{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNtgrdYo65/2ZUx+OOrdzqT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amakoko1/DocsGPT_test/blob/main/113_04_16_aecxpopx_%E7%90%86%E8%A7%A3%E7%A8%8B%E5%BC%8F%E7%A2%BC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jb6iFpgdzANN",
        "outputId": "24ca8a65-46fc-48f2-dce4-3830121f38d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.6.1-py3-none-any.whl (225 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.4/225.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken\n",
            "  Downloading tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.26.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Collecting typing-extensions<5,>=4.7 (from openai)\n",
            "  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Installing collected packages: typing-extensions, h11, tiktoken, httpcore, httpx, openai\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed h11-0.14.0 httpcore-1.0.2 httpx-0.26.0 openai-1.6.1 tiktoken-0.5.2 typing-extensions-4.9.0\n",
            "Collecting gitpython\n",
            "  Downloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1 (from gitpython)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, gitdb, gitpython\n",
            "Successfully installed gitdb-4.0.11 gitpython-3.1.40 smmap-5.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install openai tiktoken\n",
        "!pip install gitpython"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1kvOM2SlY0jg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!\n",
        "!pip install typing-inspect==0.8.0\n",
        "!typing_extensions==4.5.\n",
        "!pip install chromadb==0.3.26\n",
        "\n",
        "!pip install sentence_transformers\n",
        "!pip install python-dotenv\n",
        "!pip install pdfminer.six\n",
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fG4dYmlz0ENY",
        "outputId": "d8d2794b-5fe5-44d4-8162-c301678bf019"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting typing-inspect==0.8.0\n",
            "  Downloading typing_inspect-0.8.0-py3-none-any.whl (8.7 kB)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect==0.8.0)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from typing-inspect==0.8.0) (4.9.0)\n",
            "Installing collected packages: mypy-extensions, typing-inspect\n",
            "Successfully installed mypy-extensions-1.0.0 typing-inspect-0.8.0\n",
            "Collecting chromadb==0.3.26\n",
            "  Downloading chromadb-0.3.26-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.6/123.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas>=1.3 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.3.26) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.28 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.3.26) (2.31.0)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.3.26) (1.10.13)\n",
            "Collecting hnswlib>=0.7 (from chromadb==0.3.26)\n",
            "  Downloading hnswlib-0.8.0.tar.gz (36 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting clickhouse-connect>=0.5.7 (from chromadb==0.3.26)\n",
            "  Downloading clickhouse_connect-0.6.23-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (964 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m964.5/964.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: duckdb>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.3.26) (0.9.2)\n",
            "Collecting fastapi>=0.85.1 (from chromadb==0.3.26)\n",
            "  Downloading fastapi-0.108.0-py3-none-any.whl (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn[standard]>=0.18.3 (from chromadb==0.3.26)\n",
            "  Downloading uvicorn-0.25.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.3/60.3 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.3.26) (1.23.5)\n",
            "Collecting posthog>=2.4.0 (from chromadb==0.3.26)\n",
            "  Downloading posthog-3.1.0-py2.py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.3.26) (4.9.0)\n",
            "Collecting pulsar-client>=3.1.0 (from chromadb==0.3.26)\n",
            "  Downloading pulsar_client-3.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m69.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxruntime>=1.14.1 (from chromadb==0.3.26)\n",
            "  Downloading onnxruntime-1.16.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m74.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.3.26) (0.15.0)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.3.26) (4.66.1)\n",
            "Collecting overrides>=7.3.1 (from chromadb==0.3.26)\n",
            "  Downloading overrides-7.4.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.26) (2023.11.17)\n",
            "Requirement already satisfied: urllib3>=1.26 in /usr/local/lib/python3.10/dist-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.26) (2.0.7)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.26) (2023.3.post1)\n",
            "Collecting zstandard (from clickhouse-connect>=0.5.7->chromadb==0.3.26)\n",
            "  Downloading zstandard-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lz4 (from clickhouse-connect>=0.5.7->chromadb==0.3.26)\n",
            "  Downloading lz4-4.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m84.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting starlette<0.33.0,>=0.29.0 (from fastapi>=0.85.1->chromadb==0.3.26)\n",
            "  Downloading starlette-0.32.0.post1-py3-none-any.whl (70 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.0/70.0 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting coloredlogs (from onnxruntime>=1.14.1->chromadb==0.3.26)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb==0.3.26) (23.5.26)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb==0.3.26) (23.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb==0.3.26) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb==0.3.26) (1.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3->chromadb==0.3.26) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb==0.3.26) (1.16.0)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb==0.3.26)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb==0.3.26)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb==0.3.26) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb==0.3.26) (3.6)\n",
            "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.2->chromadb==0.3.26) (0.20.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.26) (8.1.7)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.26) (0.14.0)\n",
            "Collecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb==0.3.26)\n",
            "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb==0.3.26)\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.26) (6.0.1)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb==0.3.26)\n",
            "  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m108.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb==0.3.26)\n",
            "  Downloading watchfiles-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb==0.3.26)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb==0.3.26) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb==0.3.26) (2023.6.0)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.33.0,>=0.29.0->fastapi>=0.85.1->chromadb==0.3.26) (3.7.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb==0.3.26)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb==0.3.26) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.33.0,>=0.29.0->fastapi>=0.85.1->chromadb==0.3.26) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.33.0,>=0.29.0->fastapi>=0.85.1->chromadb==0.3.26) (1.2.0)\n",
            "Building wheels for collected packages: hnswlib\n",
            "  Building wheel for hnswlib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hnswlib: filename=hnswlib-0.8.0-cp310-cp310-linux_x86_64.whl size=2287874 sha256=1cb30f165b54711cb2d24c0a6c9919b1cfdb102de5c7a8775b4687f63f44021e\n",
            "  Stored in directory: /root/.cache/pip/wheels/af/a9/3e/3e5d59ee41664eb31a4e6de67d1846f86d16d93c45f277c4e7\n",
            "Successfully built hnswlib\n",
            "Installing collected packages: monotonic, zstandard, websockets, uvloop, uvicorn, python-dotenv, pulsar-client, overrides, lz4, humanfriendly, httptools, hnswlib, backoff, watchfiles, starlette, posthog, coloredlogs, clickhouse-connect, onnxruntime, fastapi, chromadb\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed backoff-2.2.1 chromadb-0.3.26 clickhouse-connect-0.6.23 coloredlogs-15.0.1 fastapi-0.108.0 hnswlib-0.8.0 httptools-0.6.1 humanfriendly-10.0 lz4-4.3.3 monotonic-1.6 onnxruntime-1.16.3 overrides-7.4.0 posthog-3.1.0 pulsar-client-3.4.0 python-dotenv-1.0.0 starlette-0.32.0.post1 uvicorn-0.25.0 uvloop-0.19.0 watchfiles-0.21.0 websockets-12.0 zstandard-0.22.0\n",
            "Collecting sentence_transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.35.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.16.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.11.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (3.8.1)\n",
            "Collecting sentencepiece (from sentence_transformers)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.20.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.9.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.4.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence_transformers) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\n",
            "Building wheels for collected packages: sentence_transformers\n",
            "  Building wheel for sentence_transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=0c8c1fc85d04f3b0a4274c2b364ff93fa67361a60c2503a85b1bc2180c11900e\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "Successfully built sentence_transformers\n",
            "Installing collected packages: sentencepiece, sentence_transformers\n",
            "Successfully installed sentence_transformers-2.2.2 sentencepiece-0.1.99\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
            "Collecting pdfminer.six\n",
            "  Downloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (41.0.7)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.21)\n",
            "Installing collected packages: pdfminer.six\n",
            "Successfully installed pdfminer.six-20231228\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.7.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.7.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain==0.0.350"
      ],
      "metadata": {
        "id": "tdOyobC8TPXo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c7460e4-4119-47e0-f60c-93bbd45e3d87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain==0.0.350\n",
            "  Downloading langchain-0.0.350-py3-none-any.whl (809 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m809.1/809.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.350) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.350) (2.0.23)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.350) (3.9.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.350) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain==0.0.350)\n",
            "  Downloading dataclasses_json-0.6.3-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain==0.0.350)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.2 (from langchain==0.0.350)\n",
            "  Downloading langchain_community-0.0.8-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.2,>=0.1 (from langchain==0.0.350)\n",
            "  Downloading langchain_core-0.1.5-py3-none-any.whl (205 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m205.3/205.3 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langsmith<0.1.0,>=0.0.63 (from langchain==0.0.350)\n",
            "  Downloading langsmith-0.0.77-py3-none-any.whl (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.350) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.350) (1.10.8)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.350) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.350) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.350) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.350) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.350) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.350) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.350) (1.3.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.350)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.350) (0.8.0)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain==0.0.350)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1->langchain==0.0.350) (3.7.1)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1->langchain==0.0.350) (23.2)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain==0.0.350) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.350) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.350) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.350) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.350) (2023.11.17)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.350) (3.0.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1->langchain==0.0.350) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1->langchain==0.0.350) (1.2.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.350) (1.0.0)\n",
            "Installing collected packages: marshmallow, jsonpointer, langsmith, jsonpatch, dataclasses-json, langchain-core, langchain-community, langchain\n",
            "Successfully installed dataclasses-json-0.6.3 jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.350 langchain-community-0.0.8 langchain-core-0.1.5 langsmith-0.0.77 marshmallow-3.20.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1lNZ3bobTPfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pydantic\n",
        "!pip install typing-inspect\n",
        "!typing_extensions\n",
        "!pip install chromadb\n",
        "\n",
        "!pip install sentence_transformers\n",
        "!pip install python-dotenv\n",
        "!pip install pdfminer.six\n",
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhZwClb1S_p3",
        "outputId": "b73012eb-a8f4-4011-c0c8-e4229fec763f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (1.10.8)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic) (4.9.0)\n",
            "Requirement already satisfied: typing-inspect in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect) (1.0.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from typing-inspect) (4.9.0)\n",
            "/bin/bash: line 1: typing_extensions: command not found\n",
            "Requirement already satisfied: chromadb in /usr/local/lib/python3.10/dist-packages (0.3.26)\n",
            "Requirement already satisfied: pandas>=1.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.28 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.31.0)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.10.8)\n",
            "Requirement already satisfied: hnswlib>=0.7 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.8.0)\n",
            "Requirement already satisfied: clickhouse-connect>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.6.23)\n",
            "Requirement already satisfied: duckdb>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.9.2)\n",
            "Requirement already satisfied: fastapi>=0.85.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.108.0)\n",
            "Requirement already satisfied: uvicorn[standard]>=0.18.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.25.0)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.23.5)\n",
            "Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.9.0)\n",
            "Requirement already satisfied: pulsar-client>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.3.0)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.16.3)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.15.0)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.66.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (7.4.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from clickhouse-connect>=0.5.7->chromadb) (2023.11.17)\n",
            "Requirement already satisfied: urllib3>=1.26 in /usr/local/lib/python3.10/dist-packages (from clickhouse-connect>=0.5.7->chromadb) (2.0.7)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from clickhouse-connect>=0.5.7->chromadb) (2023.3.post1)\n",
            "Requirement already satisfied: zstandard in /usr/local/lib/python3.10/dist-packages (from clickhouse-connect>=0.5.7->chromadb) (0.22.0)\n",
            "Requirement already satisfied: lz4 in /usr/local/lib/python3.10/dist-packages (from clickhouse-connect>=0.5.7->chromadb) (4.3.2)\n",
            "Requirement already satisfied: starlette<0.33.0,>=0.29.0 in /usr/local/lib/python3.10/dist-packages (from fastapi>=0.85.1->chromadb) (0.32.0.post1)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (23.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3->chromadb) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (1.16.0)\n",
            "Requirement already satisfied: monotonic>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (3.6)\n",
            "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.2->chromadb) (0.19.4)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (8.1.7)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.14.0)\n",
            "Requirement already satisfied: httptools>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.1)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (6.0.1)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.19.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (12.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2023.6.0)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.33.0,>=0.29.0->fastapi>=0.85.1->chromadb) (3.7.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.33.0,>=0.29.0->fastapi>=0.85.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.33.0,>=0.29.0->fastapi>=0.85.1->chromadb) (1.2.0)\n",
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.35.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.16.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.11.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.1.99)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.19.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.9.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.4.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence_transformers) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
            "Requirement already satisfied: pdfminer.six in /usr/local/lib/python3.10/dist-packages (20221105)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (41.0.7)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.21)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.7.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8vgwdNyxTO-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install langchain\n",
        "!pip install openai\n",
        "!pip install streamlit\n",
        "!pip install tiktoken\n",
        "\n",
        "!pip install pypdf\n",
        "!pip install pycryptodome\n",
        "!pip install \"unstructured[local-inference]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "85MekaQk0HsA",
        "outputId": "0801ae5e-bbe4-45bf-df8a-6445c3b396a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.6.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.26.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.8)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Collecting streamlit\n",
            "  Downloading streamlit-1.29.0-py2.py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.3.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Collecting importlib-metadata<7,>=1.4 (from streamlit)\n",
            "  Downloading importlib_metadata-6.11.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: numpy<2,>=1.19.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.23.5)\n",
            "Requirement already satisfied: packaging<24,>=16.8 in /usr/local/lib/python3.10/dist-packages (from streamlit) (23.2)\n",
            "Requirement already satisfied: pandas<3,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.5.3)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.4.0)\n",
            "Requirement already satisfied: protobuf<5,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=6.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (10.0.1)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.7.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.8.2)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.31.0)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.7.0)\n",
            "Requirement already satisfied: tenacity<9,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.2.3)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.9.0)\n",
            "Requirement already satisfied: tzlocal<6,>=1.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.2)\n",
            "Collecting validators<1,>=0.2 (from streamlit)\n",
            "  Downloading validators-0.22.0-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.1.40)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.8.1b0-py2.py3-none-any.whl (4.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.2)\n",
            "Collecting watchdog>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-3.0.0-py3-none-manylinux2014_x86_64.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.2)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<7,>=1.4->streamlit) (3.17.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3,>=2.7.3->streamlit) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2023.11.17)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.16.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (23.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.11.2)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.32.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.15.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Installing collected packages: watchdog, validators, importlib-metadata, pydeck, streamlit\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 7.0.0\n",
            "    Uninstalling importlib-metadata-7.0.0:\n",
            "      Successfully uninstalled importlib-metadata-7.0.0\n",
            "Successfully installed importlib-metadata-6.11.0 pydeck-0.8.1b0 streamlit-1.29.0 validators-0.22.0 watchdog-3.0.0\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.5.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.11.17)\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-3.17.4-py3-none-any.whl (278 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-3.17.4\n",
            "Collecting pycryptodome\n",
            "  Downloading pycryptodome-3.19.1-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pycryptodome\n",
            "Successfully installed pycryptodome-3.19.1\n",
            "Collecting unstructured[local-inference]\n",
            "  Downloading unstructured-0.11.6-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from unstructured[local-inference]) (5.2.0)\n",
            "Collecting filetype (from unstructured[local-inference])\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Collecting python-magic (from unstructured[local-inference])\n",
            "  Downloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from unstructured[local-inference]) (4.9.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from unstructured[local-inference]) (3.8.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from unstructured[local-inference]) (0.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from unstructured[local-inference]) (2.31.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from unstructured[local-inference]) (4.11.2)\n",
            "Collecting emoji (from unstructured[local-inference])\n",
            "  Downloading emoji-2.9.0-py2.py3-none-any.whl (397 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m397.5/397.5 kB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from unstructured[local-inference]) (0.6.3)\n",
            "Collecting python-iso639 (from unstructured[local-inference])\n",
            "  Downloading python_iso639-2023.12.11-py3-none-any.whl (275 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.1/275.1 kB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langdetect (from unstructured[local-inference])\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from unstructured[local-inference]) (1.23.5)\n",
            "Collecting rapidfuzz (from unstructured[local-inference])\n",
            "  Downloading rapidfuzz-3.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m52.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: backoff in /usr/local/lib/python3.10/dist-packages (from unstructured[local-inference]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from unstructured[local-inference]) (4.9.0)\n",
            "Collecting unstructured-client (from unstructured[local-inference])\n",
            "  Downloading unstructured_client-0.15.1-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from unstructured[local-inference]) (1.14.1)\n",
            "Collecting pypandoc (from unstructured[local-inference])\n",
            "  Downloading pypandoc-1.12-py3-none-any.whl (20 kB)\n",
            "Collecting pdf2image (from unstructured[local-inference])\n",
            "  Downloading pdf2image-1.16.3-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: pdfminer.six in /usr/local/lib/python3.10/dist-packages (from unstructured[local-inference]) (20231228)\n",
            "Requirement already satisfied: xlrd in /usr/local/lib/python3.10/dist-packages (from unstructured[local-inference]) (2.0.1)\n",
            "Requirement already satisfied: markdown in /usr/local/lib/python3.10/dist-packages (from unstructured[local-inference]) (3.5.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from unstructured[local-inference]) (3.2.1)\n",
            "Collecting pikepdf (from unstructured[local-inference])\n",
            "  Downloading pikepdf-8.11.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m97.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting unstructured.pytesseract>=0.3.12 (from unstructured[local-inference])\n",
            "  Downloading unstructured.pytesseract-0.3.12-py3-none-any.whl (14 kB)\n",
            "Collecting python-docx>=1.1.0 (from unstructured[local-inference])\n",
            "  Downloading python_docx-1.1.0-py3-none-any.whl (239 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.6/239.6 kB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting unstructured-inference==0.7.18 (from unstructured[local-inference])\n",
            "  Downloading unstructured_inference-0.7.18-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting msg-parser (from unstructured[local-inference])\n",
            "  Downloading msg_parser-1.2.0-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.8/101.8 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from unstructured[local-inference]) (1.5.3)\n",
            "Collecting python-pptx<=0.6.23 (from unstructured[local-inference])\n",
            "  Downloading python_pptx-0.6.23-py3-none-any.whl (471 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (from unstructured[local-inference]) (3.1.2)\n",
            "Collecting onnx (from unstructured[local-inference])\n",
            "  Downloading onnx-1.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (from unstructured[local-inference]) (3.17.4)\n",
            "Collecting layoutparser[layoutmodels,tesseract] (from unstructured-inference==0.7.18->unstructured[local-inference])\n",
            "  Downloading layoutparser-0.3.4-py3-none-any.whl (19.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-multipart (from unstructured-inference==0.7.18->unstructured[local-inference])\n",
            "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.18->unstructured[local-inference]) (0.19.4)\n",
            "Requirement already satisfied: opencv-python!=4.7.0.68 in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.18->unstructured[local-inference]) (4.8.0.76)\n",
            "Collecting onnxruntime<1.16 (from unstructured-inference==0.7.18->unstructured[local-inference])\n",
            "  Downloading onnxruntime-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers>=4.25.1 in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.18->unstructured[local-inference]) (4.35.2)\n",
            "Requirement already satisfied: Pillow>=3.3.2 in /usr/local/lib/python3.10/dist-packages (from python-pptx<=0.6.23->unstructured[local-inference]) (9.4.0)\n",
            "Collecting XlsxWriter>=0.5.7 (from python-pptx<=0.6.23->unstructured[local-inference])\n",
            "  Downloading XlsxWriter-3.1.9-py3-none-any.whl (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from unstructured.pytesseract>=0.3.12->unstructured[local-inference]) (23.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->unstructured[local-inference]) (2.5)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->unstructured[local-inference]) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->unstructured[local-inference]) (0.8.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect->unstructured[local-inference]) (1.16.0)\n",
            "Collecting olefile>=0.46 (from msg-parser->unstructured[local-inference])\n",
            "  Downloading olefile-0.47-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured[local-inference]) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured[local-inference]) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured[local-inference]) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured[local-inference]) (4.66.1)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx->unstructured[local-inference]) (3.20.3)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl->unstructured[local-inference]) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->unstructured[local-inference]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->unstructured[local-inference]) (2023.3.post1)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six->unstructured[local-inference]) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six->unstructured[local-inference]) (41.0.7)\n",
            "Collecting Pillow>=3.3.2 (from python-pptx<=0.6.23->unstructured[local-inference])\n",
            "  Downloading Pillow-10.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (3.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting Deprecated (from pikepdf->unstructured[local-inference])\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured[local-inference]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured[local-inference]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured[local-inference]) (2023.11.17)\n",
            "Collecting jsonpath-python>=1.0.6 (from unstructured-client->unstructured[local-inference])\n",
            "  Downloading jsonpath_python-1.0.6-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: mypy-extensions>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured[local-inference]) (1.0.0)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json->unstructured[local-inference])\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six->unstructured[local-inference]) (1.16.0)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime<1.16->unstructured-inference==0.7.18->unstructured[local-inference]) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime<1.16->unstructured-inference==0.7.18->unstructured[local-inference]) (23.5.26)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime<1.16->unstructured-inference==0.7.18->unstructured[local-inference]) (1.12)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->unstructured-inference==0.7.18->unstructured[local-inference]) (3.13.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->unstructured-inference==0.7.18->unstructured[local-inference]) (6.0.1)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->unstructured-inference==0.7.18->unstructured[local-inference]) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->unstructured-inference==0.7.18->unstructured[local-inference]) (0.4.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->unstructured-inference==0.7.18->unstructured[local-inference]) (2023.6.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.18->unstructured[local-inference]) (1.11.4)\n",
            "Collecting iopath (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.18->unstructured[local-inference])\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pdfplumber (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.18->unstructured[local-inference])\n",
            "  Downloading pdfplumber-0.10.3-py3-none-any.whl (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.0/49.0 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.18->unstructured[local-inference]) (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.18->unstructured[local-inference]) (0.16.0+cu121)\n",
            "Collecting effdet (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.18->unstructured[local-inference])\n",
            "  Downloading effdet-0.4.1-py3-none-any.whl (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.5/112.5 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytesseract (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.18->unstructured[local-inference])\n",
            "  Downloading pytesseract-0.3.10-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->unstructured[local-inference]) (2.21)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime<1.16->unstructured-inference==0.7.18->unstructured[local-inference]) (10.0)\n",
            "Collecting timm>=0.9.2 (from effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.18->unstructured[local-inference])\n",
            "  Downloading timm-0.9.12-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.18->unstructured[local-inference]) (2.0.7)\n",
            "Collecting omegaconf>=2.0 (from effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.18->unstructured[local-inference])\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.18->unstructured[local-inference]) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.18->unstructured[local-inference]) (2.1.0)\n",
            "Collecting portalocker (from iopath->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.18->unstructured[local-inference])\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Collecting pdfminer.six (from unstructured[local-inference])\n",
            "  Downloading pdfminer.six-20221105-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pypdfium2>=4.18.0 (from pdfplumber->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.18->unstructured[local-inference])\n",
            "  Downloading pypdfium2-4.25.0-py3-none-manylinux_2_17_x86_64.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m102.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime<1.16->unstructured-inference==0.7.18->unstructured[local-inference]) (1.3.0)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from omegaconf>=2.0->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.18->unstructured[local-inference])\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.18->unstructured[local-inference]) (3.7.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.18->unstructured[local-inference]) (2.1.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.18->unstructured[local-inference]) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.18->unstructured[local-inference]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.18->unstructured[local-inference]) (4.46.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.18->unstructured[local-inference]) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.18->unstructured[local-inference]) (3.1.1)\n",
            "Building wheels for collected packages: langdetect, iopath, antlr4-python3-runtime\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993225 sha256=0f000005ebc616cc0dfb1b9cb64ce7e7205afcf015a07e518a60f65e3ec1f7e4\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31532 sha256=0a24853c4b6e5704a2370112176369731b80ca4af1e9df8befb1af3192fa1532\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=71febd5a4cb1db6939f596efcec7e217b10728a8da1c40d2ee16b66bf7d53223\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "Successfully built langdetect iopath antlr4-python3-runtime\n",
            "Installing collected packages: filetype, antlr4-python3-runtime, XlsxWriter, typing-inspect, rapidfuzz, python-multipart, python-magic, python-iso639, python-docx, pypdfium2, pypandoc, portalocker, Pillow, onnx, omegaconf, olefile, langdetect, jsonpath-python, emoji, Deprecated, unstructured.pytesseract, python-pptx, pytesseract, pikepdf, pdf2image, onnxruntime, msg-parser, iopath, unstructured-client, pdfminer.six, unstructured, timm, pdfplumber, layoutparser, effdet, unstructured-inference\n",
            "  Attempting uninstall: typing-inspect\n",
            "    Found existing installation: typing-inspect 0.8.0\n",
            "    Uninstalling typing-inspect-0.8.0:\n",
            "      Successfully uninstalled typing-inspect-0.8.0\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: Pillow 9.4.0\n",
            "    Uninstalling Pillow-9.4.0:\n",
            "      Successfully uninstalled Pillow-9.4.0\n",
            "  Attempting uninstall: onnxruntime\n",
            "    Found existing installation: onnxruntime 1.16.3\n",
            "    Uninstalling onnxruntime-1.16.3:\n",
            "      Successfully uninstalled onnxruntime-1.16.3\n",
            "  Attempting uninstall: pdfminer.six\n",
            "    Found existing installation: pdfminer.six 20231228\n",
            "    Uninstalling pdfminer.six-20231228:\n",
            "      Successfully uninstalled pdfminer.six-20231228\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "imageio 2.31.6 requires pillow<10.1.0,>=8.3.2, but you have pillow 10.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Deprecated-1.2.14 Pillow-10.1.0 XlsxWriter-3.1.9 antlr4-python3-runtime-4.9.3 effdet-0.4.1 emoji-2.9.0 filetype-1.2.0 iopath-0.1.10 jsonpath-python-1.0.6 langdetect-1.0.9 layoutparser-0.3.4 msg-parser-1.2.0 olefile-0.47 omegaconf-2.3.0 onnx-1.15.0 onnxruntime-1.15.1 pdf2image-1.16.3 pdfminer.six-20221105 pdfplumber-0.10.3 pikepdf-8.11.2 portalocker-2.8.2 pypandoc-1.12 pypdfium2-4.25.0 pytesseract-0.3.10 python-docx-1.1.0 python-iso639-2023.12.11 python-magic-0.4.27 python-multipart-0.0.6 python-pptx-0.6.23 rapidfuzz-3.6.1 timm-0.9.12 typing-inspect-0.9.0 unstructured-0.11.6 unstructured-client-0.15.1 unstructured-inference-0.7.18 unstructured.pytesseract-0.3.12\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "pydevd_plugins"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4AVJDGES21Mg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install uuid"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288
        },
        "id": "ux8kb80w1Gq_",
        "outputId": "2756168a-b45b-48e9-bf9c-b27c417bd358"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting uuid\n",
            "  Downloading uuid-1.30.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: uuid\n",
            "  Building wheel for uuid (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for uuid: filename=uuid-1.30-py3-none-any.whl size=6479 sha256=7b3e952963d3418eb353e499f5afe035ba28ee2e187a95487736f77731e3291a\n",
            "  Stored in directory: /root/.cache/pip/wheels/ed/08/9e/f0a977dfe55051a07e21af89200125d65f1efa60cbac61ed88\n",
            "Successfully built uuid\n",
            "Installing collected packages: uuid\n",
            "Successfully installed uuid-1.30\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "uuid"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/docs\n",
        "!mkdir /content/db\n"
      ],
      "metadata": {
        "id": "SLg-GSLH06k-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/docs_temp"
      ],
      "metadata": {
        "id": "2XOAg2AJBh8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from git import Repo"
      ],
      "metadata": {
        "id": "HSXLQhgD1W8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "repo_path = \"/content/docs\"\n",
        "repo = Repo.clone_from(\"https://github.com/langchain-ai/langchain\", to_path=repo_path)"
      ],
      "metadata": {
        "id": "7FEhPjqN1W_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u1rXPQSuAx8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NU2E81FqAyAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZbNFnXWuAyD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#定義路徑\n",
        "persist_directory = '/content/db'\n",
        "source_directory = '/content/docs'\n",
        "embeddings_model_name = 'shibing624/text2vec-base-chinese'"
      ],
      "metadata": {
        "id": "URJrJ_GW2WOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_yfZ9w6A2n2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from git import Repo\n",
        "from langchain.text_splitter import Language\n",
        "from langchain.document_loaders.generic import GenericLoader\n",
        "from langchain.document_loaders.parsers import LanguageParser"
      ],
      "metadata": {
        "id": "BmuHIAtE2WRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kQ-Drgn20J0c"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XE4ecdqyAro6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TgPOEdEmArsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DQ4qioqlArvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SzGKSx3W2WUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "psiM5dIgehC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3QVwa4oqehF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Chroma\n"
      ],
      "metadata": {
        "id": "4ZD1R2_BehIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wX3dBrsIVhqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.docstore.document import Document"
      ],
      "metadata": {
        "id": "1GrOTvWoehLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from chromadb.config import Settings"
      ],
      "metadata": {
        "id": "TZv_4ka5ehQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iKNBFyc96Mfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client_settings"
      ],
      "metadata": {
        "id": "DCEftLyj6Mif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Chroma settings\n",
        "client_settings = Settings(\n",
        "        chroma_db_impl='duckdb+parquet',\n",
        "        persist_directory=persist_directory,\n",
        "        anonymized_telemetry=False\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "w5F6ndaqzC6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load\n",
        "repo_path=source_directory\n",
        "loader = GenericLoader.from_filesystem(\n",
        "    repo_path+\"/libs/langchain/langchain\",\n",
        "    glob=\"**/*\",\n",
        "    suffixes=[\".py\"],\n",
        "    parser=LanguageParser(language=Language.PYTHON, parser_threshold=500)\n",
        ")\n",
        "documents = loader.load()\n",
        "\n",
        "#len(documents)"
      ],
      "metadata": {
        "id": "g2KDBd8C4G12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "python_splitter = RecursiveCharacterTextSplitter.from_language(language=Language.PYTHON,\n",
        "                                                               chunk_size=2000,\n",
        "                                                               chunk_overlap=200)\n",
        "texts = python_splitter.split_documents(documents)\n",
        "len(texts)"
      ],
      "metadata": {
        "id": "LUH1axyBehTB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3187d84-ba78-4d2d-af1f-32997e3d3e4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2618"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings"
      ],
      "metadata": {
        "id": "8RieYJdwehV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fQ0jA5gsehbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import  os\n",
        "import glob"
      ],
      "metadata": {
        "id": "vEAu9_db6sOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.exists(os.path.join(persist_directory, 'index')):\n",
        "  if os.path.exists(os.path.join(persist_directory, 'chroma-collections.parquet')) and os.path.exists(os.path.join(persist_directory, 'chroma-embeddings.parquet')):\n",
        "    list_index_files = glob.glob(os.path.join(persist_directory, 'index/*.bin'))\n",
        "    list_index_files += glob.glob(os.path.join(persist_directory, 'index/*.pkl'))"
      ],
      "metadata": {
        "id": "jAED5Jv51XCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9Ewqf1ad7xLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings"
      ],
      "metadata": {
        "id": "R242Umr67-q3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade chromadb==0.3.29"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3bGT48kUUka1",
        "outputId": "81765070-2520-4a9a-f828-ec24d34e338c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting chromadb==0.3.29\n",
            "  Downloading chromadb-0.3.29-py3-none-any.whl (396 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/396.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/396.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m396.4/396.4 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas>=1.3 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.3.29) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.28 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.3.29) (2.31.0)\n",
            "Requirement already satisfied: pydantic<2.0,>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.3.29) (1.10.13)\n",
            "Collecting hnswlib>=0.7 (from chromadb==0.3.29)\n",
            "  Downloading hnswlib-0.8.0.tar.gz (36 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting clickhouse-connect>=0.5.7 (from chromadb==0.3.29)\n",
            "  Downloading clickhouse_connect-0.6.23-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (964 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m964.5/964.5 kB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: duckdb>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.3.29) (0.9.2)\n",
            "Collecting fastapi==0.85.1 (from chromadb==0.3.29)\n",
            "  Downloading fastapi-0.85.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: uvicorn[standard]>=0.18.3 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.3.29) (0.25.0)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.3.29) (1.23.5)\n",
            "Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.3.29) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.3.29) (4.9.0)\n",
            "Requirement already satisfied: pulsar-client>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.3.29) (3.3.0)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.3.29) (1.15.1)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.3.29) (0.15.0)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.3.29) (4.66.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.3.29) (7.4.0)\n",
            "Collecting starlette==0.20.4 (from fastapi==0.85.1->chromadb==0.3.29)\n",
            "  Downloading starlette-0.20.4-py3-none-any.whl (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette==0.20.4->fastapi==0.85.1->chromadb==0.3.29) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.29) (2023.11.17)\n",
            "Requirement already satisfied: urllib3>=1.26 in /usr/local/lib/python3.10/dist-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.29) (1.26.18)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.29) (2023.3.post1)\n",
            "Collecting zstandard (from clickhouse-connect>=0.5.7->chromadb==0.3.29)\n",
            "  Downloading zstandard-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m93.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lz4 (from clickhouse-connect>=0.5.7->chromadb==0.3.29)\n",
            "  Downloading lz4-4.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb==0.3.29) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb==0.3.29) (23.5.26)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb==0.3.29) (23.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb==0.3.29) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb==0.3.29) (1.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3->chromadb==0.3.29) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb==0.3.29) (1.16.0)\n",
            "Requirement already satisfied: monotonic>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb==0.3.29) (1.6)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb==0.3.29) (2.2.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb==0.3.29) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb==0.3.29) (3.6)\n",
            "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.2->chromadb==0.3.29) (0.19.4)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.29) (8.1.7)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.29) (0.14.0)\n",
            "Requirement already satisfied: httptools>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.29) (0.6.1)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.29) (1.0.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.29) (6.0.1)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.29) (0.19.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.29) (0.21.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.29) (12.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb==0.3.29) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb==0.3.29) (2023.6.0)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb==0.3.29) (10.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb==0.3.29) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette==0.20.4->fastapi==0.85.1->chromadb==0.3.29) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette==0.20.4->fastapi==0.85.1->chromadb==0.3.29) (1.2.0)\n",
            "Building wheels for collected packages: hnswlib\n",
            "  Building wheel for hnswlib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hnswlib: filename=hnswlib-0.8.0-cp310-cp310-linux_x86_64.whl size=2287871 sha256=4e35163128cdccd001bafcbbe31a1442f45812a79424522d783954797096793d\n",
            "  Stored in directory: /root/.cache/pip/wheels/af/a9/3e/3e5d59ee41664eb31a4e6de67d1846f86d16d93c45f277c4e7\n",
            "Successfully built hnswlib\n",
            "Installing collected packages: zstandard, lz4, hnswlib, starlette, clickhouse-connect, fastapi, chromadb\n",
            "  Attempting uninstall: starlette\n",
            "    Found existing installation: starlette 0.32.0.post1\n",
            "    Uninstalling starlette-0.32.0.post1:\n",
            "      Successfully uninstalled starlette-0.32.0.post1\n",
            "  Attempting uninstall: fastapi\n",
            "    Found existing installation: fastapi 0.108.0\n",
            "    Uninstalling fastapi-0.108.0:\n",
            "      Successfully uninstalled fastapi-0.108.0\n",
            "  Attempting uninstall: chromadb\n",
            "    Found existing installation: chromadb 0.4.21\n",
            "    Uninstalling chromadb-0.4.21:\n",
            "      Successfully uninstalled chromadb-0.4.21\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed chromadb-0.3.29 clickhouse-connect-0.6.23 fastapi-0.85.1 hnswlib-0.8.0 lz4-4.3.2 starlette-0.20.4 zstandard-0.22.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "chromadb",
                  "starlette"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = HuggingFaceEmbeddings(model_name=embeddings_model_name)\n",
        "db = Chroma.from_documents(texts, embeddings, persist_directory=persist_directory, client_settings=client_settings)"
      ],
      "metadata": {
        "id": "CYzvamII7xOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "--V5od3x21lz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(db.similarity_search(\"litellm\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2XQRJA721pK",
        "outputId": "fbefe5e8-1902-4ade-8834-2b6205edca8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(page_content='from langchain_community.chat_message_histories.cassandra import (\\n    CassandraChatMessageHistory,\\n)\\n\\n__all__ = [\"CassandraChatMessageHistory\"]', metadata={'source': '/content/docs/libs/langchain/langchain/memory/chat_message_histories/cassandra.py', 'language': 'python'}), Document(page_content='from langchain_community.chat_message_histories.cassandra import (\\n    CassandraChatMessageHistory,\\n)\\n\\n__all__ = [\"CassandraChatMessageHistory\"]', metadata={'source': '/content/docs/libs/langchain/langchain/memory/chat_message_histories/cassandra.py', 'language': 'python'})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bClgDv1h21r6",
        "outputId": "bd694a69-e06a-490a-c17c-393deb524fac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "client=SentenceTransformer(\n",
            "  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: BertModel \n",
            "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
            ") model_name='shibing624/text2vec-base-chinese' cache_folder=None model_kwargs={} encode_kwargs={} multi_process=False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load from disk\n",
        "#db3 = Chroma(persist_directory=\"./chroma_db\", embedding_function=embedding_function)\n",
        "db3=Chroma(embedding_function=embeddings,persist_directory=persist_directory, client_settings=client_settings)\n",
        "print(db3.similarity_search(\"litellm\"))\n",
        "#docs = db3.similarity_search(\"litellm\")\n",
        "#print(docs[0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GaYfXk1r21uy",
        "outputId": "45b65bbc-bfef-4605-a9ea-d059848408ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(page_content='from langchain_community.chat_message_histories.cassandra import (\\n    CassandraChatMessageHistory,\\n)\\n\\n__all__ = [\"CassandraChatMessageHistory\"]', metadata={'source': '/content/docs/libs/langchain/langchain/memory/chat_message_histories/cassandra.py', 'language': 'python'})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(db3.similarity_search(\"litellm\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qy5WNN267xRI",
        "outputId": "573cb9c8-357e-4d06-e8dd-04ffe423a667"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(page_content='from langchain_community.chat_message_histories.cassandra import (\\n    CassandraChatMessageHistory,\\n)\\n\\n__all__ = [\"CassandraChatMessageHistory\"]', metadata={'source': '/content/docs/libs/langchain/langchain/memory/chat_message_histories/cassandra.py', 'language': 'python'})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs = db3.similarity_search(\"litellm\")\n",
        "print(docs[0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNhNhqk1_cuW",
        "outputId": "b36de488-8f6c-4571-a622-fa039891291c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "from langchain_community.chat_message_histories.cassandra import (\n",
            "    CassandraChatMessageHistory,\n",
            ")\n",
            "\n",
            "__all__ = [\"CassandraChatMessageHistory\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_function)"
      ],
      "metadata": {
        "id": "Kfo-ZMYYeoYt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        },
        "outputId": "287e56da-5b02-4e9e-d375-a4520878d0ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-e973f6f2ab72>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'embedding_function' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aXDIa4aw_fon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y-o8bF8T_frd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: 將Chroma的db持久化保存\n",
        "\n",
        "db.persist()\n"
      ],
      "metadata": {
        "id": "cK6vNKbbeobz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPHlt1uishOc",
        "outputId": "1677082c-2918-46c4-fac1-f5cdc1fb8760"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "client=SentenceTransformer(\n",
            "  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: BertModel \n",
            "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
            ") model_name='shibing624/text2vec-base-chinese' cache_folder=None model_kwargs={} encode_kwargs={} multi_process=False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "URoeV7jEZwmD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HNp9-jgru11j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_db = Chroma(persist_directory=persist_directory)"
      ],
      "metadata": {
        "id": "3y1D0pvbu14b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6aF71NCNwZpf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import chromadb\n",
        "loaded_db = Chroma(persist_directory=persist_directory)"
      ],
      "metadata": {
        "id": "l1lxyNCFviE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"litellm\"\n",
        "results = loaded_db.similarity_search(query)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "hEoDLwyku17d",
        "outputId": "20b5f876-df09-44d9-874d-f178e6a46fab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-06b9be9d0779>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"litellm\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloaded_db\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarity_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_community/vectorstores/chroma.py\u001b[0m in \u001b[0;36msimilarity_search\u001b[0;34m(self, query, k, filter, **kwargs)\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDocument\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mdocuments\u001b[0m \u001b[0mmost\u001b[0m \u001b[0msimilar\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mquery\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \"\"\"\n\u001b[0;32m--> 348\u001b[0;31m         \u001b[0mdocs_and_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarity_search_with_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs_and_scores\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_community/vectorstores/chroma.py\u001b[0m in \u001b[0;36msimilarity_search_with_score\u001b[0;34m(self, query, k, filter, where_document, **kwargs)\u001b[0m\n\u001b[1;32m    423\u001b[0m         \"\"\"\n\u001b[1;32m    424\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_embedding_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m             results = self.__query_collection(\n\u001b[0m\u001b[1;32m    426\u001b[0m                 \u001b[0mquery_texts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m                 \u001b[0mn_results\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/utils/utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m                     \u001b[0;34mf\" {', '.join(invalid_group_names)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                 )\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_community/vectorstores/chroma.py\u001b[0m in \u001b[0;36m__query_collection\u001b[0;34m(self, query_texts, query_embeddings, n_results, where, where_document, **kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m                 \u001b[0;34m\"Please install it with `pip install chromadb`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m             )\n\u001b[0;32m--> 155\u001b[0;31m         return self._collection.query(\n\u001b[0m\u001b[1;32m    156\u001b[0m             \u001b[0mquery_texts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_texts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0mquery_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_embeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/chromadb/api/models/Collection.py\u001b[0m in \u001b[0;36mquery\u001b[0;34m(self, query_embeddings, query_texts, n_results, where, where_document, include)\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mquery_embeddings\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_embedding_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    210\u001b[0m                     \u001b[0;34m\"You must provide embeddings or a function to compute them\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m                 )\n",
            "\u001b[0;31mValueError\u001b[0m: You must provide embeddings or a function to compute them"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"litellm\"\n",
        "loaded_db = Chroma(persist_directory=persist_directory, client_settings=CHROMA_SETTINGS)\n",
        "#Chroma.from_documents(texts, embeddings, persist_directory=persist_directory, client_settings=CHROMA_SETTINGS)\n",
        "\n",
        "# 加载数据库\n",
        "#loaded_db.load()\n",
        "\n",
        "# 执行相似性搜索查询操作\n",
        "#query = \"查询文本\"\n",
        "#results = loaded_db.similarity_search(query)\n",
        "results = loaded_db.similarity_search(\"litellm\",)\n",
        "\n",
        "# 打印查询结果\n",
        "for result in results:\n",
        "    print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 975
        },
        "id": "ShyA6pwHxs2Z",
        "outputId": "2180a56b-4aae-4e08-e521-ae6fedcc0671"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-67-2fbd73af77eb>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#query = \"查询文本\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#results = loaded_db.similarity_search(query)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloaded_db\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarity_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"litellm\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# 打印查询结果\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_community/vectorstores/chroma.py\u001b[0m in \u001b[0;36msimilarity_search\u001b[0;34m(self, query, k, filter, **kwargs)\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDocument\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mdocuments\u001b[0m \u001b[0mmost\u001b[0m \u001b[0msimilar\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mquery\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \"\"\"\n\u001b[0;32m--> 348\u001b[0;31m         \u001b[0mdocs_and_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarity_search_with_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs_and_scores\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_community/vectorstores/chroma.py\u001b[0m in \u001b[0;36msimilarity_search_with_score\u001b[0;34m(self, query, k, filter, where_document, **kwargs)\u001b[0m\n\u001b[1;32m    423\u001b[0m         \"\"\"\n\u001b[1;32m    424\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_embedding_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m             results = self.__query_collection(\n\u001b[0m\u001b[1;32m    426\u001b[0m                 \u001b[0mquery_texts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m                 \u001b[0mn_results\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/utils/utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m                     \u001b[0;34mf\" {', '.join(invalid_group_names)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                 )\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_community/vectorstores/chroma.py\u001b[0m in \u001b[0;36m__query_collection\u001b[0;34m(self, query_texts, query_embeddings, n_results, where, where_document, **kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m                 \u001b[0;34m\"Please install it with `pip install chromadb`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m             )\n\u001b[0;32m--> 155\u001b[0;31m         return self._collection.query(\n\u001b[0m\u001b[1;32m    156\u001b[0m             \u001b[0mquery_texts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_texts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0mquery_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_embeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/chromadb/api/models/Collection.py\u001b[0m in \u001b[0;36mquery\u001b[0;34m(self, query_embeddings, query_texts, n_results, where, where_document, include)\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mquery_embeddings\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_embedding_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    210\u001b[0m                     \u001b[0;34m\"You must provide embeddings or a function to compute them\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m                 )\n",
            "\u001b[0;31mValueError\u001b[0m: You must provide embeddings or a function to compute them"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVL9sz9J4l4C",
        "outputId": "ad00a3b7-1e10-40a6-9698-4edcbec152c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "client=SentenceTransformer(\n",
            "  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: BertModel \n",
            "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
            ") model_name='shibing624/text2vec-base-chinese' cache_folder=None model_kwargs={} encode_kwargs={} multi_process=False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_db = Chroma(persist_directory=persist_directory, embeddings=embeddings)\n",
        "\n",
        "# 加载数据库\n",
        "loaded_db.load()\n",
        "\n",
        "# 执行相似性搜索查询操作\n",
        "query = \"litellm\"\n",
        "\n",
        "# 计算查询文本的嵌入向量\n",
        "embedding = embeddings.embed(query)\n",
        "\n",
        "# 执行相似性搜索\n",
        "results = loaded_db.similarity_search_by_vector(embedding, search_kwargs={\"k\": 8})\n",
        "#search_kwargs={\"k\": 8}\n",
        "\n",
        "# 打印查询结果\n",
        "for result in results:\n",
        "    print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "Q7ZlyqSa1hWE",
        "outputId": "f6760e33-571e-4dd6-9447-29df93f02e36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-72-f228dae0ad45>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloaded_db\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChroma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpersist_directory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpersist_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# 加载数据库\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mloaded_db\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Chroma.__init__() got an unexpected keyword argument 'embeddings'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rEen4wetu2Aj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "owfu9gbRshRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "collection = db.get()"
      ],
      "metadata": {
        "id": "sbMWdfwt7xTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metadatas = collection['metadatas']"
      ],
      "metadata": {
        "id": "SV9aKoMwecaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sources = [metadata['source'] for metadata in metadatas]"
      ],
      "metadata": {
        "id": "rdbkIVIFhOJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [open(source, 'r').read() for source in sources]"
      ],
      "metadata": {
        "id": "KwABzB-OecdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "documents = [Document(page_content=text) for text in texts]\n",
        "db.add_documents(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atB8KOalecgd",
        "outputId": "cd23b132-6982-401c-86c6-1dc42ab77157"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['3cd2ccce-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2ce7c-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2cefe-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2cf6c-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2d0a2-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2d142-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2d1d8-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2d246-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2d2a0-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2d304-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2d368-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2d3d6-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2d430-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2d494-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2d4f8-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2d552-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2d5b6-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2d610-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2d688-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2d6ec-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2d750-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2d7b4-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2d80e-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2d872-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2d8d6-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2d930-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2d994-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2d9ee-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2da5c-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2dab6-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2db1a-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2db74-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2dbd8-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2dc3c-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2dca0-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2dd04-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2dd68-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2ddc2-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2de30-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2de94-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2deee-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2df52-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2dfac-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2e010-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2e074-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2e0ce-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2e164-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2e1d2-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2e22c-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2e290-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2e2f4-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2e34e-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2e3b2-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2e416-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2e470-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2e4d4-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2e538-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2e59c-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2e600-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2e65a-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2e6be-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2e718-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2e772-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2e7d6-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2e83a-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2e89e-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2e8f8-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2e95c-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2e9b6-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2ea1a-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2ea74-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2eae2-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2eb3c-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2ebe6-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2ec40-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2eca4-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2ecfe-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2ed6c-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2edc6-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2ee20-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2ee84-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2eede-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2ef38-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2f046-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2f0aa-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2f10e-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2f172-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2f1d6-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2f23a-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2f29e-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2f302-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2f366-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2f3f2-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2f460-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2f4c4-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2f528-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2f58c-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2f5f0-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2f668-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2f6cc-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2f730-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2f7f8-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2f866-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2f8ca-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2f924-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2f988-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2f9ec-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2fa50-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2fab4-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2fb18-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2fb86-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2fc1c-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2fc80-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2fce4-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2fd52-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2fdb6-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2fe24-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2fe88-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2feec-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2ff50-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd2ffb4-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd30018-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd30072-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd300e0-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd30144-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd301a8-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3020c-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3027a-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd302de-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd30342-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd303e2-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd30446-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd304aa-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3050e-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd30572-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd305d6-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd30644-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd306a8-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3070c-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3077a-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd307de-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3086a-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd308d8-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3093c-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd309a0-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd30a04-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd30b12-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd30b6c-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd30bc6-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd30c66-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd30cc0-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd30d24-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd30d7e-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd30dd8-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd30e3c-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd30e96-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd30efa-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd30f5e-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd30fc2-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3101c-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd31080-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd310da-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3113e-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd31198-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd311fc-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd31256-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd312b0-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd31314-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd313a0-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd313fa-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3145e-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd314b8-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd31512-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd31580-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd315da-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3163e-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd316a2-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd316fc-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd31756-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd317ba-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd31814-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3186e-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd318d2-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3192c-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd31990-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd319ea-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd31a44-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd31ad0-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd31b2a-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd31b8e-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd31be8-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd31c42-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd31ca6-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd31d00-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd31d64-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd31dbe-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd31e22-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd31ec2-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd31f26-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd31f94-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd31fee-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd32052-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd320ac-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd32110-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3216a-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd321ce-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3225a-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3230e-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd323a4-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3243a-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd324b2-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd32516-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd32570-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd325d4-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd32638-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3269c-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd326f6-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd32750-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd327b4-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3280e-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd32868-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd328cc-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd32926-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd32980-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd329e4-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd32a7a-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd32ad4-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd32b2e-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd32b92-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd32bec-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd32c46-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd32ca0-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd32d04-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd32d68-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd32dcc-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd32e30-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd32e8a-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd32ee4-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd32f70-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd32fd4-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd33038-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd33092-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd330f6-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd33150-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd331f0-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd33254-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd332ae-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd33312-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3336c-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd333c6-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd33434-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3348e-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd334f2-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3354c-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd335a6-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3360a-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd33664-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd336be-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd33718-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3377c-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd337d6-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3383a-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd33894-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd33920-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd33984-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd339e8-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd33a42-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd33aa6-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd33b00-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd33b64-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd33bbe-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd33c18-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd33c7c-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd33ce0-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd33d3a-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd33d94-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd33df8-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd33e5c-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd33eb6-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd33f1a-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd33f74-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd33fd8-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd34064-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd340c8-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd34122-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd34186-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd341e0-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3424e-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd342a8-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3430c-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd34424-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd34488-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3451e-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd34636-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd34762-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd347bc-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd34820-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3487a-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd348d4-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd34938-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd34992-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd34a28-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd34a82-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd34adc-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd34b36-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd34ba4-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd34bfe-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd34c58-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd34cb2-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd34d16-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd34d7a-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd34dd4-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd34e38-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd34e92-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd34eec-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd34f46-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd34fa0-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd35004-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3505e-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd350b8-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3514e-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd351b2-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3520c-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd35266-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd352ca-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd35324-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3537e-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd353e2-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3543c-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd35496-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd35504-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3555e-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd355b8-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3561c-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd356a8-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd35702-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd35766-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd357c0-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3581a-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd358ba-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3591e-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd35978-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd359d2-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd35a2c-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd35a90-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd35af4-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd35b4e-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd35ba8-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd35c16-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd35c70-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd35cca-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd35d2e-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd35d88-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd35dec-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd35e50-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd35eaa-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd35f04-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd35f68-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd35ffe-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd36058-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd360b2-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd36116-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3617a-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd361de-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd36242-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3629c-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd362f6-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd36350-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd363b4-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3640e-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd36468-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd364c2-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3651c-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd36580-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd365da-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd36634-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3668e-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3671a-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd36774-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd367d8-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd36832-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3688c-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd368f0-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3694a-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd369ae-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd36a12-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd36a6c-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd36ac6-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd36b20-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd36b7a-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd36c42-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd36cb0-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd36d0a-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd36d64-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd36dbe-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd36e22-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd36e7c-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd36ee0-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd36f3a-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd36f94-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3702a-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd37084-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd370e8-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd37142-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd371a6-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd37200-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd37264-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd372be-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd37318-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3737c-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd373d6-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd37430-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd37494-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd374ee-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd37552-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd375ac-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd37606-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3766a-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd376c4-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd37750-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd377aa-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd37804-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd37868-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd378c2-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3791c-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd37976-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd379d0-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd37a3e-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd37aa2-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd37afc-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd37b60-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd37bba-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd37c14-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd37c6e-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd37d90-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd37dfe-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd37e58-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd37f70-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd38010-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd38088-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd380f6-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd38164-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd381d2-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3824a-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd382b8-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd38326-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd38394-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd38402-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd38470-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd384de-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3854c-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd385c4-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd38632-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd38696-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd38704-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd38772-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd387e0-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3888a-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd388f8-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3897a-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd389f2-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd38a60-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd38ace-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd38b3c-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd38baa-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd38c18-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd38c86-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd38cf4-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd38d62-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd38dd0-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd38e3e-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd38eac-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd38f1a-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd38f88-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd38ff6-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3906e-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3910e-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3917c-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd391ea-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd39258-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd392da-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd39348-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd39410-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3947e-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd394f6-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd39564-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd395d2-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd39640-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd396ae-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd39712-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd39780-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd397f8-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd39870-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd398de-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd39942-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd399e2-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd39a50-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd39abe-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd39b2c-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd39b9a-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd39c08-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd39c76-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd39ce4-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd39d52-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd39dc0-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd39e2e-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd39e9c-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd39f14-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd39f82-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd39ff0-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3a05e-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3a0cc-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3a13a-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3a1a8-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3a248-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3a2ac-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3a31a-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3a388-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3a3f6-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3a464-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3a4fa-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3a586-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3a5f4-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3a66c-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3a6da-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3a748-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3a7b6-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3a824-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3a892-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3a900-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3a96e-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3a9dc-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3aa4a-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3aaea-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3ab58-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3abbc-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3ac2a-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3ac98-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3ad06-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3adf6-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3ae64-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3af40-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3af9a-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3aff4-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3b058-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3b0bc-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3b116-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3b170-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3b1ca-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3b224-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3b288-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3b2e2-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3b36e-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3b3c8-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3b422-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3b486-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3b4ea-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3b544-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3b59e-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3b5f8-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3b65c-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3b6b6-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3b710-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3b76a-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3b7ce-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3b828-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3b882-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3b8dc-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3b936-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3b99a-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3b9f4-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3ba80-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3bb20-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3bb84-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3bbde-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3bc38-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3bc92-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3bcf6-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3bd50-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3bdaa-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3be04-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3be68-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3bec2-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3bf1c-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3bf76-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3bfd0-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3c034-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3c098-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3c0fc-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3c156-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3c1e2-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3c23c-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3c2a0-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3c304-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3c35e-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3c3b8-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3c41c-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3c476-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3c4d0-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3c52a-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3c58e-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3c5e8-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3c642-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3c6a6-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3c70a-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3c764-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3c7be-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3c818-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3c87c-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3c8fe-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3c962-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3c9bc-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3ca16-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3ca70-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3cad4-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3cb2e-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3cba6-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3cc0a-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3cc64-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3ccc8-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3cd2c-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3cd86-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3cde0-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3ce3a-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3ce9e-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3cef8-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3cf52-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3cfb6-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3d056-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3d0b0-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3d10a-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3d164-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3d1be-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3d222-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3d27c-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3d2e0-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3d33a-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3d39e-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3d3f8-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3d45c-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3d4b6-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3d510-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3d56a-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3d5ce-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3d628-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3d682-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3d6dc-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3d772-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3d7cc-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3d826-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3d880-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3d8ee-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3d948-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3d9ac-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3da06-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3da60-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3dac4-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3db1e-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3db78-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3dbd2-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3dc36-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3dc86-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3dcea-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3dd44-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3dd9e-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3de02-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3de84-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3def2-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3df56-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3dfb0-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3e00a-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3e06e-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3e0c8-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3e1ae-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3e244-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3e2d0-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3e334-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3e38e-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3e3e8-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3e44c-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3e4a6-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3e50a-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3e564-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3e5c8-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3e62c-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3e6b8-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3e712-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3e76c-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3e7d0-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3e82a-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3e884-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3e8e8-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3e94c-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3e9e2-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3ec62-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3ecda-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3ed34-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3ed98-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3edf2-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3ef14-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3ef78-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3efdc-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3f036-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3f09a-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3f130-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3f194-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3f1f8-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3f252-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3f2fc-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3f360-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3f3ce-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3f432-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3f496-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3f504-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3f568-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3f5e0-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3f644-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3f6a8-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3f70c-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3f766-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3f7ca-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3f82e-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3f892-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3f928-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3f98c-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3f9e6-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3fa4a-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3faae-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3fb12-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3fb76-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3fbda-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3fc3e-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3fca2-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3fd06-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3fd6a-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3fdc4-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3fe28-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3fe8c-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3fee6-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3ff4a-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd3ffae-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd40012-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd4009e-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd40102-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd40166-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd401ca-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd40224-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd40292-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd402f6-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd40350-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd403b4-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd40418-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd40472-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd404d6-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd4053a-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd4059e-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd405f8-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd4065c-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd406c0-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd4071a-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd4077e-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd40d46-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd40dbe-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd40e22-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd40e86-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd40eea-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd40f4e-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd40fb2-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd410a2-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd41138-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd411e2-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd41250-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd412b4-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd41318-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd4137c-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd413e0-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd41444-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd4149e-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd41502-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd41566-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd415d4-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd41638-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd41692-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd416f6-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd4175a-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd417be-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd41822-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd4187c-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd418e0-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd41944-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd419da-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd41a48-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd41aac-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd41b10-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd41b6a-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd41bce-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd41c32-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd41c96-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd41cfa-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd41d5e-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd41dc2-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd41e26-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd41e8a-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd41ee4-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd41f48-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd41fac-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd42010-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd42074-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd420d8-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd4213c-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd42196-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd421fa-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd4225e-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd422c2-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd42326-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd4238a-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd423ee-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd42452-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd424b6-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd42510-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd42574-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd425d8-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd4263c-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd426a0-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd426fa-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd4275e-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd427c2-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd42826-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd4288a-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd428e4-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd42948-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd429ac-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd42a10-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd42a74-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd42ad8-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd42b3c-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd42ba0-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd42c04-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd42c68-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd42ccc-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd42d26-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd42d8a-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd42dee-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd42eb6-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd42f1a-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd42fba-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd4301e-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd4308c-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd430e6-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd4314a-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd431a4-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd43208-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd43262-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd432bc-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd43316-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd4337a-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd433de-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd43438-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd4349c-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd434f6-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd4355a-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd435d2-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd4362c-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd43686-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd436e0-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd43744-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd4379e-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd43802-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd4385c-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd438b6-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd43910-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd43974-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd439ce-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd43a28-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd43a8c-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd43af0-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd43b4a-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd43ba4-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd43bfe-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd43c62-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd43cbc-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd43d20-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd43d7a-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd43dd4-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd43e38-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd43e92-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd43ef6-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd43f50-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd43fb4-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd4400e-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd44072-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd440fe-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd44158-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd441bc-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd44216-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd44270-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd442d4-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd4432e-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd44388-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd443ec-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd44446-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd444a0-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd444fa-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd4455e-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd445b8-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd44612-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd4466c-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd446d0-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd44734-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd4478e-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd447e8-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd4484c-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd448b0-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd4490a-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd44964-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd449c8-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd44a4a-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd44aa4-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd44afe-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd44b58-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd44bb2-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd44c16-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd44c70-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd44cca-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd44d2e-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd44d88-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd44de2-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd44e3c-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd44ea0-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd44efa-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd44f54-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd44fae-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd45008-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd4506c-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd450c6-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd45148-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd451a2-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd451fc-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd45260-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd452ba-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd45314-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd4536e-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd453c8-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd45422-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd45486-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd454e0-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd4553a-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd45594-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd455ee-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd4568e-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd456e8-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd4574c-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd457a6-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd45800-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd45882-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd458dc-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd4594a-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd459a4-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd459fe-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd45a62-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd45abc-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd45b16-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd45b7a-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd45bd4-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd45c2e-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd45c88-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd45ce2-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd45d3c-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd45da0-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd45dfa-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd45e54-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd45eae-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd45f12-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd45fb2-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd46016-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd46070-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd460ca-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd46124-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd46188-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd461e2-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd4623c-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd46296-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd462fa-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd46354-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd46462-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd464c6-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd4652a-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd46584-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd465f2-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd46656-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd466ba-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd4671e-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd467a0-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd46836-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd4689a-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd468fe-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd46962-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd469bc-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd46a20-a3ad-11ee-aabd-0242ac1c000c',\n",
              " '3cd46a84-a3ad-11ee-aabd-0242ac1c000c',\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tvzxYKg6konh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 創建 Chroma 實例時將 persist_directory 設置為 '/content/db'\n",
        "db = Chroma(persist_directory='/content/db', embedding_function=embeddings, client_settings=CHROMA_SETTINGS)\n",
        "\n",
        "# 使用 process_documents() 函數創建 documents\n",
        "texts = process_documents([metadata['source'] for metadata in collection['metadatas']])\n",
        "documents = [Document(page_content=text) for text in texts]\n",
        "\n",
        "# 將 documents 添加到 Chroma 中\n",
        "db.add_documents(documents)\n",
        "\n",
        "# 持久化\n",
        "db.persist()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        },
        "id": "HrlSTrfYkoqo",
        "outputId": "7472b92e-f400-4e1c-e9a9-f081a77b8409"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading documents from /content/docs\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-96f0a0394468>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# 使用 process_documents() 函數創建 documents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'source'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmetadata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollection\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'metadatas'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mdocuments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mDocument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-1aa3c4569a19>\u001b[0m in \u001b[0;36mprocess_documents\u001b[0;34m(ignored_files)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \"\"\"\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Loading documents from {source_directory}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mdocuments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignored_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No new documents to load\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'load_documents' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GalEGsYIv0Q1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1X1fgCwAnZRP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import UnstructuredEmailLoader\n",
        "# Custom document loaders\n",
        "class MyElmLoader(UnstructuredEmailLoader):\n",
        "    \"\"\"Wrapper to fallback to text/plain when default does not work\"\"\"\n",
        "\n",
        "    def load(self) -> List[Document]:\n",
        "        \"\"\"Wrapper adding fallback for elm without html\"\"\"\n",
        "        try:\n",
        "            try:\n",
        "                doc = UnstructuredEmailLoader.load(self)\n",
        "            except ValueError as e:\n",
        "                if 'text/html content not found in email' in str(e):\n",
        "                    # Try plain text\n",
        "                    self.unstructured_kwargs[\"content_source\"]=\"text/plain\"\n",
        "                    doc = UnstructuredEmailLoader.load(self)\n",
        "                else:\n",
        "                    raise\n",
        "        except Exception as e:\n",
        "            # Add file_path to exception message\n",
        "            raise type(e)(f\"{self.file_path}: {e}\") from e\n",
        "\n",
        "        return doc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "id": "oitNQwhYnZUW",
        "outputId": "70c0524b-d38b-416c-9880-913a9f441036"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-bff2acdd1491>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocument_loaders\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mUnstructuredEmailLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Custom document loaders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mMyElmLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUnstructuredEmailLoader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;34m\"\"\"Wrapper to fallback to text/plain when default does not work\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-36-bff2acdd1491>\u001b[0m in \u001b[0;36mMyElmLoader\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m\"\"\"Wrapper to fallback to text/plain when default does not work\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDocument\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0;34m\"\"\"Wrapper adding fallback for elm without html\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'List' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_documents(source_dir: str, ignored_files: List[str] = []) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Loads all documents from the source documents directory, ignoring specified files\n",
        "    \"\"\"\n",
        "    all_files = []\n",
        "    for ext in LOADER_MAPPING:\n",
        "        all_files.extend(\n",
        "            glob.glob(os.path.join(source_dir, f\"**/*{ext}\"), recursive=True)\n",
        "        )\n",
        "    filtered_files = [file_path for file_path in all_files if file_path not in ignored_files]\n",
        "\n",
        "    with Pool(processes=os.cpu_count()) as pool:\n",
        "        results = []\n",
        "        with tqdm(total=len(filtered_files), desc='Loading new documents', ncols=80) as pbar:\n",
        "            for i, docs in enumerate(pool.imap_unordered(load_single_document, filtered_files)):\n",
        "                results.extend(docs)\n",
        "                pbar.update()\n",
        "\n",
        "    return results\n",
        "\n",
        "def process_documents(ignored_files: List[str] = []) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Load documents and split in chunks\n",
        "    \"\"\"\n",
        "    print(f\"Loading documents from {source_directory}\")\n",
        "    documents = load_documents(source_directory, ignored_files)\n",
        "    if not documents:\n",
        "        print(\"No new documents to load\")\n",
        "        exit(0)\n",
        "    print(f\"Loaded {len(documents)} new documents from {source_directory}\")\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "    texts = text_splitter.split_documents(documents)\n",
        "    print(f\"Split into {len(texts)} chunks of text (max. {chunk_size} tokens each)\")\n",
        "    return texts\n",
        "\n",
        "def does_vectorstore_exist(persist_directory: str) -> bool:\n",
        "    \"\"\"\n",
        "    Checks if vectorstore exists\n",
        "    \"\"\"\n",
        "    if os.path.exists(os.path.join(persist_directory, 'index')):\n",
        "        if os.path.exists(os.path.join(persist_directory, 'chroma-collections.parquet')) and os.path.exists(os.path.join(persist_directory, 'chroma-embeddings.parquet')):\n",
        "            list_index_files = glob.glob(os.path.join(persist_directory, 'index/*.bin'))\n",
        "            list_index_files += glob.glob(os.path.join(persist_directory, 'index/*.pkl'))\n",
        "            # At least 3 documents are needed in a working vectorstore\n",
        "            if len(list_index_files) > 3:\n",
        "                return True\n",
        "    return False\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "OFlTqRS1kouD",
        "outputId": "f5c45bb4-4e09-41d1-db55-9a575528b2a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-323bdc7fcb9e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mload_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_dir\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignored_files\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDocument\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \"\"\"\n\u001b[1;32m      3\u001b[0m     \u001b[0mLoads\u001b[0m \u001b[0mall\u001b[0m \u001b[0mdocuments\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msource\u001b[0m \u001b[0mdocuments\u001b[0m \u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignoring\u001b[0m \u001b[0mspecified\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \"\"\"\n\u001b[1;32m      5\u001b[0m     \u001b[0mall_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'List' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2GkYTrJSeckF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 保存数据到磁盘\n",
        "db.persist()\n"
      ],
      "metadata": {
        "id": "O5bATa5A5PIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3B0mGOIM5PLB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jpm7UJLs9OeK",
        "outputId": "af245b20-8790-4644-ea33-be4d61b819ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 指定文件路徑\n",
        "file_path = '/content/db/chroma-collections.parquet'\n",
        "\n",
        "# 讀取 Parquet 文件\n",
        "collections_df = pd.read_parquet(file_path)\n",
        "\n",
        "# 打印 DataFrame 的前幾行\n",
        "print(collections_df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_ZIV0zr9Ohy",
        "outputId": "679fa71d-1b4d-4d48-e253-c0a948ec8eee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                   uuid       name metadata\n",
            "0  a5c54cb8-269c-40f7-ac73-553957841741  langchain     null\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 指定文件路徑\n",
        "file_path = '/content/db/chroma-embeddings.parquet'\n",
        "\n",
        "# 讀取 Parquet 文件\n",
        "embeddings_df = pd.read_parquet(file_path)\n",
        "\n",
        "# 打印 DataFrame 的前幾行\n",
        "print(embeddings_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-kIVA5AF9pHh",
        "outputId": "27848a89-5e50-4ed4-efce-90e729aaf2b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                        collection_uuid                                  uuid  \\\n",
            "0  a5c54cb8-269c-40f7-ac73-553957841741  e9b5da0d-814b-483d-9b72-f23351cf58b8   \n",
            "1  a5c54cb8-269c-40f7-ac73-553957841741  cd53ccba-51fe-44a7-add6-1827e5bcf59c   \n",
            "2  a5c54cb8-269c-40f7-ac73-553957841741  4e2695a3-6186-4995-9615-83a7392b83e4   \n",
            "3  a5c54cb8-269c-40f7-ac73-553957841741  8b89f80c-9b95-491d-af0c-bb7935cb74be   \n",
            "4  a5c54cb8-269c-40f7-ac73-553957841741  396b7abb-0a95-4cc6-8871-ca7e7d0b9656   \n",
            "\n",
            "                                           embedding  \\\n",
            "0  [0.36408740282058716, 0.1827491819858551, 0.86...   \n",
            "1  [0.5019861459732056, 0.22415126860141754, 0.67...   \n",
            "2  [0.8026653528213501, 0.09918830543756485, 0.86...   \n",
            "3  [0.2523344159126282, -0.5297975540161133, 0.54...   \n",
            "4  [1.137739896774292, 0.5334827899932861, 0.4318...   \n",
            "\n",
            "                                            document  \\\n",
            "0  \"\"\"**Tools** are classes that an Agent uses to...   \n",
            "1  def _import_azure_cognitive_services_AzureCogs...   \n",
            "2  def _import_edenai_EdenAiParsingInvoiceTool() ...   \n",
            "3  def _import_file_management_WriteFileTool() ->...   \n",
            "4  def _import_searchapi_tool_SearchAPIResults() ...   \n",
            "\n",
            "                                     id metadata  \n",
            "0  b652a096-a3b3-11ee-aabd-0242ac1c000c     None  \n",
            "1  b652a1a4-a3b3-11ee-aabd-0242ac1c000c     None  \n",
            "2  b652a226-a3b3-11ee-aabd-0242ac1c000c     None  \n",
            "3  b652a29e-a3b3-11ee-aabd-0242ac1c000c     None  \n",
            "4  b652a302-a3b3-11ee-aabd-0242ac1c000c     None  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_-_Yg-iw9pQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zFQkw1Fd9pag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "keevLPQG9OkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db.get()"
      ],
      "metadata": {
        "id": "3NjfY3CQ7xWr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0799ade6-0244-4edf-ef24-997466dce458"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ids': ['b666e7d6-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666eb0a-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666eab0-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666ea56-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666e9fc-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666e9a2-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666e948-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666e8e4-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666e88a-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666e830-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666eb6e-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666e772-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666e718-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666e6be-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666e664-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666e60a-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666e5a6-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666e54c-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666e4f2-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666eeac-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666f212-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666f1ae-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666f154-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666f0fa-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666f096-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666f03c-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666efe2-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666ef60-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666ef06-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666e48e-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666ee48-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666edee-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666ed94-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666ed3a-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666ece0-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666ec7c-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666ec22-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666ebc8-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666d9d0-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666dd18-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666dcb4-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666dc5a-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666dc00-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666dba6-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666db42-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666dae8-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666da8e-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666da34-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666dd72-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666d976-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666d91c-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666d8c2-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666d868-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666d804-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666d7aa-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666d750-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666d6f6-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666e0f6-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666e434-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666e3da-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666e380-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666e31c-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666e2c2-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666e268-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666e20e-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666e1b4-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666e15a-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666f26c-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666e09c-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666e042-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666dfe8-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666df84-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666df2a-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666dec6-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666de62-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666ddcc-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6670324-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b667069e-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6670644-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66705ea-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6670586-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66704f0-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b667048c-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6670432-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66703d8-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b667037e-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6670702-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66702c0-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6670266-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6670202-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66701a8-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b667014e-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66700f4-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6670090-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6670036-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6670a40-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6670d74-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6670d1a-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6670cc0-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6670c66-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6670c02-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6670ba8-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6670b4e-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6670af4-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6670a9a-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666ffdc-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66709dc-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6670982-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6670928-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66708ce-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b667086a-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6670810-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66707b6-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b667075c-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666f5aa-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666f8f2-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666f898-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666f83e-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666f7da-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666f780-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666f71c-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666f6c2-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666f668-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666f604-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666f956-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666f550-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666f4f6-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666f49c-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666f442-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666f3de-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666f384-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666f320-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666f2c6-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666fc94-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666ff78-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666ff1e-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666fec4-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666fe6a-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666fe06-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666fdac-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666fd52-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666fcf8-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666d692-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666fc3a-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666fbd6-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666fb7c-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666fb22-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666fac8-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666fa64-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666fa0a-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666f9b0-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666a974-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666acc6-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666ac6c-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666ac12-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666abae-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666ab40-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666aae6-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666aa8c-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666aa32-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666a9ce-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666ad20-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666a91a-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666a8c0-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666a85c-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666a802-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666a7a8-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666a74e-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666a6ea-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666a690-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666b07c-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666b48c-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666b428-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666b3c4-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666b360-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666b252-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666b1ee-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666b194-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666b13a-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666b0e0-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666a636-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666b004-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666afaa-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666af50-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666aeec-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666ae92-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666ae38-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666adde-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666ad7a-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6669b3c-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6669f1a-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6669ec0-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6669e5c-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6669e02-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6669d9e-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6669d30-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6669cd6-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6669c04-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6669ba0-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6669f74-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6669ae2-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6669a7e-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6669a1a-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66699b6-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666995c-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66698f8-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6669894-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6669830-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666a2e4-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666a5dc-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666a582-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666a528-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666a4ce-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666a456-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666a3f2-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666a398-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666a33e-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666b4e6-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666a28a-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666a230-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666a1d6-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666a140-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666a0e6-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666a08c-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666a032-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6669fce-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666cc24-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666cf62-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666cf08-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666ceae-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666ce54-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666cdfa-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666cd96-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666cd3c-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666cce2-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666cc88-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666cfc6-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666cbca-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666cb70-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666cb16-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666cabc-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666ca58-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666c9fe-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666c9a4-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666c94a-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666d2fa-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666d638-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666d5de-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666d584-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666d520-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666d4c6-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666d46c-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666d412-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666d3b8-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666d35e-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666c8e6-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666d2a0-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666d246-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666d1ec-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666d188-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666d12e-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666d0d4-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666d07a-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666d016-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666be1e-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666c1de-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666c17a-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666c120-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666c0c6-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666c06c-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666c008-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666bfae-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666bedc-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666be82-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666c238-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666bdb0-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666b7a2-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666b73e-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666b6da-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666b66c-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666b612-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666b5ae-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666b54a-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666c576-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666c85a-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666c7f6-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666c79c-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666c742-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666c6e8-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666c684-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666c62a-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666c5d0-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6670dce-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666c512-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666c4b8-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666c45e-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666c404-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666c3aa-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666c350-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666c2ec-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666c292-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6675bda-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6675f18-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6675eb4-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6675e5a-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6675e00-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6675da6-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6675d4c-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6675ce8-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6675c8e-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6675c34-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6675f72-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6675b80-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6675b1c-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6675ac2-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6675a68-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6675a04-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66759aa-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6675946-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b667534c-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66762b0-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6676616-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66765b2-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6676558-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66764fe-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b667647c-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6676418-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66763be-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6676364-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b667630a-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66752e8-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6676256-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66761f2-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6676198-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b667613e-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66760da-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6676080-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6676026-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6675fcc-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66748d4-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6674c12-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6674bb8-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6674b54-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6674afa-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6674aa0-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6674a46-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66749e2-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6674988-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b667492e-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6674c6c-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b667487a-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6674820-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66747c6-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b667476c-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6674708-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66746ae-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6674640-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66745e6-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6674fb4-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b667528e-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6675234-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66751da-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6675180-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6675126-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66750cc-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6675068-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b667500e-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6676670-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6674f5a-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6674ef6-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6674e9c-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6674e42-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6674de8-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6674d8e-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6674d20-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6674cc6-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b667784a-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6677be2-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6677b88-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6677b2e-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6677aca-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6677a16-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66779bc-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6677962-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66778fe-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66778a4-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6677c3c-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66777f0-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6677796-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b667773c-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66776ce-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6677674-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b667761a-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66775c0-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6677566-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6677f7a-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66782b8-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b667825e-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6678204-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66781aa-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6678150-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66780ec-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6678092-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6678038-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6677fde-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6677502-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6677f20-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6677ebc-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6677e62-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6677e08-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6677dae-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6677d54-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6677cfa-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6677ca0-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66769ae-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6676cec-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6676c88-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6676c2e-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6676bd4-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6676b7a-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6676b20-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6676abc-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6676a62-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6676a08-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6676d46-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6676954-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66768f0-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6676896-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b667683c-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66767e2-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b667677e-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6676724-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66766ca-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66770b6-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66774a8-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b667744e-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66773ea-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6677368-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66772d2-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6677250-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66771ce-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6677142-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b667458c-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b667702a-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6676fd0-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6676f76-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6676f1c-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6676eb8-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6676e54-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6676dfa-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6676da0-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6671e90-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66721ce-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6672174-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b667211a-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66720c0-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b667205c-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6672002-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6671fa8-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6671f4e-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6671ef4-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6672228-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6671e36-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6671ddc-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6671d82-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6671d28-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6671cc4-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6671c6a-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6671c10-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6671b98-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6672584-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66728b8-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b667285e-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6672804-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66727aa-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6672750-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66726ec-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6672692-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6672638-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66725de-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6671b3e-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6672520-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66724c6-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b667246c-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6672412-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66723b8-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6672354-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66722f0-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6672282-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6671102-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b667144a-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66713e6-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b667138c-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6671332-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66712d8-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b667127e-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b667121a-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66711c0-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6671166-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66714a4-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66710b2-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b667104e-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6670ff4-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6670f9a-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6670f40-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6670ee6-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6670e82-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6670e28-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6671800-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6671ada-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6671a80-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6671a26-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66719cc-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6671972-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6671918-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66718b4-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b667185a-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b667291c-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b667179c-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6671742-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66716e8-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6671666-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b667160c-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66715b2-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6671558-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66714fe-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6673aec-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6673e52-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6673dee-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6673d6c-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6673d08-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6673cae-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6673c54-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6673bfa-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6673ba0-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6673b46-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6673eac-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6673a88-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6673a38-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66739d4-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b667397a-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6673916-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66738bc-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b667384e-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66737f4-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66741fe-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6674532-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66744ce-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6674474-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b667441a-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66743c0-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6674366-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b667430c-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66742b2-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6674258-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b667379a-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66741a4-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b667414a-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66740f0-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b667408c-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6674032-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6673fd8-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6673f6a-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6673f06-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6672cd2-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66730b0-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6673056-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6672ffc-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6672f98-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6672f3e-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6672ee4-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6672e80-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6672e08-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6672d72-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b667310a-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6672c14-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6672bb0-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6672b56-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6672afc-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6672aa2-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6672a48-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66729e4-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6672976-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6673466-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6673740-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66736e6-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b667368c-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6673628-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66735ce-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6673574-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b667351a-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66734c0-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66697cc-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6673402-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66733a8-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6673344-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66732ea-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6673290-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b667322c-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66731d2-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6673164-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665f7d6-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665fb28-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665fac4-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665fa6a-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665fa10-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665f9ac-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665f952-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665f8ee-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665f894-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665f83a-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665fb82-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665f77c-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665f722-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665f6c8-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665f664-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665f60a-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665f588-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665f4f2-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665f45c-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665feca-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6660208-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66601ae-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666014a-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66600f0-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6660096-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666003c-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665ffd8-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665ff7e-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665ff24-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665f39e-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665fe66-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665fe0c-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665fdb2-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665fd58-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665fcf4-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665fc9a-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665fc40-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665fbe6-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665e96c-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665ecb4-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665ec5a-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665ebf6-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665eb9c-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665eb42-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665eade-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665ea84-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665ea2a-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665e9c6-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665ed0e-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665e912-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665e8ae-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665e854-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665e7fa-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665e796-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665e73c-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665e6e2-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665e688-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665f056-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665f344-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665f2e0-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665f286-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665f22c-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665f1d2-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665f16e-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665f114-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665f0ba-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6660262-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665effc-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665efa2-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665ef48-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665eee4-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665ee8a-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665ee30-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665edd6-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665ed72-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666137e-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66616c6-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6661662-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6661608-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66615a4-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666154a-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66614f0-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6661496-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6661432-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66613d8-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6661720-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6661324-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66612ca-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666125c-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6661202-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666119e-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6661144-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66610ea-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6661090-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6661a68-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666235a-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66622c4-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666226a-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6662210-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66621b6-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6662152-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66620f8-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6662094-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6661ac2-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666102c-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6661a0e-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66619b4-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6661946-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66618ec-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6661892-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666182e-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66617d4-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666177a-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66605dc-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6660924-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66608d4-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6660870-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6660816-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66607bc-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6660758-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66606fe-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666069a-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6660636-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6660988-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6660578-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66604ec-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6660492-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6660438-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66603d4-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666037a-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6660316-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66602bc-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6660cee-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6660fc8-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6660f6e-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6660f14-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6660eba-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6660e60-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6660e06-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6660da2-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6660d48-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665e624-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6660c8a-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6660c30-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6660bcc-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6660b54-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6660afa-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6660aa0-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6660a46-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66609e2-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665bfa0-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665c2de-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665c284-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665c22a-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665c1d0-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665c176-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665c112-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665c0b8-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665c05e-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665c004-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665c342-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665bf46-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665beec-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665be88-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665be2e-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665bdd4-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665bd7a-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665bd16-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665bcbc-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665c680-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665c9c8-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665c964-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665c90a-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665c8a6-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665c84c-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665c7f2-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665c798-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665c73e-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665c6da-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665bc62-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665c626-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665c5cc-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665c568-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665c50e-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665c4b4-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665c450-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665c3f6-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665c39c-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665b212-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665b550-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665b4f6-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665b49c-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665b442-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665b3e8-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665b384-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665b32a-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665b2d0-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665b26c-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665b5b4-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665b1b8-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665b154-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665b0fa-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665b0a0-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665b046-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665afe2-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665af88-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665af2e-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665b91a-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665bc08-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665bba4-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665bb4a-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665baf0-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665ba96-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665ba3c-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665b9d8-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665b97e-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665ca22-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665b8c0-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665b866-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665b80c-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665b7a8-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665b74e-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665b6cc-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665b668-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665b60e-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665db52-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665dec2-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665de5e-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665dddc-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665dd82-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665dd28-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665dcc4-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665dc6a-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665dc06-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665dbac-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665df1c-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665daee-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665da94-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665da3a-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665d9e0-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665d986-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665d922-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665d8c8-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665d86e-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665e26e-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665e5c0-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665e566-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665e502-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665e4a8-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665e444-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665e3e0-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665e386-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665e32c-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665e2c8-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665d814-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665e20a-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665e1b0-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665e156-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665e0fc-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665e098-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665e03e-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665dfda-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665df80-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665cdc4-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665d10c-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665d0b2-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665d058-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665cffe-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665cf9a-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665cf40-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665cedc-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665ce82-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665ce1e-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665d170-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665cd60-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665ccfc-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665cc70-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665cc0c-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665cbb2-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665cb58-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665caea-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665ca86-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665d4c2-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665d7b0-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665d756-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665d6f2-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665d68e-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665d634-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665d5da-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665d576-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665d51c-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66623b4-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665d468-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665d404-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665d3aa-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665d346-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665d2ec-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665d288-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665d22e-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b665d1d4-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6666c98-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666701c-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6666fc2-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6666f5e-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6666efa-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6666e96-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6666e32-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6666dc4-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6666d60-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6666cfc-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6667080-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6666c34-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6666bd0-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6666b76-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6666b12-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6666aae-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6666a4a-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66668ec-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6666892-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6667404-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666779c-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6667738-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66676d4-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6667670-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666760c-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66675a8-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6667530-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66674cc-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6667468-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6666838-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66673a0-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666733c-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66672d8-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6667274-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6667210-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66671ac-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6667148-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66670e4-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6665de8-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666613a-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66660d6-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666607c-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6666022-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6665fc8-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6665f6e-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6665f0a-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6665e9c-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6665e42-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6666194-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6665d84-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6665d2a-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6665cc6-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6665c6c-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6665c12-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6665bb8-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6665b5e-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6665b04-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66664c8-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66667d4-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666677a-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6666716-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66666bc-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6666662-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66665fe-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6666590-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666652c-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6667800-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666646e-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6666414-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66663ba-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6666360-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6666306-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66662a2-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6666248-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66661ee-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6668a0c-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6668d90-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6668d36-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6668cd2-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6668c6e-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6668c0a-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6668b92-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6668b38-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6668ad4-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6668a70-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6668df4-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66689a8-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666894e-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66688ea-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666887c-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6668822-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66687be-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b666875a-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66686f6-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6669358-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6669768-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6669704-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b66696a0-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  'b6669646-a3b3-11ee-aabd-0242ac1c000c',\n",
              "  ...],\n",
              " 'embeddings': None,\n",
              " 'documents': ['if current_content:\\n            lines_with_metadata.append(\\n                {\"content\": \"\\\\n\".join(current_content), \"metadata\": current_metadata}\\n            )\\n\\n        # lines_with_metadata has each line with associated header metadata\\n        # aggregate these into chunks based on common metadata\\n        if not self.return_each_line:\\n            return self.aggregate_lines_to_chunks(lines_with_metadata)\\n        else:\\n            return [\\n                Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n                for chunk in lines_with_metadata\\n            ]',\n",
              "  'class Language(str, Enum):\\n    \"\"\"Enum of the programming languages.\"\"\"\\n\\n    CPP = \"cpp\"\\n    GO = \"go\"\\n    JAVA = \"java\"\\n    KOTLIN = \"kotlin\"\\n    JS = \"js\"\\n    TS = \"ts\"\\n    PHP = \"php\"\\n    PROTO = \"proto\"\\n    PYTHON = \"python\"\\n    RST = \"rst\"\\n    RUBY = \"ruby\"\\n    RUST = \"rust\"\\n    SCALA = \"scala\"\\n    SWIFT = \"swift\"\\n    MARKDOWN = \"markdown\"\\n    LATEX = \"latex\"\\n    HTML = \"html\"\\n    SOL = \"sol\"\\n    CSHARP = \"csharp\"\\n    COBOL = \"cobol\"',\n",
              "  'def split_text(self, text: str) -> List[str]:\\n        def encode_strip_start_and_stop_token_ids(text: str) -> List[int]:\\n            return self._encode(text)[1:-1]\\n\\n        tokenizer = Tokenizer(\\n            chunk_overlap=self._chunk_overlap,\\n            tokens_per_chunk=self.tokens_per_chunk,\\n            decode=self.tokenizer.decode,\\n            encode=encode_strip_start_and_stop_token_ids,\\n        )\\n\\n        return split_text_on_tokens(text=text, tokenizer=tokenizer)\\n\\n    def count_tokens(self, *, text: str) -> int:\\n        return len(self._encode(text))\\n\\n    _max_length_equal_32_bit_integer: int = 2**32\\n\\n    def _encode(self, text: str) -> List[int]:\\n        token_ids_with_start_and_end_token_ids = self.tokenizer.encode(\\n            text,\\n            max_length=self._max_length_equal_32_bit_integer,\\n            truncation=\"do_not_truncate\",\\n        )\\n        return token_ids_with_start_and_end_token_ids',\n",
              "  'class SentenceTransformersTokenTextSplitter(TextSplitter):\\n    \"\"\"Splitting text to tokens using sentence model tokenizer.\"\"\"\\n\\n    def __init__(\\n        self,\\n        chunk_overlap: int = 50,\\n        model_name: str = \"sentence-transformers/all-mpnet-base-v2\",\\n        tokens_per_chunk: Optional[int] = None,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(**kwargs, chunk_overlap=chunk_overlap)\\n\\n        try:\\n            from sentence_transformers import SentenceTransformer\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import sentence_transformer python package. \"\\n                \"This is needed in order to for SentenceTransformersTokenTextSplitter. \"\\n                \"Please install it with `pip install sentence-transformers`.\"\\n            )\\n\\n        self.model_name = model_name\\n        self._model = SentenceTransformer(self.model_name)\\n        self.tokenizer = self._model.tokenizer\\n        self._initialize_chunk_configuration(tokens_per_chunk=tokens_per_chunk)\\n\\n    def _initialize_chunk_configuration(\\n        self, *, tokens_per_chunk: Optional[int]\\n    ) -> None:\\n        self.maximum_tokens_per_chunk = cast(int, self._model.max_seq_length)\\n\\n        if tokens_per_chunk is None:\\n            self.tokens_per_chunk = self.maximum_tokens_per_chunk\\n        else:\\n            self.tokens_per_chunk = tokens_per_chunk\\n\\n        if self.tokens_per_chunk > self.maximum_tokens_per_chunk:\\n            raise ValueError(\\n                f\"The token limit of the models \\'{self.model_name}\\'\"\\n                f\" is: {self.maximum_tokens_per_chunk}.\"\\n                f\" Argument tokens_per_chunk={self.tokens_per_chunk}\"\\n                f\" > maximum token limit.\"\\n            )\\n\\n    def split_text(self, text: str) -> List[str]:\\n        def encode_strip_start_and_stop_token_ids(text: str) -> List[int]:\\n            return self._encode(text)[1:-1]',\n",
              "  'class TokenTextSplitter(TextSplitter):\\n    \"\"\"Splitting text to tokens using model tokenizer.\"\"\"\\n\\n    def __init__(\\n        self,\\n        encoding_name: str = \"gpt2\",\\n        model_name: Optional[str] = None,\\n        allowed_special: Union[Literal[\"all\"], AbstractSet[str]] = set(),\\n        disallowed_special: Union[Literal[\"all\"], Collection[str]] = \"all\",\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(**kwargs)\\n        try:\\n            import tiktoken\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import tiktoken python package. \"\\n                \"This is needed in order to for TokenTextSplitter. \"\\n                \"Please install it with `pip install tiktoken`.\"\\n            )\\n\\n        if model_name is not None:\\n            enc = tiktoken.encoding_for_model(model_name)\\n        else:\\n            enc = tiktoken.get_encoding(encoding_name)\\n        self._tokenizer = enc\\n        self._allowed_special = allowed_special\\n        self._disallowed_special = disallowed_special\\n\\n    def split_text(self, text: str) -> List[str]:\\n        def _encode(_text: str) -> List[int]:\\n            return self._tokenizer.encode(\\n                _text,\\n                allowed_special=self._allowed_special,\\n                disallowed_special=self._disallowed_special,\\n            )\\n\\n        tokenizer = Tokenizer(\\n            chunk_overlap=self._chunk_overlap,\\n            tokens_per_chunk=self._chunk_size,\\n            decode=self._tokenizer.decode,\\n            encode=_encode,\\n        )\\n\\n        return split_text_on_tokens(text=text, tokenizer=tokenizer)',\n",
              "  'class Tokenizer:\\n    \"\"\"Tokenizer data class.\"\"\"\\n\\n    chunk_overlap: int\\n    \"\"\"Overlap in tokens between chunks\"\"\"\\n    tokens_per_chunk: int\\n    \"\"\"Maximum number of tokens per chunk\"\"\"\\n    decode: Callable[[List[int]], str]\\n    \"\"\" Function to decode a list of token ids to a string\"\"\"\\n    encode: Callable[[str], List[int]]\\n    \"\"\" Function to encode a string to a list of token ids\"\"\"\\n\\n\\ndef split_text_on_tokens(*, text: str, tokenizer: Tokenizer) -> List[str]:\\n    \"\"\"Split incoming text and return chunks using tokenizer.\"\"\"\\n    splits: List[str] = []\\n    input_ids = tokenizer.encode(text)\\n    start_idx = 0\\n    cur_idx = min(start_idx + tokenizer.tokens_per_chunk, len(input_ids))\\n    chunk_ids = input_ids[start_idx:cur_idx]\\n    while start_idx < len(input_ids):\\n        splits.append(tokenizer.decode(chunk_ids))\\n        if cur_idx == len(input_ids):\\n            break\\n        start_idx += tokenizer.tokens_per_chunk - tokenizer.chunk_overlap\\n        cur_idx = min(start_idx + tokenizer.tokens_per_chunk, len(input_ids))\\n        chunk_ids = input_ids[start_idx:cur_idx]\\n    return splits',\n",
              "  '# map xhtml namespace prefix\\n        ns_map = {\"h\": \"http://www.w3.org/1999/xhtml\"}\\n\\n        # build list of elements from DOM\\n        elements = []\\n        for element in result_dom.findall(\"*//*\", ns_map):\\n            if element.findall(\"*[@class=\\'headers\\']\") or element.findall(\\n                \"*[@class=\\'chunk\\']\"\\n            ):\\n                elements.append(\\n                    ElementType(\\n                        url=file,\\n                        xpath=\"\".join(\\n                            [\\n                                node.text\\n                                for node in element.findall(\"*[@class=\\'xpath\\']\", ns_map)\\n                            ]\\n                        ),\\n                        content=\"\".join(\\n                            [\\n                                node.text\\n                                for node in element.findall(\"*[@class=\\'chunk\\']\", ns_map)\\n                            ]\\n                        ),\\n                        metadata={\\n                            # Add text of specified headers to metadata using header\\n                            # mapping.\\n                            header_mapping[node.tag]: node.text\\n                            for node in filter(\\n                                lambda x: x.tag in header_filter,\\n                                element.findall(\"*[@class=\\'headers\\']/*\", ns_map),\\n                            )\\n                        },\\n                    )\\n                )\\n\\n        if not self.return_each_element:\\n            return self.aggregate_elements_to_chunks(elements)\\n        else:\\n            return [\\n                Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n                for chunk in elements\\n            ]\\n\\n\\n# should be in newer Python versions (3.10+)\\n# @dataclass(frozen=True, kw_only=True, slots=True)\\n@dataclass(frozen=True)',\n",
              "  'return [\\n            Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n            for chunk in aggregated_chunks\\n        ]\\n\\n    def split_text_from_url(self, url: str) -> List[Document]:\\n        \"\"\"Split HTML from web URL\\n\\n        Args:\\n            url: web URL\\n        \"\"\"\\n        r = requests.get(url)\\n        return self.split_text_from_file(BytesIO(r.content))\\n\\n    def split_text(self, text: str) -> List[Document]:\\n        \"\"\"Split HTML text string\\n\\n        Args:\\n            text: HTML text\\n        \"\"\"\\n        return self.split_text_from_file(StringIO(text))\\n\\n    def split_text_from_file(self, file: Any) -> List[Document]:\\n        \"\"\"Split HTML file\\n\\n        Args:\\n            file: HTML file\\n        \"\"\"\\n        try:\\n            from lxml import etree\\n        except ImportError as e:\\n            raise ImportError(\\n                \"Unable to import lxml, please install with `pip install lxml`.\"\\n            ) from e\\n        # use lxml library to parse html document and return xml ElementTree\\n        parser = etree.HTMLParser()\\n        tree = etree.parse(file, parser)\\n\\n        # document transformation for \"structure-aware\" chunking is handled with xsl.\\n        # see comments in html_chunks_with_headers.xslt for more detailed information.\\n        xslt_path = (\\n            pathlib.Path(__file__).parent\\n            / \"document_transformers/xsl/html_chunks_with_headers.xslt\"\\n        )\\n        xslt_tree = etree.parse(xslt_path)\\n        transform = etree.XSLT(xslt_tree)\\n        result = transform(tree)\\n        result_dom = etree.fromstring(str(result))\\n\\n        # create filter and mapping for header metadata\\n        header_filter = [header[0] for header in self.headers_to_split_on]\\n        header_mapping = dict(self.headers_to_split_on)\\n\\n        # map xhtml namespace prefix\\n        ns_map = {\"h\": \"http://www.w3.org/1999/xhtml\"}',\n",
              "  'class HTMLHeaderTextSplitter:\\n    \"\"\"\\n    Splitting HTML files based on specified headers.\\n    Requires lxml package.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        headers_to_split_on: List[Tuple[str, str]],\\n        return_each_element: bool = False,\\n    ):\\n        \"\"\"Create a new HTMLHeaderTextSplitter.\\n\\n        Args:\\n            headers_to_split_on: list of tuples of headers we want to track mapped to\\n                (arbitrary) keys for metadata. Allowed header values: h1, h2, h3, h4,\\n                h5, h6 e.g. [(\"h1\", \"Header 1\"), (\"h2\", \"Header 2)].\\n            return_each_element: Return each element w/ associated headers.\\n        \"\"\"\\n        # Output element-by-element or aggregated into chunks w/ common headers\\n        self.return_each_element = return_each_element\\n        self.headers_to_split_on = sorted(headers_to_split_on)\\n\\n    def aggregate_elements_to_chunks(\\n        self, elements: List[ElementType]\\n    ) -> List[Document]:\\n        \"\"\"Combine elements with common metadata into chunks\\n\\n        Args:\\n            elements: HTML element content with associated identifying info and metadata\\n        \"\"\"\\n        aggregated_chunks: List[ElementType] = []\\n\\n        for element in elements:\\n            if (\\n                aggregated_chunks\\n                and aggregated_chunks[-1][\"metadata\"] == element[\"metadata\"]\\n            ):\\n                # If the last element in the aggregated list\\n                # has the same metadata as the current element,\\n                # append the current content to the last element\\'s content\\n                aggregated_chunks[-1][\"content\"] += \"  \\\\n\" + element[\"content\"]\\n            else:\\n                # Otherwise, append the current element to the aggregated list\\n                aggregated_chunks.append(element)\\n\\n        return [\\n            Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n            for chunk in aggregated_chunks\\n        ]',\n",
              "  'class ElementType(TypedDict):\\n    \"\"\"Element type as typed dict.\"\"\"\\n\\n    url: str\\n    xpath: str\\n    content: str\\n    metadata: Dict[str, str]',\n",
              "  'class RecursiveCharacterTextSplitter(TextSplitter):\\n    \"\"\"Splitting text by recursively look at characters.\\n\\n    Recursively tries to split by different characters to find one\\n    that works.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        separators: Optional[List[str]] = None,\\n        keep_separator: bool = True,\\n        is_separator_regex: bool = False,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(keep_separator=keep_separator, **kwargs)\\n        self._separators = separators or [\"\\\\n\\\\n\", \"\\\\n\", \" \", \"\"]\\n        self._is_separator_regex = is_separator_regex\\n\\n    def _split_text(self, text: str, separators: List[str]) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        final_chunks = []\\n        # Get appropriate separator to use\\n        separator = separators[-1]\\n        new_separators = []\\n        for i, _s in enumerate(separators):\\n            _separator = _s if self._is_separator_regex else re.escape(_s)\\n            if _s == \"\":\\n                separator = _s\\n                break\\n            if re.search(_separator, text):\\n                separator = _s\\n                new_separators = separators[i + 1 :]\\n                break\\n\\n        _separator = separator if self._is_separator_regex else re.escape(separator)\\n        splits = _split_text_with_regex(text, _separator, self._keep_separator)',\n",
              "  '# Push the current header to the stack\\n                        header: HeaderType = {\\n                            \"level\": current_header_level,\\n                            \"name\": name,\\n                            \"data\": stripped_line[len(sep) :].strip(),\\n                        }\\n                        header_stack.append(header)\\n                        # Update initial_metadata with the current header\\n                        initial_metadata[name] = header[\"data\"]\\n\\n                    # Add the previous line to the lines_with_metadata\\n                    # only if current_content is not empty\\n                    if current_content:\\n                        lines_with_metadata.append(\\n                            {\\n                                \"content\": \"\\\\n\".join(current_content),\\n                                \"metadata\": current_metadata.copy(),\\n                            }\\n                        )\\n                        current_content.clear()\\n\\n                    break\\n            else:\\n                if stripped_line:\\n                    current_content.append(stripped_line)\\n                elif current_content:\\n                    lines_with_metadata.append(\\n                        {\\n                            \"content\": \"\\\\n\".join(current_content),\\n                            \"metadata\": current_metadata.copy(),\\n                        }\\n                    )\\n                    current_content.clear()\\n\\n            current_metadata = initial_metadata.copy()\\n\\n        if current_content:\\n            lines_with_metadata.append(\\n                {\"content\": \"\\\\n\".join(current_content), \"metadata\": current_metadata}\\n            )',\n",
              "  'if in_code_block:\\n                current_content.append(stripped_line)\\n                continue\\n\\n            # Check each line against each of the header types (e.g., #, ##)\\n            for sep, name in self.headers_to_split_on:\\n                # Check if line starts with a header that we intend to split on\\n                if stripped_line.startswith(sep) and (\\n                    # Header with no text OR header is followed by space\\n                    # Both are valid conditions that sep is being used a header\\n                    len(stripped_line) == len(sep) or stripped_line[len(sep)] == \" \"\\n                ):\\n                    # Ensure we are tracking the header as metadata\\n                    if name is not None:\\n                        # Get the current header level\\n                        current_header_level = sep.count(\"#\")\\n\\n                        # Pop out headers of lower or same level from the stack\\n                        while (\\n                            header_stack\\n                            and header_stack[-1][\"level\"] >= current_header_level\\n                        ):\\n                            # We have encountered a new header\\n                            # at the same or higher level\\n                            popped_header = header_stack.pop()\\n                            # Clear the metadata for the\\n                            # popped header in initial_metadata\\n                            if popped_header[\"name\"] in initial_metadata:\\n                                initial_metadata.pop(popped_header[\"name\"])',\n",
              "  'def split_text(self, text: str) -> List[Document]:\\n        \"\"\"Split markdown file\\n        Args:\\n            text: Markdown file\"\"\"\\n\\n        # Split the input text by newline character (\"\\\\n\").\\n        lines = text.split(\"\\\\n\")\\n        # Final output\\n        lines_with_metadata: List[LineType] = []\\n        # Content and metadata of the chunk currently being processed\\n        current_content: List[str] = []\\n        current_metadata: Dict[str, str] = {}\\n        # Keep track of the nested header structure\\n        # header_stack: List[Dict[str, Union[int, str]]] = []\\n        header_stack: List[HeaderType] = []\\n        initial_metadata: Dict[str, str] = {}\\n\\n        in_code_block = False\\n        opening_fence = \"\"\\n\\n        for line in lines:\\n            stripped_line = line.strip()\\n\\n            if not in_code_block:\\n                # Exclude inline code spans\\n                if stripped_line.startswith(\"```\") and stripped_line.count(\"```\") == 1:\\n                    in_code_block = True\\n                    opening_fence = \"```\"\\n                elif stripped_line.startswith(\"~~~\"):\\n                    in_code_block = True\\n                    opening_fence = \"~~~\"\\n            else:\\n                if stripped_line.startswith(opening_fence):\\n                    in_code_block = False\\n                    opening_fence = \"\"\\n\\n            if in_code_block:\\n                current_content.append(stripped_line)\\n                continue',\n",
              "  'class MarkdownHeaderTextSplitter:\\n    \"\"\"Splitting markdown files based on specified headers.\"\"\"\\n\\n    def __init__(\\n        self, headers_to_split_on: List[Tuple[str, str]], return_each_line: bool = False\\n    ):\\n        \"\"\"Create a new MarkdownHeaderTextSplitter.\\n\\n        Args:\\n            headers_to_split_on: Headers we want to track\\n            return_each_line: Return each line w/ associated headers\\n        \"\"\"\\n        # Output line-by-line or aggregated into chunks w/ common headers\\n        self.return_each_line = return_each_line\\n        # Given the headers we want to split on,\\n        # (e.g., \"#, ##, etc\") order by length\\n        self.headers_to_split_on = sorted(\\n            headers_to_split_on, key=lambda split: len(split[0]), reverse=True\\n        )\\n\\n    def aggregate_lines_to_chunks(self, lines: List[LineType]) -> List[Document]:\\n        \"\"\"Combine lines with common metadata into chunks\\n        Args:\\n            lines: Line of text / associated header metadata\\n        \"\"\"\\n        aggregated_chunks: List[LineType] = []\\n\\n        for line in lines:\\n            if (\\n                aggregated_chunks\\n                and aggregated_chunks[-1][\"metadata\"] == line[\"metadata\"]\\n            ):\\n                # If the last line in the aggregated list\\n                # has the same metadata as the current line,\\n                # append the current content to the last lines\\'s content\\n                aggregated_chunks[-1][\"content\"] += \"  \\\\n\" + line[\"content\"]\\n            else:\\n                # Otherwise, append the current line to the aggregated list\\n                aggregated_chunks.append(line)\\n\\n        return [\\n            Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n            for chunk in aggregated_chunks\\n        ]\\n\\n    def split_text(self, text: str) -> List[Document]:\\n        \"\"\"Split markdown file\\n        Args:\\n            text: Markdown file\"\"\"',\n",
              "  'class CharacterTextSplitter(TextSplitter):\\n    \"\"\"Splitting text that looks at characters.\"\"\"\\n\\n    def __init__(\\n        self, separator: str = \"\\\\n\\\\n\", is_separator_regex: bool = False, **kwargs: Any\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(**kwargs)\\n        self._separator = separator\\n        self._is_separator_regex = is_separator_regex\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        # First we naively split the large input into a bunch of smaller ones.\\n        separator = (\\n            self._separator if self._is_separator_regex else re.escape(self._separator)\\n        )\\n        splits = _split_text_with_regex(text, separator, self._keep_separator)\\n        _separator = \"\" if self._keep_separator else self._separator\\n        return self._merge_splits(splits, _separator)\\n\\n\\nclass LineType(TypedDict):\\n    \"\"\"Line type as typed dict.\"\"\"\\n\\n    metadata: Dict[str, str]\\n    content: str\\n\\n\\nclass HeaderType(TypedDict):\\n    \"\"\"Header type as typed dict.\"\"\"\\n\\n    level: int\\n    name: str\\n    data: str',\n",
              "  'if issubclass(cls, TokenTextSplitter):\\n            extra_kwargs = {\\n                \"encoding_name\": encoding_name,\\n                \"model_name\": model_name,\\n                \"allowed_special\": allowed_special,\\n                \"disallowed_special\": disallowed_special,\\n            }\\n            kwargs = {**kwargs, **extra_kwargs}\\n\\n        return cls(length_function=_tiktoken_encoder, **kwargs)\\n\\n    def transform_documents(\\n        self, documents: Sequence[Document], **kwargs: Any\\n    ) -> Sequence[Document]:\\n        \"\"\"Transform sequence of documents by splitting them.\"\"\"\\n        return self.split_documents(list(documents))\\n\\n    async def atransform_documents(\\n        self, documents: Sequence[Document], **kwargs: Any\\n    ) -> Sequence[Document]:\\n        \"\"\"Asynchronously transform a sequence of documents by splitting them.\"\"\"\\n        return await asyncio.get_running_loop().run_in_executor(\\n            None, partial(self.transform_documents, **kwargs), documents\\n        )',\n",
              "  'if not isinstance(tokenizer, PreTrainedTokenizerBase):\\n                raise ValueError(\\n                    \"Tokenizer received was not an instance of PreTrainedTokenizerBase\"\\n                )\\n\\n            def _huggingface_tokenizer_length(text: str) -> int:\\n                return len(tokenizer.encode(text))\\n\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import transformers python package. \"\\n                \"Please install it with `pip install transformers`.\"\\n            )\\n        return cls(length_function=_huggingface_tokenizer_length, **kwargs)\\n\\n    @classmethod\\n    def from_tiktoken_encoder(\\n        cls: Type[TS],\\n        encoding_name: str = \"gpt2\",\\n        model_name: Optional[str] = None,\\n        allowed_special: Union[Literal[\"all\"], AbstractSet[str]] = set(),\\n        disallowed_special: Union[Literal[\"all\"], Collection[str]] = \"all\",\\n        **kwargs: Any,\\n    ) -> TS:\\n        \"\"\"Text splitter that uses tiktoken encoder to count length.\"\"\"\\n        try:\\n            import tiktoken\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import tiktoken python package. \"\\n                \"This is needed in order to calculate max_tokens_for_prompt. \"\\n                \"Please install it with `pip install tiktoken`.\"\\n            )\\n\\n        if model_name is not None:\\n            enc = tiktoken.encoding_for_model(model_name)\\n        else:\\n            enc = tiktoken.get_encoding(encoding_name)\\n\\n        def _tiktoken_encoder(text: str) -> int:\\n            return len(\\n                enc.encode(\\n                    text,\\n                    allowed_special=allowed_special,\\n                    disallowed_special=disallowed_special,\\n                )\\n            )',\n",
              "  'docs = []\\n        current_doc: List[str] = []\\n        total = 0\\n        for d in splits:\\n            _len = self._length_function(d)\\n            if (\\n                total + _len + (separator_len if len(current_doc) > 0 else 0)\\n                > self._chunk_size\\n            ):\\n                if total > self._chunk_size:\\n                    logger.warning(\\n                        f\"Created a chunk of size {total}, \"\\n                        f\"which is longer than the specified {self._chunk_size}\"\\n                    )\\n                if len(current_doc) > 0:\\n                    doc = self._join_docs(current_doc, separator)\\n                    if doc is not None:\\n                        docs.append(doc)\\n                    # Keep on popping if:\\n                    # - we have a larger chunk than in the chunk overlap\\n                    # - or if we still have any chunks and the length is long\\n                    while total > self._chunk_overlap or (\\n                        total + _len + (separator_len if len(current_doc) > 0 else 0)\\n                        > self._chunk_size\\n                        and total > 0\\n                    ):\\n                        total -= self._length_function(current_doc[0]) + (\\n                            separator_len if len(current_doc) > 1 else 0\\n                        )\\n                        current_doc = current_doc[1:]\\n            current_doc.append(d)\\n            total += _len + (separator_len if len(current_doc) > 1 else 0)\\n        doc = self._join_docs(current_doc, separator)\\n        if doc is not None:\\n            docs.append(doc)\\n        return docs\\n\\n    @classmethod\\n    def from_huggingface_tokenizer(cls, tokenizer: Any, **kwargs: Any) -> TextSplitter:\\n        \"\"\"Text splitter that uses HuggingFace tokenizer to count length.\"\"\"\\n        try:\\n            from transformers import PreTrainedTokenizerBase',\n",
              "  '\"\\\\nevent \",\\n                \"\\\\nmodifier \",\\n                \"\\\\nerror \",\\n                \"\\\\nstruct \",\\n                \"\\\\nenum \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo while \",\\n                \"\\\\nassembly \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.COBOL:\\n            return [\\n                # Split along divisions\\n                \"\\\\nIDENTIFICATION DIVISION.\",\\n                \"\\\\nENVIRONMENT DIVISION.\",\\n                \"\\\\nDATA DIVISION.\",\\n                \"\\\\nPROCEDURE DIVISION.\",\\n                # Split along sections within DATA DIVISION\\n                \"\\\\nWORKING-STORAGE SECTION.\",\\n                \"\\\\nLINKAGE SECTION.\",\\n                \"\\\\nFILE SECTION.\",\\n                # Split along sections within PROCEDURE DIVISION\\n                \"\\\\nINPUT-OUTPUT SECTION.\",\\n                # Split along paragraphs and common statements\\n                \"\\\\nOPEN \",\\n                \"\\\\nCLOSE \",\\n                \"\\\\nREAD \",\\n                \"\\\\nWRITE \",\\n                \"\\\\nIF \",\\n                \"\\\\nELSE \",\\n                \"\\\\nMOVE \",\\n                \"\\\\nPERFORM \",\\n                \"\\\\nUNTIL \",\\n                \"\\\\nVARYING \",\\n                \"\\\\nACCEPT \",\\n                \"\\\\nDISPLAY \",\\n                \"\\\\nSTOP RUN.\",\\n                # Split by the normal type of lines\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]',\n",
              "  'if not isinstance(tokenizer, PreTrainedTokenizerBase):\\n                raise ValueError(\\n                    \"Tokenizer received was not an instance of PreTrainedTokenizerBase\"\\n                )\\n\\n            def _huggingface_tokenizer_length(text: str) -> int:\\n                return len(tokenizer.encode(text))\\n\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import transformers python package. \"\\n                \"Please install it with `pip install transformers`.\"\\n            )\\n        return cls(length_function=_huggingface_tokenizer_length, **kwargs)\\n\\n    @classmethod\\n    def from_tiktoken_encoder(\\n        cls: Type[TS],\\n        encoding_name: str = \"gpt2\",\\n        model_name: Optional[str] = None,\\n        allowed_special: Union[Literal[\"all\"], AbstractSet[str]] = set(),\\n        disallowed_special: Union[Literal[\"all\"], Collection[str]] = \"all\",\\n        **kwargs: Any,\\n    ) -> TS:\\n        \"\"\"Text splitter that uses tiktoken encoder to count length.\"\"\"\\n        try:\\n            import tiktoken\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import tiktoken python package. \"\\n                \"This is needed in order to calculate max_tokens_for_prompt. \"\\n                \"Please install it with `pip install tiktoken`.\"\\n            )\\n\\n        if model_name is not None:\\n            enc = tiktoken.encoding_for_model(model_name)\\n        else:\\n            enc = tiktoken.get_encoding(encoding_name)\\n\\n        def _tiktoken_encoder(text: str) -> int:\\n            return len(\\n                enc.encode(\\n                    text,\\n                    allowed_special=allowed_special,\\n                    disallowed_special=disallowed_special,\\n                )\\n            )',\n",
              "  'docs = []\\n        current_doc: List[str] = []\\n        total = 0\\n        for d in splits:\\n            _len = self._length_function(d)\\n            if (\\n                total + _len + (separator_len if len(current_doc) > 0 else 0)\\n                > self._chunk_size\\n            ):\\n                if total > self._chunk_size:\\n                    logger.warning(\\n                        f\"Created a chunk of size {total}, \"\\n                        f\"which is longer than the specified {self._chunk_size}\"\\n                    )\\n                if len(current_doc) > 0:\\n                    doc = self._join_docs(current_doc, separator)\\n                    if doc is not None:\\n                        docs.append(doc)\\n                    # Keep on popping if:\\n                    # - we have a larger chunk than in the chunk overlap\\n                    # - or if we still have any chunks and the length is long\\n                    while total > self._chunk_overlap or (\\n                        total + _len + (separator_len if len(current_doc) > 0 else 0)\\n                        > self._chunk_size\\n                        and total > 0\\n                    ):\\n                        total -= self._length_function(current_doc[0]) + (\\n                            separator_len if len(current_doc) > 1 else 0\\n                        )\\n                        current_doc = current_doc[1:]\\n            current_doc.append(d)\\n            total += _len + (separator_len if len(current_doc) > 1 else 0)\\n        doc = self._join_docs(current_doc, separator)\\n        if doc is not None:\\n            docs.append(doc)\\n        return docs\\n\\n    @classmethod\\n    def from_huggingface_tokenizer(cls, tokenizer: Any, **kwargs: Any) -> TextSplitter:\\n        \"\"\"Text splitter that uses HuggingFace tokenizer to count length.\"\"\"\\n        try:\\n            from transformers import PreTrainedTokenizerBase',\n",
              "  '@abstractmethod\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split text into multiple components.\"\"\"\\n\\n    def create_documents(\\n        self, texts: List[str], metadatas: Optional[List[dict]] = None\\n    ) -> List[Document]:\\n        \"\"\"Create documents from a list of texts.\"\"\"\\n        _metadatas = metadatas or [{}] * len(texts)\\n        documents = []\\n        for i, text in enumerate(texts):\\n            index = -1\\n            for chunk in self.split_text(text):\\n                metadata = copy.deepcopy(_metadatas[i])\\n                if self._add_start_index:\\n                    index = text.find(chunk, index + 1)\\n                    metadata[\"start_index\"] = index\\n                new_doc = Document(page_content=chunk, metadata=metadata)\\n                documents.append(new_doc)\\n        return documents\\n\\n    def split_documents(self, documents: Iterable[Document]) -> List[Document]:\\n        \"\"\"Split documents.\"\"\"\\n        texts, metadatas = [], []\\n        for doc in documents:\\n            texts.append(doc.page_content)\\n            metadatas.append(doc.metadata)\\n        return self.create_documents(texts, metadatas=metadatas)\\n\\n    def _join_docs(self, docs: List[str], separator: str) -> Optional[str]:\\n        text = separator.join(docs)\\n        if self._strip_whitespace:\\n            text = text.strip()\\n        if text == \"\":\\n            return None\\n        else:\\n            return text\\n\\n    def _merge_splits(self, splits: Iterable[str], separator: str) -> List[str]:\\n        # We now want to combine these smaller pieces into medium size\\n        # chunks to send to the LLM.\\n        separator_len = self._length_function(separator)',\n",
              "  'class TextSplitter(BaseDocumentTransformer, ABC):\\n    \"\"\"Interface for splitting text into chunks.\"\"\"\\n\\n    def __init__(\\n        self,\\n        chunk_size: int = 4000,\\n        chunk_overlap: int = 200,\\n        length_function: Callable[[str], int] = len,\\n        keep_separator: bool = False,\\n        add_start_index: bool = False,\\n        strip_whitespace: bool = True,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\\n\\n        Args:\\n            chunk_size: Maximum size of chunks to return\\n            chunk_overlap: Overlap in characters between chunks\\n            length_function: Function that measures the length of given chunks\\n            keep_separator: Whether to keep the separator in the chunks\\n            add_start_index: If `True`, includes chunk\\'s start index in metadata\\n            strip_whitespace: If `True`, strips whitespace from the start and end of\\n                              every document\\n        \"\"\"\\n        if chunk_overlap > chunk_size:\\n            raise ValueError(\\n                f\"Got a larger chunk overlap ({chunk_overlap}) than chunk size \"\\n                f\"({chunk_size}), should be smaller.\"\\n            )\\n        self._chunk_size = chunk_size\\n        self._chunk_overlap = chunk_overlap\\n        self._length_function = length_function\\n        self._keep_separator = keep_separator\\n        self._add_start_index = add_start_index\\n        self._strip_whitespace = strip_whitespace\\n\\n    @abstractmethod\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split text into multiple components.\"\"\"',\n",
              "  'def _split_text_with_regex(\\n    text: str, separator: str, keep_separator: bool\\n) -> List[str]:\\n    # Now that we have the separator, split the text\\n    if separator:\\n        if keep_separator:\\n            # The parentheses in the pattern keep the delimiters in the result.\\n            _splits = re.split(f\"({separator})\", text)\\n            splits = [_splits[i] + _splits[i + 1] for i in range(1, len(_splits), 2)]\\n            if len(_splits) % 2 == 0:\\n                splits += _splits[-1:]\\n            splits = [_splits[0]] + splits\\n        else:\\n            splits = re.split(separator, text)\\n    else:\\n        splits = list(text)\\n    return [s for s in splits if s != \"\"]',\n",
              "  '\"\"\"**Text Splitters** are classes for splitting text.\\n\\n\\n**Class hierarchy:**\\n\\n.. code-block::\\n\\n    BaseDocumentTransformer --> TextSplitter --> <name>TextSplitter  # Example: CharacterTextSplitter\\n                                                 RecursiveCharacterTextSplitter -->  <name>TextSplitter\\n\\nNote: **MarkdownHeaderTextSplitter** and **HTMLHeaderTextSplitter do not derive from TextSplitter.\\n\\n\\n**Main helpers:**\\n\\n.. code-block::\\n\\n    Document, Tokenizer, Language, LineType, HeaderType\\n\\n\"\"\"  # noqa: E501\\n\\nfrom __future__ import annotations\\n\\nimport asyncio\\nimport copy\\nimport logging\\nimport pathlib\\nimport re\\nfrom abc import ABC, abstractmethod\\nfrom dataclasses import dataclass\\nfrom enum import Enum\\nfrom functools import partial\\nfrom io import BytesIO, StringIO\\nfrom typing import (\\n    AbstractSet,\\n    Any,\\n    Callable,\\n    Collection,\\n    Dict,\\n    Iterable,\\n    List,\\n    Literal,\\n    Optional,\\n    Sequence,\\n    Tuple,\\n    Type,\\n    TypedDict,\\n    TypeVar,\\n    Union,\\n    cast,\\n)\\n\\nimport requests\\nfrom langchain_core.documents import BaseDocumentTransformer, Document\\n\\nlogger = logging.getLogger(__name__)\\n\\nTS = TypeVar(\"TS\", bound=\"TextSplitter\")\\n\\n\\ndef _make_spacy_pipeline_for_splitting(\\n    pipeline: str, *, max_length: int = 1_000_000\\n) -> Any:  # avoid importing spacy\\n    try:\\n        import spacy\\n    except ImportError:\\n        raise ImportError(\\n            \"Spacy is not installed, please install it with `pip install spacy`.\"\\n        )\\n    if pipeline == \"sentencizer\":\\n        from spacy.lang.en import English\\n\\n        sentencizer = English()\\n        sentencizer.add_pipe(\"sentencizer\")\\n    else:\\n        sentencizer = spacy.load(pipeline, exclude=[\"ner\", \"tagger\"])\\n        sentencizer.max_length = max_length\\n    return sentencizer',\n",
              "  'class PythonCodeTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Python syntax.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a PythonCodeTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.PYTHON)\\n        super().__init__(separators=separators, **kwargs)\\n\\n\\nclass MarkdownTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Markdown-formatted headings.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a MarkdownTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.MARKDOWN)\\n        super().__init__(separators=separators, **kwargs)\\n\\n\\nclass LatexTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Latex-formatted layout elements.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a LatexTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.LATEX)\\n        super().__init__(separators=separators, **kwargs)',\n",
              "  'class NLTKTextSplitter(TextSplitter):\\n    \"\"\"Splitting text using NLTK package.\"\"\"\\n\\n    def __init__(\\n        self, separator: str = \"\\\\n\\\\n\", language: str = \"english\", **kwargs: Any\\n    ) -> None:\\n        \"\"\"Initialize the NLTK splitter.\"\"\"\\n        super().__init__(**kwargs)\\n        try:\\n            from nltk.tokenize import sent_tokenize\\n\\n            self._tokenizer = sent_tokenize\\n        except ImportError:\\n            raise ImportError(\\n                \"NLTK is not installed, please install it with `pip install nltk`.\"\\n            )\\n        self._separator = separator\\n        self._language = language\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        # First we naively split the large input into a bunch of smaller ones.\\n        splits = self._tokenizer(text, language=self._language)\\n        return self._merge_splits(splits, self._separator)\\n\\n\\nclass SpacyTextSplitter(TextSplitter):\\n    \"\"\"Splitting text using Spacy package.\\n\\n\\n    Per default, Spacy\\'s `en_core_web_sm` model is used and\\n    its default max_length is 1000000 (it is the length of maximum character\\n    this model takes which can be increased for large files). For a faster, but\\n    potentially less accurate splitting, you can use `pipeline=\\'sentencizer\\'`.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        separator: str = \"\\\\n\\\\n\",\\n        pipeline: str = \"en_core_web_sm\",\\n        max_length: int = 1_000_000,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Initialize the spacy text splitter.\"\"\"\\n        super().__init__(**kwargs)\\n        self._tokenizer = _make_spacy_pipeline_for_splitting(\\n            pipeline, max_length=max_length\\n        )\\n        self._separator = separator\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        splits = (s.text for s in self._tokenizer(text).sents)\\n        return self._merge_splits(splits, self._separator)\\n\\n\\n# For backwards compatibility',\n",
              "  'else:\\n            raise ValueError(\\n                f\"Language {language} is not supported! \"\\n                f\"Please choose from {list(Language)}\"\\n            )',\n",
              "  '@abstractmethod\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split text into multiple components.\"\"\"\\n\\n    def create_documents(\\n        self, texts: List[str], metadatas: Optional[List[dict]] = None\\n    ) -> List[Document]:\\n        \"\"\"Create documents from a list of texts.\"\"\"\\n        _metadatas = metadatas or [{}] * len(texts)\\n        documents = []\\n        for i, text in enumerate(texts):\\n            index = -1\\n            for chunk in self.split_text(text):\\n                metadata = copy.deepcopy(_metadatas[i])\\n                if self._add_start_index:\\n                    index = text.find(chunk, index + 1)\\n                    metadata[\"start_index\"] = index\\n                new_doc = Document(page_content=chunk, metadata=metadata)\\n                documents.append(new_doc)\\n        return documents\\n\\n    def split_documents(self, documents: Iterable[Document]) -> List[Document]:\\n        \"\"\"Split documents.\"\"\"\\n        texts, metadatas = [], []\\n        for doc in documents:\\n            texts.append(doc.page_content)\\n            metadatas.append(doc.metadata)\\n        return self.create_documents(texts, metadatas=metadatas)\\n\\n    def _join_docs(self, docs: List[str], separator: str) -> Optional[str]:\\n        text = separator.join(docs)\\n        if self._strip_whitespace:\\n            text = text.strip()\\n        if text == \"\":\\n            return None\\n        else:\\n            return text\\n\\n    def _merge_splits(self, splits: Iterable[str], separator: str) -> List[str]:\\n        # We now want to combine these smaller pieces into medium size\\n        # chunks to send to the LLM.\\n        separator_len = self._length_function(separator)',\n",
              "  '\"<nav\",\\n                # Head\\n                \"<head\",\\n                \"<style\",\\n                \"<script\",\\n                \"<meta\",\\n                \"<title\",\\n                \"\",\\n            ]\\n        elif language == Language.CSHARP:\\n            return [\\n                \"\\\\ninterface \",\\n                \"\\\\nenum \",\\n                \"\\\\nimplements \",\\n                \"\\\\ndelegate \",\\n                \"\\\\nevent \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nabstract \",\\n                # Split along method definitions\\n                \"\\\\npublic \",\\n                \"\\\\nprotected \",\\n                \"\\\\nprivate \",\\n                \"\\\\nstatic \",\\n                \"\\\\nreturn \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\ncontinue \",\\n                \"\\\\nfor \",\\n                \"\\\\nforeach \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\nbreak \",\\n                \"\\\\ncase \",\\n                \"\\\\nelse \",\\n                # Split by exceptions\\n                \"\\\\ntry \",\\n                \"\\\\nthrow \",\\n                \"\\\\nfinally \",\\n                \"\\\\ncatch \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SOL:\\n            return [\\n                # Split along compiler information definitions\\n                \"\\\\npragma \",\\n                \"\\\\nusing \",\\n                # Split along contract definitions\\n                \"\\\\ncontract \",\\n                \"\\\\ninterface \",\\n                \"\\\\nlibrary \",\\n                # Split along method definitions\\n                \"\\\\nconstructor \",\\n                \"\\\\ntype \",\\n                \"\\\\nfunction \",\\n                \"\\\\nevent \",\\n                \"\\\\nmodifier \",\\n                \"\\\\nerror \",\\n                \"\\\\nstruct \",\\n                \"\\\\nenum \",\\n                # Split along control flow statements',\n",
              "  '\"\\\\n\\\\\\\\*\\\\\\\\*\\\\\\\\*+\\\\n\",\\n                \"\\\\n---+\\\\n\",\\n                \"\\\\n___+\\\\n\",\\n                # Note that this splitter doesn\\'t handle horizontal lines defined\\n                # by *three or more* of ***, ---, or ___, but this is not handled\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.LATEX:\\n            return [\\n                # First, try to split along Latex sections\\n                \"\\\\n\\\\\\\\\\\\\\\\chapter{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\section{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\subsection{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\subsubsection{\",\\n                # Now split by environments\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{enumerate}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{itemize}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{description}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{list}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{quote}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{quotation}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{verse}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{verbatim}\",\\n                # Now split by math environments\\n                \"\\\\n\\\\\\\\\\\\begin{align}\",\\n                \"$$\",\\n                \"$\",\\n                # Now split by the normal type of lines\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.HTML:\\n            return [\\n                # First, try to split along HTML tags\\n                \"<body\",\\n                \"<div\",\\n                \"<p\",\\n                \"<br\",\\n                \"<li\",\\n                \"<h1\",\\n                \"<h2\",\\n                \"<h3\",\\n                \"<h4\",\\n                \"<h5\",\\n                \"<h6\",\\n                \"<span\",\\n                \"<table\",\\n                \"<tr\",\\n                \"<td\",\\n                \"<th\",\\n                \"<ul\",\\n                \"<ol\",\\n                \"<header\",\\n                \"<footer\",\\n                \"<nav\",\\n                # Head\\n                \"<head\",\\n                \"<style\",\\n                \"<script\",\\n                \"<meta\",\\n                \"<title\",',\n",
              "  '\" \",\\n                \"\",\\n            ]\\n        elif language == Language.SCALA:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nobject \",\\n                # Split along method definitions\\n                \"\\\\ndef \",\\n                \"\\\\nval \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nmatch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SWIFT:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunc \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nstruct \",\\n                \"\\\\nenum \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.MARKDOWN:\\n            return [\\n                # First, try to split along Markdown headings (starting with level 2)\\n                \"\\\\n#{1,6} \",\\n                # Note the alternative syntax for headings (below) is not handled here\\n                # Heading level 2\\n                # ---------------\\n                # End of code block\\n                \"```\\\\n\",\\n                # Horizontal lines\\n                \"\\\\n\\\\\\\\*\\\\\\\\*\\\\\\\\*+\\\\n\",\\n                \"\\\\n---+\\\\n\",\\n                \"\\\\n___+\\\\n\",\\n                # Note that this splitter doesn\\'t handle horizontal lines defined',\n",
              "  'return [\\n                # First, try to split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\ndef \",\\n                \"\\\\n\\\\tdef \",\\n                # Now split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.RST:\\n            return [\\n                # Split along section titles\\n                \"\\\\n=+\\\\n\",\\n                \"\\\\n-+\\\\n\",\\n                \"\\\\n\\\\\\\\*+\\\\n\",\\n                # Split along directive markers\\n                \"\\\\n\\\\n.. *\\\\n\\\\n\",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.RUBY:\\n            return [\\n                # Split along method definitions\\n                \"\\\\ndef \",\\n                \"\\\\nclass \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nunless \",\\n                \"\\\\nwhile \",\\n                \"\\\\nfor \",\\n                \"\\\\ndo \",\\n                \"\\\\nbegin \",\\n                \"\\\\nrescue \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.RUST:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfn \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nwhile \",\\n                \"\\\\nfor \",\\n                \"\\\\nloop \",\\n                \"\\\\nmatch \",\\n                \"\\\\nconst \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SCALA:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",',\n",
              "  '\"\\\\nfunction \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                \"\\\\ndefault \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.PHP:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunction \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nforeach \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.PROTO:\\n            return [\\n                # Split along message definitions\\n                \"\\\\nmessage \",\\n                # Split along service definitions\\n                \"\\\\nservice \",\\n                # Split along enum definitions\\n                \"\\\\nenum \",\\n                # Split along option definitions\\n                \"\\\\noption \",\\n                # Split along import statements\\n                \"\\\\nimport \",\\n                # Split along syntax declarations\\n                \"\\\\nsyntax \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.PYTHON:\\n            return [\\n                # First, try to split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\ndef \",\\n                \"\\\\n\\\\tdef \",',\n",
              "  '\"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.KOTLIN:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along method definitions\\n                \"\\\\npublic \",\\n                \"\\\\nprotected \",\\n                \"\\\\nprivate \",\\n                \"\\\\ninternal \",\\n                \"\\\\ncompanion \",\\n                \"\\\\nfun \",\\n                \"\\\\nval \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nwhen \",\\n                \"\\\\ncase \",\\n                \"\\\\nelse \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.JS:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunction \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                \"\\\\nvar \",\\n                \"\\\\nclass \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                \"\\\\ndefault \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.TS:\\n            return [\\n                \"\\\\nenum \",\\n                \"\\\\ninterface \",\\n                \"\\\\nnamespace \",\\n                \"\\\\ntype \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along function definitions\\n                \"\\\\nfunction \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements',\n",
              "  '@staticmethod\\n    def get_separators_for_language(language: Language) -> List[str]:\\n        if language == Language.CPP:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along function definitions\\n                \"\\\\nvoid \",\\n                \"\\\\nint \",\\n                \"\\\\nfloat \",\\n                \"\\\\ndouble \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.GO:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunc \",\\n                \"\\\\nvar \",\\n                \"\\\\nconst \",\\n                \"\\\\ntype \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.JAVA:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along method definitions\\n                \"\\\\npublic \",\\n                \"\\\\nprotected \",\\n                \"\\\\nprivate \",\\n                \"\\\\nstatic \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.KOTLIN:\\n            return [',\n",
              "  '_separator = separator if self._is_separator_regex else re.escape(separator)\\n        splits = _split_text_with_regex(text, _separator, self._keep_separator)\\n\\n        # Now go merging things, recursively splitting longer texts.\\n        _good_splits = []\\n        _separator = \"\" if self._keep_separator else separator\\n        for s in splits:\\n            if self._length_function(s) < self._chunk_size:\\n                _good_splits.append(s)\\n            else:\\n                if _good_splits:\\n                    merged_text = self._merge_splits(_good_splits, _separator)\\n                    final_chunks.extend(merged_text)\\n                    _good_splits = []\\n                if not new_separators:\\n                    final_chunks.append(s)\\n                else:\\n                    other_info = self._split_text(s, new_separators)\\n                    final_chunks.extend(other_info)\\n        if _good_splits:\\n            merged_text = self._merge_splits(_good_splits, _separator)\\n            final_chunks.extend(merged_text)\\n        return final_chunks\\n\\n    def split_text(self, text: str) -> List[str]:\\n        return self._split_text(text, self._separators)\\n\\n    @classmethod\\n    def from_language(\\n        cls, language: Language, **kwargs: Any\\n    ) -> RecursiveCharacterTextSplitter:\\n        separators = cls.get_separators_for_language(language)\\n        return cls(separators=separators, is_separator_regex=True, **kwargs)',\n",
              "  'def split_text(self, text: str) -> List[Document]:\\n        \"\"\"Split markdown file\\n        Args:\\n            text: Markdown file\"\"\"\\n\\n        # Split the input text by newline character (\"\\\\n\").\\n        lines = text.split(\"\\\\n\")\\n        # Final output\\n        lines_with_metadata: List[LineType] = []\\n        # Content and metadata of the chunk currently being processed\\n        current_content: List[str] = []\\n        current_metadata: Dict[str, str] = {}\\n        # Keep track of the nested header structure\\n        # header_stack: List[Dict[str, Union[int, str]]] = []\\n        header_stack: List[HeaderType] = []\\n        initial_metadata: Dict[str, str] = {}\\n\\n        in_code_block = False\\n        opening_fence = \"\"\\n\\n        for line in lines:\\n            stripped_line = line.strip()\\n\\n            if not in_code_block:\\n                # Exclude inline code spans\\n                if stripped_line.startswith(\"```\") and stripped_line.count(\"```\") == 1:\\n                    in_code_block = True\\n                    opening_fence = \"```\"\\n                elif stripped_line.startswith(\"~~~\"):\\n                    in_code_block = True\\n                    opening_fence = \"~~~\"\\n            else:\\n                if stripped_line.startswith(opening_fence):\\n                    in_code_block = False\\n                    opening_fence = \"\"\\n\\n            if in_code_block:\\n                current_content.append(stripped_line)\\n                continue',\n",
              "  'class TokenTextSplitter(TextSplitter):\\n    \"\"\"Splitting text to tokens using model tokenizer.\"\"\"\\n\\n    def __init__(\\n        self,\\n        encoding_name: str = \"gpt2\",\\n        model_name: Optional[str] = None,\\n        allowed_special: Union[Literal[\"all\"], AbstractSet[str]] = set(),\\n        disallowed_special: Union[Literal[\"all\"], Collection[str]] = \"all\",\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(**kwargs)\\n        try:\\n            import tiktoken\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import tiktoken python package. \"\\n                \"This is needed in order to for TokenTextSplitter. \"\\n                \"Please install it with `pip install tiktoken`.\"\\n            )\\n\\n        if model_name is not None:\\n            enc = tiktoken.encoding_for_model(model_name)\\n        else:\\n            enc = tiktoken.get_encoding(encoding_name)\\n        self._tokenizer = enc\\n        self._allowed_special = allowed_special\\n        self._disallowed_special = disallowed_special\\n\\n    def split_text(self, text: str) -> List[str]:\\n        def _encode(_text: str) -> List[int]:\\n            return self._tokenizer.encode(\\n                _text,\\n                allowed_special=self._allowed_special,\\n                disallowed_special=self._disallowed_special,\\n            )\\n\\n        tokenizer = Tokenizer(\\n            chunk_overlap=self._chunk_overlap,\\n            tokens_per_chunk=self._chunk_size,\\n            decode=self._tokenizer.decode,\\n            encode=_encode,\\n        )\\n\\n        return split_text_on_tokens(text=text, tokenizer=tokenizer)',\n",
              "  'class Tokenizer:\\n    \"\"\"Tokenizer data class.\"\"\"\\n\\n    chunk_overlap: int\\n    \"\"\"Overlap in tokens between chunks\"\"\"\\n    tokens_per_chunk: int\\n    \"\"\"Maximum number of tokens per chunk\"\"\"\\n    decode: Callable[[List[int]], str]\\n    \"\"\" Function to decode a list of token ids to a string\"\"\"\\n    encode: Callable[[str], List[int]]\\n    \"\"\" Function to encode a string to a list of token ids\"\"\"\\n\\n\\ndef split_text_on_tokens(*, text: str, tokenizer: Tokenizer) -> List[str]:\\n    \"\"\"Split incoming text and return chunks using tokenizer.\"\"\"\\n    splits: List[str] = []\\n    input_ids = tokenizer.encode(text)\\n    start_idx = 0\\n    cur_idx = min(start_idx + tokenizer.tokens_per_chunk, len(input_ids))\\n    chunk_ids = input_ids[start_idx:cur_idx]\\n    while start_idx < len(input_ids):\\n        splits.append(tokenizer.decode(chunk_ids))\\n        if cur_idx == len(input_ids):\\n            break\\n        start_idx += tokenizer.tokens_per_chunk - tokenizer.chunk_overlap\\n        cur_idx = min(start_idx + tokenizer.tokens_per_chunk, len(input_ids))\\n        chunk_ids = input_ids[start_idx:cur_idx]\\n    return splits',\n",
              "  '# map xhtml namespace prefix\\n        ns_map = {\"h\": \"http://www.w3.org/1999/xhtml\"}\\n\\n        # build list of elements from DOM\\n        elements = []\\n        for element in result_dom.findall(\"*//*\", ns_map):\\n            if element.findall(\"*[@class=\\'headers\\']\") or element.findall(\\n                \"*[@class=\\'chunk\\']\"\\n            ):\\n                elements.append(\\n                    ElementType(\\n                        url=file,\\n                        xpath=\"\".join(\\n                            [\\n                                node.text\\n                                for node in element.findall(\"*[@class=\\'xpath\\']\", ns_map)\\n                            ]\\n                        ),\\n                        content=\"\".join(\\n                            [\\n                                node.text\\n                                for node in element.findall(\"*[@class=\\'chunk\\']\", ns_map)\\n                            ]\\n                        ),\\n                        metadata={\\n                            # Add text of specified headers to metadata using header\\n                            # mapping.\\n                            header_mapping[node.tag]: node.text\\n                            for node in filter(\\n                                lambda x: x.tag in header_filter,\\n                                element.findall(\"*[@class=\\'headers\\']/*\", ns_map),\\n                            )\\n                        },\\n                    )\\n                )\\n\\n        if not self.return_each_element:\\n            return self.aggregate_elements_to_chunks(elements)\\n        else:\\n            return [\\n                Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n                for chunk in elements\\n            ]\\n\\n\\n# should be in newer Python versions (3.10+)\\n# @dataclass(frozen=True, kw_only=True, slots=True)\\n@dataclass(frozen=True)',\n",
              "  'return [\\n            Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n            for chunk in aggregated_chunks\\n        ]\\n\\n    def split_text_from_url(self, url: str) -> List[Document]:\\n        \"\"\"Split HTML from web URL\\n\\n        Args:\\n            url: web URL\\n        \"\"\"\\n        r = requests.get(url)\\n        return self.split_text_from_file(BytesIO(r.content))\\n\\n    def split_text(self, text: str) -> List[Document]:\\n        \"\"\"Split HTML text string\\n\\n        Args:\\n            text: HTML text\\n        \"\"\"\\n        return self.split_text_from_file(StringIO(text))\\n\\n    def split_text_from_file(self, file: Any) -> List[Document]:\\n        \"\"\"Split HTML file\\n\\n        Args:\\n            file: HTML file\\n        \"\"\"\\n        try:\\n            from lxml import etree\\n        except ImportError as e:\\n            raise ImportError(\\n                \"Unable to import lxml, please install with `pip install lxml`.\"\\n            ) from e\\n        # use lxml library to parse html document and return xml ElementTree\\n        parser = etree.HTMLParser()\\n        tree = etree.parse(file, parser)\\n\\n        # document transformation for \"structure-aware\" chunking is handled with xsl.\\n        # see comments in html_chunks_with_headers.xslt for more detailed information.\\n        xslt_path = (\\n            pathlib.Path(__file__).parent\\n            / \"document_transformers/xsl/html_chunks_with_headers.xslt\"\\n        )\\n        xslt_tree = etree.parse(xslt_path)\\n        transform = etree.XSLT(xslt_tree)\\n        result = transform(tree)\\n        result_dom = etree.fromstring(str(result))\\n\\n        # create filter and mapping for header metadata\\n        header_filter = [header[0] for header in self.headers_to_split_on]\\n        header_mapping = dict(self.headers_to_split_on)\\n\\n        # map xhtml namespace prefix\\n        ns_map = {\"h\": \"http://www.w3.org/1999/xhtml\"}',\n",
              "  'class HTMLHeaderTextSplitter:\\n    \"\"\"\\n    Splitting HTML files based on specified headers.\\n    Requires lxml package.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        headers_to_split_on: List[Tuple[str, str]],\\n        return_each_element: bool = False,\\n    ):\\n        \"\"\"Create a new HTMLHeaderTextSplitter.\\n\\n        Args:\\n            headers_to_split_on: list of tuples of headers we want to track mapped to\\n                (arbitrary) keys for metadata. Allowed header values: h1, h2, h3, h4,\\n                h5, h6 e.g. [(\"h1\", \"Header 1\"), (\"h2\", \"Header 2)].\\n            return_each_element: Return each element w/ associated headers.\\n        \"\"\"\\n        # Output element-by-element or aggregated into chunks w/ common headers\\n        self.return_each_element = return_each_element\\n        self.headers_to_split_on = sorted(headers_to_split_on)\\n\\n    def aggregate_elements_to_chunks(\\n        self, elements: List[ElementType]\\n    ) -> List[Document]:\\n        \"\"\"Combine elements with common metadata into chunks\\n\\n        Args:\\n            elements: HTML element content with associated identifying info and metadata\\n        \"\"\"\\n        aggregated_chunks: List[ElementType] = []\\n\\n        for element in elements:\\n            if (\\n                aggregated_chunks\\n                and aggregated_chunks[-1][\"metadata\"] == element[\"metadata\"]\\n            ):\\n                # If the last element in the aggregated list\\n                # has the same metadata as the current element,\\n                # append the current content to the last element\\'s content\\n                aggregated_chunks[-1][\"content\"] += \"  \\\\n\" + element[\"content\"]\\n            else:\\n                # Otherwise, append the current element to the aggregated list\\n                aggregated_chunks.append(element)\\n\\n        return [\\n            Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n            for chunk in aggregated_chunks\\n        ]',\n",
              "  'class ElementType(TypedDict):\\n    \"\"\"Element type as typed dict.\"\"\"\\n\\n    url: str\\n    xpath: str\\n    content: str\\n    metadata: Dict[str, str]',\n",
              "  'if current_content:\\n            lines_with_metadata.append(\\n                {\"content\": \"\\\\n\".join(current_content), \"metadata\": current_metadata}\\n            )\\n\\n        # lines_with_metadata has each line with associated header metadata\\n        # aggregate these into chunks based on common metadata\\n        if not self.return_each_line:\\n            return self.aggregate_lines_to_chunks(lines_with_metadata)\\n        else:\\n            return [\\n                Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n                for chunk in lines_with_metadata\\n            ]',\n",
              "  '# Push the current header to the stack\\n                        header: HeaderType = {\\n                            \"level\": current_header_level,\\n                            \"name\": name,\\n                            \"data\": stripped_line[len(sep) :].strip(),\\n                        }\\n                        header_stack.append(header)\\n                        # Update initial_metadata with the current header\\n                        initial_metadata[name] = header[\"data\"]\\n\\n                    # Add the previous line to the lines_with_metadata\\n                    # only if current_content is not empty\\n                    if current_content:\\n                        lines_with_metadata.append(\\n                            {\\n                                \"content\": \"\\\\n\".join(current_content),\\n                                \"metadata\": current_metadata.copy(),\\n                            }\\n                        )\\n                        current_content.clear()\\n\\n                    break\\n            else:\\n                if stripped_line:\\n                    current_content.append(stripped_line)\\n                elif current_content:\\n                    lines_with_metadata.append(\\n                        {\\n                            \"content\": \"\\\\n\".join(current_content),\\n                            \"metadata\": current_metadata.copy(),\\n                        }\\n                    )\\n                    current_content.clear()\\n\\n            current_metadata = initial_metadata.copy()\\n\\n        if current_content:\\n            lines_with_metadata.append(\\n                {\"content\": \"\\\\n\".join(current_content), \"metadata\": current_metadata}\\n            )',\n",
              "  'if in_code_block:\\n                current_content.append(stripped_line)\\n                continue\\n\\n            # Check each line against each of the header types (e.g., #, ##)\\n            for sep, name in self.headers_to_split_on:\\n                # Check if line starts with a header that we intend to split on\\n                if stripped_line.startswith(sep) and (\\n                    # Header with no text OR header is followed by space\\n                    # Both are valid conditions that sep is being used a header\\n                    len(stripped_line) == len(sep) or stripped_line[len(sep)] == \" \"\\n                ):\\n                    # Ensure we are tracking the header as metadata\\n                    if name is not None:\\n                        # Get the current header level\\n                        current_header_level = sep.count(\"#\")\\n\\n                        # Pop out headers of lower or same level from the stack\\n                        while (\\n                            header_stack\\n                            and header_stack[-1][\"level\"] >= current_header_level\\n                        ):\\n                            # We have encountered a new header\\n                            # at the same or higher level\\n                            popped_header = header_stack.pop()\\n                            # Clear the metadata for the\\n                            # popped header in initial_metadata\\n                            if popped_header[\"name\"] in initial_metadata:\\n                                initial_metadata.pop(popped_header[\"name\"])',\n",
              "  'class SentenceTransformersTokenTextSplitter(TextSplitter):\\n    \"\"\"Splitting text to tokens using sentence model tokenizer.\"\"\"\\n\\n    def __init__(\\n        self,\\n        chunk_overlap: int = 50,\\n        model_name: str = \"sentence-transformers/all-mpnet-base-v2\",\\n        tokens_per_chunk: Optional[int] = None,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(**kwargs, chunk_overlap=chunk_overlap)\\n\\n        try:\\n            from sentence_transformers import SentenceTransformer\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import sentence_transformer python package. \"\\n                \"This is needed in order to for SentenceTransformersTokenTextSplitter. \"\\n                \"Please install it with `pip install sentence-transformers`.\"\\n            )\\n\\n        self.model_name = model_name\\n        self._model = SentenceTransformer(self.model_name)\\n        self.tokenizer = self._model.tokenizer\\n        self._initialize_chunk_configuration(tokens_per_chunk=tokens_per_chunk)\\n\\n    def _initialize_chunk_configuration(\\n        self, *, tokens_per_chunk: Optional[int]\\n    ) -> None:\\n        self.maximum_tokens_per_chunk = cast(int, self._model.max_seq_length)\\n\\n        if tokens_per_chunk is None:\\n            self.tokens_per_chunk = self.maximum_tokens_per_chunk\\n        else:\\n            self.tokens_per_chunk = tokens_per_chunk\\n\\n        if self.tokens_per_chunk > self.maximum_tokens_per_chunk:\\n            raise ValueError(\\n                f\"The token limit of the models \\'{self.model_name}\\'\"\\n                f\" is: {self.maximum_tokens_per_chunk}.\"\\n                f\" Argument tokens_per_chunk={self.tokens_per_chunk}\"\\n                f\" > maximum token limit.\"\\n            )\\n\\n    def split_text(self, text: str) -> List[str]:\\n        def encode_strip_start_and_stop_token_ids(text: str) -> List[int]:\\n            return self._encode(text)[1:-1]',\n",
              "  'class MarkdownHeaderTextSplitter:\\n    \"\"\"Splitting markdown files based on specified headers.\"\"\"\\n\\n    def __init__(\\n        self, headers_to_split_on: List[Tuple[str, str]], return_each_line: bool = False\\n    ):\\n        \"\"\"Create a new MarkdownHeaderTextSplitter.\\n\\n        Args:\\n            headers_to_split_on: Headers we want to track\\n            return_each_line: Return each line w/ associated headers\\n        \"\"\"\\n        # Output line-by-line or aggregated into chunks w/ common headers\\n        self.return_each_line = return_each_line\\n        # Given the headers we want to split on,\\n        # (e.g., \"#, ##, etc\") order by length\\n        self.headers_to_split_on = sorted(\\n            headers_to_split_on, key=lambda split: len(split[0]), reverse=True\\n        )\\n\\n    def aggregate_lines_to_chunks(self, lines: List[LineType]) -> List[Document]:\\n        \"\"\"Combine lines with common metadata into chunks\\n        Args:\\n            lines: Line of text / associated header metadata\\n        \"\"\"\\n        aggregated_chunks: List[LineType] = []\\n\\n        for line in lines:\\n            if (\\n                aggregated_chunks\\n                and aggregated_chunks[-1][\"metadata\"] == line[\"metadata\"]\\n            ):\\n                # If the last line in the aggregated list\\n                # has the same metadata as the current line,\\n                # append the current content to the last lines\\'s content\\n                aggregated_chunks[-1][\"content\"] += \"  \\\\n\" + line[\"content\"]\\n            else:\\n                # Otherwise, append the current line to the aggregated list\\n                aggregated_chunks.append(line)\\n\\n        return [\\n            Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n            for chunk in aggregated_chunks\\n        ]\\n\\n    def split_text(self, text: str) -> List[Document]:\\n        \"\"\"Split markdown file\\n        Args:\\n            text: Markdown file\"\"\"',\n",
              "  'class CharacterTextSplitter(TextSplitter):\\n    \"\"\"Splitting text that looks at characters.\"\"\"\\n\\n    def __init__(\\n        self, separator: str = \"\\\\n\\\\n\", is_separator_regex: bool = False, **kwargs: Any\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(**kwargs)\\n        self._separator = separator\\n        self._is_separator_regex = is_separator_regex\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        # First we naively split the large input into a bunch of smaller ones.\\n        separator = (\\n            self._separator if self._is_separator_regex else re.escape(self._separator)\\n        )\\n        splits = _split_text_with_regex(text, separator, self._keep_separator)\\n        _separator = \"\" if self._keep_separator else self._separator\\n        return self._merge_splits(splits, _separator)\\n\\n\\nclass LineType(TypedDict):\\n    \"\"\"Line type as typed dict.\"\"\"\\n\\n    metadata: Dict[str, str]\\n    content: str\\n\\n\\nclass HeaderType(TypedDict):\\n    \"\"\"Header type as typed dict.\"\"\"\\n\\n    level: int\\n    name: str\\n    data: str',\n",
              "  'if issubclass(cls, TokenTextSplitter):\\n            extra_kwargs = {\\n                \"encoding_name\": encoding_name,\\n                \"model_name\": model_name,\\n                \"allowed_special\": allowed_special,\\n                \"disallowed_special\": disallowed_special,\\n            }\\n            kwargs = {**kwargs, **extra_kwargs}\\n\\n        return cls(length_function=_tiktoken_encoder, **kwargs)\\n\\n    def transform_documents(\\n        self, documents: Sequence[Document], **kwargs: Any\\n    ) -> Sequence[Document]:\\n        \"\"\"Transform sequence of documents by splitting them.\"\"\"\\n        return self.split_documents(list(documents))\\n\\n    async def atransform_documents(\\n        self, documents: Sequence[Document], **kwargs: Any\\n    ) -> Sequence[Document]:\\n        \"\"\"Asynchronously transform a sequence of documents by splitting them.\"\"\"\\n        return await asyncio.get_running_loop().run_in_executor(\\n            None, partial(self.transform_documents, **kwargs), documents\\n        )',\n",
              "  'if not isinstance(tokenizer, PreTrainedTokenizerBase):\\n                raise ValueError(\\n                    \"Tokenizer received was not an instance of PreTrainedTokenizerBase\"\\n                )\\n\\n            def _huggingface_tokenizer_length(text: str) -> int:\\n                return len(tokenizer.encode(text))\\n\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import transformers python package. \"\\n                \"Please install it with `pip install transformers`.\"\\n            )\\n        return cls(length_function=_huggingface_tokenizer_length, **kwargs)\\n\\n    @classmethod\\n    def from_tiktoken_encoder(\\n        cls: Type[TS],\\n        encoding_name: str = \"gpt2\",\\n        model_name: Optional[str] = None,\\n        allowed_special: Union[Literal[\"all\"], AbstractSet[str]] = set(),\\n        disallowed_special: Union[Literal[\"all\"], Collection[str]] = \"all\",\\n        **kwargs: Any,\\n    ) -> TS:\\n        \"\"\"Text splitter that uses tiktoken encoder to count length.\"\"\"\\n        try:\\n            import tiktoken\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import tiktoken python package. \"\\n                \"This is needed in order to calculate max_tokens_for_prompt. \"\\n                \"Please install it with `pip install tiktoken`.\"\\n            )\\n\\n        if model_name is not None:\\n            enc = tiktoken.encoding_for_model(model_name)\\n        else:\\n            enc = tiktoken.get_encoding(encoding_name)\\n\\n        def _tiktoken_encoder(text: str) -> int:\\n            return len(\\n                enc.encode(\\n                    text,\\n                    allowed_special=allowed_special,\\n                    disallowed_special=disallowed_special,\\n                )\\n            )',\n",
              "  'docs = []\\n        current_doc: List[str] = []\\n        total = 0\\n        for d in splits:\\n            _len = self._length_function(d)\\n            if (\\n                total + _len + (separator_len if len(current_doc) > 0 else 0)\\n                > self._chunk_size\\n            ):\\n                if total > self._chunk_size:\\n                    logger.warning(\\n                        f\"Created a chunk of size {total}, \"\\n                        f\"which is longer than the specified {self._chunk_size}\"\\n                    )\\n                if len(current_doc) > 0:\\n                    doc = self._join_docs(current_doc, separator)\\n                    if doc is not None:\\n                        docs.append(doc)\\n                    # Keep on popping if:\\n                    # - we have a larger chunk than in the chunk overlap\\n                    # - or if we still have any chunks and the length is long\\n                    while total > self._chunk_overlap or (\\n                        total + _len + (separator_len if len(current_doc) > 0 else 0)\\n                        > self._chunk_size\\n                        and total > 0\\n                    ):\\n                        total -= self._length_function(current_doc[0]) + (\\n                            separator_len if len(current_doc) > 1 else 0\\n                        )\\n                        current_doc = current_doc[1:]\\n            current_doc.append(d)\\n            total += _len + (separator_len if len(current_doc) > 1 else 0)\\n        doc = self._join_docs(current_doc, separator)\\n        if doc is not None:\\n            docs.append(doc)\\n        return docs\\n\\n    @classmethod\\n    def from_huggingface_tokenizer(cls, tokenizer: Any, **kwargs: Any) -> TextSplitter:\\n        \"\"\"Text splitter that uses HuggingFace tokenizer to count length.\"\"\"\\n        try:\\n            from transformers import PreTrainedTokenizerBase',\n",
              "  '@abstractmethod\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split text into multiple components.\"\"\"\\n\\n    def create_documents(\\n        self, texts: List[str], metadatas: Optional[List[dict]] = None\\n    ) -> List[Document]:\\n        \"\"\"Create documents from a list of texts.\"\"\"\\n        _metadatas = metadatas or [{}] * len(texts)\\n        documents = []\\n        for i, text in enumerate(texts):\\n            index = -1\\n            for chunk in self.split_text(text):\\n                metadata = copy.deepcopy(_metadatas[i])\\n                if self._add_start_index:\\n                    index = text.find(chunk, index + 1)\\n                    metadata[\"start_index\"] = index\\n                new_doc = Document(page_content=chunk, metadata=metadata)\\n                documents.append(new_doc)\\n        return documents\\n\\n    def split_documents(self, documents: Iterable[Document]) -> List[Document]:\\n        \"\"\"Split documents.\"\"\"\\n        texts, metadatas = [], []\\n        for doc in documents:\\n            texts.append(doc.page_content)\\n            metadatas.append(doc.metadata)\\n        return self.create_documents(texts, metadatas=metadatas)\\n\\n    def _join_docs(self, docs: List[str], separator: str) -> Optional[str]:\\n        text = separator.join(docs)\\n        if self._strip_whitespace:\\n            text = text.strip()\\n        if text == \"\":\\n            return None\\n        else:\\n            return text\\n\\n    def _merge_splits(self, splits: Iterable[str], separator: str) -> List[str]:\\n        # We now want to combine these smaller pieces into medium size\\n        # chunks to send to the LLM.\\n        separator_len = self._length_function(separator)',\n",
              "  'class TextSplitter(BaseDocumentTransformer, ABC):\\n    \"\"\"Interface for splitting text into chunks.\"\"\"\\n\\n    def __init__(\\n        self,\\n        chunk_size: int = 4000,\\n        chunk_overlap: int = 200,\\n        length_function: Callable[[str], int] = len,\\n        keep_separator: bool = False,\\n        add_start_index: bool = False,\\n        strip_whitespace: bool = True,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\\n\\n        Args:\\n            chunk_size: Maximum size of chunks to return\\n            chunk_overlap: Overlap in characters between chunks\\n            length_function: Function that measures the length of given chunks\\n            keep_separator: Whether to keep the separator in the chunks\\n            add_start_index: If `True`, includes chunk\\'s start index in metadata\\n            strip_whitespace: If `True`, strips whitespace from the start and end of\\n                              every document\\n        \"\"\"\\n        if chunk_overlap > chunk_size:\\n            raise ValueError(\\n                f\"Got a larger chunk overlap ({chunk_overlap}) than chunk size \"\\n                f\"({chunk_size}), should be smaller.\"\\n            )\\n        self._chunk_size = chunk_size\\n        self._chunk_overlap = chunk_overlap\\n        self._length_function = length_function\\n        self._keep_separator = keep_separator\\n        self._add_start_index = add_start_index\\n        self._strip_whitespace = strip_whitespace\\n\\n    @abstractmethod\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split text into multiple components.\"\"\"',\n",
              "  'def _split_text_with_regex(\\n    text: str, separator: str, keep_separator: bool\\n) -> List[str]:\\n    # Now that we have the separator, split the text\\n    if separator:\\n        if keep_separator:\\n            # The parentheses in the pattern keep the delimiters in the result.\\n            _splits = re.split(f\"({separator})\", text)\\n            splits = [_splits[i] + _splits[i + 1] for i in range(1, len(_splits), 2)]\\n            if len(_splits) % 2 == 0:\\n                splits += _splits[-1:]\\n            splits = [_splits[0]] + splits\\n        else:\\n            splits = re.split(separator, text)\\n    else:\\n        splits = list(text)\\n    return [s for s in splits if s != \"\"]',\n",
              "  '\" \",\\n                \"\",\\n            ]\\n        elif language == Language.SCALA:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nobject \",\\n                # Split along method definitions\\n                \"\\\\ndef \",\\n                \"\\\\nval \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nmatch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SWIFT:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunc \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nstruct \",\\n                \"\\\\nenum \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.MARKDOWN:\\n            return [\\n                # First, try to split along Markdown headings (starting with level 2)\\n                \"\\\\n#{1,6} \",\\n                # Note the alternative syntax for headings (below) is not handled here\\n                # Heading level 2\\n                # ---------------\\n                # End of code block\\n                \"```\\\\n\",\\n                # Horizontal lines\\n                \"\\\\n\\\\\\\\*\\\\\\\\*\\\\\\\\*+\\\\n\",\\n                \"\\\\n---+\\\\n\",\\n                \"\\\\n___+\\\\n\",\\n                # Note that this splitter doesn\\'t handle horizontal lines defined',\n",
              "  'class TextSplitter(BaseDocumentTransformer, ABC):\\n    \"\"\"Interface for splitting text into chunks.\"\"\"\\n\\n    def __init__(\\n        self,\\n        chunk_size: int = 4000,\\n        chunk_overlap: int = 200,\\n        length_function: Callable[[str], int] = len,\\n        keep_separator: bool = False,\\n        add_start_index: bool = False,\\n        strip_whitespace: bool = True,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\\n\\n        Args:\\n            chunk_size: Maximum size of chunks to return\\n            chunk_overlap: Overlap in characters between chunks\\n            length_function: Function that measures the length of given chunks\\n            keep_separator: Whether to keep the separator in the chunks\\n            add_start_index: If `True`, includes chunk\\'s start index in metadata\\n            strip_whitespace: If `True`, strips whitespace from the start and end of\\n                              every document\\n        \"\"\"\\n        if chunk_overlap > chunk_size:\\n            raise ValueError(\\n                f\"Got a larger chunk overlap ({chunk_overlap}) than chunk size \"\\n                f\"({chunk_size}), should be smaller.\"\\n            )\\n        self._chunk_size = chunk_size\\n        self._chunk_overlap = chunk_overlap\\n        self._length_function = length_function\\n        self._keep_separator = keep_separator\\n        self._add_start_index = add_start_index\\n        self._strip_whitespace = strip_whitespace\\n\\n    @abstractmethod\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split text into multiple components.\"\"\"',\n",
              "  'def _split_text_with_regex(\\n    text: str, separator: str, keep_separator: bool\\n) -> List[str]:\\n    # Now that we have the separator, split the text\\n    if separator:\\n        if keep_separator:\\n            # The parentheses in the pattern keep the delimiters in the result.\\n            _splits = re.split(f\"({separator})\", text)\\n            splits = [_splits[i] + _splits[i + 1] for i in range(1, len(_splits), 2)]\\n            if len(_splits) % 2 == 0:\\n                splits += _splits[-1:]\\n            splits = [_splits[0]] + splits\\n        else:\\n            splits = re.split(separator, text)\\n    else:\\n        splits = list(text)\\n    return [s for s in splits if s != \"\"]',\n",
              "  '\"\"\"**Text Splitters** are classes for splitting text.\\n\\n\\n**Class hierarchy:**\\n\\n.. code-block::\\n\\n    BaseDocumentTransformer --> TextSplitter --> <name>TextSplitter  # Example: CharacterTextSplitter\\n                                                 RecursiveCharacterTextSplitter -->  <name>TextSplitter\\n\\nNote: **MarkdownHeaderTextSplitter** and **HTMLHeaderTextSplitter do not derive from TextSplitter.\\n\\n\\n**Main helpers:**\\n\\n.. code-block::\\n\\n    Document, Tokenizer, Language, LineType, HeaderType\\n\\n\"\"\"  # noqa: E501\\n\\nfrom __future__ import annotations\\n\\nimport asyncio\\nimport copy\\nimport logging\\nimport pathlib\\nimport re\\nfrom abc import ABC, abstractmethod\\nfrom dataclasses import dataclass\\nfrom enum import Enum\\nfrom functools import partial\\nfrom io import BytesIO, StringIO\\nfrom typing import (\\n    AbstractSet,\\n    Any,\\n    Callable,\\n    Collection,\\n    Dict,\\n    Iterable,\\n    List,\\n    Literal,\\n    Optional,\\n    Sequence,\\n    Tuple,\\n    Type,\\n    TypedDict,\\n    TypeVar,\\n    Union,\\n    cast,\\n)\\n\\nimport requests\\nfrom langchain_core.documents import BaseDocumentTransformer, Document\\n\\nlogger = logging.getLogger(__name__)\\n\\nTS = TypeVar(\"TS\", bound=\"TextSplitter\")\\n\\n\\ndef _make_spacy_pipeline_for_splitting(\\n    pipeline: str, *, max_length: int = 1_000_000\\n) -> Any:  # avoid importing spacy\\n    try:\\n        import spacy\\n    except ImportError:\\n        raise ImportError(\\n            \"Spacy is not installed, please install it with `pip install spacy`.\"\\n        )\\n    if pipeline == \"sentencizer\":\\n        from spacy.lang.en import English\\n\\n        sentencizer = English()\\n        sentencizer.add_pipe(\"sentencizer\")\\n    else:\\n        sentencizer = spacy.load(pipeline, exclude=[\"ner\", \"tagger\"])\\n        sentencizer.max_length = max_length\\n    return sentencizer',\n",
              "  'class PythonCodeTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Python syntax.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a PythonCodeTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.PYTHON)\\n        super().__init__(separators=separators, **kwargs)\\n\\n\\nclass MarkdownTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Markdown-formatted headings.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a MarkdownTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.MARKDOWN)\\n        super().__init__(separators=separators, **kwargs)\\n\\n\\nclass LatexTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Latex-formatted layout elements.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a LatexTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.LATEX)\\n        super().__init__(separators=separators, **kwargs)',\n",
              "  'class NLTKTextSplitter(TextSplitter):\\n    \"\"\"Splitting text using NLTK package.\"\"\"\\n\\n    def __init__(\\n        self, separator: str = \"\\\\n\\\\n\", language: str = \"english\", **kwargs: Any\\n    ) -> None:\\n        \"\"\"Initialize the NLTK splitter.\"\"\"\\n        super().__init__(**kwargs)\\n        try:\\n            from nltk.tokenize import sent_tokenize\\n\\n            self._tokenizer = sent_tokenize\\n        except ImportError:\\n            raise ImportError(\\n                \"NLTK is not installed, please install it with `pip install nltk`.\"\\n            )\\n        self._separator = separator\\n        self._language = language\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        # First we naively split the large input into a bunch of smaller ones.\\n        splits = self._tokenizer(text, language=self._language)\\n        return self._merge_splits(splits, self._separator)\\n\\n\\nclass SpacyTextSplitter(TextSplitter):\\n    \"\"\"Splitting text using Spacy package.\\n\\n\\n    Per default, Spacy\\'s `en_core_web_sm` model is used and\\n    its default max_length is 1000000 (it is the length of maximum character\\n    this model takes which can be increased for large files). For a faster, but\\n    potentially less accurate splitting, you can use `pipeline=\\'sentencizer\\'`.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        separator: str = \"\\\\n\\\\n\",\\n        pipeline: str = \"en_core_web_sm\",\\n        max_length: int = 1_000_000,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Initialize the spacy text splitter.\"\"\"\\n        super().__init__(**kwargs)\\n        self._tokenizer = _make_spacy_pipeline_for_splitting(\\n            pipeline, max_length=max_length\\n        )\\n        self._separator = separator\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        splits = (s.text for s in self._tokenizer(text).sents)\\n        return self._merge_splits(splits, self._separator)\\n\\n\\n# For backwards compatibility',\n",
              "  'else:\\n            raise ValueError(\\n                f\"Language {language} is not supported! \"\\n                f\"Please choose from {list(Language)}\"\\n            )',\n",
              "  '\"\\\\nevent \",\\n                \"\\\\nmodifier \",\\n                \"\\\\nerror \",\\n                \"\\\\nstruct \",\\n                \"\\\\nenum \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo while \",\\n                \"\\\\nassembly \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.COBOL:\\n            return [\\n                # Split along divisions\\n                \"\\\\nIDENTIFICATION DIVISION.\",\\n                \"\\\\nENVIRONMENT DIVISION.\",\\n                \"\\\\nDATA DIVISION.\",\\n                \"\\\\nPROCEDURE DIVISION.\",\\n                # Split along sections within DATA DIVISION\\n                \"\\\\nWORKING-STORAGE SECTION.\",\\n                \"\\\\nLINKAGE SECTION.\",\\n                \"\\\\nFILE SECTION.\",\\n                # Split along sections within PROCEDURE DIVISION\\n                \"\\\\nINPUT-OUTPUT SECTION.\",\\n                # Split along paragraphs and common statements\\n                \"\\\\nOPEN \",\\n                \"\\\\nCLOSE \",\\n                \"\\\\nREAD \",\\n                \"\\\\nWRITE \",\\n                \"\\\\nIF \",\\n                \"\\\\nELSE \",\\n                \"\\\\nMOVE \",\\n                \"\\\\nPERFORM \",\\n                \"\\\\nUNTIL \",\\n                \"\\\\nVARYING \",\\n                \"\\\\nACCEPT \",\\n                \"\\\\nDISPLAY \",\\n                \"\\\\nSTOP RUN.\",\\n                # Split by the normal type of lines\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]',\n",
              "  '\"<nav\",\\n                # Head\\n                \"<head\",\\n                \"<style\",\\n                \"<script\",\\n                \"<meta\",\\n                \"<title\",\\n                \"\",\\n            ]\\n        elif language == Language.CSHARP:\\n            return [\\n                \"\\\\ninterface \",\\n                \"\\\\nenum \",\\n                \"\\\\nimplements \",\\n                \"\\\\ndelegate \",\\n                \"\\\\nevent \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nabstract \",\\n                # Split along method definitions\\n                \"\\\\npublic \",\\n                \"\\\\nprotected \",\\n                \"\\\\nprivate \",\\n                \"\\\\nstatic \",\\n                \"\\\\nreturn \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\ncontinue \",\\n                \"\\\\nfor \",\\n                \"\\\\nforeach \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\nbreak \",\\n                \"\\\\ncase \",\\n                \"\\\\nelse \",\\n                # Split by exceptions\\n                \"\\\\ntry \",\\n                \"\\\\nthrow \",\\n                \"\\\\nfinally \",\\n                \"\\\\ncatch \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SOL:\\n            return [\\n                # Split along compiler information definitions\\n                \"\\\\npragma \",\\n                \"\\\\nusing \",\\n                # Split along contract definitions\\n                \"\\\\ncontract \",\\n                \"\\\\ninterface \",\\n                \"\\\\nlibrary \",\\n                # Split along method definitions\\n                \"\\\\nconstructor \",\\n                \"\\\\ntype \",\\n                \"\\\\nfunction \",\\n                \"\\\\nevent \",\\n                \"\\\\nmodifier \",\\n                \"\\\\nerror \",\\n                \"\\\\nstruct \",\\n                \"\\\\nenum \",\\n                # Split along control flow statements',\n",
              "  '\"\\\\n\\\\\\\\*\\\\\\\\*\\\\\\\\*+\\\\n\",\\n                \"\\\\n---+\\\\n\",\\n                \"\\\\n___+\\\\n\",\\n                # Note that this splitter doesn\\'t handle horizontal lines defined\\n                # by *three or more* of ***, ---, or ___, but this is not handled\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.LATEX:\\n            return [\\n                # First, try to split along Latex sections\\n                \"\\\\n\\\\\\\\\\\\\\\\chapter{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\section{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\subsection{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\subsubsection{\",\\n                # Now split by environments\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{enumerate}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{itemize}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{description}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{list}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{quote}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{quotation}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{verse}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{verbatim}\",\\n                # Now split by math environments\\n                \"\\\\n\\\\\\\\\\\\begin{align}\",\\n                \"$$\",\\n                \"$\",\\n                # Now split by the normal type of lines\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.HTML:\\n            return [\\n                # First, try to split along HTML tags\\n                \"<body\",\\n                \"<div\",\\n                \"<p\",\\n                \"<br\",\\n                \"<li\",\\n                \"<h1\",\\n                \"<h2\",\\n                \"<h3\",\\n                \"<h4\",\\n                \"<h5\",\\n                \"<h6\",\\n                \"<span\",\\n                \"<table\",\\n                \"<tr\",\\n                \"<td\",\\n                \"<th\",\\n                \"<ul\",\\n                \"<ol\",\\n                \"<header\",\\n                \"<footer\",\\n                \"<nav\",\\n                # Head\\n                \"<head\",\\n                \"<style\",\\n                \"<script\",\\n                \"<meta\",\\n                \"<title\",',\n",
              "  'if issubclass(cls, TokenTextSplitter):\\n            extra_kwargs = {\\n                \"encoding_name\": encoding_name,\\n                \"model_name\": model_name,\\n                \"allowed_special\": allowed_special,\\n                \"disallowed_special\": disallowed_special,\\n            }\\n            kwargs = {**kwargs, **extra_kwargs}\\n\\n        return cls(length_function=_tiktoken_encoder, **kwargs)\\n\\n    def transform_documents(\\n        self, documents: Sequence[Document], **kwargs: Any\\n    ) -> Sequence[Document]:\\n        \"\"\"Transform sequence of documents by splitting them.\"\"\"\\n        return self.split_documents(list(documents))\\n\\n    async def atransform_documents(\\n        self, documents: Sequence[Document], **kwargs: Any\\n    ) -> Sequence[Document]:\\n        \"\"\"Asynchronously transform a sequence of documents by splitting them.\"\"\"\\n        return await asyncio.get_running_loop().run_in_executor(\\n            None, partial(self.transform_documents, **kwargs), documents\\n        )',\n",
              "  'return [\\n                # First, try to split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\ndef \",\\n                \"\\\\n\\\\tdef \",\\n                # Now split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.RST:\\n            return [\\n                # Split along section titles\\n                \"\\\\n=+\\\\n\",\\n                \"\\\\n-+\\\\n\",\\n                \"\\\\n\\\\\\\\*+\\\\n\",\\n                # Split along directive markers\\n                \"\\\\n\\\\n.. *\\\\n\\\\n\",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.RUBY:\\n            return [\\n                # Split along method definitions\\n                \"\\\\ndef \",\\n                \"\\\\nclass \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nunless \",\\n                \"\\\\nwhile \",\\n                \"\\\\nfor \",\\n                \"\\\\ndo \",\\n                \"\\\\nbegin \",\\n                \"\\\\nrescue \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.RUST:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfn \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nwhile \",\\n                \"\\\\nfor \",\\n                \"\\\\nloop \",\\n                \"\\\\nmatch \",\\n                \"\\\\nconst \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SCALA:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",',\n",
              "  '\"\\\\nfunction \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                \"\\\\ndefault \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.PHP:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunction \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nforeach \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.PROTO:\\n            return [\\n                # Split along message definitions\\n                \"\\\\nmessage \",\\n                # Split along service definitions\\n                \"\\\\nservice \",\\n                # Split along enum definitions\\n                \"\\\\nenum \",\\n                # Split along option definitions\\n                \"\\\\noption \",\\n                # Split along import statements\\n                \"\\\\nimport \",\\n                # Split along syntax declarations\\n                \"\\\\nsyntax \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.PYTHON:\\n            return [\\n                # First, try to split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\ndef \",\\n                \"\\\\n\\\\tdef \",',\n",
              "  '\"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.KOTLIN:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along method definitions\\n                \"\\\\npublic \",\\n                \"\\\\nprotected \",\\n                \"\\\\nprivate \",\\n                \"\\\\ninternal \",\\n                \"\\\\ncompanion \",\\n                \"\\\\nfun \",\\n                \"\\\\nval \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nwhen \",\\n                \"\\\\ncase \",\\n                \"\\\\nelse \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.JS:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunction \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                \"\\\\nvar \",\\n                \"\\\\nclass \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                \"\\\\ndefault \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.TS:\\n            return [\\n                \"\\\\nenum \",\\n                \"\\\\ninterface \",\\n                \"\\\\nnamespace \",\\n                \"\\\\ntype \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along function definitions\\n                \"\\\\nfunction \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements',\n",
              "  '@staticmethod\\n    def get_separators_for_language(language: Language) -> List[str]:\\n        if language == Language.CPP:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along function definitions\\n                \"\\\\nvoid \",\\n                \"\\\\nint \",\\n                \"\\\\nfloat \",\\n                \"\\\\ndouble \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.GO:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunc \",\\n                \"\\\\nvar \",\\n                \"\\\\nconst \",\\n                \"\\\\ntype \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.JAVA:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along method definitions\\n                \"\\\\npublic \",\\n                \"\\\\nprotected \",\\n                \"\\\\nprivate \",\\n                \"\\\\nstatic \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.KOTLIN:\\n            return [',\n",
              "  '_separator = separator if self._is_separator_regex else re.escape(separator)\\n        splits = _split_text_with_regex(text, _separator, self._keep_separator)\\n\\n        # Now go merging things, recursively splitting longer texts.\\n        _good_splits = []\\n        _separator = \"\" if self._keep_separator else separator\\n        for s in splits:\\n            if self._length_function(s) < self._chunk_size:\\n                _good_splits.append(s)\\n            else:\\n                if _good_splits:\\n                    merged_text = self._merge_splits(_good_splits, _separator)\\n                    final_chunks.extend(merged_text)\\n                    _good_splits = []\\n                if not new_separators:\\n                    final_chunks.append(s)\\n                else:\\n                    other_info = self._split_text(s, new_separators)\\n                    final_chunks.extend(other_info)\\n        if _good_splits:\\n            merged_text = self._merge_splits(_good_splits, _separator)\\n            final_chunks.extend(merged_text)\\n        return final_chunks\\n\\n    def split_text(self, text: str) -> List[str]:\\n        return self._split_text(text, self._separators)\\n\\n    @classmethod\\n    def from_language(\\n        cls, language: Language, **kwargs: Any\\n    ) -> RecursiveCharacterTextSplitter:\\n        separators = cls.get_separators_for_language(language)\\n        return cls(separators=separators, is_separator_regex=True, **kwargs)',\n",
              "  'class RecursiveCharacterTextSplitter(TextSplitter):\\n    \"\"\"Splitting text by recursively look at characters.\\n\\n    Recursively tries to split by different characters to find one\\n    that works.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        separators: Optional[List[str]] = None,\\n        keep_separator: bool = True,\\n        is_separator_regex: bool = False,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(keep_separator=keep_separator, **kwargs)\\n        self._separators = separators or [\"\\\\n\\\\n\", \"\\\\n\", \" \", \"\"]\\n        self._is_separator_regex = is_separator_regex\\n\\n    def _split_text(self, text: str, separators: List[str]) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        final_chunks = []\\n        # Get appropriate separator to use\\n        separator = separators[-1]\\n        new_separators = []\\n        for i, _s in enumerate(separators):\\n            _separator = _s if self._is_separator_regex else re.escape(_s)\\n            if _s == \"\":\\n                separator = _s\\n                break\\n            if re.search(_separator, text):\\n                separator = _s\\n                new_separators = separators[i + 1 :]\\n                break\\n\\n        _separator = separator if self._is_separator_regex else re.escape(separator)\\n        splits = _split_text_with_regex(text, _separator, self._keep_separator)',\n",
              "  'class Language(str, Enum):\\n    \"\"\"Enum of the programming languages.\"\"\"\\n\\n    CPP = \"cpp\"\\n    GO = \"go\"\\n    JAVA = \"java\"\\n    KOTLIN = \"kotlin\"\\n    JS = \"js\"\\n    TS = \"ts\"\\n    PHP = \"php\"\\n    PROTO = \"proto\"\\n    PYTHON = \"python\"\\n    RST = \"rst\"\\n    RUBY = \"ruby\"\\n    RUST = \"rust\"\\n    SCALA = \"scala\"\\n    SWIFT = \"swift\"\\n    MARKDOWN = \"markdown\"\\n    LATEX = \"latex\"\\n    HTML = \"html\"\\n    SOL = \"sol\"\\n    CSHARP = \"csharp\"\\n    COBOL = \"cobol\"',\n",
              "  'def split_text(self, text: str) -> List[str]:\\n        def encode_strip_start_and_stop_token_ids(text: str) -> List[int]:\\n            return self._encode(text)[1:-1]\\n\\n        tokenizer = Tokenizer(\\n            chunk_overlap=self._chunk_overlap,\\n            tokens_per_chunk=self.tokens_per_chunk,\\n            decode=self.tokenizer.decode,\\n            encode=encode_strip_start_and_stop_token_ids,\\n        )\\n\\n        return split_text_on_tokens(text=text, tokenizer=tokenizer)\\n\\n    def count_tokens(self, *, text: str) -> int:\\n        return len(self._encode(text))\\n\\n    _max_length_equal_32_bit_integer: int = 2**32\\n\\n    def _encode(self, text: str) -> List[int]:\\n        token_ids_with_start_and_end_token_ids = self.tokenizer.encode(\\n            text,\\n            max_length=self._max_length_equal_32_bit_integer,\\n            truncation=\"do_not_truncate\",\\n        )\\n        return token_ids_with_start_and_end_token_ids',\n",
              "  'class Tokenizer:\\n    \"\"\"Tokenizer data class.\"\"\"\\n\\n    chunk_overlap: int\\n    \"\"\"Overlap in tokens between chunks\"\"\"\\n    tokens_per_chunk: int\\n    \"\"\"Maximum number of tokens per chunk\"\"\"\\n    decode: Callable[[List[int]], str]\\n    \"\"\" Function to decode a list of token ids to a string\"\"\"\\n    encode: Callable[[str], List[int]]\\n    \"\"\" Function to encode a string to a list of token ids\"\"\"\\n\\n\\ndef split_text_on_tokens(*, text: str, tokenizer: Tokenizer) -> List[str]:\\n    \"\"\"Split incoming text and return chunks using tokenizer.\"\"\"\\n    splits: List[str] = []\\n    input_ids = tokenizer.encode(text)\\n    start_idx = 0\\n    cur_idx = min(start_idx + tokenizer.tokens_per_chunk, len(input_ids))\\n    chunk_ids = input_ids[start_idx:cur_idx]\\n    while start_idx < len(input_ids):\\n        splits.append(tokenizer.decode(chunk_ids))\\n        if cur_idx == len(input_ids):\\n            break\\n        start_idx += tokenizer.tokens_per_chunk - tokenizer.chunk_overlap\\n        cur_idx = min(start_idx + tokenizer.tokens_per_chunk, len(input_ids))\\n        chunk_ids = input_ids[start_idx:cur_idx]\\n    return splits',\n",
              "  '\"\\\\nfunction \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                \"\\\\ndefault \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.PHP:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunction \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nforeach \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.PROTO:\\n            return [\\n                # Split along message definitions\\n                \"\\\\nmessage \",\\n                # Split along service definitions\\n                \"\\\\nservice \",\\n                # Split along enum definitions\\n                \"\\\\nenum \",\\n                # Split along option definitions\\n                \"\\\\noption \",\\n                # Split along import statements\\n                \"\\\\nimport \",\\n                # Split along syntax declarations\\n                \"\\\\nsyntax \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.PYTHON:\\n            return [\\n                # First, try to split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\ndef \",\\n                \"\\\\n\\\\tdef \",',\n",
              "  '\"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.KOTLIN:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along method definitions\\n                \"\\\\npublic \",\\n                \"\\\\nprotected \",\\n                \"\\\\nprivate \",\\n                \"\\\\ninternal \",\\n                \"\\\\ncompanion \",\\n                \"\\\\nfun \",\\n                \"\\\\nval \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nwhen \",\\n                \"\\\\ncase \",\\n                \"\\\\nelse \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.JS:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunction \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                \"\\\\nvar \",\\n                \"\\\\nclass \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                \"\\\\ndefault \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.TS:\\n            return [\\n                \"\\\\nenum \",\\n                \"\\\\ninterface \",\\n                \"\\\\nnamespace \",\\n                \"\\\\ntype \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along function definitions\\n                \"\\\\nfunction \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements',\n",
              "  '@staticmethod\\n    def get_separators_for_language(language: Language) -> List[str]:\\n        if language == Language.CPP:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along function definitions\\n                \"\\\\nvoid \",\\n                \"\\\\nint \",\\n                \"\\\\nfloat \",\\n                \"\\\\ndouble \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.GO:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunc \",\\n                \"\\\\nvar \",\\n                \"\\\\nconst \",\\n                \"\\\\ntype \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.JAVA:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along method definitions\\n                \"\\\\npublic \",\\n                \"\\\\nprotected \",\\n                \"\\\\nprivate \",\\n                \"\\\\nstatic \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.KOTLIN:\\n            return [',\n",
              "  '_separator = separator if self._is_separator_regex else re.escape(separator)\\n        splits = _split_text_with_regex(text, _separator, self._keep_separator)\\n\\n        # Now go merging things, recursively splitting longer texts.\\n        _good_splits = []\\n        _separator = \"\" if self._keep_separator else separator\\n        for s in splits:\\n            if self._length_function(s) < self._chunk_size:\\n                _good_splits.append(s)\\n            else:\\n                if _good_splits:\\n                    merged_text = self._merge_splits(_good_splits, _separator)\\n                    final_chunks.extend(merged_text)\\n                    _good_splits = []\\n                if not new_separators:\\n                    final_chunks.append(s)\\n                else:\\n                    other_info = self._split_text(s, new_separators)\\n                    final_chunks.extend(other_info)\\n        if _good_splits:\\n            merged_text = self._merge_splits(_good_splits, _separator)\\n            final_chunks.extend(merged_text)\\n        return final_chunks\\n\\n    def split_text(self, text: str) -> List[str]:\\n        return self._split_text(text, self._separators)\\n\\n    @classmethod\\n    def from_language(\\n        cls, language: Language, **kwargs: Any\\n    ) -> RecursiveCharacterTextSplitter:\\n        separators = cls.get_separators_for_language(language)\\n        return cls(separators=separators, is_separator_regex=True, **kwargs)',\n",
              "  'class RecursiveCharacterTextSplitter(TextSplitter):\\n    \"\"\"Splitting text by recursively look at characters.\\n\\n    Recursively tries to split by different characters to find one\\n    that works.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        separators: Optional[List[str]] = None,\\n        keep_separator: bool = True,\\n        is_separator_regex: bool = False,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(keep_separator=keep_separator, **kwargs)\\n        self._separators = separators or [\"\\\\n\\\\n\", \"\\\\n\", \" \", \"\"]\\n        self._is_separator_regex = is_separator_regex\\n\\n    def _split_text(self, text: str, separators: List[str]) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        final_chunks = []\\n        # Get appropriate separator to use\\n        separator = separators[-1]\\n        new_separators = []\\n        for i, _s in enumerate(separators):\\n            _separator = _s if self._is_separator_regex else re.escape(_s)\\n            if _s == \"\":\\n                separator = _s\\n                break\\n            if re.search(_separator, text):\\n                separator = _s\\n                new_separators = separators[i + 1 :]\\n                break\\n\\n        _separator = separator if self._is_separator_regex else re.escape(separator)\\n        splits = _split_text_with_regex(text, _separator, self._keep_separator)',\n",
              "  'class Language(str, Enum):\\n    \"\"\"Enum of the programming languages.\"\"\"\\n\\n    CPP = \"cpp\"\\n    GO = \"go\"\\n    JAVA = \"java\"\\n    KOTLIN = \"kotlin\"\\n    JS = \"js\"\\n    TS = \"ts\"\\n    PHP = \"php\"\\n    PROTO = \"proto\"\\n    PYTHON = \"python\"\\n    RST = \"rst\"\\n    RUBY = \"ruby\"\\n    RUST = \"rust\"\\n    SCALA = \"scala\"\\n    SWIFT = \"swift\"\\n    MARKDOWN = \"markdown\"\\n    LATEX = \"latex\"\\n    HTML = \"html\"\\n    SOL = \"sol\"\\n    CSHARP = \"csharp\"\\n    COBOL = \"cobol\"',\n",
              "  'def split_text(self, text: str) -> List[str]:\\n        def encode_strip_start_and_stop_token_ids(text: str) -> List[int]:\\n            return self._encode(text)[1:-1]\\n\\n        tokenizer = Tokenizer(\\n            chunk_overlap=self._chunk_overlap,\\n            tokens_per_chunk=self.tokens_per_chunk,\\n            decode=self.tokenizer.decode,\\n            encode=encode_strip_start_and_stop_token_ids,\\n        )\\n\\n        return split_text_on_tokens(text=text, tokenizer=tokenizer)\\n\\n    def count_tokens(self, *, text: str) -> int:\\n        return len(self._encode(text))\\n\\n    _max_length_equal_32_bit_integer: int = 2**32\\n\\n    def _encode(self, text: str) -> List[int]:\\n        token_ids_with_start_and_end_token_ids = self.tokenizer.encode(\\n            text,\\n            max_length=self._max_length_equal_32_bit_integer,\\n            truncation=\"do_not_truncate\",\\n        )\\n        return token_ids_with_start_and_end_token_ids',\n",
              "  'class SentenceTransformersTokenTextSplitter(TextSplitter):\\n    \"\"\"Splitting text to tokens using sentence model tokenizer.\"\"\"\\n\\n    def __init__(\\n        self,\\n        chunk_overlap: int = 50,\\n        model_name: str = \"sentence-transformers/all-mpnet-base-v2\",\\n        tokens_per_chunk: Optional[int] = None,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(**kwargs, chunk_overlap=chunk_overlap)\\n\\n        try:\\n            from sentence_transformers import SentenceTransformer\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import sentence_transformer python package. \"\\n                \"This is needed in order to for SentenceTransformersTokenTextSplitter. \"\\n                \"Please install it with `pip install sentence-transformers`.\"\\n            )\\n\\n        self.model_name = model_name\\n        self._model = SentenceTransformer(self.model_name)\\n        self.tokenizer = self._model.tokenizer\\n        self._initialize_chunk_configuration(tokens_per_chunk=tokens_per_chunk)\\n\\n    def _initialize_chunk_configuration(\\n        self, *, tokens_per_chunk: Optional[int]\\n    ) -> None:\\n        self.maximum_tokens_per_chunk = cast(int, self._model.max_seq_length)\\n\\n        if tokens_per_chunk is None:\\n            self.tokens_per_chunk = self.maximum_tokens_per_chunk\\n        else:\\n            self.tokens_per_chunk = tokens_per_chunk\\n\\n        if self.tokens_per_chunk > self.maximum_tokens_per_chunk:\\n            raise ValueError(\\n                f\"The token limit of the models \\'{self.model_name}\\'\"\\n                f\" is: {self.maximum_tokens_per_chunk}.\"\\n                f\" Argument tokens_per_chunk={self.tokens_per_chunk}\"\\n                f\" > maximum token limit.\"\\n            )\\n\\n    def split_text(self, text: str) -> List[str]:\\n        def encode_strip_start_and_stop_token_ids(text: str) -> List[int]:\\n            return self._encode(text)[1:-1]',\n",
              "  'class TokenTextSplitter(TextSplitter):\\n    \"\"\"Splitting text to tokens using model tokenizer.\"\"\"\\n\\n    def __init__(\\n        self,\\n        encoding_name: str = \"gpt2\",\\n        model_name: Optional[str] = None,\\n        allowed_special: Union[Literal[\"all\"], AbstractSet[str]] = set(),\\n        disallowed_special: Union[Literal[\"all\"], Collection[str]] = \"all\",\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(**kwargs)\\n        try:\\n            import tiktoken\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import tiktoken python package. \"\\n                \"This is needed in order to for TokenTextSplitter. \"\\n                \"Please install it with `pip install tiktoken`.\"\\n            )\\n\\n        if model_name is not None:\\n            enc = tiktoken.encoding_for_model(model_name)\\n        else:\\n            enc = tiktoken.get_encoding(encoding_name)\\n        self._tokenizer = enc\\n        self._allowed_special = allowed_special\\n        self._disallowed_special = disallowed_special\\n\\n    def split_text(self, text: str) -> List[str]:\\n        def _encode(_text: str) -> List[int]:\\n            return self._tokenizer.encode(\\n                _text,\\n                allowed_special=self._allowed_special,\\n                disallowed_special=self._disallowed_special,\\n            )\\n\\n        tokenizer = Tokenizer(\\n            chunk_overlap=self._chunk_overlap,\\n            tokens_per_chunk=self._chunk_size,\\n            decode=self._tokenizer.decode,\\n            encode=_encode,\\n        )\\n\\n        return split_text_on_tokens(text=text, tokenizer=tokenizer)',\n",
              "  'return [\\n                # First, try to split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\ndef \",\\n                \"\\\\n\\\\tdef \",\\n                # Now split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.RST:\\n            return [\\n                # Split along section titles\\n                \"\\\\n=+\\\\n\",\\n                \"\\\\n-+\\\\n\",\\n                \"\\\\n\\\\\\\\*+\\\\n\",\\n                # Split along directive markers\\n                \"\\\\n\\\\n.. *\\\\n\\\\n\",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.RUBY:\\n            return [\\n                # Split along method definitions\\n                \"\\\\ndef \",\\n                \"\\\\nclass \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nunless \",\\n                \"\\\\nwhile \",\\n                \"\\\\nfor \",\\n                \"\\\\ndo \",\\n                \"\\\\nbegin \",\\n                \"\\\\nrescue \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.RUST:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfn \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nwhile \",\\n                \"\\\\nfor \",\\n                \"\\\\nloop \",\\n                \"\\\\nmatch \",\\n                \"\\\\nconst \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SCALA:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",',\n",
              "  '# map xhtml namespace prefix\\n        ns_map = {\"h\": \"http://www.w3.org/1999/xhtml\"}\\n\\n        # build list of elements from DOM\\n        elements = []\\n        for element in result_dom.findall(\"*//*\", ns_map):\\n            if element.findall(\"*[@class=\\'headers\\']\") or element.findall(\\n                \"*[@class=\\'chunk\\']\"\\n            ):\\n                elements.append(\\n                    ElementType(\\n                        url=file,\\n                        xpath=\"\".join(\\n                            [\\n                                node.text\\n                                for node in element.findall(\"*[@class=\\'xpath\\']\", ns_map)\\n                            ]\\n                        ),\\n                        content=\"\".join(\\n                            [\\n                                node.text\\n                                for node in element.findall(\"*[@class=\\'chunk\\']\", ns_map)\\n                            ]\\n                        ),\\n                        metadata={\\n                            # Add text of specified headers to metadata using header\\n                            # mapping.\\n                            header_mapping[node.tag]: node.text\\n                            for node in filter(\\n                                lambda x: x.tag in header_filter,\\n                                element.findall(\"*[@class=\\'headers\\']/*\", ns_map),\\n                            )\\n                        },\\n                    )\\n                )\\n\\n        if not self.return_each_element:\\n            return self.aggregate_elements_to_chunks(elements)\\n        else:\\n            return [\\n                Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n                for chunk in elements\\n            ]\\n\\n\\n# should be in newer Python versions (3.10+)\\n# @dataclass(frozen=True, kw_only=True, slots=True)\\n@dataclass(frozen=True)',\n",
              "  'return [\\n            Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n            for chunk in aggregated_chunks\\n        ]\\n\\n    def split_text_from_url(self, url: str) -> List[Document]:\\n        \"\"\"Split HTML from web URL\\n\\n        Args:\\n            url: web URL\\n        \"\"\"\\n        r = requests.get(url)\\n        return self.split_text_from_file(BytesIO(r.content))\\n\\n    def split_text(self, text: str) -> List[Document]:\\n        \"\"\"Split HTML text string\\n\\n        Args:\\n            text: HTML text\\n        \"\"\"\\n        return self.split_text_from_file(StringIO(text))\\n\\n    def split_text_from_file(self, file: Any) -> List[Document]:\\n        \"\"\"Split HTML file\\n\\n        Args:\\n            file: HTML file\\n        \"\"\"\\n        try:\\n            from lxml import etree\\n        except ImportError as e:\\n            raise ImportError(\\n                \"Unable to import lxml, please install with `pip install lxml`.\"\\n            ) from e\\n        # use lxml library to parse html document and return xml ElementTree\\n        parser = etree.HTMLParser()\\n        tree = etree.parse(file, parser)\\n\\n        # document transformation for \"structure-aware\" chunking is handled with xsl.\\n        # see comments in html_chunks_with_headers.xslt for more detailed information.\\n        xslt_path = (\\n            pathlib.Path(__file__).parent\\n            / \"document_transformers/xsl/html_chunks_with_headers.xslt\"\\n        )\\n        xslt_tree = etree.parse(xslt_path)\\n        transform = etree.XSLT(xslt_tree)\\n        result = transform(tree)\\n        result_dom = etree.fromstring(str(result))\\n\\n        # create filter and mapping for header metadata\\n        header_filter = [header[0] for header in self.headers_to_split_on]\\n        header_mapping = dict(self.headers_to_split_on)\\n\\n        # map xhtml namespace prefix\\n        ns_map = {\"h\": \"http://www.w3.org/1999/xhtml\"}',\n",
              "  'class HTMLHeaderTextSplitter:\\n    \"\"\"\\n    Splitting HTML files based on specified headers.\\n    Requires lxml package.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        headers_to_split_on: List[Tuple[str, str]],\\n        return_each_element: bool = False,\\n    ):\\n        \"\"\"Create a new HTMLHeaderTextSplitter.\\n\\n        Args:\\n            headers_to_split_on: list of tuples of headers we want to track mapped to\\n                (arbitrary) keys for metadata. Allowed header values: h1, h2, h3, h4,\\n                h5, h6 e.g. [(\"h1\", \"Header 1\"), (\"h2\", \"Header 2)].\\n            return_each_element: Return each element w/ associated headers.\\n        \"\"\"\\n        # Output element-by-element or aggregated into chunks w/ common headers\\n        self.return_each_element = return_each_element\\n        self.headers_to_split_on = sorted(headers_to_split_on)\\n\\n    def aggregate_elements_to_chunks(\\n        self, elements: List[ElementType]\\n    ) -> List[Document]:\\n        \"\"\"Combine elements with common metadata into chunks\\n\\n        Args:\\n            elements: HTML element content with associated identifying info and metadata\\n        \"\"\"\\n        aggregated_chunks: List[ElementType] = []\\n\\n        for element in elements:\\n            if (\\n                aggregated_chunks\\n                and aggregated_chunks[-1][\"metadata\"] == element[\"metadata\"]\\n            ):\\n                # If the last element in the aggregated list\\n                # has the same metadata as the current element,\\n                # append the current content to the last element\\'s content\\n                aggregated_chunks[-1][\"content\"] += \"  \\\\n\" + element[\"content\"]\\n            else:\\n                # Otherwise, append the current element to the aggregated list\\n                aggregated_chunks.append(element)\\n\\n        return [\\n            Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n            for chunk in aggregated_chunks\\n        ]',\n",
              "  'class ElementType(TypedDict):\\n    \"\"\"Element type as typed dict.\"\"\"\\n\\n    url: str\\n    xpath: str\\n    content: str\\n    metadata: Dict[str, str]',\n",
              "  'if current_content:\\n            lines_with_metadata.append(\\n                {\"content\": \"\\\\n\".join(current_content), \"metadata\": current_metadata}\\n            )\\n\\n        # lines_with_metadata has each line with associated header metadata\\n        # aggregate these into chunks based on common metadata\\n        if not self.return_each_line:\\n            return self.aggregate_lines_to_chunks(lines_with_metadata)\\n        else:\\n            return [\\n                Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n                for chunk in lines_with_metadata\\n            ]',\n",
              "  '# Push the current header to the stack\\n                        header: HeaderType = {\\n                            \"level\": current_header_level,\\n                            \"name\": name,\\n                            \"data\": stripped_line[len(sep) :].strip(),\\n                        }\\n                        header_stack.append(header)\\n                        # Update initial_metadata with the current header\\n                        initial_metadata[name] = header[\"data\"]\\n\\n                    # Add the previous line to the lines_with_metadata\\n                    # only if current_content is not empty\\n                    if current_content:\\n                        lines_with_metadata.append(\\n                            {\\n                                \"content\": \"\\\\n\".join(current_content),\\n                                \"metadata\": current_metadata.copy(),\\n                            }\\n                        )\\n                        current_content.clear()\\n\\n                    break\\n            else:\\n                if stripped_line:\\n                    current_content.append(stripped_line)\\n                elif current_content:\\n                    lines_with_metadata.append(\\n                        {\\n                            \"content\": \"\\\\n\".join(current_content),\\n                            \"metadata\": current_metadata.copy(),\\n                        }\\n                    )\\n                    current_content.clear()\\n\\n            current_metadata = initial_metadata.copy()\\n\\n        if current_content:\\n            lines_with_metadata.append(\\n                {\"content\": \"\\\\n\".join(current_content), \"metadata\": current_metadata}\\n            )',\n",
              "  'if in_code_block:\\n                current_content.append(stripped_line)\\n                continue\\n\\n            # Check each line against each of the header types (e.g., #, ##)\\n            for sep, name in self.headers_to_split_on:\\n                # Check if line starts with a header that we intend to split on\\n                if stripped_line.startswith(sep) and (\\n                    # Header with no text OR header is followed by space\\n                    # Both are valid conditions that sep is being used a header\\n                    len(stripped_line) == len(sep) or stripped_line[len(sep)] == \" \"\\n                ):\\n                    # Ensure we are tracking the header as metadata\\n                    if name is not None:\\n                        # Get the current header level\\n                        current_header_level = sep.count(\"#\")\\n\\n                        # Pop out headers of lower or same level from the stack\\n                        while (\\n                            header_stack\\n                            and header_stack[-1][\"level\"] >= current_header_level\\n                        ):\\n                            # We have encountered a new header\\n                            # at the same or higher level\\n                            popped_header = header_stack.pop()\\n                            # Clear the metadata for the\\n                            # popped header in initial_metadata\\n                            if popped_header[\"name\"] in initial_metadata:\\n                                initial_metadata.pop(popped_header[\"name\"])',\n",
              "  'def split_text(self, text: str) -> List[Document]:\\n        \"\"\"Split markdown file\\n        Args:\\n            text: Markdown file\"\"\"\\n\\n        # Split the input text by newline character (\"\\\\n\").\\n        lines = text.split(\"\\\\n\")\\n        # Final output\\n        lines_with_metadata: List[LineType] = []\\n        # Content and metadata of the chunk currently being processed\\n        current_content: List[str] = []\\n        current_metadata: Dict[str, str] = {}\\n        # Keep track of the nested header structure\\n        # header_stack: List[Dict[str, Union[int, str]]] = []\\n        header_stack: List[HeaderType] = []\\n        initial_metadata: Dict[str, str] = {}\\n\\n        in_code_block = False\\n        opening_fence = \"\"\\n\\n        for line in lines:\\n            stripped_line = line.strip()\\n\\n            if not in_code_block:\\n                # Exclude inline code spans\\n                if stripped_line.startswith(\"```\") and stripped_line.count(\"```\") == 1:\\n                    in_code_block = True\\n                    opening_fence = \"```\"\\n                elif stripped_line.startswith(\"~~~\"):\\n                    in_code_block = True\\n                    opening_fence = \"~~~\"\\n            else:\\n                if stripped_line.startswith(opening_fence):\\n                    in_code_block = False\\n                    opening_fence = \"\"\\n\\n            if in_code_block:\\n                current_content.append(stripped_line)\\n                continue',\n",
              "  'def _split_text_with_regex(\\n    text: str, separator: str, keep_separator: bool\\n) -> List[str]:\\n    # Now that we have the separator, split the text\\n    if separator:\\n        if keep_separator:\\n            # The parentheses in the pattern keep the delimiters in the result.\\n            _splits = re.split(f\"({separator})\", text)\\n            splits = [_splits[i] + _splits[i + 1] for i in range(1, len(_splits), 2)]\\n            if len(_splits) % 2 == 0:\\n                splits += _splits[-1:]\\n            splits = [_splits[0]] + splits\\n        else:\\n            splits = re.split(separator, text)\\n    else:\\n        splits = list(text)\\n    return [s for s in splits if s != \"\"]',\n",
              "  'if in_code_block:\\n                current_content.append(stripped_line)\\n                continue\\n\\n            # Check each line against each of the header types (e.g., #, ##)\\n            for sep, name in self.headers_to_split_on:\\n                # Check if line starts with a header that we intend to split on\\n                if stripped_line.startswith(sep) and (\\n                    # Header with no text OR header is followed by space\\n                    # Both are valid conditions that sep is being used a header\\n                    len(stripped_line) == len(sep) or stripped_line[len(sep)] == \" \"\\n                ):\\n                    # Ensure we are tracking the header as metadata\\n                    if name is not None:\\n                        # Get the current header level\\n                        current_header_level = sep.count(\"#\")\\n\\n                        # Pop out headers of lower or same level from the stack\\n                        while (\\n                            header_stack\\n                            and header_stack[-1][\"level\"] >= current_header_level\\n                        ):\\n                            # We have encountered a new header\\n                            # at the same or higher level\\n                            popped_header = header_stack.pop()\\n                            # Clear the metadata for the\\n                            # popped header in initial_metadata\\n                            if popped_header[\"name\"] in initial_metadata:\\n                                initial_metadata.pop(popped_header[\"name\"])',\n",
              "  'def split_text(self, text: str) -> List[Document]:\\n        \"\"\"Split markdown file\\n        Args:\\n            text: Markdown file\"\"\"\\n\\n        # Split the input text by newline character (\"\\\\n\").\\n        lines = text.split(\"\\\\n\")\\n        # Final output\\n        lines_with_metadata: List[LineType] = []\\n        # Content and metadata of the chunk currently being processed\\n        current_content: List[str] = []\\n        current_metadata: Dict[str, str] = {}\\n        # Keep track of the nested header structure\\n        # header_stack: List[Dict[str, Union[int, str]]] = []\\n        header_stack: List[HeaderType] = []\\n        initial_metadata: Dict[str, str] = {}\\n\\n        in_code_block = False\\n        opening_fence = \"\"\\n\\n        for line in lines:\\n            stripped_line = line.strip()\\n\\n            if not in_code_block:\\n                # Exclude inline code spans\\n                if stripped_line.startswith(\"```\") and stripped_line.count(\"```\") == 1:\\n                    in_code_block = True\\n                    opening_fence = \"```\"\\n                elif stripped_line.startswith(\"~~~\"):\\n                    in_code_block = True\\n                    opening_fence = \"~~~\"\\n            else:\\n                if stripped_line.startswith(opening_fence):\\n                    in_code_block = False\\n                    opening_fence = \"\"\\n\\n            if in_code_block:\\n                current_content.append(stripped_line)\\n                continue',\n",
              "  'class MarkdownHeaderTextSplitter:\\n    \"\"\"Splitting markdown files based on specified headers.\"\"\"\\n\\n    def __init__(\\n        self, headers_to_split_on: List[Tuple[str, str]], return_each_line: bool = False\\n    ):\\n        \"\"\"Create a new MarkdownHeaderTextSplitter.\\n\\n        Args:\\n            headers_to_split_on: Headers we want to track\\n            return_each_line: Return each line w/ associated headers\\n        \"\"\"\\n        # Output line-by-line or aggregated into chunks w/ common headers\\n        self.return_each_line = return_each_line\\n        # Given the headers we want to split on,\\n        # (e.g., \"#, ##, etc\") order by length\\n        self.headers_to_split_on = sorted(\\n            headers_to_split_on, key=lambda split: len(split[0]), reverse=True\\n        )\\n\\n    def aggregate_lines_to_chunks(self, lines: List[LineType]) -> List[Document]:\\n        \"\"\"Combine lines with common metadata into chunks\\n        Args:\\n            lines: Line of text / associated header metadata\\n        \"\"\"\\n        aggregated_chunks: List[LineType] = []\\n\\n        for line in lines:\\n            if (\\n                aggregated_chunks\\n                and aggregated_chunks[-1][\"metadata\"] == line[\"metadata\"]\\n            ):\\n                # If the last line in the aggregated list\\n                # has the same metadata as the current line,\\n                # append the current content to the last lines\\'s content\\n                aggregated_chunks[-1][\"content\"] += \"  \\\\n\" + line[\"content\"]\\n            else:\\n                # Otherwise, append the current line to the aggregated list\\n                aggregated_chunks.append(line)\\n\\n        return [\\n            Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n            for chunk in aggregated_chunks\\n        ]\\n\\n    def split_text(self, text: str) -> List[Document]:\\n        \"\"\"Split markdown file\\n        Args:\\n            text: Markdown file\"\"\"',\n",
              "  'class CharacterTextSplitter(TextSplitter):\\n    \"\"\"Splitting text that looks at characters.\"\"\"\\n\\n    def __init__(\\n        self, separator: str = \"\\\\n\\\\n\", is_separator_regex: bool = False, **kwargs: Any\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(**kwargs)\\n        self._separator = separator\\n        self._is_separator_regex = is_separator_regex\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        # First we naively split the large input into a bunch of smaller ones.\\n        separator = (\\n            self._separator if self._is_separator_regex else re.escape(self._separator)\\n        )\\n        splits = _split_text_with_regex(text, separator, self._keep_separator)\\n        _separator = \"\" if self._keep_separator else self._separator\\n        return self._merge_splits(splits, _separator)\\n\\n\\nclass LineType(TypedDict):\\n    \"\"\"Line type as typed dict.\"\"\"\\n\\n    metadata: Dict[str, str]\\n    content: str\\n\\n\\nclass HeaderType(TypedDict):\\n    \"\"\"Header type as typed dict.\"\"\"\\n\\n    level: int\\n    name: str\\n    data: str',\n",
              "  'if issubclass(cls, TokenTextSplitter):\\n            extra_kwargs = {\\n                \"encoding_name\": encoding_name,\\n                \"model_name\": model_name,\\n                \"allowed_special\": allowed_special,\\n                \"disallowed_special\": disallowed_special,\\n            }\\n            kwargs = {**kwargs, **extra_kwargs}\\n\\n        return cls(length_function=_tiktoken_encoder, **kwargs)\\n\\n    def transform_documents(\\n        self, documents: Sequence[Document], **kwargs: Any\\n    ) -> Sequence[Document]:\\n        \"\"\"Transform sequence of documents by splitting them.\"\"\"\\n        return self.split_documents(list(documents))\\n\\n    async def atransform_documents(\\n        self, documents: Sequence[Document], **kwargs: Any\\n    ) -> Sequence[Document]:\\n        \"\"\"Asynchronously transform a sequence of documents by splitting them.\"\"\"\\n        return await asyncio.get_running_loop().run_in_executor(\\n            None, partial(self.transform_documents, **kwargs), documents\\n        )',\n",
              "  'if not isinstance(tokenizer, PreTrainedTokenizerBase):\\n                raise ValueError(\\n                    \"Tokenizer received was not an instance of PreTrainedTokenizerBase\"\\n                )\\n\\n            def _huggingface_tokenizer_length(text: str) -> int:\\n                return len(tokenizer.encode(text))\\n\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import transformers python package. \"\\n                \"Please install it with `pip install transformers`.\"\\n            )\\n        return cls(length_function=_huggingface_tokenizer_length, **kwargs)\\n\\n    @classmethod\\n    def from_tiktoken_encoder(\\n        cls: Type[TS],\\n        encoding_name: str = \"gpt2\",\\n        model_name: Optional[str] = None,\\n        allowed_special: Union[Literal[\"all\"], AbstractSet[str]] = set(),\\n        disallowed_special: Union[Literal[\"all\"], Collection[str]] = \"all\",\\n        **kwargs: Any,\\n    ) -> TS:\\n        \"\"\"Text splitter that uses tiktoken encoder to count length.\"\"\"\\n        try:\\n            import tiktoken\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import tiktoken python package. \"\\n                \"This is needed in order to calculate max_tokens_for_prompt. \"\\n                \"Please install it with `pip install tiktoken`.\"\\n            )\\n\\n        if model_name is not None:\\n            enc = tiktoken.encoding_for_model(model_name)\\n        else:\\n            enc = tiktoken.get_encoding(encoding_name)\\n\\n        def _tiktoken_encoder(text: str) -> int:\\n            return len(\\n                enc.encode(\\n                    text,\\n                    allowed_special=allowed_special,\\n                    disallowed_special=disallowed_special,\\n                )\\n            )',\n",
              "  'docs = []\\n        current_doc: List[str] = []\\n        total = 0\\n        for d in splits:\\n            _len = self._length_function(d)\\n            if (\\n                total + _len + (separator_len if len(current_doc) > 0 else 0)\\n                > self._chunk_size\\n            ):\\n                if total > self._chunk_size:\\n                    logger.warning(\\n                        f\"Created a chunk of size {total}, \"\\n                        f\"which is longer than the specified {self._chunk_size}\"\\n                    )\\n                if len(current_doc) > 0:\\n                    doc = self._join_docs(current_doc, separator)\\n                    if doc is not None:\\n                        docs.append(doc)\\n                    # Keep on popping if:\\n                    # - we have a larger chunk than in the chunk overlap\\n                    # - or if we still have any chunks and the length is long\\n                    while total > self._chunk_overlap or (\\n                        total + _len + (separator_len if len(current_doc) > 0 else 0)\\n                        > self._chunk_size\\n                        and total > 0\\n                    ):\\n                        total -= self._length_function(current_doc[0]) + (\\n                            separator_len if len(current_doc) > 1 else 0\\n                        )\\n                        current_doc = current_doc[1:]\\n            current_doc.append(d)\\n            total += _len + (separator_len if len(current_doc) > 1 else 0)\\n        doc = self._join_docs(current_doc, separator)\\n        if doc is not None:\\n            docs.append(doc)\\n        return docs\\n\\n    @classmethod\\n    def from_huggingface_tokenizer(cls, tokenizer: Any, **kwargs: Any) -> TextSplitter:\\n        \"\"\"Text splitter that uses HuggingFace tokenizer to count length.\"\"\"\\n        try:\\n            from transformers import PreTrainedTokenizerBase',\n",
              "  '@abstractmethod\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split text into multiple components.\"\"\"\\n\\n    def create_documents(\\n        self, texts: List[str], metadatas: Optional[List[dict]] = None\\n    ) -> List[Document]:\\n        \"\"\"Create documents from a list of texts.\"\"\"\\n        _metadatas = metadatas or [{}] * len(texts)\\n        documents = []\\n        for i, text in enumerate(texts):\\n            index = -1\\n            for chunk in self.split_text(text):\\n                metadata = copy.deepcopy(_metadatas[i])\\n                if self._add_start_index:\\n                    index = text.find(chunk, index + 1)\\n                    metadata[\"start_index\"] = index\\n                new_doc = Document(page_content=chunk, metadata=metadata)\\n                documents.append(new_doc)\\n        return documents\\n\\n    def split_documents(self, documents: Iterable[Document]) -> List[Document]:\\n        \"\"\"Split documents.\"\"\"\\n        texts, metadatas = [], []\\n        for doc in documents:\\n            texts.append(doc.page_content)\\n            metadatas.append(doc.metadata)\\n        return self.create_documents(texts, metadatas=metadatas)\\n\\n    def _join_docs(self, docs: List[str], separator: str) -> Optional[str]:\\n        text = separator.join(docs)\\n        if self._strip_whitespace:\\n            text = text.strip()\\n        if text == \"\":\\n            return None\\n        else:\\n            return text\\n\\n    def _merge_splits(self, splits: Iterable[str], separator: str) -> List[str]:\\n        # We now want to combine these smaller pieces into medium size\\n        # chunks to send to the LLM.\\n        separator_len = self._length_function(separator)',\n",
              "  'class TextSplitter(BaseDocumentTransformer, ABC):\\n    \"\"\"Interface for splitting text into chunks.\"\"\"\\n\\n    def __init__(\\n        self,\\n        chunk_size: int = 4000,\\n        chunk_overlap: int = 200,\\n        length_function: Callable[[str], int] = len,\\n        keep_separator: bool = False,\\n        add_start_index: bool = False,\\n        strip_whitespace: bool = True,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\\n\\n        Args:\\n            chunk_size: Maximum size of chunks to return\\n            chunk_overlap: Overlap in characters between chunks\\n            length_function: Function that measures the length of given chunks\\n            keep_separator: Whether to keep the separator in the chunks\\n            add_start_index: If `True`, includes chunk\\'s start index in metadata\\n            strip_whitespace: If `True`, strips whitespace from the start and end of\\n                              every document\\n        \"\"\"\\n        if chunk_overlap > chunk_size:\\n            raise ValueError(\\n                f\"Got a larger chunk overlap ({chunk_overlap}) than chunk size \"\\n                f\"({chunk_size}), should be smaller.\"\\n            )\\n        self._chunk_size = chunk_size\\n        self._chunk_overlap = chunk_overlap\\n        self._length_function = length_function\\n        self._keep_separator = keep_separator\\n        self._add_start_index = add_start_index\\n        self._strip_whitespace = strip_whitespace\\n\\n    @abstractmethod\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split text into multiple components.\"\"\"',\n",
              "  'class MarkdownHeaderTextSplitter:\\n    \"\"\"Splitting markdown files based on specified headers.\"\"\"\\n\\n    def __init__(\\n        self, headers_to_split_on: List[Tuple[str, str]], return_each_line: bool = False\\n    ):\\n        \"\"\"Create a new MarkdownHeaderTextSplitter.\\n\\n        Args:\\n            headers_to_split_on: Headers we want to track\\n            return_each_line: Return each line w/ associated headers\\n        \"\"\"\\n        # Output line-by-line or aggregated into chunks w/ common headers\\n        self.return_each_line = return_each_line\\n        # Given the headers we want to split on,\\n        # (e.g., \"#, ##, etc\") order by length\\n        self.headers_to_split_on = sorted(\\n            headers_to_split_on, key=lambda split: len(split[0]), reverse=True\\n        )\\n\\n    def aggregate_lines_to_chunks(self, lines: List[LineType]) -> List[Document]:\\n        \"\"\"Combine lines with common metadata into chunks\\n        Args:\\n            lines: Line of text / associated header metadata\\n        \"\"\"\\n        aggregated_chunks: List[LineType] = []\\n\\n        for line in lines:\\n            if (\\n                aggregated_chunks\\n                and aggregated_chunks[-1][\"metadata\"] == line[\"metadata\"]\\n            ):\\n                # If the last line in the aggregated list\\n                # has the same metadata as the current line,\\n                # append the current content to the last lines\\'s content\\n                aggregated_chunks[-1][\"content\"] += \"  \\\\n\" + line[\"content\"]\\n            else:\\n                # Otherwise, append the current line to the aggregated list\\n                aggregated_chunks.append(line)\\n\\n        return [\\n            Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n            for chunk in aggregated_chunks\\n        ]\\n\\n    def split_text(self, text: str) -> List[Document]:\\n        \"\"\"Split markdown file\\n        Args:\\n            text: Markdown file\"\"\"',\n",
              "  '\"\"\"**Text Splitters** are classes for splitting text.\\n\\n\\n**Class hierarchy:**\\n\\n.. code-block::\\n\\n    BaseDocumentTransformer --> TextSplitter --> <name>TextSplitter  # Example: CharacterTextSplitter\\n                                                 RecursiveCharacterTextSplitter -->  <name>TextSplitter\\n\\nNote: **MarkdownHeaderTextSplitter** and **HTMLHeaderTextSplitter do not derive from TextSplitter.\\n\\n\\n**Main helpers:**\\n\\n.. code-block::\\n\\n    Document, Tokenizer, Language, LineType, HeaderType\\n\\n\"\"\"  # noqa: E501\\n\\nfrom __future__ import annotations\\n\\nimport asyncio\\nimport copy\\nimport logging\\nimport pathlib\\nimport re\\nfrom abc import ABC, abstractmethod\\nfrom dataclasses import dataclass\\nfrom enum import Enum\\nfrom functools import partial\\nfrom io import BytesIO, StringIO\\nfrom typing import (\\n    AbstractSet,\\n    Any,\\n    Callable,\\n    Collection,\\n    Dict,\\n    Iterable,\\n    List,\\n    Literal,\\n    Optional,\\n    Sequence,\\n    Tuple,\\n    Type,\\n    TypedDict,\\n    TypeVar,\\n    Union,\\n    cast,\\n)\\n\\nimport requests\\nfrom langchain_core.documents import BaseDocumentTransformer, Document\\n\\nlogger = logging.getLogger(__name__)\\n\\nTS = TypeVar(\"TS\", bound=\"TextSplitter\")\\n\\n\\ndef _make_spacy_pipeline_for_splitting(\\n    pipeline: str, *, max_length: int = 1_000_000\\n) -> Any:  # avoid importing spacy\\n    try:\\n        import spacy\\n    except ImportError:\\n        raise ImportError(\\n            \"Spacy is not installed, please install it with `pip install spacy`.\"\\n        )\\n    if pipeline == \"sentencizer\":\\n        from spacy.lang.en import English\\n\\n        sentencizer = English()\\n        sentencizer.add_pipe(\"sentencizer\")\\n    else:\\n        sentencizer = spacy.load(pipeline, exclude=[\"ner\", \"tagger\"])\\n        sentencizer.max_length = max_length\\n    return sentencizer',\n",
              "  'class PythonCodeTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Python syntax.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a PythonCodeTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.PYTHON)\\n        super().__init__(separators=separators, **kwargs)\\n\\n\\nclass MarkdownTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Markdown-formatted headings.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a MarkdownTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.MARKDOWN)\\n        super().__init__(separators=separators, **kwargs)\\n\\n\\nclass LatexTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Latex-formatted layout elements.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a LatexTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.LATEX)\\n        super().__init__(separators=separators, **kwargs)',\n",
              "  'class NLTKTextSplitter(TextSplitter):\\n    \"\"\"Splitting text using NLTK package.\"\"\"\\n\\n    def __init__(\\n        self, separator: str = \"\\\\n\\\\n\", language: str = \"english\", **kwargs: Any\\n    ) -> None:\\n        \"\"\"Initialize the NLTK splitter.\"\"\"\\n        super().__init__(**kwargs)\\n        try:\\n            from nltk.tokenize import sent_tokenize\\n\\n            self._tokenizer = sent_tokenize\\n        except ImportError:\\n            raise ImportError(\\n                \"NLTK is not installed, please install it with `pip install nltk`.\"\\n            )\\n        self._separator = separator\\n        self._language = language\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        # First we naively split the large input into a bunch of smaller ones.\\n        splits = self._tokenizer(text, language=self._language)\\n        return self._merge_splits(splits, self._separator)\\n\\n\\nclass SpacyTextSplitter(TextSplitter):\\n    \"\"\"Splitting text using Spacy package.\\n\\n\\n    Per default, Spacy\\'s `en_core_web_sm` model is used and\\n    its default max_length is 1000000 (it is the length of maximum character\\n    this model takes which can be increased for large files). For a faster, but\\n    potentially less accurate splitting, you can use `pipeline=\\'sentencizer\\'`.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        separator: str = \"\\\\n\\\\n\",\\n        pipeline: str = \"en_core_web_sm\",\\n        max_length: int = 1_000_000,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Initialize the spacy text splitter.\"\"\"\\n        super().__init__(**kwargs)\\n        self._tokenizer = _make_spacy_pipeline_for_splitting(\\n            pipeline, max_length=max_length\\n        )\\n        self._separator = separator\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        splits = (s.text for s in self._tokenizer(text).sents)\\n        return self._merge_splits(splits, self._separator)\\n\\n\\n# For backwards compatibility',\n",
              "  'else:\\n            raise ValueError(\\n                f\"Language {language} is not supported! \"\\n                f\"Please choose from {list(Language)}\"\\n            )',\n",
              "  '\"\\\\nevent \",\\n                \"\\\\nmodifier \",\\n                \"\\\\nerror \",\\n                \"\\\\nstruct \",\\n                \"\\\\nenum \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo while \",\\n                \"\\\\nassembly \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.COBOL:\\n            return [\\n                # Split along divisions\\n                \"\\\\nIDENTIFICATION DIVISION.\",\\n                \"\\\\nENVIRONMENT DIVISION.\",\\n                \"\\\\nDATA DIVISION.\",\\n                \"\\\\nPROCEDURE DIVISION.\",\\n                # Split along sections within DATA DIVISION\\n                \"\\\\nWORKING-STORAGE SECTION.\",\\n                \"\\\\nLINKAGE SECTION.\",\\n                \"\\\\nFILE SECTION.\",\\n                # Split along sections within PROCEDURE DIVISION\\n                \"\\\\nINPUT-OUTPUT SECTION.\",\\n                # Split along paragraphs and common statements\\n                \"\\\\nOPEN \",\\n                \"\\\\nCLOSE \",\\n                \"\\\\nREAD \",\\n                \"\\\\nWRITE \",\\n                \"\\\\nIF \",\\n                \"\\\\nELSE \",\\n                \"\\\\nMOVE \",\\n                \"\\\\nPERFORM \",\\n                \"\\\\nUNTIL \",\\n                \"\\\\nVARYING \",\\n                \"\\\\nACCEPT \",\\n                \"\\\\nDISPLAY \",\\n                \"\\\\nSTOP RUN.\",\\n                # Split by the normal type of lines\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]',\n",
              "  '\"<nav\",\\n                # Head\\n                \"<head\",\\n                \"<style\",\\n                \"<script\",\\n                \"<meta\",\\n                \"<title\",\\n                \"\",\\n            ]\\n        elif language == Language.CSHARP:\\n            return [\\n                \"\\\\ninterface \",\\n                \"\\\\nenum \",\\n                \"\\\\nimplements \",\\n                \"\\\\ndelegate \",\\n                \"\\\\nevent \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nabstract \",\\n                # Split along method definitions\\n                \"\\\\npublic \",\\n                \"\\\\nprotected \",\\n                \"\\\\nprivate \",\\n                \"\\\\nstatic \",\\n                \"\\\\nreturn \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\ncontinue \",\\n                \"\\\\nfor \",\\n                \"\\\\nforeach \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\nbreak \",\\n                \"\\\\ncase \",\\n                \"\\\\nelse \",\\n                # Split by exceptions\\n                \"\\\\ntry \",\\n                \"\\\\nthrow \",\\n                \"\\\\nfinally \",\\n                \"\\\\ncatch \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SOL:\\n            return [\\n                # Split along compiler information definitions\\n                \"\\\\npragma \",\\n                \"\\\\nusing \",\\n                # Split along contract definitions\\n                \"\\\\ncontract \",\\n                \"\\\\ninterface \",\\n                \"\\\\nlibrary \",\\n                # Split along method definitions\\n                \"\\\\nconstructor \",\\n                \"\\\\ntype \",\\n                \"\\\\nfunction \",\\n                \"\\\\nevent \",\\n                \"\\\\nmodifier \",\\n                \"\\\\nerror \",\\n                \"\\\\nstruct \",\\n                \"\\\\nenum \",\\n                # Split along control flow statements',\n",
              "  '\"\\\\n\\\\\\\\*\\\\\\\\*\\\\\\\\*+\\\\n\",\\n                \"\\\\n---+\\\\n\",\\n                \"\\\\n___+\\\\n\",\\n                # Note that this splitter doesn\\'t handle horizontal lines defined\\n                # by *three or more* of ***, ---, or ___, but this is not handled\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.LATEX:\\n            return [\\n                # First, try to split along Latex sections\\n                \"\\\\n\\\\\\\\\\\\\\\\chapter{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\section{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\subsection{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\subsubsection{\",\\n                # Now split by environments\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{enumerate}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{itemize}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{description}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{list}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{quote}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{quotation}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{verse}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{verbatim}\",\\n                # Now split by math environments\\n                \"\\\\n\\\\\\\\\\\\begin{align}\",\\n                \"$$\",\\n                \"$\",\\n                # Now split by the normal type of lines\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.HTML:\\n            return [\\n                # First, try to split along HTML tags\\n                \"<body\",\\n                \"<div\",\\n                \"<p\",\\n                \"<br\",\\n                \"<li\",\\n                \"<h1\",\\n                \"<h2\",\\n                \"<h3\",\\n                \"<h4\",\\n                \"<h5\",\\n                \"<h6\",\\n                \"<span\",\\n                \"<table\",\\n                \"<tr\",\\n                \"<td\",\\n                \"<th\",\\n                \"<ul\",\\n                \"<ol\",\\n                \"<header\",\\n                \"<footer\",\\n                \"<nav\",\\n                # Head\\n                \"<head\",\\n                \"<style\",\\n                \"<script\",\\n                \"<meta\",\\n                \"<title\",',\n",
              "  '\" \",\\n                \"\",\\n            ]\\n        elif language == Language.SCALA:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nobject \",\\n                # Split along method definitions\\n                \"\\\\ndef \",\\n                \"\\\\nval \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nmatch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SWIFT:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunc \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nstruct \",\\n                \"\\\\nenum \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.MARKDOWN:\\n            return [\\n                # First, try to split along Markdown headings (starting with level 2)\\n                \"\\\\n#{1,6} \",\\n                # Note the alternative syntax for headings (below) is not handled here\\n                # Heading level 2\\n                # ---------------\\n                # End of code block\\n                \"```\\\\n\",\\n                # Horizontal lines\\n                \"\\\\n\\\\\\\\*\\\\\\\\*\\\\\\\\*+\\\\n\",\\n                \"\\\\n---+\\\\n\",\\n                \"\\\\n___+\\\\n\",\\n                # Note that this splitter doesn\\'t handle horizontal lines defined',\n",
              "  'return [\\n            Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n            for chunk in aggregated_chunks\\n        ]\\n\\n    def split_text_from_url(self, url: str) -> List[Document]:\\n        \"\"\"Split HTML from web URL\\n\\n        Args:\\n            url: web URL\\n        \"\"\"\\n        r = requests.get(url)\\n        return self.split_text_from_file(BytesIO(r.content))\\n\\n    def split_text(self, text: str) -> List[Document]:\\n        \"\"\"Split HTML text string\\n\\n        Args:\\n            text: HTML text\\n        \"\"\"\\n        return self.split_text_from_file(StringIO(text))\\n\\n    def split_text_from_file(self, file: Any) -> List[Document]:\\n        \"\"\"Split HTML file\\n\\n        Args:\\n            file: HTML file\\n        \"\"\"\\n        try:\\n            from lxml import etree\\n        except ImportError as e:\\n            raise ImportError(\\n                \"Unable to import lxml, please install with `pip install lxml`.\"\\n            ) from e\\n        # use lxml library to parse html document and return xml ElementTree\\n        parser = etree.HTMLParser()\\n        tree = etree.parse(file, parser)\\n\\n        # document transformation for \"structure-aware\" chunking is handled with xsl.\\n        # see comments in html_chunks_with_headers.xslt for more detailed information.\\n        xslt_path = (\\n            pathlib.Path(__file__).parent\\n            / \"document_transformers/xsl/html_chunks_with_headers.xslt\"\\n        )\\n        xslt_tree = etree.parse(xslt_path)\\n        transform = etree.XSLT(xslt_tree)\\n        result = transform(tree)\\n        result_dom = etree.fromstring(str(result))\\n\\n        # create filter and mapping for header metadata\\n        header_filter = [header[0] for header in self.headers_to_split_on]\\n        header_mapping = dict(self.headers_to_split_on)\\n\\n        # map xhtml namespace prefix\\n        ns_map = {\"h\": \"http://www.w3.org/1999/xhtml\"}',\n",
              "  '@staticmethod\\n    def get_separators_for_language(language: Language) -> List[str]:\\n        if language == Language.CPP:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along function definitions\\n                \"\\\\nvoid \",\\n                \"\\\\nint \",\\n                \"\\\\nfloat \",\\n                \"\\\\ndouble \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.GO:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunc \",\\n                \"\\\\nvar \",\\n                \"\\\\nconst \",\\n                \"\\\\ntype \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.JAVA:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along method definitions\\n                \"\\\\npublic \",\\n                \"\\\\nprotected \",\\n                \"\\\\nprivate \",\\n                \"\\\\nstatic \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.KOTLIN:\\n            return [',\n",
              "  '_separator = separator if self._is_separator_regex else re.escape(separator)\\n        splits = _split_text_with_regex(text, _separator, self._keep_separator)\\n\\n        # Now go merging things, recursively splitting longer texts.\\n        _good_splits = []\\n        _separator = \"\" if self._keep_separator else separator\\n        for s in splits:\\n            if self._length_function(s) < self._chunk_size:\\n                _good_splits.append(s)\\n            else:\\n                if _good_splits:\\n                    merged_text = self._merge_splits(_good_splits, _separator)\\n                    final_chunks.extend(merged_text)\\n                    _good_splits = []\\n                if not new_separators:\\n                    final_chunks.append(s)\\n                else:\\n                    other_info = self._split_text(s, new_separators)\\n                    final_chunks.extend(other_info)\\n        if _good_splits:\\n            merged_text = self._merge_splits(_good_splits, _separator)\\n            final_chunks.extend(merged_text)\\n        return final_chunks\\n\\n    def split_text(self, text: str) -> List[str]:\\n        return self._split_text(text, self._separators)\\n\\n    @classmethod\\n    def from_language(\\n        cls, language: Language, **kwargs: Any\\n    ) -> RecursiveCharacterTextSplitter:\\n        separators = cls.get_separators_for_language(language)\\n        return cls(separators=separators, is_separator_regex=True, **kwargs)',\n",
              "  'class RecursiveCharacterTextSplitter(TextSplitter):\\n    \"\"\"Splitting text by recursively look at characters.\\n\\n    Recursively tries to split by different characters to find one\\n    that works.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        separators: Optional[List[str]] = None,\\n        keep_separator: bool = True,\\n        is_separator_regex: bool = False,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(keep_separator=keep_separator, **kwargs)\\n        self._separators = separators or [\"\\\\n\\\\n\", \"\\\\n\", \" \", \"\"]\\n        self._is_separator_regex = is_separator_regex\\n\\n    def _split_text(self, text: str, separators: List[str]) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        final_chunks = []\\n        # Get appropriate separator to use\\n        separator = separators[-1]\\n        new_separators = []\\n        for i, _s in enumerate(separators):\\n            _separator = _s if self._is_separator_regex else re.escape(_s)\\n            if _s == \"\":\\n                separator = _s\\n                break\\n            if re.search(_separator, text):\\n                separator = _s\\n                new_separators = separators[i + 1 :]\\n                break\\n\\n        _separator = separator if self._is_separator_regex else re.escape(separator)\\n        splits = _split_text_with_regex(text, _separator, self._keep_separator)',\n",
              "  'class Language(str, Enum):\\n    \"\"\"Enum of the programming languages.\"\"\"\\n\\n    CPP = \"cpp\"\\n    GO = \"go\"\\n    JAVA = \"java\"\\n    KOTLIN = \"kotlin\"\\n    JS = \"js\"\\n    TS = \"ts\"\\n    PHP = \"php\"\\n    PROTO = \"proto\"\\n    PYTHON = \"python\"\\n    RST = \"rst\"\\n    RUBY = \"ruby\"\\n    RUST = \"rust\"\\n    SCALA = \"scala\"\\n    SWIFT = \"swift\"\\n    MARKDOWN = \"markdown\"\\n    LATEX = \"latex\"\\n    HTML = \"html\"\\n    SOL = \"sol\"\\n    CSHARP = \"csharp\"\\n    COBOL = \"cobol\"',\n",
              "  'def split_text(self, text: str) -> List[str]:\\n        def encode_strip_start_and_stop_token_ids(text: str) -> List[int]:\\n            return self._encode(text)[1:-1]\\n\\n        tokenizer = Tokenizer(\\n            chunk_overlap=self._chunk_overlap,\\n            tokens_per_chunk=self.tokens_per_chunk,\\n            decode=self.tokenizer.decode,\\n            encode=encode_strip_start_and_stop_token_ids,\\n        )\\n\\n        return split_text_on_tokens(text=text, tokenizer=tokenizer)\\n\\n    def count_tokens(self, *, text: str) -> int:\\n        return len(self._encode(text))\\n\\n    _max_length_equal_32_bit_integer: int = 2**32\\n\\n    def _encode(self, text: str) -> List[int]:\\n        token_ids_with_start_and_end_token_ids = self.tokenizer.encode(\\n            text,\\n            max_length=self._max_length_equal_32_bit_integer,\\n            truncation=\"do_not_truncate\",\\n        )\\n        return token_ids_with_start_and_end_token_ids',\n",
              "  'class SentenceTransformersTokenTextSplitter(TextSplitter):\\n    \"\"\"Splitting text to tokens using sentence model tokenizer.\"\"\"\\n\\n    def __init__(\\n        self,\\n        chunk_overlap: int = 50,\\n        model_name: str = \"sentence-transformers/all-mpnet-base-v2\",\\n        tokens_per_chunk: Optional[int] = None,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(**kwargs, chunk_overlap=chunk_overlap)\\n\\n        try:\\n            from sentence_transformers import SentenceTransformer\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import sentence_transformer python package. \"\\n                \"This is needed in order to for SentenceTransformersTokenTextSplitter. \"\\n                \"Please install it with `pip install sentence-transformers`.\"\\n            )\\n\\n        self.model_name = model_name\\n        self._model = SentenceTransformer(self.model_name)\\n        self.tokenizer = self._model.tokenizer\\n        self._initialize_chunk_configuration(tokens_per_chunk=tokens_per_chunk)\\n\\n    def _initialize_chunk_configuration(\\n        self, *, tokens_per_chunk: Optional[int]\\n    ) -> None:\\n        self.maximum_tokens_per_chunk = cast(int, self._model.max_seq_length)\\n\\n        if tokens_per_chunk is None:\\n            self.tokens_per_chunk = self.maximum_tokens_per_chunk\\n        else:\\n            self.tokens_per_chunk = tokens_per_chunk\\n\\n        if self.tokens_per_chunk > self.maximum_tokens_per_chunk:\\n            raise ValueError(\\n                f\"The token limit of the models \\'{self.model_name}\\'\"\\n                f\" is: {self.maximum_tokens_per_chunk}.\"\\n                f\" Argument tokens_per_chunk={self.tokens_per_chunk}\"\\n                f\" > maximum token limit.\"\\n            )\\n\\n    def split_text(self, text: str) -> List[str]:\\n        def encode_strip_start_and_stop_token_ids(text: str) -> List[int]:\\n            return self._encode(text)[1:-1]',\n",
              "  'class TokenTextSplitter(TextSplitter):\\n    \"\"\"Splitting text to tokens using model tokenizer.\"\"\"\\n\\n    def __init__(\\n        self,\\n        encoding_name: str = \"gpt2\",\\n        model_name: Optional[str] = None,\\n        allowed_special: Union[Literal[\"all\"], AbstractSet[str]] = set(),\\n        disallowed_special: Union[Literal[\"all\"], Collection[str]] = \"all\",\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(**kwargs)\\n        try:\\n            import tiktoken\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import tiktoken python package. \"\\n                \"This is needed in order to for TokenTextSplitter. \"\\n                \"Please install it with `pip install tiktoken`.\"\\n            )\\n\\n        if model_name is not None:\\n            enc = tiktoken.encoding_for_model(model_name)\\n        else:\\n            enc = tiktoken.get_encoding(encoding_name)\\n        self._tokenizer = enc\\n        self._allowed_special = allowed_special\\n        self._disallowed_special = disallowed_special\\n\\n    def split_text(self, text: str) -> List[str]:\\n        def _encode(_text: str) -> List[int]:\\n            return self._tokenizer.encode(\\n                _text,\\n                allowed_special=self._allowed_special,\\n                disallowed_special=self._disallowed_special,\\n            )\\n\\n        tokenizer = Tokenizer(\\n            chunk_overlap=self._chunk_overlap,\\n            tokens_per_chunk=self._chunk_size,\\n            decode=self._tokenizer.decode,\\n            encode=_encode,\\n        )\\n\\n        return split_text_on_tokens(text=text, tokenizer=tokenizer)',\n",
              "  'class Tokenizer:\\n    \"\"\"Tokenizer data class.\"\"\"\\n\\n    chunk_overlap: int\\n    \"\"\"Overlap in tokens between chunks\"\"\"\\n    tokens_per_chunk: int\\n    \"\"\"Maximum number of tokens per chunk\"\"\"\\n    decode: Callable[[List[int]], str]\\n    \"\"\" Function to decode a list of token ids to a string\"\"\"\\n    encode: Callable[[str], List[int]]\\n    \"\"\" Function to encode a string to a list of token ids\"\"\"\\n\\n\\ndef split_text_on_tokens(*, text: str, tokenizer: Tokenizer) -> List[str]:\\n    \"\"\"Split incoming text and return chunks using tokenizer.\"\"\"\\n    splits: List[str] = []\\n    input_ids = tokenizer.encode(text)\\n    start_idx = 0\\n    cur_idx = min(start_idx + tokenizer.tokens_per_chunk, len(input_ids))\\n    chunk_ids = input_ids[start_idx:cur_idx]\\n    while start_idx < len(input_ids):\\n        splits.append(tokenizer.decode(chunk_ids))\\n        if cur_idx == len(input_ids):\\n            break\\n        start_idx += tokenizer.tokens_per_chunk - tokenizer.chunk_overlap\\n        cur_idx = min(start_idx + tokenizer.tokens_per_chunk, len(input_ids))\\n        chunk_ids = input_ids[start_idx:cur_idx]\\n    return splits',\n",
              "  '# map xhtml namespace prefix\\n        ns_map = {\"h\": \"http://www.w3.org/1999/xhtml\"}\\n\\n        # build list of elements from DOM\\n        elements = []\\n        for element in result_dom.findall(\"*//*\", ns_map):\\n            if element.findall(\"*[@class=\\'headers\\']\") or element.findall(\\n                \"*[@class=\\'chunk\\']\"\\n            ):\\n                elements.append(\\n                    ElementType(\\n                        url=file,\\n                        xpath=\"\".join(\\n                            [\\n                                node.text\\n                                for node in element.findall(\"*[@class=\\'xpath\\']\", ns_map)\\n                            ]\\n                        ),\\n                        content=\"\".join(\\n                            [\\n                                node.text\\n                                for node in element.findall(\"*[@class=\\'chunk\\']\", ns_map)\\n                            ]\\n                        ),\\n                        metadata={\\n                            # Add text of specified headers to metadata using header\\n                            # mapping.\\n                            header_mapping[node.tag]: node.text\\n                            for node in filter(\\n                                lambda x: x.tag in header_filter,\\n                                element.findall(\"*[@class=\\'headers\\']/*\", ns_map),\\n                            )\\n                        },\\n                    )\\n                )\\n\\n        if not self.return_each_element:\\n            return self.aggregate_elements_to_chunks(elements)\\n        else:\\n            return [\\n                Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n                for chunk in elements\\n            ]\\n\\n\\n# should be in newer Python versions (3.10+)\\n# @dataclass(frozen=True, kw_only=True, slots=True)\\n@dataclass(frozen=True)',\n",
              "  '\"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.KOTLIN:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along method definitions\\n                \"\\\\npublic \",\\n                \"\\\\nprotected \",\\n                \"\\\\nprivate \",\\n                \"\\\\ninternal \",\\n                \"\\\\ncompanion \",\\n                \"\\\\nfun \",\\n                \"\\\\nval \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nwhen \",\\n                \"\\\\ncase \",\\n                \"\\\\nelse \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.JS:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunction \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                \"\\\\nvar \",\\n                \"\\\\nclass \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                \"\\\\ndefault \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.TS:\\n            return [\\n                \"\\\\nenum \",\\n                \"\\\\ninterface \",\\n                \"\\\\nnamespace \",\\n                \"\\\\ntype \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along function definitions\\n                \"\\\\nfunction \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements',\n",
              "  'class HTMLHeaderTextSplitter:\\n    \"\"\"\\n    Splitting HTML files based on specified headers.\\n    Requires lxml package.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        headers_to_split_on: List[Tuple[str, str]],\\n        return_each_element: bool = False,\\n    ):\\n        \"\"\"Create a new HTMLHeaderTextSplitter.\\n\\n        Args:\\n            headers_to_split_on: list of tuples of headers we want to track mapped to\\n                (arbitrary) keys for metadata. Allowed header values: h1, h2, h3, h4,\\n                h5, h6 e.g. [(\"h1\", \"Header 1\"), (\"h2\", \"Header 2)].\\n            return_each_element: Return each element w/ associated headers.\\n        \"\"\"\\n        # Output element-by-element or aggregated into chunks w/ common headers\\n        self.return_each_element = return_each_element\\n        self.headers_to_split_on = sorted(headers_to_split_on)\\n\\n    def aggregate_elements_to_chunks(\\n        self, elements: List[ElementType]\\n    ) -> List[Document]:\\n        \"\"\"Combine elements with common metadata into chunks\\n\\n        Args:\\n            elements: HTML element content with associated identifying info and metadata\\n        \"\"\"\\n        aggregated_chunks: List[ElementType] = []\\n\\n        for element in elements:\\n            if (\\n                aggregated_chunks\\n                and aggregated_chunks[-1][\"metadata\"] == element[\"metadata\"]\\n            ):\\n                # If the last element in the aggregated list\\n                # has the same metadata as the current element,\\n                # append the current content to the last element\\'s content\\n                aggregated_chunks[-1][\"content\"] += \"  \\\\n\" + element[\"content\"]\\n            else:\\n                # Otherwise, append the current element to the aggregated list\\n                aggregated_chunks.append(element)\\n\\n        return [\\n            Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n            for chunk in aggregated_chunks\\n        ]',\n",
              "  'class ElementType(TypedDict):\\n    \"\"\"Element type as typed dict.\"\"\"\\n\\n    url: str\\n    xpath: str\\n    content: str\\n    metadata: Dict[str, str]',\n",
              "  'if current_content:\\n            lines_with_metadata.append(\\n                {\"content\": \"\\\\n\".join(current_content), \"metadata\": current_metadata}\\n            )\\n\\n        # lines_with_metadata has each line with associated header metadata\\n        # aggregate these into chunks based on common metadata\\n        if not self.return_each_line:\\n            return self.aggregate_lines_to_chunks(lines_with_metadata)\\n        else:\\n            return [\\n                Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n                for chunk in lines_with_metadata\\n            ]',\n",
              "  '# Push the current header to the stack\\n                        header: HeaderType = {\\n                            \"level\": current_header_level,\\n                            \"name\": name,\\n                            \"data\": stripped_line[len(sep) :].strip(),\\n                        }\\n                        header_stack.append(header)\\n                        # Update initial_metadata with the current header\\n                        initial_metadata[name] = header[\"data\"]\\n\\n                    # Add the previous line to the lines_with_metadata\\n                    # only if current_content is not empty\\n                    if current_content:\\n                        lines_with_metadata.append(\\n                            {\\n                                \"content\": \"\\\\n\".join(current_content),\\n                                \"metadata\": current_metadata.copy(),\\n                            }\\n                        )\\n                        current_content.clear()\\n\\n                    break\\n            else:\\n                if stripped_line:\\n                    current_content.append(stripped_line)\\n                elif current_content:\\n                    lines_with_metadata.append(\\n                        {\\n                            \"content\": \"\\\\n\".join(current_content),\\n                            \"metadata\": current_metadata.copy(),\\n                        }\\n                    )\\n                    current_content.clear()\\n\\n            current_metadata = initial_metadata.copy()\\n\\n        if current_content:\\n            lines_with_metadata.append(\\n                {\"content\": \"\\\\n\".join(current_content), \"metadata\": current_metadata}\\n            )',\n",
              "  'if in_code_block:\\n                current_content.append(stripped_line)\\n                continue\\n\\n            # Check each line against each of the header types (e.g., #, ##)\\n            for sep, name in self.headers_to_split_on:\\n                # Check if line starts with a header that we intend to split on\\n                if stripped_line.startswith(sep) and (\\n                    # Header with no text OR header is followed by space\\n                    # Both are valid conditions that sep is being used a header\\n                    len(stripped_line) == len(sep) or stripped_line[len(sep)] == \" \"\\n                ):\\n                    # Ensure we are tracking the header as metadata\\n                    if name is not None:\\n                        # Get the current header level\\n                        current_header_level = sep.count(\"#\")\\n\\n                        # Pop out headers of lower or same level from the stack\\n                        while (\\n                            header_stack\\n                            and header_stack[-1][\"level\"] >= current_header_level\\n                        ):\\n                            # We have encountered a new header\\n                            # at the same or higher level\\n                            popped_header = header_stack.pop()\\n                            # Clear the metadata for the\\n                            # popped header in initial_metadata\\n                            if popped_header[\"name\"] in initial_metadata:\\n                                initial_metadata.pop(popped_header[\"name\"])',\n",
              "  'def split_text(self, text: str) -> List[Document]:\\n        \"\"\"Split markdown file\\n        Args:\\n            text: Markdown file\"\"\"\\n\\n        # Split the input text by newline character (\"\\\\n\").\\n        lines = text.split(\"\\\\n\")\\n        # Final output\\n        lines_with_metadata: List[LineType] = []\\n        # Content and metadata of the chunk currently being processed\\n        current_content: List[str] = []\\n        current_metadata: Dict[str, str] = {}\\n        # Keep track of the nested header structure\\n        # header_stack: List[Dict[str, Union[int, str]]] = []\\n        header_stack: List[HeaderType] = []\\n        initial_metadata: Dict[str, str] = {}\\n\\n        in_code_block = False\\n        opening_fence = \"\"\\n\\n        for line in lines:\\n            stripped_line = line.strip()\\n\\n            if not in_code_block:\\n                # Exclude inline code spans\\n                if stripped_line.startswith(\"```\") and stripped_line.count(\"```\") == 1:\\n                    in_code_block = True\\n                    opening_fence = \"```\"\\n                elif stripped_line.startswith(\"~~~\"):\\n                    in_code_block = True\\n                    opening_fence = \"~~~\"\\n            else:\\n                if stripped_line.startswith(opening_fence):\\n                    in_code_block = False\\n                    opening_fence = \"\"\\n\\n            if in_code_block:\\n                current_content.append(stripped_line)\\n                continue',\n",
              "  'class MarkdownHeaderTextSplitter:\\n    \"\"\"Splitting markdown files based on specified headers.\"\"\"\\n\\n    def __init__(\\n        self, headers_to_split_on: List[Tuple[str, str]], return_each_line: bool = False\\n    ):\\n        \"\"\"Create a new MarkdownHeaderTextSplitter.\\n\\n        Args:\\n            headers_to_split_on: Headers we want to track\\n            return_each_line: Return each line w/ associated headers\\n        \"\"\"\\n        # Output line-by-line or aggregated into chunks w/ common headers\\n        self.return_each_line = return_each_line\\n        # Given the headers we want to split on,\\n        # (e.g., \"#, ##, etc\") order by length\\n        self.headers_to_split_on = sorted(\\n            headers_to_split_on, key=lambda split: len(split[0]), reverse=True\\n        )\\n\\n    def aggregate_lines_to_chunks(self, lines: List[LineType]) -> List[Document]:\\n        \"\"\"Combine lines with common metadata into chunks\\n        Args:\\n            lines: Line of text / associated header metadata\\n        \"\"\"\\n        aggregated_chunks: List[LineType] = []\\n\\n        for line in lines:\\n            if (\\n                aggregated_chunks\\n                and aggregated_chunks[-1][\"metadata\"] == line[\"metadata\"]\\n            ):\\n                # If the last line in the aggregated list\\n                # has the same metadata as the current line,\\n                # append the current content to the last lines\\'s content\\n                aggregated_chunks[-1][\"content\"] += \"  \\\\n\" + line[\"content\"]\\n            else:\\n                # Otherwise, append the current line to the aggregated list\\n                aggregated_chunks.append(line)\\n\\n        return [\\n            Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n            for chunk in aggregated_chunks\\n        ]\\n\\n    def split_text(self, text: str) -> List[Document]:\\n        \"\"\"Split markdown file\\n        Args:\\n            text: Markdown file\"\"\"',\n",
              "  'class CharacterTextSplitter(TextSplitter):\\n    \"\"\"Splitting text that looks at characters.\"\"\"\\n\\n    def __init__(\\n        self, separator: str = \"\\\\n\\\\n\", is_separator_regex: bool = False, **kwargs: Any\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(**kwargs)\\n        self._separator = separator\\n        self._is_separator_regex = is_separator_regex\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        # First we naively split the large input into a bunch of smaller ones.\\n        separator = (\\n            self._separator if self._is_separator_regex else re.escape(self._separator)\\n        )\\n        splits = _split_text_with_regex(text, separator, self._keep_separator)\\n        _separator = \"\" if self._keep_separator else self._separator\\n        return self._merge_splits(splits, _separator)\\n\\n\\nclass LineType(TypedDict):\\n    \"\"\"Line type as typed dict.\"\"\"\\n\\n    metadata: Dict[str, str]\\n    content: str\\n\\n\\nclass HeaderType(TypedDict):\\n    \"\"\"Header type as typed dict.\"\"\"\\n\\n    level: int\\n    name: str\\n    data: str',\n",
              "  'class PythonCodeTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Python syntax.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a PythonCodeTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.PYTHON)\\n        super().__init__(separators=separators, **kwargs)\\n\\n\\nclass MarkdownTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Markdown-formatted headings.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a MarkdownTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.MARKDOWN)\\n        super().__init__(separators=separators, **kwargs)\\n\\n\\nclass LatexTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Latex-formatted layout elements.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a LatexTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.LATEX)\\n        super().__init__(separators=separators, **kwargs)',\n",
              "  'class CharacterTextSplitter(TextSplitter):\\n    \"\"\"Splitting text that looks at characters.\"\"\"\\n\\n    def __init__(\\n        self, separator: str = \"\\\\n\\\\n\", is_separator_regex: bool = False, **kwargs: Any\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(**kwargs)\\n        self._separator = separator\\n        self._is_separator_regex = is_separator_regex\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        # First we naively split the large input into a bunch of smaller ones.\\n        separator = (\\n            self._separator if self._is_separator_regex else re.escape(self._separator)\\n        )\\n        splits = _split_text_with_regex(text, separator, self._keep_separator)\\n        _separator = \"\" if self._keep_separator else self._separator\\n        return self._merge_splits(splits, _separator)\\n\\n\\nclass LineType(TypedDict):\\n    \"\"\"Line type as typed dict.\"\"\"\\n\\n    metadata: Dict[str, str]\\n    content: str\\n\\n\\nclass HeaderType(TypedDict):\\n    \"\"\"Header type as typed dict.\"\"\"\\n\\n    level: int\\n    name: str\\n    data: str',\n",
              "  'if issubclass(cls, TokenTextSplitter):\\n            extra_kwargs = {\\n                \"encoding_name\": encoding_name,\\n                \"model_name\": model_name,\\n                \"allowed_special\": allowed_special,\\n                \"disallowed_special\": disallowed_special,\\n            }\\n            kwargs = {**kwargs, **extra_kwargs}\\n\\n        return cls(length_function=_tiktoken_encoder, **kwargs)\\n\\n    def transform_documents(\\n        self, documents: Sequence[Document], **kwargs: Any\\n    ) -> Sequence[Document]:\\n        \"\"\"Transform sequence of documents by splitting them.\"\"\"\\n        return self.split_documents(list(documents))\\n\\n    async def atransform_documents(\\n        self, documents: Sequence[Document], **kwargs: Any\\n    ) -> Sequence[Document]:\\n        \"\"\"Asynchronously transform a sequence of documents by splitting them.\"\"\"\\n        return await asyncio.get_running_loop().run_in_executor(\\n            None, partial(self.transform_documents, **kwargs), documents\\n        )',\n",
              "  'if not isinstance(tokenizer, PreTrainedTokenizerBase):\\n                raise ValueError(\\n                    \"Tokenizer received was not an instance of PreTrainedTokenizerBase\"\\n                )\\n\\n            def _huggingface_tokenizer_length(text: str) -> int:\\n                return len(tokenizer.encode(text))\\n\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import transformers python package. \"\\n                \"Please install it with `pip install transformers`.\"\\n            )\\n        return cls(length_function=_huggingface_tokenizer_length, **kwargs)\\n\\n    @classmethod\\n    def from_tiktoken_encoder(\\n        cls: Type[TS],\\n        encoding_name: str = \"gpt2\",\\n        model_name: Optional[str] = None,\\n        allowed_special: Union[Literal[\"all\"], AbstractSet[str]] = set(),\\n        disallowed_special: Union[Literal[\"all\"], Collection[str]] = \"all\",\\n        **kwargs: Any,\\n    ) -> TS:\\n        \"\"\"Text splitter that uses tiktoken encoder to count length.\"\"\"\\n        try:\\n            import tiktoken\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import tiktoken python package. \"\\n                \"This is needed in order to calculate max_tokens_for_prompt. \"\\n                \"Please install it with `pip install tiktoken`.\"\\n            )\\n\\n        if model_name is not None:\\n            enc = tiktoken.encoding_for_model(model_name)\\n        else:\\n            enc = tiktoken.get_encoding(encoding_name)\\n\\n        def _tiktoken_encoder(text: str) -> int:\\n            return len(\\n                enc.encode(\\n                    text,\\n                    allowed_special=allowed_special,\\n                    disallowed_special=disallowed_special,\\n                )\\n            )',\n",
              "  'docs = []\\n        current_doc: List[str] = []\\n        total = 0\\n        for d in splits:\\n            _len = self._length_function(d)\\n            if (\\n                total + _len + (separator_len if len(current_doc) > 0 else 0)\\n                > self._chunk_size\\n            ):\\n                if total > self._chunk_size:\\n                    logger.warning(\\n                        f\"Created a chunk of size {total}, \"\\n                        f\"which is longer than the specified {self._chunk_size}\"\\n                    )\\n                if len(current_doc) > 0:\\n                    doc = self._join_docs(current_doc, separator)\\n                    if doc is not None:\\n                        docs.append(doc)\\n                    # Keep on popping if:\\n                    # - we have a larger chunk than in the chunk overlap\\n                    # - or if we still have any chunks and the length is long\\n                    while total > self._chunk_overlap or (\\n                        total + _len + (separator_len if len(current_doc) > 0 else 0)\\n                        > self._chunk_size\\n                        and total > 0\\n                    ):\\n                        total -= self._length_function(current_doc[0]) + (\\n                            separator_len if len(current_doc) > 1 else 0\\n                        )\\n                        current_doc = current_doc[1:]\\n            current_doc.append(d)\\n            total += _len + (separator_len if len(current_doc) > 1 else 0)\\n        doc = self._join_docs(current_doc, separator)\\n        if doc is not None:\\n            docs.append(doc)\\n        return docs\\n\\n    @classmethod\\n    def from_huggingface_tokenizer(cls, tokenizer: Any, **kwargs: Any) -> TextSplitter:\\n        \"\"\"Text splitter that uses HuggingFace tokenizer to count length.\"\"\"\\n        try:\\n            from transformers import PreTrainedTokenizerBase',\n",
              "  '@abstractmethod\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split text into multiple components.\"\"\"\\n\\n    def create_documents(\\n        self, texts: List[str], metadatas: Optional[List[dict]] = None\\n    ) -> List[Document]:\\n        \"\"\"Create documents from a list of texts.\"\"\"\\n        _metadatas = metadatas or [{}] * len(texts)\\n        documents = []\\n        for i, text in enumerate(texts):\\n            index = -1\\n            for chunk in self.split_text(text):\\n                metadata = copy.deepcopy(_metadatas[i])\\n                if self._add_start_index:\\n                    index = text.find(chunk, index + 1)\\n                    metadata[\"start_index\"] = index\\n                new_doc = Document(page_content=chunk, metadata=metadata)\\n                documents.append(new_doc)\\n        return documents\\n\\n    def split_documents(self, documents: Iterable[Document]) -> List[Document]:\\n        \"\"\"Split documents.\"\"\"\\n        texts, metadatas = [], []\\n        for doc in documents:\\n            texts.append(doc.page_content)\\n            metadatas.append(doc.metadata)\\n        return self.create_documents(texts, metadatas=metadatas)\\n\\n    def _join_docs(self, docs: List[str], separator: str) -> Optional[str]:\\n        text = separator.join(docs)\\n        if self._strip_whitespace:\\n            text = text.strip()\\n        if text == \"\":\\n            return None\\n        else:\\n            return text\\n\\n    def _merge_splits(self, splits: Iterable[str], separator: str) -> List[str]:\\n        # We now want to combine these smaller pieces into medium size\\n        # chunks to send to the LLM.\\n        separator_len = self._length_function(separator)',\n",
              "  'class TextSplitter(BaseDocumentTransformer, ABC):\\n    \"\"\"Interface for splitting text into chunks.\"\"\"\\n\\n    def __init__(\\n        self,\\n        chunk_size: int = 4000,\\n        chunk_overlap: int = 200,\\n        length_function: Callable[[str], int] = len,\\n        keep_separator: bool = False,\\n        add_start_index: bool = False,\\n        strip_whitespace: bool = True,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\\n\\n        Args:\\n            chunk_size: Maximum size of chunks to return\\n            chunk_overlap: Overlap in characters between chunks\\n            length_function: Function that measures the length of given chunks\\n            keep_separator: Whether to keep the separator in the chunks\\n            add_start_index: If `True`, includes chunk\\'s start index in metadata\\n            strip_whitespace: If `True`, strips whitespace from the start and end of\\n                              every document\\n        \"\"\"\\n        if chunk_overlap > chunk_size:\\n            raise ValueError(\\n                f\"Got a larger chunk overlap ({chunk_overlap}) than chunk size \"\\n                f\"({chunk_size}), should be smaller.\"\\n            )\\n        self._chunk_size = chunk_size\\n        self._chunk_overlap = chunk_overlap\\n        self._length_function = length_function\\n        self._keep_separator = keep_separator\\n        self._add_start_index = add_start_index\\n        self._strip_whitespace = strip_whitespace\\n\\n    @abstractmethod\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split text into multiple components.\"\"\"',\n",
              "  'def _split_text_with_regex(\\n    text: str, separator: str, keep_separator: bool\\n) -> List[str]:\\n    # Now that we have the separator, split the text\\n    if separator:\\n        if keep_separator:\\n            # The parentheses in the pattern keep the delimiters in the result.\\n            _splits = re.split(f\"({separator})\", text)\\n            splits = [_splits[i] + _splits[i + 1] for i in range(1, len(_splits), 2)]\\n            if len(_splits) % 2 == 0:\\n                splits += _splits[-1:]\\n            splits = [_splits[0]] + splits\\n        else:\\n            splits = re.split(separator, text)\\n    else:\\n        splits = list(text)\\n    return [s for s in splits if s != \"\"]',\n",
              "  '\"\"\"**Text Splitters** are classes for splitting text.\\n\\n\\n**Class hierarchy:**\\n\\n.. code-block::\\n\\n    BaseDocumentTransformer --> TextSplitter --> <name>TextSplitter  # Example: CharacterTextSplitter\\n                                                 RecursiveCharacterTextSplitter -->  <name>TextSplitter\\n\\nNote: **MarkdownHeaderTextSplitter** and **HTMLHeaderTextSplitter do not derive from TextSplitter.\\n\\n\\n**Main helpers:**\\n\\n.. code-block::\\n\\n    Document, Tokenizer, Language, LineType, HeaderType\\n\\n\"\"\"  # noqa: E501\\n\\nfrom __future__ import annotations\\n\\nimport asyncio\\nimport copy\\nimport logging\\nimport pathlib\\nimport re\\nfrom abc import ABC, abstractmethod\\nfrom dataclasses import dataclass\\nfrom enum import Enum\\nfrom functools import partial\\nfrom io import BytesIO, StringIO\\nfrom typing import (\\n    AbstractSet,\\n    Any,\\n    Callable,\\n    Collection,\\n    Dict,\\n    Iterable,\\n    List,\\n    Literal,\\n    Optional,\\n    Sequence,\\n    Tuple,\\n    Type,\\n    TypedDict,\\n    TypeVar,\\n    Union,\\n    cast,\\n)\\n\\nimport requests\\nfrom langchain_core.documents import BaseDocumentTransformer, Document\\n\\nlogger = logging.getLogger(__name__)\\n\\nTS = TypeVar(\"TS\", bound=\"TextSplitter\")\\n\\n\\ndef _make_spacy_pipeline_for_splitting(\\n    pipeline: str, *, max_length: int = 1_000_000\\n) -> Any:  # avoid importing spacy\\n    try:\\n        import spacy\\n    except ImportError:\\n        raise ImportError(\\n            \"Spacy is not installed, please install it with `pip install spacy`.\"\\n        )\\n    if pipeline == \"sentencizer\":\\n        from spacy.lang.en import English\\n\\n        sentencizer = English()\\n        sentencizer.add_pipe(\"sentencizer\")\\n    else:\\n        sentencizer = spacy.load(pipeline, exclude=[\"ner\", \"tagger\"])\\n        sentencizer.max_length = max_length\\n    return sentencizer',\n",
              "  '\"\"\"**Text Splitters** are classes for splitting text.\\n\\n\\n**Class hierarchy:**\\n\\n.. code-block::\\n\\n    BaseDocumentTransformer --> TextSplitter --> <name>TextSplitter  # Example: CharacterTextSplitter\\n                                                 RecursiveCharacterTextSplitter -->  <name>TextSplitter\\n\\nNote: **MarkdownHeaderTextSplitter** and **HTMLHeaderTextSplitter do not derive from TextSplitter.\\n\\n\\n**Main helpers:**\\n\\n.. code-block::\\n\\n    Document, Tokenizer, Language, LineType, HeaderType\\n\\n\"\"\"  # noqa: E501\\n\\nfrom __future__ import annotations\\n\\nimport asyncio\\nimport copy\\nimport logging\\nimport pathlib\\nimport re\\nfrom abc import ABC, abstractmethod\\nfrom dataclasses import dataclass\\nfrom enum import Enum\\nfrom functools import partial\\nfrom io import BytesIO, StringIO\\nfrom typing import (\\n    AbstractSet,\\n    Any,\\n    Callable,\\n    Collection,\\n    Dict,\\n    Iterable,\\n    List,\\n    Literal,\\n    Optional,\\n    Sequence,\\n    Tuple,\\n    Type,\\n    TypedDict,\\n    TypeVar,\\n    Union,\\n    cast,\\n)\\n\\nimport requests\\nfrom langchain_core.documents import BaseDocumentTransformer, Document\\n\\nlogger = logging.getLogger(__name__)\\n\\nTS = TypeVar(\"TS\", bound=\"TextSplitter\")\\n\\n\\ndef _make_spacy_pipeline_for_splitting(\\n    pipeline: str, *, max_length: int = 1_000_000\\n) -> Any:  # avoid importing spacy\\n    try:\\n        import spacy\\n    except ImportError:\\n        raise ImportError(\\n            \"Spacy is not installed, please install it with `pip install spacy`.\"\\n        )\\n    if pipeline == \"sentencizer\":\\n        from spacy.lang.en import English\\n\\n        sentencizer = English()\\n        sentencizer.add_pipe(\"sentencizer\")\\n    else:\\n        sentencizer = spacy.load(pipeline, exclude=[\"ner\", \"tagger\"])\\n        sentencizer.max_length = max_length\\n    return sentencizer',\n",
              "  'class NLTKTextSplitter(TextSplitter):\\n    \"\"\"Splitting text using NLTK package.\"\"\"\\n\\n    def __init__(\\n        self, separator: str = \"\\\\n\\\\n\", language: str = \"english\", **kwargs: Any\\n    ) -> None:\\n        \"\"\"Initialize the NLTK splitter.\"\"\"\\n        super().__init__(**kwargs)\\n        try:\\n            from nltk.tokenize import sent_tokenize\\n\\n            self._tokenizer = sent_tokenize\\n        except ImportError:\\n            raise ImportError(\\n                \"NLTK is not installed, please install it with `pip install nltk`.\"\\n            )\\n        self._separator = separator\\n        self._language = language\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        # First we naively split the large input into a bunch of smaller ones.\\n        splits = self._tokenizer(text, language=self._language)\\n        return self._merge_splits(splits, self._separator)\\n\\n\\nclass SpacyTextSplitter(TextSplitter):\\n    \"\"\"Splitting text using Spacy package.\\n\\n\\n    Per default, Spacy\\'s `en_core_web_sm` model is used and\\n    its default max_length is 1000000 (it is the length of maximum character\\n    this model takes which can be increased for large files). For a faster, but\\n    potentially less accurate splitting, you can use `pipeline=\\'sentencizer\\'`.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        separator: str = \"\\\\n\\\\n\",\\n        pipeline: str = \"en_core_web_sm\",\\n        max_length: int = 1_000_000,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Initialize the spacy text splitter.\"\"\"\\n        super().__init__(**kwargs)\\n        self._tokenizer = _make_spacy_pipeline_for_splitting(\\n            pipeline, max_length=max_length\\n        )\\n        self._separator = separator\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        splits = (s.text for s in self._tokenizer(text).sents)\\n        return self._merge_splits(splits, self._separator)\\n\\n\\n# For backwards compatibility',\n",
              "  'else:\\n            raise ValueError(\\n                f\"Language {language} is not supported! \"\\n                f\"Please choose from {list(Language)}\"\\n            )',\n",
              "  '\"\\\\nevent \",\\n                \"\\\\nmodifier \",\\n                \"\\\\nerror \",\\n                \"\\\\nstruct \",\\n                \"\\\\nenum \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo while \",\\n                \"\\\\nassembly \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.COBOL:\\n            return [\\n                # Split along divisions\\n                \"\\\\nIDENTIFICATION DIVISION.\",\\n                \"\\\\nENVIRONMENT DIVISION.\",\\n                \"\\\\nDATA DIVISION.\",\\n                \"\\\\nPROCEDURE DIVISION.\",\\n                # Split along sections within DATA DIVISION\\n                \"\\\\nWORKING-STORAGE SECTION.\",\\n                \"\\\\nLINKAGE SECTION.\",\\n                \"\\\\nFILE SECTION.\",\\n                # Split along sections within PROCEDURE DIVISION\\n                \"\\\\nINPUT-OUTPUT SECTION.\",\\n                # Split along paragraphs and common statements\\n                \"\\\\nOPEN \",\\n                \"\\\\nCLOSE \",\\n                \"\\\\nREAD \",\\n                \"\\\\nWRITE \",\\n                \"\\\\nIF \",\\n                \"\\\\nELSE \",\\n                \"\\\\nMOVE \",\\n                \"\\\\nPERFORM \",\\n                \"\\\\nUNTIL \",\\n                \"\\\\nVARYING \",\\n                \"\\\\nACCEPT \",\\n                \"\\\\nDISPLAY \",\\n                \"\\\\nSTOP RUN.\",\\n                # Split by the normal type of lines\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]',\n",
              "  '\"<nav\",\\n                # Head\\n                \"<head\",\\n                \"<style\",\\n                \"<script\",\\n                \"<meta\",\\n                \"<title\",\\n                \"\",\\n            ]\\n        elif language == Language.CSHARP:\\n            return [\\n                \"\\\\ninterface \",\\n                \"\\\\nenum \",\\n                \"\\\\nimplements \",\\n                \"\\\\ndelegate \",\\n                \"\\\\nevent \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nabstract \",\\n                # Split along method definitions\\n                \"\\\\npublic \",\\n                \"\\\\nprotected \",\\n                \"\\\\nprivate \",\\n                \"\\\\nstatic \",\\n                \"\\\\nreturn \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\ncontinue \",\\n                \"\\\\nfor \",\\n                \"\\\\nforeach \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\nbreak \",\\n                \"\\\\ncase \",\\n                \"\\\\nelse \",\\n                # Split by exceptions\\n                \"\\\\ntry \",\\n                \"\\\\nthrow \",\\n                \"\\\\nfinally \",\\n                \"\\\\ncatch \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SOL:\\n            return [\\n                # Split along compiler information definitions\\n                \"\\\\npragma \",\\n                \"\\\\nusing \",\\n                # Split along contract definitions\\n                \"\\\\ncontract \",\\n                \"\\\\ninterface \",\\n                \"\\\\nlibrary \",\\n                # Split along method definitions\\n                \"\\\\nconstructor \",\\n                \"\\\\ntype \",\\n                \"\\\\nfunction \",\\n                \"\\\\nevent \",\\n                \"\\\\nmodifier \",\\n                \"\\\\nerror \",\\n                \"\\\\nstruct \",\\n                \"\\\\nenum \",\\n                # Split along control flow statements',\n",
              "  '\"\\\\n\\\\\\\\*\\\\\\\\*\\\\\\\\*+\\\\n\",\\n                \"\\\\n---+\\\\n\",\\n                \"\\\\n___+\\\\n\",\\n                # Note that this splitter doesn\\'t handle horizontal lines defined\\n                # by *three or more* of ***, ---, or ___, but this is not handled\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.LATEX:\\n            return [\\n                # First, try to split along Latex sections\\n                \"\\\\n\\\\\\\\\\\\\\\\chapter{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\section{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\subsection{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\subsubsection{\",\\n                # Now split by environments\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{enumerate}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{itemize}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{description}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{list}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{quote}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{quotation}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{verse}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{verbatim}\",\\n                # Now split by math environments\\n                \"\\\\n\\\\\\\\\\\\begin{align}\",\\n                \"$$\",\\n                \"$\",\\n                # Now split by the normal type of lines\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.HTML:\\n            return [\\n                # First, try to split along HTML tags\\n                \"<body\",\\n                \"<div\",\\n                \"<p\",\\n                \"<br\",\\n                \"<li\",\\n                \"<h1\",\\n                \"<h2\",\\n                \"<h3\",\\n                \"<h4\",\\n                \"<h5\",\\n                \"<h6\",\\n                \"<span\",\\n                \"<table\",\\n                \"<tr\",\\n                \"<td\",\\n                \"<th\",\\n                \"<ul\",\\n                \"<ol\",\\n                \"<header\",\\n                \"<footer\",\\n                \"<nav\",\\n                # Head\\n                \"<head\",\\n                \"<style\",\\n                \"<script\",\\n                \"<meta\",\\n                \"<title\",',\n",
              "  '\" \",\\n                \"\",\\n            ]\\n        elif language == Language.SCALA:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nobject \",\\n                # Split along method definitions\\n                \"\\\\ndef \",\\n                \"\\\\nval \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nmatch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SWIFT:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunc \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nstruct \",\\n                \"\\\\nenum \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.MARKDOWN:\\n            return [\\n                # First, try to split along Markdown headings (starting with level 2)\\n                \"\\\\n#{1,6} \",\\n                # Note the alternative syntax for headings (below) is not handled here\\n                # Heading level 2\\n                # ---------------\\n                # End of code block\\n                \"```\\\\n\",\\n                # Horizontal lines\\n                \"\\\\n\\\\\\\\*\\\\\\\\*\\\\\\\\*+\\\\n\",\\n                \"\\\\n---+\\\\n\",\\n                \"\\\\n___+\\\\n\",\\n                # Note that this splitter doesn\\'t handle horizontal lines defined',\n",
              "  'return [\\n                # First, try to split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\ndef \",\\n                \"\\\\n\\\\tdef \",\\n                # Now split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.RST:\\n            return [\\n                # Split along section titles\\n                \"\\\\n=+\\\\n\",\\n                \"\\\\n-+\\\\n\",\\n                \"\\\\n\\\\\\\\*+\\\\n\",\\n                # Split along directive markers\\n                \"\\\\n\\\\n.. *\\\\n\\\\n\",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.RUBY:\\n            return [\\n                # Split along method definitions\\n                \"\\\\ndef \",\\n                \"\\\\nclass \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nunless \",\\n                \"\\\\nwhile \",\\n                \"\\\\nfor \",\\n                \"\\\\ndo \",\\n                \"\\\\nbegin \",\\n                \"\\\\nrescue \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.RUST:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfn \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nwhile \",\\n                \"\\\\nfor \",\\n                \"\\\\nloop \",\\n                \"\\\\nmatch \",\\n                \"\\\\nconst \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SCALA:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",',\n",
              "  '\"\\\\nfunction \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                \"\\\\ndefault \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.PHP:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunction \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nforeach \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.PROTO:\\n            return [\\n                # Split along message definitions\\n                \"\\\\nmessage \",\\n                # Split along service definitions\\n                \"\\\\nservice \",\\n                # Split along enum definitions\\n                \"\\\\nenum \",\\n                # Split along option definitions\\n                \"\\\\noption \",\\n                # Split along import statements\\n                \"\\\\nimport \",\\n                # Split along syntax declarations\\n                \"\\\\nsyntax \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.PYTHON:\\n            return [\\n                # First, try to split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\ndef \",\\n                \"\\\\n\\\\tdef \",',\n",
              "  'def _split_text_with_regex(\\n    text: str, separator: str, keep_separator: bool\\n) -> List[str]:\\n    # Now that we have the separator, split the text\\n    if separator:\\n        if keep_separator:\\n            # The parentheses in the pattern keep the delimiters in the result.\\n            _splits = re.split(f\"({separator})\", text)\\n            splits = [_splits[i] + _splits[i + 1] for i in range(1, len(_splits), 2)]\\n            if len(_splits) % 2 == 0:\\n                splits += _splits[-1:]\\n            splits = [_splits[0]] + splits\\n        else:\\n            splits = re.split(separator, text)\\n    else:\\n        splits = list(text)\\n    return [s for s in splits if s != \"\"]',\n",
              "  'if in_code_block:\\n                current_content.append(stripped_line)\\n                continue\\n\\n            # Check each line against each of the header types (e.g., #, ##)\\n            for sep, name in self.headers_to_split_on:\\n                # Check if line starts with a header that we intend to split on\\n                if stripped_line.startswith(sep) and (\\n                    # Header with no text OR header is followed by space\\n                    # Both are valid conditions that sep is being used a header\\n                    len(stripped_line) == len(sep) or stripped_line[len(sep)] == \" \"\\n                ):\\n                    # Ensure we are tracking the header as metadata\\n                    if name is not None:\\n                        # Get the current header level\\n                        current_header_level = sep.count(\"#\")\\n\\n                        # Pop out headers of lower or same level from the stack\\n                        while (\\n                            header_stack\\n                            and header_stack[-1][\"level\"] >= current_header_level\\n                        ):\\n                            # We have encountered a new header\\n                            # at the same or higher level\\n                            popped_header = header_stack.pop()\\n                            # Clear the metadata for the\\n                            # popped header in initial_metadata\\n                            if popped_header[\"name\"] in initial_metadata:\\n                                initial_metadata.pop(popped_header[\"name\"])',\n",
              "  'def split_text(self, text: str) -> List[Document]:\\n        \"\"\"Split markdown file\\n        Args:\\n            text: Markdown file\"\"\"\\n\\n        # Split the input text by newline character (\"\\\\n\").\\n        lines = text.split(\"\\\\n\")\\n        # Final output\\n        lines_with_metadata: List[LineType] = []\\n        # Content and metadata of the chunk currently being processed\\n        current_content: List[str] = []\\n        current_metadata: Dict[str, str] = {}\\n        # Keep track of the nested header structure\\n        # header_stack: List[Dict[str, Union[int, str]]] = []\\n        header_stack: List[HeaderType] = []\\n        initial_metadata: Dict[str, str] = {}\\n\\n        in_code_block = False\\n        opening_fence = \"\"\\n\\n        for line in lines:\\n            stripped_line = line.strip()\\n\\n            if not in_code_block:\\n                # Exclude inline code spans\\n                if stripped_line.startswith(\"```\") and stripped_line.count(\"```\") == 1:\\n                    in_code_block = True\\n                    opening_fence = \"```\"\\n                elif stripped_line.startswith(\"~~~\"):\\n                    in_code_block = True\\n                    opening_fence = \"~~~\"\\n            else:\\n                if stripped_line.startswith(opening_fence):\\n                    in_code_block = False\\n                    opening_fence = \"\"\\n\\n            if in_code_block:\\n                current_content.append(stripped_line)\\n                continue',\n",
              "  'class MarkdownHeaderTextSplitter:\\n    \"\"\"Splitting markdown files based on specified headers.\"\"\"\\n\\n    def __init__(\\n        self, headers_to_split_on: List[Tuple[str, str]], return_each_line: bool = False\\n    ):\\n        \"\"\"Create a new MarkdownHeaderTextSplitter.\\n\\n        Args:\\n            headers_to_split_on: Headers we want to track\\n            return_each_line: Return each line w/ associated headers\\n        \"\"\"\\n        # Output line-by-line or aggregated into chunks w/ common headers\\n        self.return_each_line = return_each_line\\n        # Given the headers we want to split on,\\n        # (e.g., \"#, ##, etc\") order by length\\n        self.headers_to_split_on = sorted(\\n            headers_to_split_on, key=lambda split: len(split[0]), reverse=True\\n        )\\n\\n    def aggregate_lines_to_chunks(self, lines: List[LineType]) -> List[Document]:\\n        \"\"\"Combine lines with common metadata into chunks\\n        Args:\\n            lines: Line of text / associated header metadata\\n        \"\"\"\\n        aggregated_chunks: List[LineType] = []\\n\\n        for line in lines:\\n            if (\\n                aggregated_chunks\\n                and aggregated_chunks[-1][\"metadata\"] == line[\"metadata\"]\\n            ):\\n                # If the last line in the aggregated list\\n                # has the same metadata as the current line,\\n                # append the current content to the last lines\\'s content\\n                aggregated_chunks[-1][\"content\"] += \"  \\\\n\" + line[\"content\"]\\n            else:\\n                # Otherwise, append the current line to the aggregated list\\n                aggregated_chunks.append(line)\\n\\n        return [\\n            Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n            for chunk in aggregated_chunks\\n        ]\\n\\n    def split_text(self, text: str) -> List[Document]:\\n        \"\"\"Split markdown file\\n        Args:\\n            text: Markdown file\"\"\"',\n",
              "  'class CharacterTextSplitter(TextSplitter):\\n    \"\"\"Splitting text that looks at characters.\"\"\"\\n\\n    def __init__(\\n        self, separator: str = \"\\\\n\\\\n\", is_separator_regex: bool = False, **kwargs: Any\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(**kwargs)\\n        self._separator = separator\\n        self._is_separator_regex = is_separator_regex\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        # First we naively split the large input into a bunch of smaller ones.\\n        separator = (\\n            self._separator if self._is_separator_regex else re.escape(self._separator)\\n        )\\n        splits = _split_text_with_regex(text, separator, self._keep_separator)\\n        _separator = \"\" if self._keep_separator else self._separator\\n        return self._merge_splits(splits, _separator)\\n\\n\\nclass LineType(TypedDict):\\n    \"\"\"Line type as typed dict.\"\"\"\\n\\n    metadata: Dict[str, str]\\n    content: str\\n\\n\\nclass HeaderType(TypedDict):\\n    \"\"\"Header type as typed dict.\"\"\"\\n\\n    level: int\\n    name: str\\n    data: str',\n",
              "  'if issubclass(cls, TokenTextSplitter):\\n            extra_kwargs = {\\n                \"encoding_name\": encoding_name,\\n                \"model_name\": model_name,\\n                \"allowed_special\": allowed_special,\\n                \"disallowed_special\": disallowed_special,\\n            }\\n            kwargs = {**kwargs, **extra_kwargs}\\n\\n        return cls(length_function=_tiktoken_encoder, **kwargs)\\n\\n    def transform_documents(\\n        self, documents: Sequence[Document], **kwargs: Any\\n    ) -> Sequence[Document]:\\n        \"\"\"Transform sequence of documents by splitting them.\"\"\"\\n        return self.split_documents(list(documents))\\n\\n    async def atransform_documents(\\n        self, documents: Sequence[Document], **kwargs: Any\\n    ) -> Sequence[Document]:\\n        \"\"\"Asynchronously transform a sequence of documents by splitting them.\"\"\"\\n        return await asyncio.get_running_loop().run_in_executor(\\n            None, partial(self.transform_documents, **kwargs), documents\\n        )',\n",
              "  'if not isinstance(tokenizer, PreTrainedTokenizerBase):\\n                raise ValueError(\\n                    \"Tokenizer received was not an instance of PreTrainedTokenizerBase\"\\n                )\\n\\n            def _huggingface_tokenizer_length(text: str) -> int:\\n                return len(tokenizer.encode(text))\\n\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import transformers python package. \"\\n                \"Please install it with `pip install transformers`.\"\\n            )\\n        return cls(length_function=_huggingface_tokenizer_length, **kwargs)\\n\\n    @classmethod\\n    def from_tiktoken_encoder(\\n        cls: Type[TS],\\n        encoding_name: str = \"gpt2\",\\n        model_name: Optional[str] = None,\\n        allowed_special: Union[Literal[\"all\"], AbstractSet[str]] = set(),\\n        disallowed_special: Union[Literal[\"all\"], Collection[str]] = \"all\",\\n        **kwargs: Any,\\n    ) -> TS:\\n        \"\"\"Text splitter that uses tiktoken encoder to count length.\"\"\"\\n        try:\\n            import tiktoken\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import tiktoken python package. \"\\n                \"This is needed in order to calculate max_tokens_for_prompt. \"\\n                \"Please install it with `pip install tiktoken`.\"\\n            )\\n\\n        if model_name is not None:\\n            enc = tiktoken.encoding_for_model(model_name)\\n        else:\\n            enc = tiktoken.get_encoding(encoding_name)\\n\\n        def _tiktoken_encoder(text: str) -> int:\\n            return len(\\n                enc.encode(\\n                    text,\\n                    allowed_special=allowed_special,\\n                    disallowed_special=disallowed_special,\\n                )\\n            )',\n",
              "  'docs = []\\n        current_doc: List[str] = []\\n        total = 0\\n        for d in splits:\\n            _len = self._length_function(d)\\n            if (\\n                total + _len + (separator_len if len(current_doc) > 0 else 0)\\n                > self._chunk_size\\n            ):\\n                if total > self._chunk_size:\\n                    logger.warning(\\n                        f\"Created a chunk of size {total}, \"\\n                        f\"which is longer than the specified {self._chunk_size}\"\\n                    )\\n                if len(current_doc) > 0:\\n                    doc = self._join_docs(current_doc, separator)\\n                    if doc is not None:\\n                        docs.append(doc)\\n                    # Keep on popping if:\\n                    # - we have a larger chunk than in the chunk overlap\\n                    # - or if we still have any chunks and the length is long\\n                    while total > self._chunk_overlap or (\\n                        total + _len + (separator_len if len(current_doc) > 0 else 0)\\n                        > self._chunk_size\\n                        and total > 0\\n                    ):\\n                        total -= self._length_function(current_doc[0]) + (\\n                            separator_len if len(current_doc) > 1 else 0\\n                        )\\n                        current_doc = current_doc[1:]\\n            current_doc.append(d)\\n            total += _len + (separator_len if len(current_doc) > 1 else 0)\\n        doc = self._join_docs(current_doc, separator)\\n        if doc is not None:\\n            docs.append(doc)\\n        return docs\\n\\n    @classmethod\\n    def from_huggingface_tokenizer(cls, tokenizer: Any, **kwargs: Any) -> TextSplitter:\\n        \"\"\"Text splitter that uses HuggingFace tokenizer to count length.\"\"\"\\n        try:\\n            from transformers import PreTrainedTokenizerBase',\n",
              "  '@abstractmethod\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split text into multiple components.\"\"\"\\n\\n    def create_documents(\\n        self, texts: List[str], metadatas: Optional[List[dict]] = None\\n    ) -> List[Document]:\\n        \"\"\"Create documents from a list of texts.\"\"\"\\n        _metadatas = metadatas or [{}] * len(texts)\\n        documents = []\\n        for i, text in enumerate(texts):\\n            index = -1\\n            for chunk in self.split_text(text):\\n                metadata = copy.deepcopy(_metadatas[i])\\n                if self._add_start_index:\\n                    index = text.find(chunk, index + 1)\\n                    metadata[\"start_index\"] = index\\n                new_doc = Document(page_content=chunk, metadata=metadata)\\n                documents.append(new_doc)\\n        return documents\\n\\n    def split_documents(self, documents: Iterable[Document]) -> List[Document]:\\n        \"\"\"Split documents.\"\"\"\\n        texts, metadatas = [], []\\n        for doc in documents:\\n            texts.append(doc.page_content)\\n            metadatas.append(doc.metadata)\\n        return self.create_documents(texts, metadatas=metadatas)\\n\\n    def _join_docs(self, docs: List[str], separator: str) -> Optional[str]:\\n        text = separator.join(docs)\\n        if self._strip_whitespace:\\n            text = text.strip()\\n        if text == \"\":\\n            return None\\n        else:\\n            return text\\n\\n    def _merge_splits(self, splits: Iterable[str], separator: str) -> List[str]:\\n        # We now want to combine these smaller pieces into medium size\\n        # chunks to send to the LLM.\\n        separator_len = self._length_function(separator)',\n",
              "  'class TextSplitter(BaseDocumentTransformer, ABC):\\n    \"\"\"Interface for splitting text into chunks.\"\"\"\\n\\n    def __init__(\\n        self,\\n        chunk_size: int = 4000,\\n        chunk_overlap: int = 200,\\n        length_function: Callable[[str], int] = len,\\n        keep_separator: bool = False,\\n        add_start_index: bool = False,\\n        strip_whitespace: bool = True,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\\n\\n        Args:\\n            chunk_size: Maximum size of chunks to return\\n            chunk_overlap: Overlap in characters between chunks\\n            length_function: Function that measures the length of given chunks\\n            keep_separator: Whether to keep the separator in the chunks\\n            add_start_index: If `True`, includes chunk\\'s start index in metadata\\n            strip_whitespace: If `True`, strips whitespace from the start and end of\\n                              every document\\n        \"\"\"\\n        if chunk_overlap > chunk_size:\\n            raise ValueError(\\n                f\"Got a larger chunk overlap ({chunk_overlap}) than chunk size \"\\n                f\"({chunk_size}), should be smaller.\"\\n            )\\n        self._chunk_size = chunk_size\\n        self._chunk_overlap = chunk_overlap\\n        self._length_function = length_function\\n        self._keep_separator = keep_separator\\n        self._add_start_index = add_start_index\\n        self._strip_whitespace = strip_whitespace\\n\\n    @abstractmethod\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split text into multiple components.\"\"\"',\n",
              "  '# Push the current header to the stack\\n                        header: HeaderType = {\\n                            \"level\": current_header_level,\\n                            \"name\": name,\\n                            \"data\": stripped_line[len(sep) :].strip(),\\n                        }\\n                        header_stack.append(header)\\n                        # Update initial_metadata with the current header\\n                        initial_metadata[name] = header[\"data\"]\\n\\n                    # Add the previous line to the lines_with_metadata\\n                    # only if current_content is not empty\\n                    if current_content:\\n                        lines_with_metadata.append(\\n                            {\\n                                \"content\": \"\\\\n\".join(current_content),\\n                                \"metadata\": current_metadata.copy(),\\n                            }\\n                        )\\n                        current_content.clear()\\n\\n                    break\\n            else:\\n                if stripped_line:\\n                    current_content.append(stripped_line)\\n                elif current_content:\\n                    lines_with_metadata.append(\\n                        {\\n                            \"content\": \"\\\\n\".join(current_content),\\n                            \"metadata\": current_metadata.copy(),\\n                        }\\n                    )\\n                    current_content.clear()\\n\\n            current_metadata = initial_metadata.copy()\\n\\n        if current_content:\\n            lines_with_metadata.append(\\n                {\"content\": \"\\\\n\".join(current_content), \"metadata\": current_metadata}\\n            )',\n",
              "  '\"\"\"**Text Splitters** are classes for splitting text.\\n\\n\\n**Class hierarchy:**\\n\\n.. code-block::\\n\\n    BaseDocumentTransformer --> TextSplitter --> <name>TextSplitter  # Example: CharacterTextSplitter\\n                                                 RecursiveCharacterTextSplitter -->  <name>TextSplitter\\n\\nNote: **MarkdownHeaderTextSplitter** and **HTMLHeaderTextSplitter do not derive from TextSplitter.\\n\\n\\n**Main helpers:**\\n\\n.. code-block::\\n\\n    Document, Tokenizer, Language, LineType, HeaderType\\n\\n\"\"\"  # noqa: E501\\n\\nfrom __future__ import annotations\\n\\nimport asyncio\\nimport copy\\nimport logging\\nimport pathlib\\nimport re\\nfrom abc import ABC, abstractmethod\\nfrom dataclasses import dataclass\\nfrom enum import Enum\\nfrom functools import partial\\nfrom io import BytesIO, StringIO\\nfrom typing import (\\n    AbstractSet,\\n    Any,\\n    Callable,\\n    Collection,\\n    Dict,\\n    Iterable,\\n    List,\\n    Literal,\\n    Optional,\\n    Sequence,\\n    Tuple,\\n    Type,\\n    TypedDict,\\n    TypeVar,\\n    Union,\\n    cast,\\n)\\n\\nimport requests\\nfrom langchain_core.documents import BaseDocumentTransformer, Document\\n\\nlogger = logging.getLogger(__name__)\\n\\nTS = TypeVar(\"TS\", bound=\"TextSplitter\")\\n\\n\\ndef _make_spacy_pipeline_for_splitting(\\n    pipeline: str, *, max_length: int = 1_000_000\\n) -> Any:  # avoid importing spacy\\n    try:\\n        import spacy\\n    except ImportError:\\n        raise ImportError(\\n            \"Spacy is not installed, please install it with `pip install spacy`.\"\\n        )\\n    if pipeline == \"sentencizer\":\\n        from spacy.lang.en import English\\n\\n        sentencizer = English()\\n        sentencizer.add_pipe(\"sentencizer\")\\n    else:\\n        sentencizer = spacy.load(pipeline, exclude=[\"ner\", \"tagger\"])\\n        sentencizer.max_length = max_length\\n    return sentencizer',\n",
              "  'class PythonCodeTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Python syntax.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a PythonCodeTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.PYTHON)\\n        super().__init__(separators=separators, **kwargs)\\n\\n\\nclass MarkdownTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Markdown-formatted headings.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a MarkdownTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.MARKDOWN)\\n        super().__init__(separators=separators, **kwargs)\\n\\n\\nclass LatexTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Latex-formatted layout elements.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a LatexTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.LATEX)\\n        super().__init__(separators=separators, **kwargs)',\n",
              "  'class NLTKTextSplitter(TextSplitter):\\n    \"\"\"Splitting text using NLTK package.\"\"\"\\n\\n    def __init__(\\n        self, separator: str = \"\\\\n\\\\n\", language: str = \"english\", **kwargs: Any\\n    ) -> None:\\n        \"\"\"Initialize the NLTK splitter.\"\"\"\\n        super().__init__(**kwargs)\\n        try:\\n            from nltk.tokenize import sent_tokenize\\n\\n            self._tokenizer = sent_tokenize\\n        except ImportError:\\n            raise ImportError(\\n                \"NLTK is not installed, please install it with `pip install nltk`.\"\\n            )\\n        self._separator = separator\\n        self._language = language\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        # First we naively split the large input into a bunch of smaller ones.\\n        splits = self._tokenizer(text, language=self._language)\\n        return self._merge_splits(splits, self._separator)\\n\\n\\nclass SpacyTextSplitter(TextSplitter):\\n    \"\"\"Splitting text using Spacy package.\\n\\n\\n    Per default, Spacy\\'s `en_core_web_sm` model is used and\\n    its default max_length is 1000000 (it is the length of maximum character\\n    this model takes which can be increased for large files). For a faster, but\\n    potentially less accurate splitting, you can use `pipeline=\\'sentencizer\\'`.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        separator: str = \"\\\\n\\\\n\",\\n        pipeline: str = \"en_core_web_sm\",\\n        max_length: int = 1_000_000,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Initialize the spacy text splitter.\"\"\"\\n        super().__init__(**kwargs)\\n        self._tokenizer = _make_spacy_pipeline_for_splitting(\\n            pipeline, max_length=max_length\\n        )\\n        self._separator = separator\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        splits = (s.text for s in self._tokenizer(text).sents)\\n        return self._merge_splits(splits, self._separator)\\n\\n\\n# For backwards compatibility',\n",
              "  'else:\\n            raise ValueError(\\n                f\"Language {language} is not supported! \"\\n                f\"Please choose from {list(Language)}\"\\n            )',\n",
              "  '\"\\\\nevent \",\\n                \"\\\\nmodifier \",\\n                \"\\\\nerror \",\\n                \"\\\\nstruct \",\\n                \"\\\\nenum \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo while \",\\n                \"\\\\nassembly \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.COBOL:\\n            return [\\n                # Split along divisions\\n                \"\\\\nIDENTIFICATION DIVISION.\",\\n                \"\\\\nENVIRONMENT DIVISION.\",\\n                \"\\\\nDATA DIVISION.\",\\n                \"\\\\nPROCEDURE DIVISION.\",\\n                # Split along sections within DATA DIVISION\\n                \"\\\\nWORKING-STORAGE SECTION.\",\\n                \"\\\\nLINKAGE SECTION.\",\\n                \"\\\\nFILE SECTION.\",\\n                # Split along sections within PROCEDURE DIVISION\\n                \"\\\\nINPUT-OUTPUT SECTION.\",\\n                # Split along paragraphs and common statements\\n                \"\\\\nOPEN \",\\n                \"\\\\nCLOSE \",\\n                \"\\\\nREAD \",\\n                \"\\\\nWRITE \",\\n                \"\\\\nIF \",\\n                \"\\\\nELSE \",\\n                \"\\\\nMOVE \",\\n                \"\\\\nPERFORM \",\\n                \"\\\\nUNTIL \",\\n                \"\\\\nVARYING \",\\n                \"\\\\nACCEPT \",\\n                \"\\\\nDISPLAY \",\\n                \"\\\\nSTOP RUN.\",\\n                # Split by the normal type of lines\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]',\n",
              "  '\"<nav\",\\n                # Head\\n                \"<head\",\\n                \"<style\",\\n                \"<script\",\\n                \"<meta\",\\n                \"<title\",\\n                \"\",\\n            ]\\n        elif language == Language.CSHARP:\\n            return [\\n                \"\\\\ninterface \",\\n                \"\\\\nenum \",\\n                \"\\\\nimplements \",\\n                \"\\\\ndelegate \",\\n                \"\\\\nevent \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nabstract \",\\n                # Split along method definitions\\n                \"\\\\npublic \",\\n                \"\\\\nprotected \",\\n                \"\\\\nprivate \",\\n                \"\\\\nstatic \",\\n                \"\\\\nreturn \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\ncontinue \",\\n                \"\\\\nfor \",\\n                \"\\\\nforeach \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\nbreak \",\\n                \"\\\\ncase \",\\n                \"\\\\nelse \",\\n                # Split by exceptions\\n                \"\\\\ntry \",\\n                \"\\\\nthrow \",\\n                \"\\\\nfinally \",\\n                \"\\\\ncatch \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SOL:\\n            return [\\n                # Split along compiler information definitions\\n                \"\\\\npragma \",\\n                \"\\\\nusing \",\\n                # Split along contract definitions\\n                \"\\\\ncontract \",\\n                \"\\\\ninterface \",\\n                \"\\\\nlibrary \",\\n                # Split along method definitions\\n                \"\\\\nconstructor \",\\n                \"\\\\ntype \",\\n                \"\\\\nfunction \",\\n                \"\\\\nevent \",\\n                \"\\\\nmodifier \",\\n                \"\\\\nerror \",\\n                \"\\\\nstruct \",\\n                \"\\\\nenum \",\\n                # Split along control flow statements',\n",
              "  '\"\\\\n\\\\\\\\*\\\\\\\\*\\\\\\\\*+\\\\n\",\\n                \"\\\\n---+\\\\n\",\\n                \"\\\\n___+\\\\n\",\\n                # Note that this splitter doesn\\'t handle horizontal lines defined\\n                # by *three or more* of ***, ---, or ___, but this is not handled\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.LATEX:\\n            return [\\n                # First, try to split along Latex sections\\n                \"\\\\n\\\\\\\\\\\\\\\\chapter{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\section{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\subsection{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\subsubsection{\",\\n                # Now split by environments\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{enumerate}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{itemize}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{description}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{list}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{quote}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{quotation}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{verse}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{verbatim}\",\\n                # Now split by math environments\\n                \"\\\\n\\\\\\\\\\\\begin{align}\",\\n                \"$$\",\\n                \"$\",\\n                # Now split by the normal type of lines\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.HTML:\\n            return [\\n                # First, try to split along HTML tags\\n                \"<body\",\\n                \"<div\",\\n                \"<p\",\\n                \"<br\",\\n                \"<li\",\\n                \"<h1\",\\n                \"<h2\",\\n                \"<h3\",\\n                \"<h4\",\\n                \"<h5\",\\n                \"<h6\",\\n                \"<span\",\\n                \"<table\",\\n                \"<tr\",\\n                \"<td\",\\n                \"<th\",\\n                \"<ul\",\\n                \"<ol\",\\n                \"<header\",\\n                \"<footer\",\\n                \"<nav\",\\n                # Head\\n                \"<head\",\\n                \"<style\",\\n                \"<script\",\\n                \"<meta\",\\n                \"<title\",',\n",
              "  '\" \",\\n                \"\",\\n            ]\\n        elif language == Language.SCALA:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nobject \",\\n                # Split along method definitions\\n                \"\\\\ndef \",\\n                \"\\\\nval \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nmatch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SWIFT:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunc \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nstruct \",\\n                \"\\\\nenum \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.MARKDOWN:\\n            return [\\n                # First, try to split along Markdown headings (starting with level 2)\\n                \"\\\\n#{1,6} \",\\n                # Note the alternative syntax for headings (below) is not handled here\\n                # Heading level 2\\n                # ---------------\\n                # End of code block\\n                \"```\\\\n\",\\n                # Horizontal lines\\n                \"\\\\n\\\\\\\\*\\\\\\\\*\\\\\\\\*+\\\\n\",\\n                \"\\\\n---+\\\\n\",\\n                \"\\\\n___+\\\\n\",\\n                # Note that this splitter doesn\\'t handle horizontal lines defined',\n",
              "  'def split_text(self, text: str) -> List[str]:\\n        def encode_strip_start_and_stop_token_ids(text: str) -> List[int]:\\n            return self._encode(text)[1:-1]\\n\\n        tokenizer = Tokenizer(\\n            chunk_overlap=self._chunk_overlap,\\n            tokens_per_chunk=self.tokens_per_chunk,\\n            decode=self.tokenizer.decode,\\n            encode=encode_strip_start_and_stop_token_ids,\\n        )\\n\\n        return split_text_on_tokens(text=text, tokenizer=tokenizer)\\n\\n    def count_tokens(self, *, text: str) -> int:\\n        return len(self._encode(text))\\n\\n    _max_length_equal_32_bit_integer: int = 2**32\\n\\n    def _encode(self, text: str) -> List[int]:\\n        token_ids_with_start_and_end_token_ids = self.tokenizer.encode(\\n            text,\\n            max_length=self._max_length_equal_32_bit_integer,\\n            truncation=\"do_not_truncate\",\\n        )\\n        return token_ids_with_start_and_end_token_ids',\n",
              "  '\"\\\\n\\\\\\\\*\\\\\\\\*\\\\\\\\*+\\\\n\",\\n                \"\\\\n---+\\\\n\",\\n                \"\\\\n___+\\\\n\",\\n                # Note that this splitter doesn\\'t handle horizontal lines defined\\n                # by *three or more* of ***, ---, or ___, but this is not handled\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.LATEX:\\n            return [\\n                # First, try to split along Latex sections\\n                \"\\\\n\\\\\\\\\\\\\\\\chapter{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\section{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\subsection{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\subsubsection{\",\\n                # Now split by environments\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{enumerate}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{itemize}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{description}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{list}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{quote}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{quotation}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{verse}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{verbatim}\",\\n                # Now split by math environments\\n                \"\\\\n\\\\\\\\\\\\begin{align}\",\\n                \"$$\",\\n                \"$\",\\n                # Now split by the normal type of lines\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.HTML:\\n            return [\\n                # First, try to split along HTML tags\\n                \"<body\",\\n                \"<div\",\\n                \"<p\",\\n                \"<br\",\\n                \"<li\",\\n                \"<h1\",\\n                \"<h2\",\\n                \"<h3\",\\n                \"<h4\",\\n                \"<h5\",\\n                \"<h6\",\\n                \"<span\",\\n                \"<table\",\\n                \"<tr\",\\n                \"<td\",\\n                \"<th\",\\n                \"<ul\",\\n                \"<ol\",\\n                \"<header\",\\n                \"<footer\",\\n                \"<nav\",\\n                # Head\\n                \"<head\",\\n                \"<style\",\\n                \"<script\",\\n                \"<meta\",\\n                \"<title\",',\n",
              "  '\" \",\\n                \"\",\\n            ]\\n        elif language == Language.SCALA:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nobject \",\\n                # Split along method definitions\\n                \"\\\\ndef \",\\n                \"\\\\nval \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nmatch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SWIFT:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunc \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nstruct \",\\n                \"\\\\nenum \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.MARKDOWN:\\n            return [\\n                # First, try to split along Markdown headings (starting with level 2)\\n                \"\\\\n#{1,6} \",\\n                # Note the alternative syntax for headings (below) is not handled here\\n                # Heading level 2\\n                # ---------------\\n                # End of code block\\n                \"```\\\\n\",\\n                # Horizontal lines\\n                \"\\\\n\\\\\\\\*\\\\\\\\*\\\\\\\\*+\\\\n\",\\n                \"\\\\n---+\\\\n\",\\n                \"\\\\n___+\\\\n\",\\n                # Note that this splitter doesn\\'t handle horizontal lines defined',\n",
              "  'return [\\n                # First, try to split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\ndef \",\\n                \"\\\\n\\\\tdef \",\\n                # Now split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.RST:\\n            return [\\n                # Split along section titles\\n                \"\\\\n=+\\\\n\",\\n                \"\\\\n-+\\\\n\",\\n                \"\\\\n\\\\\\\\*+\\\\n\",\\n                # Split along directive markers\\n                \"\\\\n\\\\n.. *\\\\n\\\\n\",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.RUBY:\\n            return [\\n                # Split along method definitions\\n                \"\\\\ndef \",\\n                \"\\\\nclass \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nunless \",\\n                \"\\\\nwhile \",\\n                \"\\\\nfor \",\\n                \"\\\\ndo \",\\n                \"\\\\nbegin \",\\n                \"\\\\nrescue \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.RUST:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfn \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nwhile \",\\n                \"\\\\nfor \",\\n                \"\\\\nloop \",\\n                \"\\\\nmatch \",\\n                \"\\\\nconst \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SCALA:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",',\n",
              "  '\"\\\\nfunction \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                \"\\\\ndefault \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.PHP:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunction \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nforeach \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.PROTO:\\n            return [\\n                # Split along message definitions\\n                \"\\\\nmessage \",\\n                # Split along service definitions\\n                \"\\\\nservice \",\\n                # Split along enum definitions\\n                \"\\\\nenum \",\\n                # Split along option definitions\\n                \"\\\\noption \",\\n                # Split along import statements\\n                \"\\\\nimport \",\\n                # Split along syntax declarations\\n                \"\\\\nsyntax \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.PYTHON:\\n            return [\\n                # First, try to split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\ndef \",\\n                \"\\\\n\\\\tdef \",',\n",
              "  '\"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.KOTLIN:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along method definitions\\n                \"\\\\npublic \",\\n                \"\\\\nprotected \",\\n                \"\\\\nprivate \",\\n                \"\\\\ninternal \",\\n                \"\\\\ncompanion \",\\n                \"\\\\nfun \",\\n                \"\\\\nval \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nwhen \",\\n                \"\\\\ncase \",\\n                \"\\\\nelse \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.JS:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunction \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                \"\\\\nvar \",\\n                \"\\\\nclass \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                \"\\\\ndefault \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.TS:\\n            return [\\n                \"\\\\nenum \",\\n                \"\\\\ninterface \",\\n                \"\\\\nnamespace \",\\n                \"\\\\ntype \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along function definitions\\n                \"\\\\nfunction \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements',\n",
              "  '@staticmethod\\n    def get_separators_for_language(language: Language) -> List[str]:\\n        if language == Language.CPP:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along function definitions\\n                \"\\\\nvoid \",\\n                \"\\\\nint \",\\n                \"\\\\nfloat \",\\n                \"\\\\ndouble \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.GO:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunc \",\\n                \"\\\\nvar \",\\n                \"\\\\nconst \",\\n                \"\\\\ntype \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.JAVA:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along method definitions\\n                \"\\\\npublic \",\\n                \"\\\\nprotected \",\\n                \"\\\\nprivate \",\\n                \"\\\\nstatic \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.KOTLIN:\\n            return [',\n",
              "  '_separator = separator if self._is_separator_regex else re.escape(separator)\\n        splits = _split_text_with_regex(text, _separator, self._keep_separator)\\n\\n        # Now go merging things, recursively splitting longer texts.\\n        _good_splits = []\\n        _separator = \"\" if self._keep_separator else separator\\n        for s in splits:\\n            if self._length_function(s) < self._chunk_size:\\n                _good_splits.append(s)\\n            else:\\n                if _good_splits:\\n                    merged_text = self._merge_splits(_good_splits, _separator)\\n                    final_chunks.extend(merged_text)\\n                    _good_splits = []\\n                if not new_separators:\\n                    final_chunks.append(s)\\n                else:\\n                    other_info = self._split_text(s, new_separators)\\n                    final_chunks.extend(other_info)\\n        if _good_splits:\\n            merged_text = self._merge_splits(_good_splits, _separator)\\n            final_chunks.extend(merged_text)\\n        return final_chunks\\n\\n    def split_text(self, text: str) -> List[str]:\\n        return self._split_text(text, self._separators)\\n\\n    @classmethod\\n    def from_language(\\n        cls, language: Language, **kwargs: Any\\n    ) -> RecursiveCharacterTextSplitter:\\n        separators = cls.get_separators_for_language(language)\\n        return cls(separators=separators, is_separator_regex=True, **kwargs)',\n",
              "  'class RecursiveCharacterTextSplitter(TextSplitter):\\n    \"\"\"Splitting text by recursively look at characters.\\n\\n    Recursively tries to split by different characters to find one\\n    that works.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        separators: Optional[List[str]] = None,\\n        keep_separator: bool = True,\\n        is_separator_regex: bool = False,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(keep_separator=keep_separator, **kwargs)\\n        self._separators = separators or [\"\\\\n\\\\n\", \"\\\\n\", \" \", \"\"]\\n        self._is_separator_regex = is_separator_regex\\n\\n    def _split_text(self, text: str, separators: List[str]) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        final_chunks = []\\n        # Get appropriate separator to use\\n        separator = separators[-1]\\n        new_separators = []\\n        for i, _s in enumerate(separators):\\n            _separator = _s if self._is_separator_regex else re.escape(_s)\\n            if _s == \"\":\\n                separator = _s\\n                break\\n            if re.search(_separator, text):\\n                separator = _s\\n                new_separators = separators[i + 1 :]\\n                break\\n\\n        _separator = separator if self._is_separator_regex else re.escape(separator)\\n        splits = _split_text_with_regex(text, _separator, self._keep_separator)',\n",
              "  'class Language(str, Enum):\\n    \"\"\"Enum of the programming languages.\"\"\"\\n\\n    CPP = \"cpp\"\\n    GO = \"go\"\\n    JAVA = \"java\"\\n    KOTLIN = \"kotlin\"\\n    JS = \"js\"\\n    TS = \"ts\"\\n    PHP = \"php\"\\n    PROTO = \"proto\"\\n    PYTHON = \"python\"\\n    RST = \"rst\"\\n    RUBY = \"ruby\"\\n    RUST = \"rust\"\\n    SCALA = \"scala\"\\n    SWIFT = \"swift\"\\n    MARKDOWN = \"markdown\"\\n    LATEX = \"latex\"\\n    HTML = \"html\"\\n    SOL = \"sol\"\\n    CSHARP = \"csharp\"\\n    COBOL = \"cobol\"',\n",
              "  'return [\\n                # First, try to split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\ndef \",\\n                \"\\\\n\\\\tdef \",\\n                # Now split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.RST:\\n            return [\\n                # Split along section titles\\n                \"\\\\n=+\\\\n\",\\n                \"\\\\n-+\\\\n\",\\n                \"\\\\n\\\\\\\\*+\\\\n\",\\n                # Split along directive markers\\n                \"\\\\n\\\\n.. *\\\\n\\\\n\",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.RUBY:\\n            return [\\n                # Split along method definitions\\n                \"\\\\ndef \",\\n                \"\\\\nclass \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nunless \",\\n                \"\\\\nwhile \",\\n                \"\\\\nfor \",\\n                \"\\\\ndo \",\\n                \"\\\\nbegin \",\\n                \"\\\\nrescue \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.RUST:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfn \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nwhile \",\\n                \"\\\\nfor \",\\n                \"\\\\nloop \",\\n                \"\\\\nmatch \",\\n                \"\\\\nconst \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SCALA:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",',\n",
              "  'class SentenceTransformersTokenTextSplitter(TextSplitter):\\n    \"\"\"Splitting text to tokens using sentence model tokenizer.\"\"\"\\n\\n    def __init__(\\n        self,\\n        chunk_overlap: int = 50,\\n        model_name: str = \"sentence-transformers/all-mpnet-base-v2\",\\n        tokens_per_chunk: Optional[int] = None,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(**kwargs, chunk_overlap=chunk_overlap)\\n\\n        try:\\n            from sentence_transformers import SentenceTransformer\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import sentence_transformer python package. \"\\n                \"This is needed in order to for SentenceTransformersTokenTextSplitter. \"\\n                \"Please install it with `pip install sentence-transformers`.\"\\n            )\\n\\n        self.model_name = model_name\\n        self._model = SentenceTransformer(self.model_name)\\n        self.tokenizer = self._model.tokenizer\\n        self._initialize_chunk_configuration(tokens_per_chunk=tokens_per_chunk)\\n\\n    def _initialize_chunk_configuration(\\n        self, *, tokens_per_chunk: Optional[int]\\n    ) -> None:\\n        self.maximum_tokens_per_chunk = cast(int, self._model.max_seq_length)\\n\\n        if tokens_per_chunk is None:\\n            self.tokens_per_chunk = self.maximum_tokens_per_chunk\\n        else:\\n            self.tokens_per_chunk = tokens_per_chunk\\n\\n        if self.tokens_per_chunk > self.maximum_tokens_per_chunk:\\n            raise ValueError(\\n                f\"The token limit of the models \\'{self.model_name}\\'\"\\n                f\" is: {self.maximum_tokens_per_chunk}.\"\\n                f\" Argument tokens_per_chunk={self.tokens_per_chunk}\"\\n                f\" > maximum token limit.\"\\n            )\\n\\n    def split_text(self, text: str) -> List[str]:\\n        def encode_strip_start_and_stop_token_ids(text: str) -> List[int]:\\n            return self._encode(text)[1:-1]',\n",
              "  'class TokenTextSplitter(TextSplitter):\\n    \"\"\"Splitting text to tokens using model tokenizer.\"\"\"\\n\\n    def __init__(\\n        self,\\n        encoding_name: str = \"gpt2\",\\n        model_name: Optional[str] = None,\\n        allowed_special: Union[Literal[\"all\"], AbstractSet[str]] = set(),\\n        disallowed_special: Union[Literal[\"all\"], Collection[str]] = \"all\",\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(**kwargs)\\n        try:\\n            import tiktoken\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import tiktoken python package. \"\\n                \"This is needed in order to for TokenTextSplitter. \"\\n                \"Please install it with `pip install tiktoken`.\"\\n            )\\n\\n        if model_name is not None:\\n            enc = tiktoken.encoding_for_model(model_name)\\n        else:\\n            enc = tiktoken.get_encoding(encoding_name)\\n        self._tokenizer = enc\\n        self._allowed_special = allowed_special\\n        self._disallowed_special = disallowed_special\\n\\n    def split_text(self, text: str) -> List[str]:\\n        def _encode(_text: str) -> List[int]:\\n            return self._tokenizer.encode(\\n                _text,\\n                allowed_special=self._allowed_special,\\n                disallowed_special=self._disallowed_special,\\n            )\\n\\n        tokenizer = Tokenizer(\\n            chunk_overlap=self._chunk_overlap,\\n            tokens_per_chunk=self._chunk_size,\\n            decode=self._tokenizer.decode,\\n            encode=_encode,\\n        )\\n\\n        return split_text_on_tokens(text=text, tokenizer=tokenizer)',\n",
              "  'class Tokenizer:\\n    \"\"\"Tokenizer data class.\"\"\"\\n\\n    chunk_overlap: int\\n    \"\"\"Overlap in tokens between chunks\"\"\"\\n    tokens_per_chunk: int\\n    \"\"\"Maximum number of tokens per chunk\"\"\"\\n    decode: Callable[[List[int]], str]\\n    \"\"\" Function to decode a list of token ids to a string\"\"\"\\n    encode: Callable[[str], List[int]]\\n    \"\"\" Function to encode a string to a list of token ids\"\"\"\\n\\n\\ndef split_text_on_tokens(*, text: str, tokenizer: Tokenizer) -> List[str]:\\n    \"\"\"Split incoming text and return chunks using tokenizer.\"\"\"\\n    splits: List[str] = []\\n    input_ids = tokenizer.encode(text)\\n    start_idx = 0\\n    cur_idx = min(start_idx + tokenizer.tokens_per_chunk, len(input_ids))\\n    chunk_ids = input_ids[start_idx:cur_idx]\\n    while start_idx < len(input_ids):\\n        splits.append(tokenizer.decode(chunk_ids))\\n        if cur_idx == len(input_ids):\\n            break\\n        start_idx += tokenizer.tokens_per_chunk - tokenizer.chunk_overlap\\n        cur_idx = min(start_idx + tokenizer.tokens_per_chunk, len(input_ids))\\n        chunk_ids = input_ids[start_idx:cur_idx]\\n    return splits',\n",
              "  '# map xhtml namespace prefix\\n        ns_map = {\"h\": \"http://www.w3.org/1999/xhtml\"}\\n\\n        # build list of elements from DOM\\n        elements = []\\n        for element in result_dom.findall(\"*//*\", ns_map):\\n            if element.findall(\"*[@class=\\'headers\\']\") or element.findall(\\n                \"*[@class=\\'chunk\\']\"\\n            ):\\n                elements.append(\\n                    ElementType(\\n                        url=file,\\n                        xpath=\"\".join(\\n                            [\\n                                node.text\\n                                for node in element.findall(\"*[@class=\\'xpath\\']\", ns_map)\\n                            ]\\n                        ),\\n                        content=\"\".join(\\n                            [\\n                                node.text\\n                                for node in element.findall(\"*[@class=\\'chunk\\']\", ns_map)\\n                            ]\\n                        ),\\n                        metadata={\\n                            # Add text of specified headers to metadata using header\\n                            # mapping.\\n                            header_mapping[node.tag]: node.text\\n                            for node in filter(\\n                                lambda x: x.tag in header_filter,\\n                                element.findall(\"*[@class=\\'headers\\']/*\", ns_map),\\n                            )\\n                        },\\n                    )\\n                )\\n\\n        if not self.return_each_element:\\n            return self.aggregate_elements_to_chunks(elements)\\n        else:\\n            return [\\n                Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n                for chunk in elements\\n            ]\\n\\n\\n# should be in newer Python versions (3.10+)\\n# @dataclass(frozen=True, kw_only=True, slots=True)\\n@dataclass(frozen=True)',\n",
              "  'return [\\n            Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n            for chunk in aggregated_chunks\\n        ]\\n\\n    def split_text_from_url(self, url: str) -> List[Document]:\\n        \"\"\"Split HTML from web URL\\n\\n        Args:\\n            url: web URL\\n        \"\"\"\\n        r = requests.get(url)\\n        return self.split_text_from_file(BytesIO(r.content))\\n\\n    def split_text(self, text: str) -> List[Document]:\\n        \"\"\"Split HTML text string\\n\\n        Args:\\n            text: HTML text\\n        \"\"\"\\n        return self.split_text_from_file(StringIO(text))\\n\\n    def split_text_from_file(self, file: Any) -> List[Document]:\\n        \"\"\"Split HTML file\\n\\n        Args:\\n            file: HTML file\\n        \"\"\"\\n        try:\\n            from lxml import etree\\n        except ImportError as e:\\n            raise ImportError(\\n                \"Unable to import lxml, please install with `pip install lxml`.\"\\n            ) from e\\n        # use lxml library to parse html document and return xml ElementTree\\n        parser = etree.HTMLParser()\\n        tree = etree.parse(file, parser)\\n\\n        # document transformation for \"structure-aware\" chunking is handled with xsl.\\n        # see comments in html_chunks_with_headers.xslt for more detailed information.\\n        xslt_path = (\\n            pathlib.Path(__file__).parent\\n            / \"document_transformers/xsl/html_chunks_with_headers.xslt\"\\n        )\\n        xslt_tree = etree.parse(xslt_path)\\n        transform = etree.XSLT(xslt_tree)\\n        result = transform(tree)\\n        result_dom = etree.fromstring(str(result))\\n\\n        # create filter and mapping for header metadata\\n        header_filter = [header[0] for header in self.headers_to_split_on]\\n        header_mapping = dict(self.headers_to_split_on)\\n\\n        # map xhtml namespace prefix\\n        ns_map = {\"h\": \"http://www.w3.org/1999/xhtml\"}',\n",
              "  'class HTMLHeaderTextSplitter:\\n    \"\"\"\\n    Splitting HTML files based on specified headers.\\n    Requires lxml package.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        headers_to_split_on: List[Tuple[str, str]],\\n        return_each_element: bool = False,\\n    ):\\n        \"\"\"Create a new HTMLHeaderTextSplitter.\\n\\n        Args:\\n            headers_to_split_on: list of tuples of headers we want to track mapped to\\n                (arbitrary) keys for metadata. Allowed header values: h1, h2, h3, h4,\\n                h5, h6 e.g. [(\"h1\", \"Header 1\"), (\"h2\", \"Header 2)].\\n            return_each_element: Return each element w/ associated headers.\\n        \"\"\"\\n        # Output element-by-element or aggregated into chunks w/ common headers\\n        self.return_each_element = return_each_element\\n        self.headers_to_split_on = sorted(headers_to_split_on)\\n\\n    def aggregate_elements_to_chunks(\\n        self, elements: List[ElementType]\\n    ) -> List[Document]:\\n        \"\"\"Combine elements with common metadata into chunks\\n\\n        Args:\\n            elements: HTML element content with associated identifying info and metadata\\n        \"\"\"\\n        aggregated_chunks: List[ElementType] = []\\n\\n        for element in elements:\\n            if (\\n                aggregated_chunks\\n                and aggregated_chunks[-1][\"metadata\"] == element[\"metadata\"]\\n            ):\\n                # If the last element in the aggregated list\\n                # has the same metadata as the current element,\\n                # append the current content to the last element\\'s content\\n                aggregated_chunks[-1][\"content\"] += \"  \\\\n\" + element[\"content\"]\\n            else:\\n                # Otherwise, append the current element to the aggregated list\\n                aggregated_chunks.append(element)\\n\\n        return [\\n            Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n            for chunk in aggregated_chunks\\n        ]',\n",
              "  'class ElementType(TypedDict):\\n    \"\"\"Element type as typed dict.\"\"\"\\n\\n    url: str\\n    xpath: str\\n    content: str\\n    metadata: Dict[str, str]',\n",
              "  'if current_content:\\n            lines_with_metadata.append(\\n                {\"content\": \"\\\\n\".join(current_content), \"metadata\": current_metadata}\\n            )\\n\\n        # lines_with_metadata has each line with associated header metadata\\n        # aggregate these into chunks based on common metadata\\n        if not self.return_each_line:\\n            return self.aggregate_lines_to_chunks(lines_with_metadata)\\n        else:\\n            return [\\n                Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n                for chunk in lines_with_metadata\\n            ]',\n",
              "  'class PythonCodeTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Python syntax.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a PythonCodeTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.PYTHON)\\n        super().__init__(separators=separators, **kwargs)\\n\\n\\nclass MarkdownTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Markdown-formatted headings.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a MarkdownTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.MARKDOWN)\\n        super().__init__(separators=separators, **kwargs)\\n\\n\\nclass LatexTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Latex-formatted layout elements.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a LatexTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.LATEX)\\n        super().__init__(separators=separators, **kwargs)',\n",
              "  'class MarkdownHeaderTextSplitter:\\n    \"\"\"Splitting markdown files based on specified headers.\"\"\"\\n\\n    def __init__(\\n        self, headers_to_split_on: List[Tuple[str, str]], return_each_line: bool = False\\n    ):\\n        \"\"\"Create a new MarkdownHeaderTextSplitter.\\n\\n        Args:\\n            headers_to_split_on: Headers we want to track\\n            return_each_line: Return each line w/ associated headers\\n        \"\"\"\\n        # Output line-by-line or aggregated into chunks w/ common headers\\n        self.return_each_line = return_each_line\\n        # Given the headers we want to split on,\\n        # (e.g., \"#, ##, etc\") order by length\\n        self.headers_to_split_on = sorted(\\n            headers_to_split_on, key=lambda split: len(split[0]), reverse=True\\n        )\\n\\n    def aggregate_lines_to_chunks(self, lines: List[LineType]) -> List[Document]:\\n        \"\"\"Combine lines with common metadata into chunks\\n        Args:\\n            lines: Line of text / associated header metadata\\n        \"\"\"\\n        aggregated_chunks: List[LineType] = []\\n\\n        for line in lines:\\n            if (\\n                aggregated_chunks\\n                and aggregated_chunks[-1][\"metadata\"] == line[\"metadata\"]\\n            ):\\n                # If the last line in the aggregated list\\n                # has the same metadata as the current line,\\n                # append the current content to the last lines\\'s content\\n                aggregated_chunks[-1][\"content\"] += \"  \\\\n\" + line[\"content\"]\\n            else:\\n                # Otherwise, append the current line to the aggregated list\\n                aggregated_chunks.append(line)\\n\\n        return [\\n            Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n            for chunk in aggregated_chunks\\n        ]\\n\\n    def split_text(self, text: str) -> List[Document]:\\n        \"\"\"Split markdown file\\n        Args:\\n            text: Markdown file\"\"\"',\n",
              "  'class CharacterTextSplitter(TextSplitter):\\n    \"\"\"Splitting text that looks at characters.\"\"\"\\n\\n    def __init__(\\n        self, separator: str = \"\\\\n\\\\n\", is_separator_regex: bool = False, **kwargs: Any\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(**kwargs)\\n        self._separator = separator\\n        self._is_separator_regex = is_separator_regex\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        # First we naively split the large input into a bunch of smaller ones.\\n        separator = (\\n            self._separator if self._is_separator_regex else re.escape(self._separator)\\n        )\\n        splits = _split_text_with_regex(text, separator, self._keep_separator)\\n        _separator = \"\" if self._keep_separator else self._separator\\n        return self._merge_splits(splits, _separator)\\n\\n\\nclass LineType(TypedDict):\\n    \"\"\"Line type as typed dict.\"\"\"\\n\\n    metadata: Dict[str, str]\\n    content: str\\n\\n\\nclass HeaderType(TypedDict):\\n    \"\"\"Header type as typed dict.\"\"\"\\n\\n    level: int\\n    name: str\\n    data: str',\n",
              "  'if issubclass(cls, TokenTextSplitter):\\n            extra_kwargs = {\\n                \"encoding_name\": encoding_name,\\n                \"model_name\": model_name,\\n                \"allowed_special\": allowed_special,\\n                \"disallowed_special\": disallowed_special,\\n            }\\n            kwargs = {**kwargs, **extra_kwargs}\\n\\n        return cls(length_function=_tiktoken_encoder, **kwargs)\\n\\n    def transform_documents(\\n        self, documents: Sequence[Document], **kwargs: Any\\n    ) -> Sequence[Document]:\\n        \"\"\"Transform sequence of documents by splitting them.\"\"\"\\n        return self.split_documents(list(documents))\\n\\n    async def atransform_documents(\\n        self, documents: Sequence[Document], **kwargs: Any\\n    ) -> Sequence[Document]:\\n        \"\"\"Asynchronously transform a sequence of documents by splitting them.\"\"\"\\n        return await asyncio.get_running_loop().run_in_executor(\\n            None, partial(self.transform_documents, **kwargs), documents\\n        )',\n",
              "  'if not isinstance(tokenizer, PreTrainedTokenizerBase):\\n                raise ValueError(\\n                    \"Tokenizer received was not an instance of PreTrainedTokenizerBase\"\\n                )\\n\\n            def _huggingface_tokenizer_length(text: str) -> int:\\n                return len(tokenizer.encode(text))\\n\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import transformers python package. \"\\n                \"Please install it with `pip install transformers`.\"\\n            )\\n        return cls(length_function=_huggingface_tokenizer_length, **kwargs)\\n\\n    @classmethod\\n    def from_tiktoken_encoder(\\n        cls: Type[TS],\\n        encoding_name: str = \"gpt2\",\\n        model_name: Optional[str] = None,\\n        allowed_special: Union[Literal[\"all\"], AbstractSet[str]] = set(),\\n        disallowed_special: Union[Literal[\"all\"], Collection[str]] = \"all\",\\n        **kwargs: Any,\\n    ) -> TS:\\n        \"\"\"Text splitter that uses tiktoken encoder to count length.\"\"\"\\n        try:\\n            import tiktoken\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import tiktoken python package. \"\\n                \"This is needed in order to calculate max_tokens_for_prompt. \"\\n                \"Please install it with `pip install tiktoken`.\"\\n            )\\n\\n        if model_name is not None:\\n            enc = tiktoken.encoding_for_model(model_name)\\n        else:\\n            enc = tiktoken.get_encoding(encoding_name)\\n\\n        def _tiktoken_encoder(text: str) -> int:\\n            return len(\\n                enc.encode(\\n                    text,\\n                    allowed_special=allowed_special,\\n                    disallowed_special=disallowed_special,\\n                )\\n            )',\n",
              "  'docs = []\\n        current_doc: List[str] = []\\n        total = 0\\n        for d in splits:\\n            _len = self._length_function(d)\\n            if (\\n                total + _len + (separator_len if len(current_doc) > 0 else 0)\\n                > self._chunk_size\\n            ):\\n                if total > self._chunk_size:\\n                    logger.warning(\\n                        f\"Created a chunk of size {total}, \"\\n                        f\"which is longer than the specified {self._chunk_size}\"\\n                    )\\n                if len(current_doc) > 0:\\n                    doc = self._join_docs(current_doc, separator)\\n                    if doc is not None:\\n                        docs.append(doc)\\n                    # Keep on popping if:\\n                    # - we have a larger chunk than in the chunk overlap\\n                    # - or if we still have any chunks and the length is long\\n                    while total > self._chunk_overlap or (\\n                        total + _len + (separator_len if len(current_doc) > 0 else 0)\\n                        > self._chunk_size\\n                        and total > 0\\n                    ):\\n                        total -= self._length_function(current_doc[0]) + (\\n                            separator_len if len(current_doc) > 1 else 0\\n                        )\\n                        current_doc = current_doc[1:]\\n            current_doc.append(d)\\n            total += _len + (separator_len if len(current_doc) > 1 else 0)\\n        doc = self._join_docs(current_doc, separator)\\n        if doc is not None:\\n            docs.append(doc)\\n        return docs\\n\\n    @classmethod\\n    def from_huggingface_tokenizer(cls, tokenizer: Any, **kwargs: Any) -> TextSplitter:\\n        \"\"\"Text splitter that uses HuggingFace tokenizer to count length.\"\"\"\\n        try:\\n            from transformers import PreTrainedTokenizerBase',\n",
              "  '@abstractmethod\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split text into multiple components.\"\"\"\\n\\n    def create_documents(\\n        self, texts: List[str], metadatas: Optional[List[dict]] = None\\n    ) -> List[Document]:\\n        \"\"\"Create documents from a list of texts.\"\"\"\\n        _metadatas = metadatas or [{}] * len(texts)\\n        documents = []\\n        for i, text in enumerate(texts):\\n            index = -1\\n            for chunk in self.split_text(text):\\n                metadata = copy.deepcopy(_metadatas[i])\\n                if self._add_start_index:\\n                    index = text.find(chunk, index + 1)\\n                    metadata[\"start_index\"] = index\\n                new_doc = Document(page_content=chunk, metadata=metadata)\\n                documents.append(new_doc)\\n        return documents\\n\\n    def split_documents(self, documents: Iterable[Document]) -> List[Document]:\\n        \"\"\"Split documents.\"\"\"\\n        texts, metadatas = [], []\\n        for doc in documents:\\n            texts.append(doc.page_content)\\n            metadatas.append(doc.metadata)\\n        return self.create_documents(texts, metadatas=metadatas)\\n\\n    def _join_docs(self, docs: List[str], separator: str) -> Optional[str]:\\n        text = separator.join(docs)\\n        if self._strip_whitespace:\\n            text = text.strip()\\n        if text == \"\":\\n            return None\\n        else:\\n            return text\\n\\n    def _merge_splits(self, splits: Iterable[str], separator: str) -> List[str]:\\n        # We now want to combine these smaller pieces into medium size\\n        # chunks to send to the LLM.\\n        separator_len = self._length_function(separator)',\n",
              "  'class TextSplitter(BaseDocumentTransformer, ABC):\\n    \"\"\"Interface for splitting text into chunks.\"\"\"\\n\\n    def __init__(\\n        self,\\n        chunk_size: int = 4000,\\n        chunk_overlap: int = 200,\\n        length_function: Callable[[str], int] = len,\\n        keep_separator: bool = False,\\n        add_start_index: bool = False,\\n        strip_whitespace: bool = True,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\\n\\n        Args:\\n            chunk_size: Maximum size of chunks to return\\n            chunk_overlap: Overlap in characters between chunks\\n            length_function: Function that measures the length of given chunks\\n            keep_separator: Whether to keep the separator in the chunks\\n            add_start_index: If `True`, includes chunk\\'s start index in metadata\\n            strip_whitespace: If `True`, strips whitespace from the start and end of\\n                              every document\\n        \"\"\"\\n        if chunk_overlap > chunk_size:\\n            raise ValueError(\\n                f\"Got a larger chunk overlap ({chunk_overlap}) than chunk size \"\\n                f\"({chunk_size}), should be smaller.\"\\n            )\\n        self._chunk_size = chunk_size\\n        self._chunk_overlap = chunk_overlap\\n        self._length_function = length_function\\n        self._keep_separator = keep_separator\\n        self._add_start_index = add_start_index\\n        self._strip_whitespace = strip_whitespace\\n\\n    @abstractmethod\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split text into multiple components.\"\"\"',\n",
              "  'def _split_text_with_regex(\\n    text: str, separator: str, keep_separator: bool\\n) -> List[str]:\\n    # Now that we have the separator, split the text\\n    if separator:\\n        if keep_separator:\\n            # The parentheses in the pattern keep the delimiters in the result.\\n            _splits = re.split(f\"({separator})\", text)\\n            splits = [_splits[i] + _splits[i + 1] for i in range(1, len(_splits), 2)]\\n            if len(_splits) % 2 == 0:\\n                splits += _splits[-1:]\\n            splits = [_splits[0]] + splits\\n        else:\\n            splits = re.split(separator, text)\\n    else:\\n        splits = list(text)\\n    return [s for s in splits if s != \"\"]',\n",
              "  '\"\"\"**Text Splitters** are classes for splitting text.\\n\\n\\n**Class hierarchy:**\\n\\n.. code-block::\\n\\n    BaseDocumentTransformer --> TextSplitter --> <name>TextSplitter  # Example: CharacterTextSplitter\\n                                                 RecursiveCharacterTextSplitter -->  <name>TextSplitter\\n\\nNote: **MarkdownHeaderTextSplitter** and **HTMLHeaderTextSplitter do not derive from TextSplitter.\\n\\n\\n**Main helpers:**\\n\\n.. code-block::\\n\\n    Document, Tokenizer, Language, LineType, HeaderType\\n\\n\"\"\"  # noqa: E501\\n\\nfrom __future__ import annotations\\n\\nimport asyncio\\nimport copy\\nimport logging\\nimport pathlib\\nimport re\\nfrom abc import ABC, abstractmethod\\nfrom dataclasses import dataclass\\nfrom enum import Enum\\nfrom functools import partial\\nfrom io import BytesIO, StringIO\\nfrom typing import (\\n    AbstractSet,\\n    Any,\\n    Callable,\\n    Collection,\\n    Dict,\\n    Iterable,\\n    List,\\n    Literal,\\n    Optional,\\n    Sequence,\\n    Tuple,\\n    Type,\\n    TypedDict,\\n    TypeVar,\\n    Union,\\n    cast,\\n)\\n\\nimport requests\\nfrom langchain_core.documents import BaseDocumentTransformer, Document\\n\\nlogger = logging.getLogger(__name__)\\n\\nTS = TypeVar(\"TS\", bound=\"TextSplitter\")\\n\\n\\ndef _make_spacy_pipeline_for_splitting(\\n    pipeline: str, *, max_length: int = 1_000_000\\n) -> Any:  # avoid importing spacy\\n    try:\\n        import spacy\\n    except ImportError:\\n        raise ImportError(\\n            \"Spacy is not installed, please install it with `pip install spacy`.\"\\n        )\\n    if pipeline == \"sentencizer\":\\n        from spacy.lang.en import English\\n\\n        sentencizer = English()\\n        sentencizer.add_pipe(\"sentencizer\")\\n    else:\\n        sentencizer = spacy.load(pipeline, exclude=[\"ner\", \"tagger\"])\\n        sentencizer.max_length = max_length\\n    return sentencizer',\n",
              "  'def split_text(self, text: str) -> List[Document]:\\n        \"\"\"Split markdown file\\n        Args:\\n            text: Markdown file\"\"\"\\n\\n        # Split the input text by newline character (\"\\\\n\").\\n        lines = text.split(\"\\\\n\")\\n        # Final output\\n        lines_with_metadata: List[LineType] = []\\n        # Content and metadata of the chunk currently being processed\\n        current_content: List[str] = []\\n        current_metadata: Dict[str, str] = {}\\n        # Keep track of the nested header structure\\n        # header_stack: List[Dict[str, Union[int, str]]] = []\\n        header_stack: List[HeaderType] = []\\n        initial_metadata: Dict[str, str] = {}\\n\\n        in_code_block = False\\n        opening_fence = \"\"\\n\\n        for line in lines:\\n            stripped_line = line.strip()\\n\\n            if not in_code_block:\\n                # Exclude inline code spans\\n                if stripped_line.startswith(\"```\") and stripped_line.count(\"```\") == 1:\\n                    in_code_block = True\\n                    opening_fence = \"```\"\\n                elif stripped_line.startswith(\"~~~\"):\\n                    in_code_block = True\\n                    opening_fence = \"~~~\"\\n            else:\\n                if stripped_line.startswith(opening_fence):\\n                    in_code_block = False\\n                    opening_fence = \"\"\\n\\n            if in_code_block:\\n                current_content.append(stripped_line)\\n                continue',\n",
              "  'class NLTKTextSplitter(TextSplitter):\\n    \"\"\"Splitting text using NLTK package.\"\"\"\\n\\n    def __init__(\\n        self, separator: str = \"\\\\n\\\\n\", language: str = \"english\", **kwargs: Any\\n    ) -> None:\\n        \"\"\"Initialize the NLTK splitter.\"\"\"\\n        super().__init__(**kwargs)\\n        try:\\n            from nltk.tokenize import sent_tokenize\\n\\n            self._tokenizer = sent_tokenize\\n        except ImportError:\\n            raise ImportError(\\n                \"NLTK is not installed, please install it with `pip install nltk`.\"\\n            )\\n        self._separator = separator\\n        self._language = language\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        # First we naively split the large input into a bunch of smaller ones.\\n        splits = self._tokenizer(text, language=self._language)\\n        return self._merge_splits(splits, self._separator)\\n\\n\\nclass SpacyTextSplitter(TextSplitter):\\n    \"\"\"Splitting text using Spacy package.\\n\\n\\n    Per default, Spacy\\'s `en_core_web_sm` model is used and\\n    its default max_length is 1000000 (it is the length of maximum character\\n    this model takes which can be increased for large files). For a faster, but\\n    potentially less accurate splitting, you can use `pipeline=\\'sentencizer\\'`.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        separator: str = \"\\\\n\\\\n\",\\n        pipeline: str = \"en_core_web_sm\",\\n        max_length: int = 1_000_000,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Initialize the spacy text splitter.\"\"\"\\n        super().__init__(**kwargs)\\n        self._tokenizer = _make_spacy_pipeline_for_splitting(\\n            pipeline, max_length=max_length\\n        )\\n        self._separator = separator\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        splits = (s.text for s in self._tokenizer(text).sents)\\n        return self._merge_splits(splits, self._separator)\\n\\n\\n# For backwards compatibility',\n",
              "  'else:\\n            raise ValueError(\\n                f\"Language {language} is not supported! \"\\n                f\"Please choose from {list(Language)}\"\\n            )',\n",
              "  '\"\\\\nevent \",\\n                \"\\\\nmodifier \",\\n                \"\\\\nerror \",\\n                \"\\\\nstruct \",\\n                \"\\\\nenum \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo while \",\\n                \"\\\\nassembly \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.COBOL:\\n            return [\\n                # Split along divisions\\n                \"\\\\nIDENTIFICATION DIVISION.\",\\n                \"\\\\nENVIRONMENT DIVISION.\",\\n                \"\\\\nDATA DIVISION.\",\\n                \"\\\\nPROCEDURE DIVISION.\",\\n                # Split along sections within DATA DIVISION\\n                \"\\\\nWORKING-STORAGE SECTION.\",\\n                \"\\\\nLINKAGE SECTION.\",\\n                \"\\\\nFILE SECTION.\",\\n                # Split along sections within PROCEDURE DIVISION\\n                \"\\\\nINPUT-OUTPUT SECTION.\",\\n                # Split along paragraphs and common statements\\n                \"\\\\nOPEN \",\\n                \"\\\\nCLOSE \",\\n                \"\\\\nREAD \",\\n                \"\\\\nWRITE \",\\n                \"\\\\nIF \",\\n                \"\\\\nELSE \",\\n                \"\\\\nMOVE \",\\n                \"\\\\nPERFORM \",\\n                \"\\\\nUNTIL \",\\n                \"\\\\nVARYING \",\\n                \"\\\\nACCEPT \",\\n                \"\\\\nDISPLAY \",\\n                \"\\\\nSTOP RUN.\",\\n                # Split by the normal type of lines\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]',\n",
              "  '\"<nav\",\\n                # Head\\n                \"<head\",\\n                \"<style\",\\n                \"<script\",\\n                \"<meta\",\\n                \"<title\",\\n                \"\",\\n            ]\\n        elif language == Language.CSHARP:\\n            return [\\n                \"\\\\ninterface \",\\n                \"\\\\nenum \",\\n                \"\\\\nimplements \",\\n                \"\\\\ndelegate \",\\n                \"\\\\nevent \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nabstract \",\\n                # Split along method definitions\\n                \"\\\\npublic \",\\n                \"\\\\nprotected \",\\n                \"\\\\nprivate \",\\n                \"\\\\nstatic \",\\n                \"\\\\nreturn \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\ncontinue \",\\n                \"\\\\nfor \",\\n                \"\\\\nforeach \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\nbreak \",\\n                \"\\\\ncase \",\\n                \"\\\\nelse \",\\n                # Split by exceptions\\n                \"\\\\ntry \",\\n                \"\\\\nthrow \",\\n                \"\\\\nfinally \",\\n                \"\\\\ncatch \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SOL:\\n            return [\\n                # Split along compiler information definitions\\n                \"\\\\npragma \",\\n                \"\\\\nusing \",\\n                # Split along contract definitions\\n                \"\\\\ncontract \",\\n                \"\\\\ninterface \",\\n                \"\\\\nlibrary \",\\n                # Split along method definitions\\n                \"\\\\nconstructor \",\\n                \"\\\\ntype \",\\n                \"\\\\nfunction \",\\n                \"\\\\nevent \",\\n                \"\\\\nmodifier \",\\n                \"\\\\nerror \",\\n                \"\\\\nstruct \",\\n                \"\\\\nenum \",\\n                # Split along control flow statements',\n",
              "  '\"\\\\n\\\\\\\\*\\\\\\\\*\\\\\\\\*+\\\\n\",\\n                \"\\\\n---+\\\\n\",\\n                \"\\\\n___+\\\\n\",\\n                # Note that this splitter doesn\\'t handle horizontal lines defined\\n                # by *three or more* of ***, ---, or ___, but this is not handled\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.LATEX:\\n            return [\\n                # First, try to split along Latex sections\\n                \"\\\\n\\\\\\\\\\\\\\\\chapter{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\section{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\subsection{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\subsubsection{\",\\n                # Now split by environments\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{enumerate}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{itemize}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{description}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{list}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{quote}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{quotation}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{verse}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{verbatim}\",\\n                # Now split by math environments\\n                \"\\\\n\\\\\\\\\\\\begin{align}\",\\n                \"$$\",\\n                \"$\",\\n                # Now split by the normal type of lines\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.HTML:\\n            return [\\n                # First, try to split along HTML tags\\n                \"<body\",\\n                \"<div\",\\n                \"<p\",\\n                \"<br\",\\n                \"<li\",\\n                \"<h1\",\\n                \"<h2\",\\n                \"<h3\",\\n                \"<h4\",\\n                \"<h5\",\\n                \"<h6\",\\n                \"<span\",\\n                \"<table\",\\n                \"<tr\",\\n                \"<td\",\\n                \"<th\",\\n                \"<ul\",\\n                \"<ol\",\\n                \"<header\",\\n                \"<footer\",\\n                \"<nav\",\\n                # Head\\n                \"<head\",\\n                \"<style\",\\n                \"<script\",\\n                \"<meta\",\\n                \"<title\",',\n",
              "  '\" \",\\n                \"\",\\n            ]\\n        elif language == Language.SCALA:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nobject \",\\n                # Split along method definitions\\n                \"\\\\ndef \",\\n                \"\\\\nval \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nmatch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SWIFT:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunc \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nstruct \",\\n                \"\\\\nenum \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.MARKDOWN:\\n            return [\\n                # First, try to split along Markdown headings (starting with level 2)\\n                \"\\\\n#{1,6} \",\\n                # Note the alternative syntax for headings (below) is not handled here\\n                # Heading level 2\\n                # ---------------\\n                # End of code block\\n                \"```\\\\n\",\\n                # Horizontal lines\\n                \"\\\\n\\\\\\\\*\\\\\\\\*\\\\\\\\*+\\\\n\",\\n                \"\\\\n---+\\\\n\",\\n                \"\\\\n___+\\\\n\",\\n                # Note that this splitter doesn\\'t handle horizontal lines defined',\n",
              "  'return [\\n                # First, try to split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\ndef \",\\n                \"\\\\n\\\\tdef \",\\n                # Now split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.RST:\\n            return [\\n                # Split along section titles\\n                \"\\\\n=+\\\\n\",\\n                \"\\\\n-+\\\\n\",\\n                \"\\\\n\\\\\\\\*+\\\\n\",\\n                # Split along directive markers\\n                \"\\\\n\\\\n.. *\\\\n\\\\n\",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.RUBY:\\n            return [\\n                # Split along method definitions\\n                \"\\\\ndef \",\\n                \"\\\\nclass \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nunless \",\\n                \"\\\\nwhile \",\\n                \"\\\\nfor \",\\n                \"\\\\ndo \",\\n                \"\\\\nbegin \",\\n                \"\\\\nrescue \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.RUST:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfn \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nwhile \",\\n                \"\\\\nfor \",\\n                \"\\\\nloop \",\\n                \"\\\\nmatch \",\\n                \"\\\\nconst \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SCALA:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",',\n",
              "  '\"\\\\nfunction \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                \"\\\\ndefault \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.PHP:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunction \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nforeach \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.PROTO:\\n            return [\\n                # Split along message definitions\\n                \"\\\\nmessage \",\\n                # Split along service definitions\\n                \"\\\\nservice \",\\n                # Split along enum definitions\\n                \"\\\\nenum \",\\n                # Split along option definitions\\n                \"\\\\noption \",\\n                # Split along import statements\\n                \"\\\\nimport \",\\n                # Split along syntax declarations\\n                \"\\\\nsyntax \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.PYTHON:\\n            return [\\n                # First, try to split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\ndef \",\\n                \"\\\\n\\\\tdef \",',\n",
              "  'class TokenTextSplitter(TextSplitter):\\n    \"\"\"Splitting text to tokens using model tokenizer.\"\"\"\\n\\n    def __init__(\\n        self,\\n        encoding_name: str = \"gpt2\",\\n        model_name: Optional[str] = None,\\n        allowed_special: Union[Literal[\"all\"], AbstractSet[str]] = set(),\\n        disallowed_special: Union[Literal[\"all\"], Collection[str]] = \"all\",\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(**kwargs)\\n        try:\\n            import tiktoken\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import tiktoken python package. \"\\n                \"This is needed in order to for TokenTextSplitter. \"\\n                \"Please install it with `pip install tiktoken`.\"\\n            )\\n\\n        if model_name is not None:\\n            enc = tiktoken.encoding_for_model(model_name)\\n        else:\\n            enc = tiktoken.get_encoding(encoding_name)\\n        self._tokenizer = enc\\n        self._allowed_special = allowed_special\\n        self._disallowed_special = disallowed_special\\n\\n    def split_text(self, text: str) -> List[str]:\\n        def _encode(_text: str) -> List[int]:\\n            return self._tokenizer.encode(\\n                _text,\\n                allowed_special=self._allowed_special,\\n                disallowed_special=self._disallowed_special,\\n            )\\n\\n        tokenizer = Tokenizer(\\n            chunk_overlap=self._chunk_overlap,\\n            tokens_per_chunk=self._chunk_size,\\n            decode=self._tokenizer.decode,\\n            encode=_encode,\\n        )\\n\\n        return split_text_on_tokens(text=text, tokenizer=tokenizer)',\n",
              "  '\"\\\\nfunction \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                \"\\\\ndefault \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.PHP:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunction \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nforeach \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.PROTO:\\n            return [\\n                # Split along message definitions\\n                \"\\\\nmessage \",\\n                # Split along service definitions\\n                \"\\\\nservice \",\\n                # Split along enum definitions\\n                \"\\\\nenum \",\\n                # Split along option definitions\\n                \"\\\\noption \",\\n                # Split along import statements\\n                \"\\\\nimport \",\\n                # Split along syntax declarations\\n                \"\\\\nsyntax \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.PYTHON:\\n            return [\\n                # First, try to split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\ndef \",\\n                \"\\\\n\\\\tdef \",',\n",
              "  '\"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.KOTLIN:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along method definitions\\n                \"\\\\npublic \",\\n                \"\\\\nprotected \",\\n                \"\\\\nprivate \",\\n                \"\\\\ninternal \",\\n                \"\\\\ncompanion \",\\n                \"\\\\nfun \",\\n                \"\\\\nval \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nwhen \",\\n                \"\\\\ncase \",\\n                \"\\\\nelse \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.JS:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunction \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                \"\\\\nvar \",\\n                \"\\\\nclass \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                \"\\\\ndefault \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.TS:\\n            return [\\n                \"\\\\nenum \",\\n                \"\\\\ninterface \",\\n                \"\\\\nnamespace \",\\n                \"\\\\ntype \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along function definitions\\n                \"\\\\nfunction \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements',\n",
              "  '@staticmethod\\n    def get_separators_for_language(language: Language) -> List[str]:\\n        if language == Language.CPP:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along function definitions\\n                \"\\\\nvoid \",\\n                \"\\\\nint \",\\n                \"\\\\nfloat \",\\n                \"\\\\ndouble \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.GO:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunc \",\\n                \"\\\\nvar \",\\n                \"\\\\nconst \",\\n                \"\\\\ntype \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.JAVA:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along method definitions\\n                \"\\\\npublic \",\\n                \"\\\\nprotected \",\\n                \"\\\\nprivate \",\\n                \"\\\\nstatic \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.KOTLIN:\\n            return [',\n",
              "  '_separator = separator if self._is_separator_regex else re.escape(separator)\\n        splits = _split_text_with_regex(text, _separator, self._keep_separator)\\n\\n        # Now go merging things, recursively splitting longer texts.\\n        _good_splits = []\\n        _separator = \"\" if self._keep_separator else separator\\n        for s in splits:\\n            if self._length_function(s) < self._chunk_size:\\n                _good_splits.append(s)\\n            else:\\n                if _good_splits:\\n                    merged_text = self._merge_splits(_good_splits, _separator)\\n                    final_chunks.extend(merged_text)\\n                    _good_splits = []\\n                if not new_separators:\\n                    final_chunks.append(s)\\n                else:\\n                    other_info = self._split_text(s, new_separators)\\n                    final_chunks.extend(other_info)\\n        if _good_splits:\\n            merged_text = self._merge_splits(_good_splits, _separator)\\n            final_chunks.extend(merged_text)\\n        return final_chunks\\n\\n    def split_text(self, text: str) -> List[str]:\\n        return self._split_text(text, self._separators)\\n\\n    @classmethod\\n    def from_language(\\n        cls, language: Language, **kwargs: Any\\n    ) -> RecursiveCharacterTextSplitter:\\n        separators = cls.get_separators_for_language(language)\\n        return cls(separators=separators, is_separator_regex=True, **kwargs)',\n",
              "  'class RecursiveCharacterTextSplitter(TextSplitter):\\n    \"\"\"Splitting text by recursively look at characters.\\n\\n    Recursively tries to split by different characters to find one\\n    that works.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        separators: Optional[List[str]] = None,\\n        keep_separator: bool = True,\\n        is_separator_regex: bool = False,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(keep_separator=keep_separator, **kwargs)\\n        self._separators = separators or [\"\\\\n\\\\n\", \"\\\\n\", \" \", \"\"]\\n        self._is_separator_regex = is_separator_regex\\n\\n    def _split_text(self, text: str, separators: List[str]) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        final_chunks = []\\n        # Get appropriate separator to use\\n        separator = separators[-1]\\n        new_separators = []\\n        for i, _s in enumerate(separators):\\n            _separator = _s if self._is_separator_regex else re.escape(_s)\\n            if _s == \"\":\\n                separator = _s\\n                break\\n            if re.search(_separator, text):\\n                separator = _s\\n                new_separators = separators[i + 1 :]\\n                break\\n\\n        _separator = separator if self._is_separator_regex else re.escape(separator)\\n        splits = _split_text_with_regex(text, _separator, self._keep_separator)',\n",
              "  'class Language(str, Enum):\\n    \"\"\"Enum of the programming languages.\"\"\"\\n\\n    CPP = \"cpp\"\\n    GO = \"go\"\\n    JAVA = \"java\"\\n    KOTLIN = \"kotlin\"\\n    JS = \"js\"\\n    TS = \"ts\"\\n    PHP = \"php\"\\n    PROTO = \"proto\"\\n    PYTHON = \"python\"\\n    RST = \"rst\"\\n    RUBY = \"ruby\"\\n    RUST = \"rust\"\\n    SCALA = \"scala\"\\n    SWIFT = \"swift\"\\n    MARKDOWN = \"markdown\"\\n    LATEX = \"latex\"\\n    HTML = \"html\"\\n    SOL = \"sol\"\\n    CSHARP = \"csharp\"\\n    COBOL = \"cobol\"',\n",
              "  'def split_text(self, text: str) -> List[str]:\\n        def encode_strip_start_and_stop_token_ids(text: str) -> List[int]:\\n            return self._encode(text)[1:-1]\\n\\n        tokenizer = Tokenizer(\\n            chunk_overlap=self._chunk_overlap,\\n            tokens_per_chunk=self.tokens_per_chunk,\\n            decode=self.tokenizer.decode,\\n            encode=encode_strip_start_and_stop_token_ids,\\n        )\\n\\n        return split_text_on_tokens(text=text, tokenizer=tokenizer)\\n\\n    def count_tokens(self, *, text: str) -> int:\\n        return len(self._encode(text))\\n\\n    _max_length_equal_32_bit_integer: int = 2**32\\n\\n    def _encode(self, text: str) -> List[int]:\\n        token_ids_with_start_and_end_token_ids = self.tokenizer.encode(\\n            text,\\n            max_length=self._max_length_equal_32_bit_integer,\\n            truncation=\"do_not_truncate\",\\n        )\\n        return token_ids_with_start_and_end_token_ids',\n",
              "  'class SentenceTransformersTokenTextSplitter(TextSplitter):\\n    \"\"\"Splitting text to tokens using sentence model tokenizer.\"\"\"\\n\\n    def __init__(\\n        self,\\n        chunk_overlap: int = 50,\\n        model_name: str = \"sentence-transformers/all-mpnet-base-v2\",\\n        tokens_per_chunk: Optional[int] = None,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(**kwargs, chunk_overlap=chunk_overlap)\\n\\n        try:\\n            from sentence_transformers import SentenceTransformer\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import sentence_transformer python package. \"\\n                \"This is needed in order to for SentenceTransformersTokenTextSplitter. \"\\n                \"Please install it with `pip install sentence-transformers`.\"\\n            )\\n\\n        self.model_name = model_name\\n        self._model = SentenceTransformer(self.model_name)\\n        self.tokenizer = self._model.tokenizer\\n        self._initialize_chunk_configuration(tokens_per_chunk=tokens_per_chunk)\\n\\n    def _initialize_chunk_configuration(\\n        self, *, tokens_per_chunk: Optional[int]\\n    ) -> None:\\n        self.maximum_tokens_per_chunk = cast(int, self._model.max_seq_length)\\n\\n        if tokens_per_chunk is None:\\n            self.tokens_per_chunk = self.maximum_tokens_per_chunk\\n        else:\\n            self.tokens_per_chunk = tokens_per_chunk\\n\\n        if self.tokens_per_chunk > self.maximum_tokens_per_chunk:\\n            raise ValueError(\\n                f\"The token limit of the models \\'{self.model_name}\\'\"\\n                f\" is: {self.maximum_tokens_per_chunk}.\"\\n                f\" Argument tokens_per_chunk={self.tokens_per_chunk}\"\\n                f\" > maximum token limit.\"\\n            )\\n\\n    def split_text(self, text: str) -> List[str]:\\n        def encode_strip_start_and_stop_token_ids(text: str) -> List[int]:\\n            return self._encode(text)[1:-1]',\n",
              "  '\"<nav\",\\n                # Head\\n                \"<head\",\\n                \"<style\",\\n                \"<script\",\\n                \"<meta\",\\n                \"<title\",\\n                \"\",\\n            ]\\n        elif language == Language.CSHARP:\\n            return [\\n                \"\\\\ninterface \",\\n                \"\\\\nenum \",\\n                \"\\\\nimplements \",\\n                \"\\\\ndelegate \",\\n                \"\\\\nevent \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nabstract \",\\n                # Split along method definitions\\n                \"\\\\npublic \",\\n                \"\\\\nprotected \",\\n                \"\\\\nprivate \",\\n                \"\\\\nstatic \",\\n                \"\\\\nreturn \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\ncontinue \",\\n                \"\\\\nfor \",\\n                \"\\\\nforeach \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\nbreak \",\\n                \"\\\\ncase \",\\n                \"\\\\nelse \",\\n                # Split by exceptions\\n                \"\\\\ntry \",\\n                \"\\\\nthrow \",\\n                \"\\\\nfinally \",\\n                \"\\\\ncatch \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SOL:\\n            return [\\n                # Split along compiler information definitions\\n                \"\\\\npragma \",\\n                \"\\\\nusing \",\\n                # Split along contract definitions\\n                \"\\\\ncontract \",\\n                \"\\\\ninterface \",\\n                \"\\\\nlibrary \",\\n                # Split along method definitions\\n                \"\\\\nconstructor \",\\n                \"\\\\ntype \",\\n                \"\\\\nfunction \",\\n                \"\\\\nevent \",\\n                \"\\\\nmodifier \",\\n                \"\\\\nerror \",\\n                \"\\\\nstruct \",\\n                \"\\\\nenum \",\\n                # Split along control flow statements',\n",
              "  'class Tokenizer:\\n    \"\"\"Tokenizer data class.\"\"\"\\n\\n    chunk_overlap: int\\n    \"\"\"Overlap in tokens between chunks\"\"\"\\n    tokens_per_chunk: int\\n    \"\"\"Maximum number of tokens per chunk\"\"\"\\n    decode: Callable[[List[int]], str]\\n    \"\"\" Function to decode a list of token ids to a string\"\"\"\\n    encode: Callable[[str], List[int]]\\n    \"\"\" Function to encode a string to a list of token ids\"\"\"\\n\\n\\ndef split_text_on_tokens(*, text: str, tokenizer: Tokenizer) -> List[str]:\\n    \"\"\"Split incoming text and return chunks using tokenizer.\"\"\"\\n    splits: List[str] = []\\n    input_ids = tokenizer.encode(text)\\n    start_idx = 0\\n    cur_idx = min(start_idx + tokenizer.tokens_per_chunk, len(input_ids))\\n    chunk_ids = input_ids[start_idx:cur_idx]\\n    while start_idx < len(input_ids):\\n        splits.append(tokenizer.decode(chunk_ids))\\n        if cur_idx == len(input_ids):\\n            break\\n        start_idx += tokenizer.tokens_per_chunk - tokenizer.chunk_overlap\\n        cur_idx = min(start_idx + tokenizer.tokens_per_chunk, len(input_ids))\\n        chunk_ids = input_ids[start_idx:cur_idx]\\n    return splits',\n",
              "  '# map xhtml namespace prefix\\n        ns_map = {\"h\": \"http://www.w3.org/1999/xhtml\"}\\n\\n        # build list of elements from DOM\\n        elements = []\\n        for element in result_dom.findall(\"*//*\", ns_map):\\n            if element.findall(\"*[@class=\\'headers\\']\") or element.findall(\\n                \"*[@class=\\'chunk\\']\"\\n            ):\\n                elements.append(\\n                    ElementType(\\n                        url=file,\\n                        xpath=\"\".join(\\n                            [\\n                                node.text\\n                                for node in element.findall(\"*[@class=\\'xpath\\']\", ns_map)\\n                            ]\\n                        ),\\n                        content=\"\".join(\\n                            [\\n                                node.text\\n                                for node in element.findall(\"*[@class=\\'chunk\\']\", ns_map)\\n                            ]\\n                        ),\\n                        metadata={\\n                            # Add text of specified headers to metadata using header\\n                            # mapping.\\n                            header_mapping[node.tag]: node.text\\n                            for node in filter(\\n                                lambda x: x.tag in header_filter,\\n                                element.findall(\"*[@class=\\'headers\\']/*\", ns_map),\\n                            )\\n                        },\\n                    )\\n                )\\n\\n        if not self.return_each_element:\\n            return self.aggregate_elements_to_chunks(elements)\\n        else:\\n            return [\\n                Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n                for chunk in elements\\n            ]\\n\\n\\n# should be in newer Python versions (3.10+)\\n# @dataclass(frozen=True, kw_only=True, slots=True)\\n@dataclass(frozen=True)',\n",
              "  'return [\\n            Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n            for chunk in aggregated_chunks\\n        ]\\n\\n    def split_text_from_url(self, url: str) -> List[Document]:\\n        \"\"\"Split HTML from web URL\\n\\n        Args:\\n            url: web URL\\n        \"\"\"\\n        r = requests.get(url)\\n        return self.split_text_from_file(BytesIO(r.content))\\n\\n    def split_text(self, text: str) -> List[Document]:\\n        \"\"\"Split HTML text string\\n\\n        Args:\\n            text: HTML text\\n        \"\"\"\\n        return self.split_text_from_file(StringIO(text))\\n\\n    def split_text_from_file(self, file: Any) -> List[Document]:\\n        \"\"\"Split HTML file\\n\\n        Args:\\n            file: HTML file\\n        \"\"\"\\n        try:\\n            from lxml import etree\\n        except ImportError as e:\\n            raise ImportError(\\n                \"Unable to import lxml, please install with `pip install lxml`.\"\\n            ) from e\\n        # use lxml library to parse html document and return xml ElementTree\\n        parser = etree.HTMLParser()\\n        tree = etree.parse(file, parser)\\n\\n        # document transformation for \"structure-aware\" chunking is handled with xsl.\\n        # see comments in html_chunks_with_headers.xslt for more detailed information.\\n        xslt_path = (\\n            pathlib.Path(__file__).parent\\n            / \"document_transformers/xsl/html_chunks_with_headers.xslt\"\\n        )\\n        xslt_tree = etree.parse(xslt_path)\\n        transform = etree.XSLT(xslt_tree)\\n        result = transform(tree)\\n        result_dom = etree.fromstring(str(result))\\n\\n        # create filter and mapping for header metadata\\n        header_filter = [header[0] for header in self.headers_to_split_on]\\n        header_mapping = dict(self.headers_to_split_on)\\n\\n        # map xhtml namespace prefix\\n        ns_map = {\"h\": \"http://www.w3.org/1999/xhtml\"}',\n",
              "  'class HTMLHeaderTextSplitter:\\n    \"\"\"\\n    Splitting HTML files based on specified headers.\\n    Requires lxml package.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        headers_to_split_on: List[Tuple[str, str]],\\n        return_each_element: bool = False,\\n    ):\\n        \"\"\"Create a new HTMLHeaderTextSplitter.\\n\\n        Args:\\n            headers_to_split_on: list of tuples of headers we want to track mapped to\\n                (arbitrary) keys for metadata. Allowed header values: h1, h2, h3, h4,\\n                h5, h6 e.g. [(\"h1\", \"Header 1\"), (\"h2\", \"Header 2)].\\n            return_each_element: Return each element w/ associated headers.\\n        \"\"\"\\n        # Output element-by-element or aggregated into chunks w/ common headers\\n        self.return_each_element = return_each_element\\n        self.headers_to_split_on = sorted(headers_to_split_on)\\n\\n    def aggregate_elements_to_chunks(\\n        self, elements: List[ElementType]\\n    ) -> List[Document]:\\n        \"\"\"Combine elements with common metadata into chunks\\n\\n        Args:\\n            elements: HTML element content with associated identifying info and metadata\\n        \"\"\"\\n        aggregated_chunks: List[ElementType] = []\\n\\n        for element in elements:\\n            if (\\n                aggregated_chunks\\n                and aggregated_chunks[-1][\"metadata\"] == element[\"metadata\"]\\n            ):\\n                # If the last element in the aggregated list\\n                # has the same metadata as the current element,\\n                # append the current content to the last element\\'s content\\n                aggregated_chunks[-1][\"content\"] += \"  \\\\n\" + element[\"content\"]\\n            else:\\n                # Otherwise, append the current element to the aggregated list\\n                aggregated_chunks.append(element)\\n\\n        return [\\n            Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n            for chunk in aggregated_chunks\\n        ]',\n",
              "  'class ElementType(TypedDict):\\n    \"\"\"Element type as typed dict.\"\"\"\\n\\n    url: str\\n    xpath: str\\n    content: str\\n    metadata: Dict[str, str]',\n",
              "  'if current_content:\\n            lines_with_metadata.append(\\n                {\"content\": \"\\\\n\".join(current_content), \"metadata\": current_metadata}\\n            )\\n\\n        # lines_with_metadata has each line with associated header metadata\\n        # aggregate these into chunks based on common metadata\\n        if not self.return_each_line:\\n            return self.aggregate_lines_to_chunks(lines_with_metadata)\\n        else:\\n            return [\\n                Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n                for chunk in lines_with_metadata\\n            ]',\n",
              "  '# Push the current header to the stack\\n                        header: HeaderType = {\\n                            \"level\": current_header_level,\\n                            \"name\": name,\\n                            \"data\": stripped_line[len(sep) :].strip(),\\n                        }\\n                        header_stack.append(header)\\n                        # Update initial_metadata with the current header\\n                        initial_metadata[name] = header[\"data\"]\\n\\n                    # Add the previous line to the lines_with_metadata\\n                    # only if current_content is not empty\\n                    if current_content:\\n                        lines_with_metadata.append(\\n                            {\\n                                \"content\": \"\\\\n\".join(current_content),\\n                                \"metadata\": current_metadata.copy(),\\n                            }\\n                        )\\n                        current_content.clear()\\n\\n                    break\\n            else:\\n                if stripped_line:\\n                    current_content.append(stripped_line)\\n                elif current_content:\\n                    lines_with_metadata.append(\\n                        {\\n                            \"content\": \"\\\\n\".join(current_content),\\n                            \"metadata\": current_metadata.copy(),\\n                        }\\n                    )\\n                    current_content.clear()\\n\\n            current_metadata = initial_metadata.copy()\\n\\n        if current_content:\\n            lines_with_metadata.append(\\n                {\"content\": \"\\\\n\".join(current_content), \"metadata\": current_metadata}\\n            )',\n",
              "  'if in_code_block:\\n                current_content.append(stripped_line)\\n                continue\\n\\n            # Check each line against each of the header types (e.g., #, ##)\\n            for sep, name in self.headers_to_split_on:\\n                # Check if line starts with a header that we intend to split on\\n                if stripped_line.startswith(sep) and (\\n                    # Header with no text OR header is followed by space\\n                    # Both are valid conditions that sep is being used a header\\n                    len(stripped_line) == len(sep) or stripped_line[len(sep)] == \" \"\\n                ):\\n                    # Ensure we are tracking the header as metadata\\n                    if name is not None:\\n                        # Get the current header level\\n                        current_header_level = sep.count(\"#\")\\n\\n                        # Pop out headers of lower or same level from the stack\\n                        while (\\n                            header_stack\\n                            and header_stack[-1][\"level\"] >= current_header_level\\n                        ):\\n                            # We have encountered a new header\\n                            # at the same or higher level\\n                            popped_header = header_stack.pop()\\n                            # Clear the metadata for the\\n                            # popped header in initial_metadata\\n                            if popped_header[\"name\"] in initial_metadata:\\n                                initial_metadata.pop(popped_header[\"name\"])',\n",
              "  'if issubclass(cls, TokenTextSplitter):\\n            extra_kwargs = {\\n                \"encoding_name\": encoding_name,\\n                \"model_name\": model_name,\\n                \"allowed_special\": allowed_special,\\n                \"disallowed_special\": disallowed_special,\\n            }\\n            kwargs = {**kwargs, **extra_kwargs}\\n\\n        return cls(length_function=_tiktoken_encoder, **kwargs)\\n\\n    def transform_documents(\\n        self, documents: Sequence[Document], **kwargs: Any\\n    ) -> Sequence[Document]:\\n        \"\"\"Transform sequence of documents by splitting them.\"\"\"\\n        return self.split_documents(list(documents))\\n\\n    async def atransform_documents(\\n        self, documents: Sequence[Document], **kwargs: Any\\n    ) -> Sequence[Document]:\\n        \"\"\"Asynchronously transform a sequence of documents by splitting them.\"\"\"\\n        return await asyncio.get_running_loop().run_in_executor(\\n            None, partial(self.transform_documents, **kwargs), documents\\n        )',\n",
              "  'return [\\n            Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n            for chunk in aggregated_chunks\\n        ]\\n\\n    def split_text_from_url(self, url: str) -> List[Document]:\\n        \"\"\"Split HTML from web URL\\n\\n        Args:\\n            url: web URL\\n        \"\"\"\\n        r = requests.get(url)\\n        return self.split_text_from_file(BytesIO(r.content))\\n\\n    def split_text(self, text: str) -> List[Document]:\\n        \"\"\"Split HTML text string\\n\\n        Args:\\n            text: HTML text\\n        \"\"\"\\n        return self.split_text_from_file(StringIO(text))\\n\\n    def split_text_from_file(self, file: Any) -> List[Document]:\\n        \"\"\"Split HTML file\\n\\n        Args:\\n            file: HTML file\\n        \"\"\"\\n        try:\\n            from lxml import etree\\n        except ImportError as e:\\n            raise ImportError(\\n                \"Unable to import lxml, please install with `pip install lxml`.\"\\n            ) from e\\n        # use lxml library to parse html document and return xml ElementTree\\n        parser = etree.HTMLParser()\\n        tree = etree.parse(file, parser)\\n\\n        # document transformation for \"structure-aware\" chunking is handled with xsl.\\n        # see comments in html_chunks_with_headers.xslt for more detailed information.\\n        xslt_path = (\\n            pathlib.Path(__file__).parent\\n            / \"document_transformers/xsl/html_chunks_with_headers.xslt\"\\n        )\\n        xslt_tree = etree.parse(xslt_path)\\n        transform = etree.XSLT(xslt_tree)\\n        result = transform(tree)\\n        result_dom = etree.fromstring(str(result))\\n\\n        # create filter and mapping for header metadata\\n        header_filter = [header[0] for header in self.headers_to_split_on]\\n        header_mapping = dict(self.headers_to_split_on)\\n\\n        # map xhtml namespace prefix\\n        ns_map = {\"h\": \"http://www.w3.org/1999/xhtml\"}',\n",
              "  'class HTMLHeaderTextSplitter:\\n    \"\"\"\\n    Splitting HTML files based on specified headers.\\n    Requires lxml package.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        headers_to_split_on: List[Tuple[str, str]],\\n        return_each_element: bool = False,\\n    ):\\n        \"\"\"Create a new HTMLHeaderTextSplitter.\\n\\n        Args:\\n            headers_to_split_on: list of tuples of headers we want to track mapped to\\n                (arbitrary) keys for metadata. Allowed header values: h1, h2, h3, h4,\\n                h5, h6 e.g. [(\"h1\", \"Header 1\"), (\"h2\", \"Header 2)].\\n            return_each_element: Return each element w/ associated headers.\\n        \"\"\"\\n        # Output element-by-element or aggregated into chunks w/ common headers\\n        self.return_each_element = return_each_element\\n        self.headers_to_split_on = sorted(headers_to_split_on)\\n\\n    def aggregate_elements_to_chunks(\\n        self, elements: List[ElementType]\\n    ) -> List[Document]:\\n        \"\"\"Combine elements with common metadata into chunks\\n\\n        Args:\\n            elements: HTML element content with associated identifying info and metadata\\n        \"\"\"\\n        aggregated_chunks: List[ElementType] = []\\n\\n        for element in elements:\\n            if (\\n                aggregated_chunks\\n                and aggregated_chunks[-1][\"metadata\"] == element[\"metadata\"]\\n            ):\\n                # If the last element in the aggregated list\\n                # has the same metadata as the current element,\\n                # append the current content to the last element\\'s content\\n                aggregated_chunks[-1][\"content\"] += \"  \\\\n\" + element[\"content\"]\\n            else:\\n                # Otherwise, append the current element to the aggregated list\\n                aggregated_chunks.append(element)\\n\\n        return [\\n            Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n            for chunk in aggregated_chunks\\n        ]',\n",
              "  'class ElementType(TypedDict):\\n    \"\"\"Element type as typed dict.\"\"\"\\n\\n    url: str\\n    xpath: str\\n    content: str\\n    metadata: Dict[str, str]',\n",
              "  'if current_content:\\n            lines_with_metadata.append(\\n                {\"content\": \"\\\\n\".join(current_content), \"metadata\": current_metadata}\\n            )\\n\\n        # lines_with_metadata has each line with associated header metadata\\n        # aggregate these into chunks based on common metadata\\n        if not self.return_each_line:\\n            return self.aggregate_lines_to_chunks(lines_with_metadata)\\n        else:\\n            return [\\n                Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n                for chunk in lines_with_metadata\\n            ]',\n",
              "  '# Push the current header to the stack\\n                        header: HeaderType = {\\n                            \"level\": current_header_level,\\n                            \"name\": name,\\n                            \"data\": stripped_line[len(sep) :].strip(),\\n                        }\\n                        header_stack.append(header)\\n                        # Update initial_metadata with the current header\\n                        initial_metadata[name] = header[\"data\"]\\n\\n                    # Add the previous line to the lines_with_metadata\\n                    # only if current_content is not empty\\n                    if current_content:\\n                        lines_with_metadata.append(\\n                            {\\n                                \"content\": \"\\\\n\".join(current_content),\\n                                \"metadata\": current_metadata.copy(),\\n                            }\\n                        )\\n                        current_content.clear()\\n\\n                    break\\n            else:\\n                if stripped_line:\\n                    current_content.append(stripped_line)\\n                elif current_content:\\n                    lines_with_metadata.append(\\n                        {\\n                            \"content\": \"\\\\n\".join(current_content),\\n                            \"metadata\": current_metadata.copy(),\\n                        }\\n                    )\\n                    current_content.clear()\\n\\n            current_metadata = initial_metadata.copy()\\n\\n        if current_content:\\n            lines_with_metadata.append(\\n                {\"content\": \"\\\\n\".join(current_content), \"metadata\": current_metadata}\\n            )',\n",
              "  'if in_code_block:\\n                current_content.append(stripped_line)\\n                continue\\n\\n            # Check each line against each of the header types (e.g., #, ##)\\n            for sep, name in self.headers_to_split_on:\\n                # Check if line starts with a header that we intend to split on\\n                if stripped_line.startswith(sep) and (\\n                    # Header with no text OR header is followed by space\\n                    # Both are valid conditions that sep is being used a header\\n                    len(stripped_line) == len(sep) or stripped_line[len(sep)] == \" \"\\n                ):\\n                    # Ensure we are tracking the header as metadata\\n                    if name is not None:\\n                        # Get the current header level\\n                        current_header_level = sep.count(\"#\")\\n\\n                        # Pop out headers of lower or same level from the stack\\n                        while (\\n                            header_stack\\n                            and header_stack[-1][\"level\"] >= current_header_level\\n                        ):\\n                            # We have encountered a new header\\n                            # at the same or higher level\\n                            popped_header = header_stack.pop()\\n                            # Clear the metadata for the\\n                            # popped header in initial_metadata\\n                            if popped_header[\"name\"] in initial_metadata:\\n                                initial_metadata.pop(popped_header[\"name\"])',\n",
              "  'def split_text(self, text: str) -> List[Document]:\\n        \"\"\"Split markdown file\\n        Args:\\n            text: Markdown file\"\"\"\\n\\n        # Split the input text by newline character (\"\\\\n\").\\n        lines = text.split(\"\\\\n\")\\n        # Final output\\n        lines_with_metadata: List[LineType] = []\\n        # Content and metadata of the chunk currently being processed\\n        current_content: List[str] = []\\n        current_metadata: Dict[str, str] = {}\\n        # Keep track of the nested header structure\\n        # header_stack: List[Dict[str, Union[int, str]]] = []\\n        header_stack: List[HeaderType] = []\\n        initial_metadata: Dict[str, str] = {}\\n\\n        in_code_block = False\\n        opening_fence = \"\"\\n\\n        for line in lines:\\n            stripped_line = line.strip()\\n\\n            if not in_code_block:\\n                # Exclude inline code spans\\n                if stripped_line.startswith(\"```\") and stripped_line.count(\"```\") == 1:\\n                    in_code_block = True\\n                    opening_fence = \"```\"\\n                elif stripped_line.startswith(\"~~~\"):\\n                    in_code_block = True\\n                    opening_fence = \"~~~\"\\n            else:\\n                if stripped_line.startswith(opening_fence):\\n                    in_code_block = False\\n                    opening_fence = \"\"\\n\\n            if in_code_block:\\n                current_content.append(stripped_line)\\n                continue',\n",
              "  'class MarkdownHeaderTextSplitter:\\n    \"\"\"Splitting markdown files based on specified headers.\"\"\"\\n\\n    def __init__(\\n        self, headers_to_split_on: List[Tuple[str, str]], return_each_line: bool = False\\n    ):\\n        \"\"\"Create a new MarkdownHeaderTextSplitter.\\n\\n        Args:\\n            headers_to_split_on: Headers we want to track\\n            return_each_line: Return each line w/ associated headers\\n        \"\"\"\\n        # Output line-by-line or aggregated into chunks w/ common headers\\n        self.return_each_line = return_each_line\\n        # Given the headers we want to split on,\\n        # (e.g., \"#, ##, etc\") order by length\\n        self.headers_to_split_on = sorted(\\n            headers_to_split_on, key=lambda split: len(split[0]), reverse=True\\n        )\\n\\n    def aggregate_lines_to_chunks(self, lines: List[LineType]) -> List[Document]:\\n        \"\"\"Combine lines with common metadata into chunks\\n        Args:\\n            lines: Line of text / associated header metadata\\n        \"\"\"\\n        aggregated_chunks: List[LineType] = []\\n\\n        for line in lines:\\n            if (\\n                aggregated_chunks\\n                and aggregated_chunks[-1][\"metadata\"] == line[\"metadata\"]\\n            ):\\n                # If the last line in the aggregated list\\n                # has the same metadata as the current line,\\n                # append the current content to the last lines\\'s content\\n                aggregated_chunks[-1][\"content\"] += \"  \\\\n\" + line[\"content\"]\\n            else:\\n                # Otherwise, append the current line to the aggregated list\\n                aggregated_chunks.append(line)\\n\\n        return [\\n            Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n            for chunk in aggregated_chunks\\n        ]\\n\\n    def split_text(self, text: str) -> List[Document]:\\n        \"\"\"Split markdown file\\n        Args:\\n            text: Markdown file\"\"\"',\n",
              "  'class CharacterTextSplitter(TextSplitter):\\n    \"\"\"Splitting text that looks at characters.\"\"\"\\n\\n    def __init__(\\n        self, separator: str = \"\\\\n\\\\n\", is_separator_regex: bool = False, **kwargs: Any\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(**kwargs)\\n        self._separator = separator\\n        self._is_separator_regex = is_separator_regex\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        # First we naively split the large input into a bunch of smaller ones.\\n        separator = (\\n            self._separator if self._is_separator_regex else re.escape(self._separator)\\n        )\\n        splits = _split_text_with_regex(text, separator, self._keep_separator)\\n        _separator = \"\" if self._keep_separator else self._separator\\n        return self._merge_splits(splits, _separator)\\n\\n\\nclass LineType(TypedDict):\\n    \"\"\"Line type as typed dict.\"\"\"\\n\\n    metadata: Dict[str, str]\\n    content: str\\n\\n\\nclass HeaderType(TypedDict):\\n    \"\"\"Header type as typed dict.\"\"\"\\n\\n    level: int\\n    name: str\\n    data: str',\n",
              "  '# map xhtml namespace prefix\\n        ns_map = {\"h\": \"http://www.w3.org/1999/xhtml\"}\\n\\n        # build list of elements from DOM\\n        elements = []\\n        for element in result_dom.findall(\"*//*\", ns_map):\\n            if element.findall(\"*[@class=\\'headers\\']\") or element.findall(\\n                \"*[@class=\\'chunk\\']\"\\n            ):\\n                elements.append(\\n                    ElementType(\\n                        url=file,\\n                        xpath=\"\".join(\\n                            [\\n                                node.text\\n                                for node in element.findall(\"*[@class=\\'xpath\\']\", ns_map)\\n                            ]\\n                        ),\\n                        content=\"\".join(\\n                            [\\n                                node.text\\n                                for node in element.findall(\"*[@class=\\'chunk\\']\", ns_map)\\n                            ]\\n                        ),\\n                        metadata={\\n                            # Add text of specified headers to metadata using header\\n                            # mapping.\\n                            header_mapping[node.tag]: node.text\\n                            for node in filter(\\n                                lambda x: x.tag in header_filter,\\n                                element.findall(\"*[@class=\\'headers\\']/*\", ns_map),\\n                            )\\n                        },\\n                    )\\n                )\\n\\n        if not self.return_each_element:\\n            return self.aggregate_elements_to_chunks(elements)\\n        else:\\n            return [\\n                Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n                for chunk in elements\\n            ]\\n\\n\\n# should be in newer Python versions (3.10+)\\n# @dataclass(frozen=True, kw_only=True, slots=True)\\n@dataclass(frozen=True)',\n",
              "  'if not isinstance(tokenizer, PreTrainedTokenizerBase):\\n                raise ValueError(\\n                    \"Tokenizer received was not an instance of PreTrainedTokenizerBase\"\\n                )\\n\\n            def _huggingface_tokenizer_length(text: str) -> int:\\n                return len(tokenizer.encode(text))\\n\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import transformers python package. \"\\n                \"Please install it with `pip install transformers`.\"\\n            )\\n        return cls(length_function=_huggingface_tokenizer_length, **kwargs)\\n\\n    @classmethod\\n    def from_tiktoken_encoder(\\n        cls: Type[TS],\\n        encoding_name: str = \"gpt2\",\\n        model_name: Optional[str] = None,\\n        allowed_special: Union[Literal[\"all\"], AbstractSet[str]] = set(),\\n        disallowed_special: Union[Literal[\"all\"], Collection[str]] = \"all\",\\n        **kwargs: Any,\\n    ) -> TS:\\n        \"\"\"Text splitter that uses tiktoken encoder to count length.\"\"\"\\n        try:\\n            import tiktoken\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import tiktoken python package. \"\\n                \"This is needed in order to calculate max_tokens_for_prompt. \"\\n                \"Please install it with `pip install tiktoken`.\"\\n            )\\n\\n        if model_name is not None:\\n            enc = tiktoken.encoding_for_model(model_name)\\n        else:\\n            enc = tiktoken.get_encoding(encoding_name)\\n\\n        def _tiktoken_encoder(text: str) -> int:\\n            return len(\\n                enc.encode(\\n                    text,\\n                    allowed_special=allowed_special,\\n                    disallowed_special=disallowed_special,\\n                )\\n            )',\n",
              "  'docs = []\\n        current_doc: List[str] = []\\n        total = 0\\n        for d in splits:\\n            _len = self._length_function(d)\\n            if (\\n                total + _len + (separator_len if len(current_doc) > 0 else 0)\\n                > self._chunk_size\\n            ):\\n                if total > self._chunk_size:\\n                    logger.warning(\\n                        f\"Created a chunk of size {total}, \"\\n                        f\"which is longer than the specified {self._chunk_size}\"\\n                    )\\n                if len(current_doc) > 0:\\n                    doc = self._join_docs(current_doc, separator)\\n                    if doc is not None:\\n                        docs.append(doc)\\n                    # Keep on popping if:\\n                    # - we have a larger chunk than in the chunk overlap\\n                    # - or if we still have any chunks and the length is long\\n                    while total > self._chunk_overlap or (\\n                        total + _len + (separator_len if len(current_doc) > 0 else 0)\\n                        > self._chunk_size\\n                        and total > 0\\n                    ):\\n                        total -= self._length_function(current_doc[0]) + (\\n                            separator_len if len(current_doc) > 1 else 0\\n                        )\\n                        current_doc = current_doc[1:]\\n            current_doc.append(d)\\n            total += _len + (separator_len if len(current_doc) > 1 else 0)\\n        doc = self._join_docs(current_doc, separator)\\n        if doc is not None:\\n            docs.append(doc)\\n        return docs\\n\\n    @classmethod\\n    def from_huggingface_tokenizer(cls, tokenizer: Any, **kwargs: Any) -> TextSplitter:\\n        \"\"\"Text splitter that uses HuggingFace tokenizer to count length.\"\"\"\\n        try:\\n            from transformers import PreTrainedTokenizerBase',\n",
              "  '@abstractmethod\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split text into multiple components.\"\"\"\\n\\n    def create_documents(\\n        self, texts: List[str], metadatas: Optional[List[dict]] = None\\n    ) -> List[Document]:\\n        \"\"\"Create documents from a list of texts.\"\"\"\\n        _metadatas = metadatas or [{}] * len(texts)\\n        documents = []\\n        for i, text in enumerate(texts):\\n            index = -1\\n            for chunk in self.split_text(text):\\n                metadata = copy.deepcopy(_metadatas[i])\\n                if self._add_start_index:\\n                    index = text.find(chunk, index + 1)\\n                    metadata[\"start_index\"] = index\\n                new_doc = Document(page_content=chunk, metadata=metadata)\\n                documents.append(new_doc)\\n        return documents\\n\\n    def split_documents(self, documents: Iterable[Document]) -> List[Document]:\\n        \"\"\"Split documents.\"\"\"\\n        texts, metadatas = [], []\\n        for doc in documents:\\n            texts.append(doc.page_content)\\n            metadatas.append(doc.metadata)\\n        return self.create_documents(texts, metadatas=metadatas)\\n\\n    def _join_docs(self, docs: List[str], separator: str) -> Optional[str]:\\n        text = separator.join(docs)\\n        if self._strip_whitespace:\\n            text = text.strip()\\n        if text == \"\":\\n            return None\\n        else:\\n            return text\\n\\n    def _merge_splits(self, splits: Iterable[str], separator: str) -> List[str]:\\n        # We now want to combine these smaller pieces into medium size\\n        # chunks to send to the LLM.\\n        separator_len = self._length_function(separator)',\n",
              "  'class TextSplitter(BaseDocumentTransformer, ABC):\\n    \"\"\"Interface for splitting text into chunks.\"\"\"\\n\\n    def __init__(\\n        self,\\n        chunk_size: int = 4000,\\n        chunk_overlap: int = 200,\\n        length_function: Callable[[str], int] = len,\\n        keep_separator: bool = False,\\n        add_start_index: bool = False,\\n        strip_whitespace: bool = True,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\\n\\n        Args:\\n            chunk_size: Maximum size of chunks to return\\n            chunk_overlap: Overlap in characters between chunks\\n            length_function: Function that measures the length of given chunks\\n            keep_separator: Whether to keep the separator in the chunks\\n            add_start_index: If `True`, includes chunk\\'s start index in metadata\\n            strip_whitespace: If `True`, strips whitespace from the start and end of\\n                              every document\\n        \"\"\"\\n        if chunk_overlap > chunk_size:\\n            raise ValueError(\\n                f\"Got a larger chunk overlap ({chunk_overlap}) than chunk size \"\\n                f\"({chunk_size}), should be smaller.\"\\n            )\\n        self._chunk_size = chunk_size\\n        self._chunk_overlap = chunk_overlap\\n        self._length_function = length_function\\n        self._keep_separator = keep_separator\\n        self._add_start_index = add_start_index\\n        self._strip_whitespace = strip_whitespace\\n\\n    @abstractmethod\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split text into multiple components.\"\"\"',\n",
              "  'def _split_text_with_regex(\\n    text: str, separator: str, keep_separator: bool\\n) -> List[str]:\\n    # Now that we have the separator, split the text\\n    if separator:\\n        if keep_separator:\\n            # The parentheses in the pattern keep the delimiters in the result.\\n            _splits = re.split(f\"({separator})\", text)\\n            splits = [_splits[i] + _splits[i + 1] for i in range(1, len(_splits), 2)]\\n            if len(_splits) % 2 == 0:\\n                splits += _splits[-1:]\\n            splits = [_splits[0]] + splits\\n        else:\\n            splits = re.split(separator, text)\\n    else:\\n        splits = list(text)\\n    return [s for s in splits if s != \"\"]',\n",
              "  '\"\"\"**Text Splitters** are classes for splitting text.\\n\\n\\n**Class hierarchy:**\\n\\n.. code-block::\\n\\n    BaseDocumentTransformer --> TextSplitter --> <name>TextSplitter  # Example: CharacterTextSplitter\\n                                                 RecursiveCharacterTextSplitter -->  <name>TextSplitter\\n\\nNote: **MarkdownHeaderTextSplitter** and **HTMLHeaderTextSplitter do not derive from TextSplitter.\\n\\n\\n**Main helpers:**\\n\\n.. code-block::\\n\\n    Document, Tokenizer, Language, LineType, HeaderType\\n\\n\"\"\"  # noqa: E501\\n\\nfrom __future__ import annotations\\n\\nimport asyncio\\nimport copy\\nimport logging\\nimport pathlib\\nimport re\\nfrom abc import ABC, abstractmethod\\nfrom dataclasses import dataclass\\nfrom enum import Enum\\nfrom functools import partial\\nfrom io import BytesIO, StringIO\\nfrom typing import (\\n    AbstractSet,\\n    Any,\\n    Callable,\\n    Collection,\\n    Dict,\\n    Iterable,\\n    List,\\n    Literal,\\n    Optional,\\n    Sequence,\\n    Tuple,\\n    Type,\\n    TypedDict,\\n    TypeVar,\\n    Union,\\n    cast,\\n)\\n\\nimport requests\\nfrom langchain_core.documents import BaseDocumentTransformer, Document\\n\\nlogger = logging.getLogger(__name__)\\n\\nTS = TypeVar(\"TS\", bound=\"TextSplitter\")\\n\\n\\ndef _make_spacy_pipeline_for_splitting(\\n    pipeline: str, *, max_length: int = 1_000_000\\n) -> Any:  # avoid importing spacy\\n    try:\\n        import spacy\\n    except ImportError:\\n        raise ImportError(\\n            \"Spacy is not installed, please install it with `pip install spacy`.\"\\n        )\\n    if pipeline == \"sentencizer\":\\n        from spacy.lang.en import English\\n\\n        sentencizer = English()\\n        sentencizer.add_pipe(\"sentencizer\")\\n    else:\\n        sentencizer = spacy.load(pipeline, exclude=[\"ner\", \"tagger\"])\\n        sentencizer.max_length = max_length\\n    return sentencizer',\n",
              "  'class PythonCodeTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Python syntax.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a PythonCodeTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.PYTHON)\\n        super().__init__(separators=separators, **kwargs)\\n\\n\\nclass MarkdownTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Markdown-formatted headings.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a MarkdownTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.MARKDOWN)\\n        super().__init__(separators=separators, **kwargs)\\n\\n\\nclass LatexTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Latex-formatted layout elements.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a LatexTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.LATEX)\\n        super().__init__(separators=separators, **kwargs)',\n",
              "  'class NLTKTextSplitter(TextSplitter):\\n    \"\"\"Splitting text using NLTK package.\"\"\"\\n\\n    def __init__(\\n        self, separator: str = \"\\\\n\\\\n\", language: str = \"english\", **kwargs: Any\\n    ) -> None:\\n        \"\"\"Initialize the NLTK splitter.\"\"\"\\n        super().__init__(**kwargs)\\n        try:\\n            from nltk.tokenize import sent_tokenize\\n\\n            self._tokenizer = sent_tokenize\\n        except ImportError:\\n            raise ImportError(\\n                \"NLTK is not installed, please install it with `pip install nltk`.\"\\n            )\\n        self._separator = separator\\n        self._language = language\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        # First we naively split the large input into a bunch of smaller ones.\\n        splits = self._tokenizer(text, language=self._language)\\n        return self._merge_splits(splits, self._separator)\\n\\n\\nclass SpacyTextSplitter(TextSplitter):\\n    \"\"\"Splitting text using Spacy package.\\n\\n\\n    Per default, Spacy\\'s `en_core_web_sm` model is used and\\n    its default max_length is 1000000 (it is the length of maximum character\\n    this model takes which can be increased for large files). For a faster, but\\n    potentially less accurate splitting, you can use `pipeline=\\'sentencizer\\'`.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        separator: str = \"\\\\n\\\\n\",\\n        pipeline: str = \"en_core_web_sm\",\\n        max_length: int = 1_000_000,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Initialize the spacy text splitter.\"\"\"\\n        super().__init__(**kwargs)\\n        self._tokenizer = _make_spacy_pipeline_for_splitting(\\n            pipeline, max_length=max_length\\n        )\\n        self._separator = separator\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        splits = (s.text for s in self._tokenizer(text).sents)\\n        return self._merge_splits(splits, self._separator)\\n\\n\\n# For backwards compatibility',\n",
              "  '\"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.KOTLIN:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along method definitions\\n                \"\\\\npublic \",\\n                \"\\\\nprotected \",\\n                \"\\\\nprivate \",\\n                \"\\\\ninternal \",\\n                \"\\\\ncompanion \",\\n                \"\\\\nfun \",\\n                \"\\\\nval \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nwhen \",\\n                \"\\\\ncase \",\\n                \"\\\\nelse \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.JS:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunction \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                \"\\\\nvar \",\\n                \"\\\\nclass \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                \"\\\\ndefault \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.TS:\\n            return [\\n                \"\\\\nenum \",\\n                \"\\\\ninterface \",\\n                \"\\\\nnamespace \",\\n                \"\\\\ntype \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along function definitions\\n                \"\\\\nfunction \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements',\n",
              "  'class PythonCodeTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Python syntax.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a PythonCodeTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.PYTHON)\\n        super().__init__(separators=separators, **kwargs)\\n\\n\\nclass MarkdownTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Markdown-formatted headings.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a MarkdownTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.MARKDOWN)\\n        super().__init__(separators=separators, **kwargs)\\n\\n\\nclass LatexTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Latex-formatted layout elements.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a LatexTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.LATEX)\\n        super().__init__(separators=separators, **kwargs)',\n",
              "  'class NLTKTextSplitter(TextSplitter):\\n    \"\"\"Splitting text using NLTK package.\"\"\"\\n\\n    def __init__(\\n        self, separator: str = \"\\\\n\\\\n\", language: str = \"english\", **kwargs: Any\\n    ) -> None:\\n        \"\"\"Initialize the NLTK splitter.\"\"\"\\n        super().__init__(**kwargs)\\n        try:\\n            from nltk.tokenize import sent_tokenize\\n\\n            self._tokenizer = sent_tokenize\\n        except ImportError:\\n            raise ImportError(\\n                \"NLTK is not installed, please install it with `pip install nltk`.\"\\n            )\\n        self._separator = separator\\n        self._language = language\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        # First we naively split the large input into a bunch of smaller ones.\\n        splits = self._tokenizer(text, language=self._language)\\n        return self._merge_splits(splits, self._separator)\\n\\n\\nclass SpacyTextSplitter(TextSplitter):\\n    \"\"\"Splitting text using Spacy package.\\n\\n\\n    Per default, Spacy\\'s `en_core_web_sm` model is used and\\n    its default max_length is 1000000 (it is the length of maximum character\\n    this model takes which can be increased for large files). For a faster, but\\n    potentially less accurate splitting, you can use `pipeline=\\'sentencizer\\'`.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        separator: str = \"\\\\n\\\\n\",\\n        pipeline: str = \"en_core_web_sm\",\\n        max_length: int = 1_000_000,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Initialize the spacy text splitter.\"\"\"\\n        super().__init__(**kwargs)\\n        self._tokenizer = _make_spacy_pipeline_for_splitting(\\n            pipeline, max_length=max_length\\n        )\\n        self._separator = separator\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        splits = (s.text for s in self._tokenizer(text).sents)\\n        return self._merge_splits(splits, self._separator)\\n\\n\\n# For backwards compatibility',\n",
              "  'else:\\n            raise ValueError(\\n                f\"Language {language} is not supported! \"\\n                f\"Please choose from {list(Language)}\"\\n            )',\n",
              "  '\"\\\\nevent \",\\n                \"\\\\nmodifier \",\\n                \"\\\\nerror \",\\n                \"\\\\nstruct \",\\n                \"\\\\nenum \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo while \",\\n                \"\\\\nassembly \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.COBOL:\\n            return [\\n                # Split along divisions\\n                \"\\\\nIDENTIFICATION DIVISION.\",\\n                \"\\\\nENVIRONMENT DIVISION.\",\\n                \"\\\\nDATA DIVISION.\",\\n                \"\\\\nPROCEDURE DIVISION.\",\\n                # Split along sections within DATA DIVISION\\n                \"\\\\nWORKING-STORAGE SECTION.\",\\n                \"\\\\nLINKAGE SECTION.\",\\n                \"\\\\nFILE SECTION.\",\\n                # Split along sections within PROCEDURE DIVISION\\n                \"\\\\nINPUT-OUTPUT SECTION.\",\\n                # Split along paragraphs and common statements\\n                \"\\\\nOPEN \",\\n                \"\\\\nCLOSE \",\\n                \"\\\\nREAD \",\\n                \"\\\\nWRITE \",\\n                \"\\\\nIF \",\\n                \"\\\\nELSE \",\\n                \"\\\\nMOVE \",\\n                \"\\\\nPERFORM \",\\n                \"\\\\nUNTIL \",\\n                \"\\\\nVARYING \",\\n                \"\\\\nACCEPT \",\\n                \"\\\\nDISPLAY \",\\n                \"\\\\nSTOP RUN.\",\\n                # Split by the normal type of lines\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]',\n",
              "  '\"<nav\",\\n                # Head\\n                \"<head\",\\n                \"<style\",\\n                \"<script\",\\n                \"<meta\",\\n                \"<title\",\\n                \"\",\\n            ]\\n        elif language == Language.CSHARP:\\n            return [\\n                \"\\\\ninterface \",\\n                \"\\\\nenum \",\\n                \"\\\\nimplements \",\\n                \"\\\\ndelegate \",\\n                \"\\\\nevent \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nabstract \",\\n                # Split along method definitions\\n                \"\\\\npublic \",\\n                \"\\\\nprotected \",\\n                \"\\\\nprivate \",\\n                \"\\\\nstatic \",\\n                \"\\\\nreturn \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\ncontinue \",\\n                \"\\\\nfor \",\\n                \"\\\\nforeach \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\nbreak \",\\n                \"\\\\ncase \",\\n                \"\\\\nelse \",\\n                # Split by exceptions\\n                \"\\\\ntry \",\\n                \"\\\\nthrow \",\\n                \"\\\\nfinally \",\\n                \"\\\\ncatch \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SOL:\\n            return [\\n                # Split along compiler information definitions\\n                \"\\\\npragma \",\\n                \"\\\\nusing \",\\n                # Split along contract definitions\\n                \"\\\\ncontract \",\\n                \"\\\\ninterface \",\\n                \"\\\\nlibrary \",\\n                # Split along method definitions\\n                \"\\\\nconstructor \",\\n                \"\\\\ntype \",\\n                \"\\\\nfunction \",\\n                \"\\\\nevent \",\\n                \"\\\\nmodifier \",\\n                \"\\\\nerror \",\\n                \"\\\\nstruct \",\\n                \"\\\\nenum \",\\n                # Split along control flow statements',\n",
              "  '\"\\\\n\\\\\\\\*\\\\\\\\*\\\\\\\\*+\\\\n\",\\n                \"\\\\n---+\\\\n\",\\n                \"\\\\n___+\\\\n\",\\n                # Note that this splitter doesn\\'t handle horizontal lines defined\\n                # by *three or more* of ***, ---, or ___, but this is not handled\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.LATEX:\\n            return [\\n                # First, try to split along Latex sections\\n                \"\\\\n\\\\\\\\\\\\\\\\chapter{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\section{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\subsection{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\subsubsection{\",\\n                # Now split by environments\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{enumerate}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{itemize}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{description}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{list}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{quote}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{quotation}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{verse}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{verbatim}\",\\n                # Now split by math environments\\n                \"\\\\n\\\\\\\\\\\\begin{align}\",\\n                \"$$\",\\n                \"$\",\\n                # Now split by the normal type of lines\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.HTML:\\n            return [\\n                # First, try to split along HTML tags\\n                \"<body\",\\n                \"<div\",\\n                \"<p\",\\n                \"<br\",\\n                \"<li\",\\n                \"<h1\",\\n                \"<h2\",\\n                \"<h3\",\\n                \"<h4\",\\n                \"<h5\",\\n                \"<h6\",\\n                \"<span\",\\n                \"<table\",\\n                \"<tr\",\\n                \"<td\",\\n                \"<th\",\\n                \"<ul\",\\n                \"<ol\",\\n                \"<header\",\\n                \"<footer\",\\n                \"<nav\",\\n                # Head\\n                \"<head\",\\n                \"<style\",\\n                \"<script\",\\n                \"<meta\",\\n                \"<title\",',\n",
              "  '\" \",\\n                \"\",\\n            ]\\n        elif language == Language.SCALA:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nobject \",\\n                # Split along method definitions\\n                \"\\\\ndef \",\\n                \"\\\\nval \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nmatch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SWIFT:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunc \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nstruct \",\\n                \"\\\\nenum \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.MARKDOWN:\\n            return [\\n                # First, try to split along Markdown headings (starting with level 2)\\n                \"\\\\n#{1,6} \",\\n                # Note the alternative syntax for headings (below) is not handled here\\n                # Heading level 2\\n                # ---------------\\n                # End of code block\\n                \"```\\\\n\",\\n                # Horizontal lines\\n                \"\\\\n\\\\\\\\*\\\\\\\\*\\\\\\\\*+\\\\n\",\\n                \"\\\\n---+\\\\n\",\\n                \"\\\\n___+\\\\n\",\\n                # Note that this splitter doesn\\'t handle horizontal lines defined',\n",
              "  'return [\\n                # First, try to split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\ndef \",\\n                \"\\\\n\\\\tdef \",\\n                # Now split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.RST:\\n            return [\\n                # Split along section titles\\n                \"\\\\n=+\\\\n\",\\n                \"\\\\n-+\\\\n\",\\n                \"\\\\n\\\\\\\\*+\\\\n\",\\n                # Split along directive markers\\n                \"\\\\n\\\\n.. *\\\\n\\\\n\",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.RUBY:\\n            return [\\n                # Split along method definitions\\n                \"\\\\ndef \",\\n                \"\\\\nclass \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nunless \",\\n                \"\\\\nwhile \",\\n                \"\\\\nfor \",\\n                \"\\\\ndo \",\\n                \"\\\\nbegin \",\\n                \"\\\\nrescue \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.RUST:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfn \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nwhile \",\\n                \"\\\\nfor \",\\n                \"\\\\nloop \",\\n                \"\\\\nmatch \",\\n                \"\\\\nconst \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SCALA:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",',\n",
              "  '\"\\\\nfunction \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                \"\\\\ndefault \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.PHP:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunction \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nforeach \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.PROTO:\\n            return [\\n                # Split along message definitions\\n                \"\\\\nmessage \",\\n                # Split along service definitions\\n                \"\\\\nservice \",\\n                # Split along enum definitions\\n                \"\\\\nenum \",\\n                # Split along option definitions\\n                \"\\\\noption \",\\n                # Split along import statements\\n                \"\\\\nimport \",\\n                # Split along syntax declarations\\n                \"\\\\nsyntax \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.PYTHON:\\n            return [\\n                # First, try to split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\ndef \",\\n                \"\\\\n\\\\tdef \",',\n",
              "  'else:\\n            raise ValueError(\\n                f\"Language {language} is not supported! \"\\n                f\"Please choose from {list(Language)}\"\\n            )',\n",
              "  '@staticmethod\\n    def get_separators_for_language(language: Language) -> List[str]:\\n        if language == Language.CPP:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along function definitions\\n                \"\\\\nvoid \",\\n                \"\\\\nint \",\\n                \"\\\\nfloat \",\\n                \"\\\\ndouble \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.GO:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunc \",\\n                \"\\\\nvar \",\\n                \"\\\\nconst \",\\n                \"\\\\ntype \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.JAVA:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along method definitions\\n                \"\\\\npublic \",\\n                \"\\\\nprotected \",\\n                \"\\\\nprivate \",\\n                \"\\\\nstatic \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.KOTLIN:\\n            return [',\n",
              "  '_separator = separator if self._is_separator_regex else re.escape(separator)\\n        splits = _split_text_with_regex(text, _separator, self._keep_separator)\\n\\n        # Now go merging things, recursively splitting longer texts.\\n        _good_splits = []\\n        _separator = \"\" if self._keep_separator else separator\\n        for s in splits:\\n            if self._length_function(s) < self._chunk_size:\\n                _good_splits.append(s)\\n            else:\\n                if _good_splits:\\n                    merged_text = self._merge_splits(_good_splits, _separator)\\n                    final_chunks.extend(merged_text)\\n                    _good_splits = []\\n                if not new_separators:\\n                    final_chunks.append(s)\\n                else:\\n                    other_info = self._split_text(s, new_separators)\\n                    final_chunks.extend(other_info)\\n        if _good_splits:\\n            merged_text = self._merge_splits(_good_splits, _separator)\\n            final_chunks.extend(merged_text)\\n        return final_chunks\\n\\n    def split_text(self, text: str) -> List[str]:\\n        return self._split_text(text, self._separators)\\n\\n    @classmethod\\n    def from_language(\\n        cls, language: Language, **kwargs: Any\\n    ) -> RecursiveCharacterTextSplitter:\\n        separators = cls.get_separators_for_language(language)\\n        return cls(separators=separators, is_separator_regex=True, **kwargs)',\n",
              "  'class RecursiveCharacterTextSplitter(TextSplitter):\\n    \"\"\"Splitting text by recursively look at characters.\\n\\n    Recursively tries to split by different characters to find one\\n    that works.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        separators: Optional[List[str]] = None,\\n        keep_separator: bool = True,\\n        is_separator_regex: bool = False,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(keep_separator=keep_separator, **kwargs)\\n        self._separators = separators or [\"\\\\n\\\\n\", \"\\\\n\", \" \", \"\"]\\n        self._is_separator_regex = is_separator_regex\\n\\n    def _split_text(self, text: str, separators: List[str]) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        final_chunks = []\\n        # Get appropriate separator to use\\n        separator = separators[-1]\\n        new_separators = []\\n        for i, _s in enumerate(separators):\\n            _separator = _s if self._is_separator_regex else re.escape(_s)\\n            if _s == \"\":\\n                separator = _s\\n                break\\n            if re.search(_separator, text):\\n                separator = _s\\n                new_separators = separators[i + 1 :]\\n                break\\n\\n        _separator = separator if self._is_separator_regex else re.escape(separator)\\n        splits = _split_text_with_regex(text, _separator, self._keep_separator)',\n",
              "  'class Language(str, Enum):\\n    \"\"\"Enum of the programming languages.\"\"\"\\n\\n    CPP = \"cpp\"\\n    GO = \"go\"\\n    JAVA = \"java\"\\n    KOTLIN = \"kotlin\"\\n    JS = \"js\"\\n    TS = \"ts\"\\n    PHP = \"php\"\\n    PROTO = \"proto\"\\n    PYTHON = \"python\"\\n    RST = \"rst\"\\n    RUBY = \"ruby\"\\n    RUST = \"rust\"\\n    SCALA = \"scala\"\\n    SWIFT = \"swift\"\\n    MARKDOWN = \"markdown\"\\n    LATEX = \"latex\"\\n    HTML = \"html\"\\n    SOL = \"sol\"\\n    CSHARP = \"csharp\"\\n    COBOL = \"cobol\"',\n",
              "  'def split_text(self, text: str) -> List[str]:\\n        def encode_strip_start_and_stop_token_ids(text: str) -> List[int]:\\n            return self._encode(text)[1:-1]\\n\\n        tokenizer = Tokenizer(\\n            chunk_overlap=self._chunk_overlap,\\n            tokens_per_chunk=self.tokens_per_chunk,\\n            decode=self.tokenizer.decode,\\n            encode=encode_strip_start_and_stop_token_ids,\\n        )\\n\\n        return split_text_on_tokens(text=text, tokenizer=tokenizer)\\n\\n    def count_tokens(self, *, text: str) -> int:\\n        return len(self._encode(text))\\n\\n    _max_length_equal_32_bit_integer: int = 2**32\\n\\n    def _encode(self, text: str) -> List[int]:\\n        token_ids_with_start_and_end_token_ids = self.tokenizer.encode(\\n            text,\\n            max_length=self._max_length_equal_32_bit_integer,\\n            truncation=\"do_not_truncate\",\\n        )\\n        return token_ids_with_start_and_end_token_ids',\n",
              "  'class SentenceTransformersTokenTextSplitter(TextSplitter):\\n    \"\"\"Splitting text to tokens using sentence model tokenizer.\"\"\"\\n\\n    def __init__(\\n        self,\\n        chunk_overlap: int = 50,\\n        model_name: str = \"sentence-transformers/all-mpnet-base-v2\",\\n        tokens_per_chunk: Optional[int] = None,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(**kwargs, chunk_overlap=chunk_overlap)\\n\\n        try:\\n            from sentence_transformers import SentenceTransformer\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import sentence_transformer python package. \"\\n                \"This is needed in order to for SentenceTransformersTokenTextSplitter. \"\\n                \"Please install it with `pip install sentence-transformers`.\"\\n            )\\n\\n        self.model_name = model_name\\n        self._model = SentenceTransformer(self.model_name)\\n        self.tokenizer = self._model.tokenizer\\n        self._initialize_chunk_configuration(tokens_per_chunk=tokens_per_chunk)\\n\\n    def _initialize_chunk_configuration(\\n        self, *, tokens_per_chunk: Optional[int]\\n    ) -> None:\\n        self.maximum_tokens_per_chunk = cast(int, self._model.max_seq_length)\\n\\n        if tokens_per_chunk is None:\\n            self.tokens_per_chunk = self.maximum_tokens_per_chunk\\n        else:\\n            self.tokens_per_chunk = tokens_per_chunk\\n\\n        if self.tokens_per_chunk > self.maximum_tokens_per_chunk:\\n            raise ValueError(\\n                f\"The token limit of the models \\'{self.model_name}\\'\"\\n                f\" is: {self.maximum_tokens_per_chunk}.\"\\n                f\" Argument tokens_per_chunk={self.tokens_per_chunk}\"\\n                f\" > maximum token limit.\"\\n            )\\n\\n    def split_text(self, text: str) -> List[str]:\\n        def encode_strip_start_and_stop_token_ids(text: str) -> List[int]:\\n            return self._encode(text)[1:-1]',\n",
              "  'class TokenTextSplitter(TextSplitter):\\n    \"\"\"Splitting text to tokens using model tokenizer.\"\"\"\\n\\n    def __init__(\\n        self,\\n        encoding_name: str = \"gpt2\",\\n        model_name: Optional[str] = None,\\n        allowed_special: Union[Literal[\"all\"], AbstractSet[str]] = set(),\\n        disallowed_special: Union[Literal[\"all\"], Collection[str]] = \"all\",\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(**kwargs)\\n        try:\\n            import tiktoken\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import tiktoken python package. \"\\n                \"This is needed in order to for TokenTextSplitter. \"\\n                \"Please install it with `pip install tiktoken`.\"\\n            )\\n\\n        if model_name is not None:\\n            enc = tiktoken.encoding_for_model(model_name)\\n        else:\\n            enc = tiktoken.get_encoding(encoding_name)\\n        self._tokenizer = enc\\n        self._allowed_special = allowed_special\\n        self._disallowed_special = disallowed_special\\n\\n    def split_text(self, text: str) -> List[str]:\\n        def _encode(_text: str) -> List[int]:\\n            return self._tokenizer.encode(\\n                _text,\\n                allowed_special=self._allowed_special,\\n                disallowed_special=self._disallowed_special,\\n            )\\n\\n        tokenizer = Tokenizer(\\n            chunk_overlap=self._chunk_overlap,\\n            tokens_per_chunk=self._chunk_size,\\n            decode=self._tokenizer.decode,\\n            encode=_encode,\\n        )\\n\\n        return split_text_on_tokens(text=text, tokenizer=tokenizer)',\n",
              "  'class Tokenizer:\\n    \"\"\"Tokenizer data class.\"\"\"\\n\\n    chunk_overlap: int\\n    \"\"\"Overlap in tokens between chunks\"\"\"\\n    tokens_per_chunk: int\\n    \"\"\"Maximum number of tokens per chunk\"\"\"\\n    decode: Callable[[List[int]], str]\\n    \"\"\" Function to decode a list of token ids to a string\"\"\"\\n    encode: Callable[[str], List[int]]\\n    \"\"\" Function to encode a string to a list of token ids\"\"\"\\n\\n\\ndef split_text_on_tokens(*, text: str, tokenizer: Tokenizer) -> List[str]:\\n    \"\"\"Split incoming text and return chunks using tokenizer.\"\"\"\\n    splits: List[str] = []\\n    input_ids = tokenizer.encode(text)\\n    start_idx = 0\\n    cur_idx = min(start_idx + tokenizer.tokens_per_chunk, len(input_ids))\\n    chunk_ids = input_ids[start_idx:cur_idx]\\n    while start_idx < len(input_ids):\\n        splits.append(tokenizer.decode(chunk_ids))\\n        if cur_idx == len(input_ids):\\n            break\\n        start_idx += tokenizer.tokens_per_chunk - tokenizer.chunk_overlap\\n        cur_idx = min(start_idx + tokenizer.tokens_per_chunk, len(input_ids))\\n        chunk_ids = input_ids[start_idx:cur_idx]\\n    return splits',\n",
              "  'docs = []\\n        current_doc: List[str] = []\\n        total = 0\\n        for d in splits:\\n            _len = self._length_function(d)\\n            if (\\n                total + _len + (separator_len if len(current_doc) > 0 else 0)\\n                > self._chunk_size\\n            ):\\n                if total > self._chunk_size:\\n                    logger.warning(\\n                        f\"Created a chunk of size {total}, \"\\n                        f\"which is longer than the specified {self._chunk_size}\"\\n                    )\\n                if len(current_doc) > 0:\\n                    doc = self._join_docs(current_doc, separator)\\n                    if doc is not None:\\n                        docs.append(doc)\\n                    # Keep on popping if:\\n                    # - we have a larger chunk than in the chunk overlap\\n                    # - or if we still have any chunks and the length is long\\n                    while total > self._chunk_overlap or (\\n                        total + _len + (separator_len if len(current_doc) > 0 else 0)\\n                        > self._chunk_size\\n                        and total > 0\\n                    ):\\n                        total -= self._length_function(current_doc[0]) + (\\n                            separator_len if len(current_doc) > 1 else 0\\n                        )\\n                        current_doc = current_doc[1:]\\n            current_doc.append(d)\\n            total += _len + (separator_len if len(current_doc) > 1 else 0)\\n        doc = self._join_docs(current_doc, separator)\\n        if doc is not None:\\n            docs.append(doc)\\n        return docs\\n\\n    @classmethod\\n    def from_huggingface_tokenizer(cls, tokenizer: Any, **kwargs: Any) -> TextSplitter:\\n        \"\"\"Text splitter that uses HuggingFace tokenizer to count length.\"\"\"\\n        try:\\n            from transformers import PreTrainedTokenizerBase',\n",
              "  'class ElementType(TypedDict):\\n    \"\"\"Element type as typed dict.\"\"\"\\n\\n    url: str\\n    xpath: str\\n    content: str\\n    metadata: Dict[str, str]',\n",
              "  'if current_content:\\n            lines_with_metadata.append(\\n                {\"content\": \"\\\\n\".join(current_content), \"metadata\": current_metadata}\\n            )\\n\\n        # lines_with_metadata has each line with associated header metadata\\n        # aggregate these into chunks based on common metadata\\n        if not self.return_each_line:\\n            return self.aggregate_lines_to_chunks(lines_with_metadata)\\n        else:\\n            return [\\n                Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n                for chunk in lines_with_metadata\\n            ]',\n",
              "  '# Push the current header to the stack\\n                        header: HeaderType = {\\n                            \"level\": current_header_level,\\n                            \"name\": name,\\n                            \"data\": stripped_line[len(sep) :].strip(),\\n                        }\\n                        header_stack.append(header)\\n                        # Update initial_metadata with the current header\\n                        initial_metadata[name] = header[\"data\"]\\n\\n                    # Add the previous line to the lines_with_metadata\\n                    # only if current_content is not empty\\n                    if current_content:\\n                        lines_with_metadata.append(\\n                            {\\n                                \"content\": \"\\\\n\".join(current_content),\\n                                \"metadata\": current_metadata.copy(),\\n                            }\\n                        )\\n                        current_content.clear()\\n\\n                    break\\n            else:\\n                if stripped_line:\\n                    current_content.append(stripped_line)\\n                elif current_content:\\n                    lines_with_metadata.append(\\n                        {\\n                            \"content\": \"\\\\n\".join(current_content),\\n                            \"metadata\": current_metadata.copy(),\\n                        }\\n                    )\\n                    current_content.clear()\\n\\n            current_metadata = initial_metadata.copy()\\n\\n        if current_content:\\n            lines_with_metadata.append(\\n                {\"content\": \"\\\\n\".join(current_content), \"metadata\": current_metadata}\\n            )',\n",
              "  'if in_code_block:\\n                current_content.append(stripped_line)\\n                continue\\n\\n            # Check each line against each of the header types (e.g., #, ##)\\n            for sep, name in self.headers_to_split_on:\\n                # Check if line starts with a header that we intend to split on\\n                if stripped_line.startswith(sep) and (\\n                    # Header with no text OR header is followed by space\\n                    # Both are valid conditions that sep is being used a header\\n                    len(stripped_line) == len(sep) or stripped_line[len(sep)] == \" \"\\n                ):\\n                    # Ensure we are tracking the header as metadata\\n                    if name is not None:\\n                        # Get the current header level\\n                        current_header_level = sep.count(\"#\")\\n\\n                        # Pop out headers of lower or same level from the stack\\n                        while (\\n                            header_stack\\n                            and header_stack[-1][\"level\"] >= current_header_level\\n                        ):\\n                            # We have encountered a new header\\n                            # at the same or higher level\\n                            popped_header = header_stack.pop()\\n                            # Clear the metadata for the\\n                            # popped header in initial_metadata\\n                            if popped_header[\"name\"] in initial_metadata:\\n                                initial_metadata.pop(popped_header[\"name\"])',\n",
              "  'def split_text(self, text: str) -> List[Document]:\\n        \"\"\"Split markdown file\\n        Args:\\n            text: Markdown file\"\"\"\\n\\n        # Split the input text by newline character (\"\\\\n\").\\n        lines = text.split(\"\\\\n\")\\n        # Final output\\n        lines_with_metadata: List[LineType] = []\\n        # Content and metadata of the chunk currently being processed\\n        current_content: List[str] = []\\n        current_metadata: Dict[str, str] = {}\\n        # Keep track of the nested header structure\\n        # header_stack: List[Dict[str, Union[int, str]]] = []\\n        header_stack: List[HeaderType] = []\\n        initial_metadata: Dict[str, str] = {}\\n\\n        in_code_block = False\\n        opening_fence = \"\"\\n\\n        for line in lines:\\n            stripped_line = line.strip()\\n\\n            if not in_code_block:\\n                # Exclude inline code spans\\n                if stripped_line.startswith(\"```\") and stripped_line.count(\"```\") == 1:\\n                    in_code_block = True\\n                    opening_fence = \"```\"\\n                elif stripped_line.startswith(\"~~~\"):\\n                    in_code_block = True\\n                    opening_fence = \"~~~\"\\n            else:\\n                if stripped_line.startswith(opening_fence):\\n                    in_code_block = False\\n                    opening_fence = \"\"\\n\\n            if in_code_block:\\n                current_content.append(stripped_line)\\n                continue',\n",
              "  'class MarkdownHeaderTextSplitter:\\n    \"\"\"Splitting markdown files based on specified headers.\"\"\"\\n\\n    def __init__(\\n        self, headers_to_split_on: List[Tuple[str, str]], return_each_line: bool = False\\n    ):\\n        \"\"\"Create a new MarkdownHeaderTextSplitter.\\n\\n        Args:\\n            headers_to_split_on: Headers we want to track\\n            return_each_line: Return each line w/ associated headers\\n        \"\"\"\\n        # Output line-by-line or aggregated into chunks w/ common headers\\n        self.return_each_line = return_each_line\\n        # Given the headers we want to split on,\\n        # (e.g., \"#, ##, etc\") order by length\\n        self.headers_to_split_on = sorted(\\n            headers_to_split_on, key=lambda split: len(split[0]), reverse=True\\n        )\\n\\n    def aggregate_lines_to_chunks(self, lines: List[LineType]) -> List[Document]:\\n        \"\"\"Combine lines with common metadata into chunks\\n        Args:\\n            lines: Line of text / associated header metadata\\n        \"\"\"\\n        aggregated_chunks: List[LineType] = []\\n\\n        for line in lines:\\n            if (\\n                aggregated_chunks\\n                and aggregated_chunks[-1][\"metadata\"] == line[\"metadata\"]\\n            ):\\n                # If the last line in the aggregated list\\n                # has the same metadata as the current line,\\n                # append the current content to the last lines\\'s content\\n                aggregated_chunks[-1][\"content\"] += \"  \\\\n\" + line[\"content\"]\\n            else:\\n                # Otherwise, append the current line to the aggregated list\\n                aggregated_chunks.append(line)\\n\\n        return [\\n            Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n            for chunk in aggregated_chunks\\n        ]\\n\\n    def split_text(self, text: str) -> List[Document]:\\n        \"\"\"Split markdown file\\n        Args:\\n            text: Markdown file\"\"\"',\n",
              "  'class CharacterTextSplitter(TextSplitter):\\n    \"\"\"Splitting text that looks at characters.\"\"\"\\n\\n    def __init__(\\n        self, separator: str = \"\\\\n\\\\n\", is_separator_regex: bool = False, **kwargs: Any\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(**kwargs)\\n        self._separator = separator\\n        self._is_separator_regex = is_separator_regex\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        # First we naively split the large input into a bunch of smaller ones.\\n        separator = (\\n            self._separator if self._is_separator_regex else re.escape(self._separator)\\n        )\\n        splits = _split_text_with_regex(text, separator, self._keep_separator)\\n        _separator = \"\" if self._keep_separator else self._separator\\n        return self._merge_splits(splits, _separator)\\n\\n\\nclass LineType(TypedDict):\\n    \"\"\"Line type as typed dict.\"\"\"\\n\\n    metadata: Dict[str, str]\\n    content: str\\n\\n\\nclass HeaderType(TypedDict):\\n    \"\"\"Header type as typed dict.\"\"\"\\n\\n    level: int\\n    name: str\\n    data: str',\n",
              "  'if issubclass(cls, TokenTextSplitter):\\n            extra_kwargs = {\\n                \"encoding_name\": encoding_name,\\n                \"model_name\": model_name,\\n                \"allowed_special\": allowed_special,\\n                \"disallowed_special\": disallowed_special,\\n            }\\n            kwargs = {**kwargs, **extra_kwargs}\\n\\n        return cls(length_function=_tiktoken_encoder, **kwargs)\\n\\n    def transform_documents(\\n        self, documents: Sequence[Document], **kwargs: Any\\n    ) -> Sequence[Document]:\\n        \"\"\"Transform sequence of documents by splitting them.\"\"\"\\n        return self.split_documents(list(documents))\\n\\n    async def atransform_documents(\\n        self, documents: Sequence[Document], **kwargs: Any\\n    ) -> Sequence[Document]:\\n        \"\"\"Asynchronously transform a sequence of documents by splitting them.\"\"\"\\n        return await asyncio.get_running_loop().run_in_executor(\\n            None, partial(self.transform_documents, **kwargs), documents\\n        )',\n",
              "  'if not isinstance(tokenizer, PreTrainedTokenizerBase):\\n                raise ValueError(\\n                    \"Tokenizer received was not an instance of PreTrainedTokenizerBase\"\\n                )\\n\\n            def _huggingface_tokenizer_length(text: str) -> int:\\n                return len(tokenizer.encode(text))\\n\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import transformers python package. \"\\n                \"Please install it with `pip install transformers`.\"\\n            )\\n        return cls(length_function=_huggingface_tokenizer_length, **kwargs)\\n\\n    @classmethod\\n    def from_tiktoken_encoder(\\n        cls: Type[TS],\\n        encoding_name: str = \"gpt2\",\\n        model_name: Optional[str] = None,\\n        allowed_special: Union[Literal[\"all\"], AbstractSet[str]] = set(),\\n        disallowed_special: Union[Literal[\"all\"], Collection[str]] = \"all\",\\n        **kwargs: Any,\\n    ) -> TS:\\n        \"\"\"Text splitter that uses tiktoken encoder to count length.\"\"\"\\n        try:\\n            import tiktoken\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import tiktoken python package. \"\\n                \"This is needed in order to calculate max_tokens_for_prompt. \"\\n                \"Please install it with `pip install tiktoken`.\"\\n            )\\n\\n        if model_name is not None:\\n            enc = tiktoken.encoding_for_model(model_name)\\n        else:\\n            enc = tiktoken.get_encoding(encoding_name)\\n\\n        def _tiktoken_encoder(text: str) -> int:\\n            return len(\\n                enc.encode(\\n                    text,\\n                    allowed_special=allowed_special,\\n                    disallowed_special=disallowed_special,\\n                )\\n            )',\n",
              "  'class HTMLHeaderTextSplitter:\\n    \"\"\"\\n    Splitting HTML files based on specified headers.\\n    Requires lxml package.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        headers_to_split_on: List[Tuple[str, str]],\\n        return_each_element: bool = False,\\n    ):\\n        \"\"\"Create a new HTMLHeaderTextSplitter.\\n\\n        Args:\\n            headers_to_split_on: list of tuples of headers we want to track mapped to\\n                (arbitrary) keys for metadata. Allowed header values: h1, h2, h3, h4,\\n                h5, h6 e.g. [(\"h1\", \"Header 1\"), (\"h2\", \"Header 2)].\\n            return_each_element: Return each element w/ associated headers.\\n        \"\"\"\\n        # Output element-by-element or aggregated into chunks w/ common headers\\n        self.return_each_element = return_each_element\\n        self.headers_to_split_on = sorted(headers_to_split_on)\\n\\n    def aggregate_elements_to_chunks(\\n        self, elements: List[ElementType]\\n    ) -> List[Document]:\\n        \"\"\"Combine elements with common metadata into chunks\\n\\n        Args:\\n            elements: HTML element content with associated identifying info and metadata\\n        \"\"\"\\n        aggregated_chunks: List[ElementType] = []\\n\\n        for element in elements:\\n            if (\\n                aggregated_chunks\\n                and aggregated_chunks[-1][\"metadata\"] == element[\"metadata\"]\\n            ):\\n                # If the last element in the aggregated list\\n                # has the same metadata as the current element,\\n                # append the current content to the last element\\'s content\\n                aggregated_chunks[-1][\"content\"] += \"  \\\\n\" + element[\"content\"]\\n            else:\\n                # Otherwise, append the current element to the aggregated list\\n                aggregated_chunks.append(element)\\n\\n        return [\\n            Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n            for chunk in aggregated_chunks\\n        ]',\n",
              "  '@abstractmethod\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split text into multiple components.\"\"\"\\n\\n    def create_documents(\\n        self, texts: List[str], metadatas: Optional[List[dict]] = None\\n    ) -> List[Document]:\\n        \"\"\"Create documents from a list of texts.\"\"\"\\n        _metadatas = metadatas or [{}] * len(texts)\\n        documents = []\\n        for i, text in enumerate(texts):\\n            index = -1\\n            for chunk in self.split_text(text):\\n                metadata = copy.deepcopy(_metadatas[i])\\n                if self._add_start_index:\\n                    index = text.find(chunk, index + 1)\\n                    metadata[\"start_index\"] = index\\n                new_doc = Document(page_content=chunk, metadata=metadata)\\n                documents.append(new_doc)\\n        return documents\\n\\n    def split_documents(self, documents: Iterable[Document]) -> List[Document]:\\n        \"\"\"Split documents.\"\"\"\\n        texts, metadatas = [], []\\n        for doc in documents:\\n            texts.append(doc.page_content)\\n            metadatas.append(doc.metadata)\\n        return self.create_documents(texts, metadatas=metadatas)\\n\\n    def _join_docs(self, docs: List[str], separator: str) -> Optional[str]:\\n        text = separator.join(docs)\\n        if self._strip_whitespace:\\n            text = text.strip()\\n        if text == \"\":\\n            return None\\n        else:\\n            return text\\n\\n    def _merge_splits(self, splits: Iterable[str], separator: str) -> List[str]:\\n        # We now want to combine these smaller pieces into medium size\\n        # chunks to send to the LLM.\\n        separator_len = self._length_function(separator)',\n",
              "  'class TextSplitter(BaseDocumentTransformer, ABC):\\n    \"\"\"Interface for splitting text into chunks.\"\"\"\\n\\n    def __init__(\\n        self,\\n        chunk_size: int = 4000,\\n        chunk_overlap: int = 200,\\n        length_function: Callable[[str], int] = len,\\n        keep_separator: bool = False,\\n        add_start_index: bool = False,\\n        strip_whitespace: bool = True,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\\n\\n        Args:\\n            chunk_size: Maximum size of chunks to return\\n            chunk_overlap: Overlap in characters between chunks\\n            length_function: Function that measures the length of given chunks\\n            keep_separator: Whether to keep the separator in the chunks\\n            add_start_index: If `True`, includes chunk\\'s start index in metadata\\n            strip_whitespace: If `True`, strips whitespace from the start and end of\\n                              every document\\n        \"\"\"\\n        if chunk_overlap > chunk_size:\\n            raise ValueError(\\n                f\"Got a larger chunk overlap ({chunk_overlap}) than chunk size \"\\n                f\"({chunk_size}), should be smaller.\"\\n            )\\n        self._chunk_size = chunk_size\\n        self._chunk_overlap = chunk_overlap\\n        self._length_function = length_function\\n        self._keep_separator = keep_separator\\n        self._add_start_index = add_start_index\\n        self._strip_whitespace = strip_whitespace\\n\\n    @abstractmethod\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split text into multiple components.\"\"\"',\n",
              "  'def _split_text_with_regex(\\n    text: str, separator: str, keep_separator: bool\\n) -> List[str]:\\n    # Now that we have the separator, split the text\\n    if separator:\\n        if keep_separator:\\n            # The parentheses in the pattern keep the delimiters in the result.\\n            _splits = re.split(f\"({separator})\", text)\\n            splits = [_splits[i] + _splits[i + 1] for i in range(1, len(_splits), 2)]\\n            if len(_splits) % 2 == 0:\\n                splits += _splits[-1:]\\n            splits = [_splits[0]] + splits\\n        else:\\n            splits = re.split(separator, text)\\n    else:\\n        splits = list(text)\\n    return [s for s in splits if s != \"\"]',\n",
              "  '\"\"\"**Text Splitters** are classes for splitting text.\\n\\n\\n**Class hierarchy:**\\n\\n.. code-block::\\n\\n    BaseDocumentTransformer --> TextSplitter --> <name>TextSplitter  # Example: CharacterTextSplitter\\n                                                 RecursiveCharacterTextSplitter -->  <name>TextSplitter\\n\\nNote: **MarkdownHeaderTextSplitter** and **HTMLHeaderTextSplitter do not derive from TextSplitter.\\n\\n\\n**Main helpers:**\\n\\n.. code-block::\\n\\n    Document, Tokenizer, Language, LineType, HeaderType\\n\\n\"\"\"  # noqa: E501\\n\\nfrom __future__ import annotations\\n\\nimport asyncio\\nimport copy\\nimport logging\\nimport pathlib\\nimport re\\nfrom abc import ABC, abstractmethod\\nfrom dataclasses import dataclass\\nfrom enum import Enum\\nfrom functools import partial\\nfrom io import BytesIO, StringIO\\nfrom typing import (\\n    AbstractSet,\\n    Any,\\n    Callable,\\n    Collection,\\n    Dict,\\n    Iterable,\\n    List,\\n    Literal,\\n    Optional,\\n    Sequence,\\n    Tuple,\\n    Type,\\n    TypedDict,\\n    TypeVar,\\n    Union,\\n    cast,\\n)\\n\\nimport requests\\nfrom langchain_core.documents import BaseDocumentTransformer, Document\\n\\nlogger = logging.getLogger(__name__)\\n\\nTS = TypeVar(\"TS\", bound=\"TextSplitter\")\\n\\n\\ndef _make_spacy_pipeline_for_splitting(\\n    pipeline: str, *, max_length: int = 1_000_000\\n) -> Any:  # avoid importing spacy\\n    try:\\n        import spacy\\n    except ImportError:\\n        raise ImportError(\\n            \"Spacy is not installed, please install it with `pip install spacy`.\"\\n        )\\n    if pipeline == \"sentencizer\":\\n        from spacy.lang.en import English\\n\\n        sentencizer = English()\\n        sentencizer.add_pipe(\"sentencizer\")\\n    else:\\n        sentencizer = spacy.load(pipeline, exclude=[\"ner\", \"tagger\"])\\n        sentencizer.max_length = max_length\\n    return sentencizer',\n",
              "  'class PythonCodeTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Python syntax.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a PythonCodeTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.PYTHON)\\n        super().__init__(separators=separators, **kwargs)\\n\\n\\nclass MarkdownTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Markdown-formatted headings.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a MarkdownTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.MARKDOWN)\\n        super().__init__(separators=separators, **kwargs)\\n\\n\\nclass LatexTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Latex-formatted layout elements.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a LatexTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.LATEX)\\n        super().__init__(separators=separators, **kwargs)',\n",
              "  'class NLTKTextSplitter(TextSplitter):\\n    \"\"\"Splitting text using NLTK package.\"\"\"\\n\\n    def __init__(\\n        self, separator: str = \"\\\\n\\\\n\", language: str = \"english\", **kwargs: Any\\n    ) -> None:\\n        \"\"\"Initialize the NLTK splitter.\"\"\"\\n        super().__init__(**kwargs)\\n        try:\\n            from nltk.tokenize import sent_tokenize\\n\\n            self._tokenizer = sent_tokenize\\n        except ImportError:\\n            raise ImportError(\\n                \"NLTK is not installed, please install it with `pip install nltk`.\"\\n            )\\n        self._separator = separator\\n        self._language = language\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        # First we naively split the large input into a bunch of smaller ones.\\n        splits = self._tokenizer(text, language=self._language)\\n        return self._merge_splits(splits, self._separator)\\n\\n\\nclass SpacyTextSplitter(TextSplitter):\\n    \"\"\"Splitting text using Spacy package.\\n\\n\\n    Per default, Spacy\\'s `en_core_web_sm` model is used and\\n    its default max_length is 1000000 (it is the length of maximum character\\n    this model takes which can be increased for large files). For a faster, but\\n    potentially less accurate splitting, you can use `pipeline=\\'sentencizer\\'`.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        separator: str = \"\\\\n\\\\n\",\\n        pipeline: str = \"en_core_web_sm\",\\n        max_length: int = 1_000_000,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Initialize the spacy text splitter.\"\"\"\\n        super().__init__(**kwargs)\\n        self._tokenizer = _make_spacy_pipeline_for_splitting(\\n            pipeline, max_length=max_length\\n        )\\n        self._separator = separator\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        splits = (s.text for s in self._tokenizer(text).sents)\\n        return self._merge_splits(splits, self._separator)\\n\\n\\n# For backwards compatibility',\n",
              "  'else:\\n            raise ValueError(\\n                f\"Language {language} is not supported! \"\\n                f\"Please choose from {list(Language)}\"\\n            )',\n",
              "  '\"\\\\nevent \",\\n                \"\\\\nmodifier \",\\n                \"\\\\nerror \",\\n                \"\\\\nstruct \",\\n                \"\\\\nenum \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo while \",\\n                \"\\\\nassembly \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.COBOL:\\n            return [\\n                # Split along divisions\\n                \"\\\\nIDENTIFICATION DIVISION.\",\\n                \"\\\\nENVIRONMENT DIVISION.\",\\n                \"\\\\nDATA DIVISION.\",\\n                \"\\\\nPROCEDURE DIVISION.\",\\n                # Split along sections within DATA DIVISION\\n                \"\\\\nWORKING-STORAGE SECTION.\",\\n                \"\\\\nLINKAGE SECTION.\",\\n                \"\\\\nFILE SECTION.\",\\n                # Split along sections within PROCEDURE DIVISION\\n                \"\\\\nINPUT-OUTPUT SECTION.\",\\n                # Split along paragraphs and common statements\\n                \"\\\\nOPEN \",\\n                \"\\\\nCLOSE \",\\n                \"\\\\nREAD \",\\n                \"\\\\nWRITE \",\\n                \"\\\\nIF \",\\n                \"\\\\nELSE \",\\n                \"\\\\nMOVE \",\\n                \"\\\\nPERFORM \",\\n                \"\\\\nUNTIL \",\\n                \"\\\\nVARYING \",\\n                \"\\\\nACCEPT \",\\n                \"\\\\nDISPLAY \",\\n                \"\\\\nSTOP RUN.\",\\n                # Split by the normal type of lines\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]',\n",
              "  '_separator = separator if self._is_separator_regex else re.escape(separator)\\n        splits = _split_text_with_regex(text, _separator, self._keep_separator)\\n\\n        # Now go merging things, recursively splitting longer texts.\\n        _good_splits = []\\n        _separator = \"\" if self._keep_separator else separator\\n        for s in splits:\\n            if self._length_function(s) < self._chunk_size:\\n                _good_splits.append(s)\\n            else:\\n                if _good_splits:\\n                    merged_text = self._merge_splits(_good_splits, _separator)\\n                    final_chunks.extend(merged_text)\\n                    _good_splits = []\\n                if not new_separators:\\n                    final_chunks.append(s)\\n                else:\\n                    other_info = self._split_text(s, new_separators)\\n                    final_chunks.extend(other_info)\\n        if _good_splits:\\n            merged_text = self._merge_splits(_good_splits, _separator)\\n            final_chunks.extend(merged_text)\\n        return final_chunks\\n\\n    def split_text(self, text: str) -> List[str]:\\n        return self._split_text(text, self._separators)\\n\\n    @classmethod\\n    def from_language(\\n        cls, language: Language, **kwargs: Any\\n    ) -> RecursiveCharacterTextSplitter:\\n        separators = cls.get_separators_for_language(language)\\n        return cls(separators=separators, is_separator_regex=True, **kwargs)',\n",
              "  '\"\\\\nevent \",\\n                \"\\\\nmodifier \",\\n                \"\\\\nerror \",\\n                \"\\\\nstruct \",\\n                \"\\\\nenum \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo while \",\\n                \"\\\\nassembly \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.COBOL:\\n            return [\\n                # Split along divisions\\n                \"\\\\nIDENTIFICATION DIVISION.\",\\n                \"\\\\nENVIRONMENT DIVISION.\",\\n                \"\\\\nDATA DIVISION.\",\\n                \"\\\\nPROCEDURE DIVISION.\",\\n                # Split along sections within DATA DIVISION\\n                \"\\\\nWORKING-STORAGE SECTION.\",\\n                \"\\\\nLINKAGE SECTION.\",\\n                \"\\\\nFILE SECTION.\",\\n                # Split along sections within PROCEDURE DIVISION\\n                \"\\\\nINPUT-OUTPUT SECTION.\",\\n                # Split along paragraphs and common statements\\n                \"\\\\nOPEN \",\\n                \"\\\\nCLOSE \",\\n                \"\\\\nREAD \",\\n                \"\\\\nWRITE \",\\n                \"\\\\nIF \",\\n                \"\\\\nELSE \",\\n                \"\\\\nMOVE \",\\n                \"\\\\nPERFORM \",\\n                \"\\\\nUNTIL \",\\n                \"\\\\nVARYING \",\\n                \"\\\\nACCEPT \",\\n                \"\\\\nDISPLAY \",\\n                \"\\\\nSTOP RUN.\",\\n                # Split by the normal type of lines\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]',\n",
              "  '\"<nav\",\\n                # Head\\n                \"<head\",\\n                \"<style\",\\n                \"<script\",\\n                \"<meta\",\\n                \"<title\",\\n                \"\",\\n            ]\\n        elif language == Language.CSHARP:\\n            return [\\n                \"\\\\ninterface \",\\n                \"\\\\nenum \",\\n                \"\\\\nimplements \",\\n                \"\\\\ndelegate \",\\n                \"\\\\nevent \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nabstract \",\\n                # Split along method definitions\\n                \"\\\\npublic \",\\n                \"\\\\nprotected \",\\n                \"\\\\nprivate \",\\n                \"\\\\nstatic \",\\n                \"\\\\nreturn \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\ncontinue \",\\n                \"\\\\nfor \",\\n                \"\\\\nforeach \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\nbreak \",\\n                \"\\\\ncase \",\\n                \"\\\\nelse \",\\n                # Split by exceptions\\n                \"\\\\ntry \",\\n                \"\\\\nthrow \",\\n                \"\\\\nfinally \",\\n                \"\\\\ncatch \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SOL:\\n            return [\\n                # Split along compiler information definitions\\n                \"\\\\npragma \",\\n                \"\\\\nusing \",\\n                # Split along contract definitions\\n                \"\\\\ncontract \",\\n                \"\\\\ninterface \",\\n                \"\\\\nlibrary \",\\n                # Split along method definitions\\n                \"\\\\nconstructor \",\\n                \"\\\\ntype \",\\n                \"\\\\nfunction \",\\n                \"\\\\nevent \",\\n                \"\\\\nmodifier \",\\n                \"\\\\nerror \",\\n                \"\\\\nstruct \",\\n                \"\\\\nenum \",\\n                # Split along control flow statements',\n",
              "  '\"\\\\n\\\\\\\\*\\\\\\\\*\\\\\\\\*+\\\\n\",\\n                \"\\\\n---+\\\\n\",\\n                \"\\\\n___+\\\\n\",\\n                # Note that this splitter doesn\\'t handle horizontal lines defined\\n                # by *three or more* of ***, ---, or ___, but this is not handled\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.LATEX:\\n            return [\\n                # First, try to split along Latex sections\\n                \"\\\\n\\\\\\\\\\\\\\\\chapter{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\section{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\subsection{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\subsubsection{\",\\n                # Now split by environments\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{enumerate}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{itemize}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{description}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{list}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{quote}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{quotation}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{verse}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{verbatim}\",\\n                # Now split by math environments\\n                \"\\\\n\\\\\\\\\\\\begin{align}\",\\n                \"$$\",\\n                \"$\",\\n                # Now split by the normal type of lines\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.HTML:\\n            return [\\n                # First, try to split along HTML tags\\n                \"<body\",\\n                \"<div\",\\n                \"<p\",\\n                \"<br\",\\n                \"<li\",\\n                \"<h1\",\\n                \"<h2\",\\n                \"<h3\",\\n                \"<h4\",\\n                \"<h5\",\\n                \"<h6\",\\n                \"<span\",\\n                \"<table\",\\n                \"<tr\",\\n                \"<td\",\\n                \"<th\",\\n                \"<ul\",\\n                \"<ol\",\\n                \"<header\",\\n                \"<footer\",\\n                \"<nav\",\\n                # Head\\n                \"<head\",\\n                \"<style\",\\n                \"<script\",\\n                \"<meta\",\\n                \"<title\",',\n",
              "  '\" \",\\n                \"\",\\n            ]\\n        elif language == Language.SCALA:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nobject \",\\n                # Split along method definitions\\n                \"\\\\ndef \",\\n                \"\\\\nval \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nmatch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SWIFT:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunc \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nstruct \",\\n                \"\\\\nenum \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.MARKDOWN:\\n            return [\\n                # First, try to split along Markdown headings (starting with level 2)\\n                \"\\\\n#{1,6} \",\\n                # Note the alternative syntax for headings (below) is not handled here\\n                # Heading level 2\\n                # ---------------\\n                # End of code block\\n                \"```\\\\n\",\\n                # Horizontal lines\\n                \"\\\\n\\\\\\\\*\\\\\\\\*\\\\\\\\*+\\\\n\",\\n                \"\\\\n---+\\\\n\",\\n                \"\\\\n___+\\\\n\",\\n                # Note that this splitter doesn\\'t handle horizontal lines defined',\n",
              "  'return [\\n                # First, try to split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\ndef \",\\n                \"\\\\n\\\\tdef \",\\n                # Now split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.RST:\\n            return [\\n                # Split along section titles\\n                \"\\\\n=+\\\\n\",\\n                \"\\\\n-+\\\\n\",\\n                \"\\\\n\\\\\\\\*+\\\\n\",\\n                # Split along directive markers\\n                \"\\\\n\\\\n.. *\\\\n\\\\n\",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.RUBY:\\n            return [\\n                # Split along method definitions\\n                \"\\\\ndef \",\\n                \"\\\\nclass \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nunless \",\\n                \"\\\\nwhile \",\\n                \"\\\\nfor \",\\n                \"\\\\ndo \",\\n                \"\\\\nbegin \",\\n                \"\\\\nrescue \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.RUST:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfn \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nwhile \",\\n                \"\\\\nfor \",\\n                \"\\\\nloop \",\\n                \"\\\\nmatch \",\\n                \"\\\\nconst \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SCALA:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",',\n",
              "  '\"\\\\nfunction \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                \"\\\\ndefault \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.PHP:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunction \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nforeach \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.PROTO:\\n            return [\\n                # Split along message definitions\\n                \"\\\\nmessage \",\\n                # Split along service definitions\\n                \"\\\\nservice \",\\n                # Split along enum definitions\\n                \"\\\\nenum \",\\n                # Split along option definitions\\n                \"\\\\noption \",\\n                # Split along import statements\\n                \"\\\\nimport \",\\n                # Split along syntax declarations\\n                \"\\\\nsyntax \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.PYTHON:\\n            return [\\n                # First, try to split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\ndef \",\\n                \"\\\\n\\\\tdef \",',\n",
              "  '\"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.KOTLIN:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along method definitions\\n                \"\\\\npublic \",\\n                \"\\\\nprotected \",\\n                \"\\\\nprivate \",\\n                \"\\\\ninternal \",\\n                \"\\\\ncompanion \",\\n                \"\\\\nfun \",\\n                \"\\\\nval \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nwhen \",\\n                \"\\\\ncase \",\\n                \"\\\\nelse \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.JS:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunction \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                \"\\\\nvar \",\\n                \"\\\\nclass \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                \"\\\\ndefault \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.TS:\\n            return [\\n                \"\\\\nenum \",\\n                \"\\\\ninterface \",\\n                \"\\\\nnamespace \",\\n                \"\\\\ntype \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along function definitions\\n                \"\\\\nfunction \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements',\n",
              "  '@staticmethod\\n    def get_separators_for_language(language: Language) -> List[str]:\\n        if language == Language.CPP:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along function definitions\\n                \"\\\\nvoid \",\\n                \"\\\\nint \",\\n                \"\\\\nfloat \",\\n                \"\\\\ndouble \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.GO:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunc \",\\n                \"\\\\nvar \",\\n                \"\\\\nconst \",\\n                \"\\\\ntype \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.JAVA:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along method definitions\\n                \"\\\\npublic \",\\n                \"\\\\nprotected \",\\n                \"\\\\nprivate \",\\n                \"\\\\nstatic \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.KOTLIN:\\n            return [',\n",
              "  '# Push the current header to the stack\\n                        header: HeaderType = {\\n                            \"level\": current_header_level,\\n                            \"name\": name,\\n                            \"data\": stripped_line[len(sep) :].strip(),\\n                        }\\n                        header_stack.append(header)\\n                        # Update initial_metadata with the current header\\n                        initial_metadata[name] = header[\"data\"]\\n\\n                    # Add the previous line to the lines_with_metadata\\n                    # only if current_content is not empty\\n                    if current_content:\\n                        lines_with_metadata.append(\\n                            {\\n                                \"content\": \"\\\\n\".join(current_content),\\n                                \"metadata\": current_metadata.copy(),\\n                            }\\n                        )\\n                        current_content.clear()\\n\\n                    break\\n            else:\\n                if stripped_line:\\n                    current_content.append(stripped_line)\\n                elif current_content:\\n                    lines_with_metadata.append(\\n                        {\\n                            \"content\": \"\\\\n\".join(current_content),\\n                            \"metadata\": current_metadata.copy(),\\n                        }\\n                    )\\n                    current_content.clear()\\n\\n            current_metadata = initial_metadata.copy()\\n\\n        if current_content:\\n            lines_with_metadata.append(\\n                {\"content\": \"\\\\n\".join(current_content), \"metadata\": current_metadata}\\n            )',\n",
              "  'class RecursiveCharacterTextSplitter(TextSplitter):\\n    \"\"\"Splitting text by recursively look at characters.\\n\\n    Recursively tries to split by different characters to find one\\n    that works.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        separators: Optional[List[str]] = None,\\n        keep_separator: bool = True,\\n        is_separator_regex: bool = False,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(keep_separator=keep_separator, **kwargs)\\n        self._separators = separators or [\"\\\\n\\\\n\", \"\\\\n\", \" \", \"\"]\\n        self._is_separator_regex = is_separator_regex\\n\\n    def _split_text(self, text: str, separators: List[str]) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        final_chunks = []\\n        # Get appropriate separator to use\\n        separator = separators[-1]\\n        new_separators = []\\n        for i, _s in enumerate(separators):\\n            _separator = _s if self._is_separator_regex else re.escape(_s)\\n            if _s == \"\":\\n                separator = _s\\n                break\\n            if re.search(_separator, text):\\n                separator = _s\\n                new_separators = separators[i + 1 :]\\n                break\\n\\n        _separator = separator if self._is_separator_regex else re.escape(separator)\\n        splits = _split_text_with_regex(text, _separator, self._keep_separator)',\n",
              "  'class Language(str, Enum):\\n    \"\"\"Enum of the programming languages.\"\"\"\\n\\n    CPP = \"cpp\"\\n    GO = \"go\"\\n    JAVA = \"java\"\\n    KOTLIN = \"kotlin\"\\n    JS = \"js\"\\n    TS = \"ts\"\\n    PHP = \"php\"\\n    PROTO = \"proto\"\\n    PYTHON = \"python\"\\n    RST = \"rst\"\\n    RUBY = \"ruby\"\\n    RUST = \"rust\"\\n    SCALA = \"scala\"\\n    SWIFT = \"swift\"\\n    MARKDOWN = \"markdown\"\\n    LATEX = \"latex\"\\n    HTML = \"html\"\\n    SOL = \"sol\"\\n    CSHARP = \"csharp\"\\n    COBOL = \"cobol\"',\n",
              "  'def split_text(self, text: str) -> List[str]:\\n        def encode_strip_start_and_stop_token_ids(text: str) -> List[int]:\\n            return self._encode(text)[1:-1]\\n\\n        tokenizer = Tokenizer(\\n            chunk_overlap=self._chunk_overlap,\\n            tokens_per_chunk=self.tokens_per_chunk,\\n            decode=self.tokenizer.decode,\\n            encode=encode_strip_start_and_stop_token_ids,\\n        )\\n\\n        return split_text_on_tokens(text=text, tokenizer=tokenizer)\\n\\n    def count_tokens(self, *, text: str) -> int:\\n        return len(self._encode(text))\\n\\n    _max_length_equal_32_bit_integer: int = 2**32\\n\\n    def _encode(self, text: str) -> List[int]:\\n        token_ids_with_start_and_end_token_ids = self.tokenizer.encode(\\n            text,\\n            max_length=self._max_length_equal_32_bit_integer,\\n            truncation=\"do_not_truncate\",\\n        )\\n        return token_ids_with_start_and_end_token_ids',\n",
              "  'class SentenceTransformersTokenTextSplitter(TextSplitter):\\n    \"\"\"Splitting text to tokens using sentence model tokenizer.\"\"\"\\n\\n    def __init__(\\n        self,\\n        chunk_overlap: int = 50,\\n        model_name: str = \"sentence-transformers/all-mpnet-base-v2\",\\n        tokens_per_chunk: Optional[int] = None,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(**kwargs, chunk_overlap=chunk_overlap)\\n\\n        try:\\n            from sentence_transformers import SentenceTransformer\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import sentence_transformer python package. \"\\n                \"This is needed in order to for SentenceTransformersTokenTextSplitter. \"\\n                \"Please install it with `pip install sentence-transformers`.\"\\n            )\\n\\n        self.model_name = model_name\\n        self._model = SentenceTransformer(self.model_name)\\n        self.tokenizer = self._model.tokenizer\\n        self._initialize_chunk_configuration(tokens_per_chunk=tokens_per_chunk)\\n\\n    def _initialize_chunk_configuration(\\n        self, *, tokens_per_chunk: Optional[int]\\n    ) -> None:\\n        self.maximum_tokens_per_chunk = cast(int, self._model.max_seq_length)\\n\\n        if tokens_per_chunk is None:\\n            self.tokens_per_chunk = self.maximum_tokens_per_chunk\\n        else:\\n            self.tokens_per_chunk = tokens_per_chunk\\n\\n        if self.tokens_per_chunk > self.maximum_tokens_per_chunk:\\n            raise ValueError(\\n                f\"The token limit of the models \\'{self.model_name}\\'\"\\n                f\" is: {self.maximum_tokens_per_chunk}.\"\\n                f\" Argument tokens_per_chunk={self.tokens_per_chunk}\"\\n                f\" > maximum token limit.\"\\n            )\\n\\n    def split_text(self, text: str) -> List[str]:\\n        def encode_strip_start_and_stop_token_ids(text: str) -> List[int]:\\n            return self._encode(text)[1:-1]',\n",
              "  'class TokenTextSplitter(TextSplitter):\\n    \"\"\"Splitting text to tokens using model tokenizer.\"\"\"\\n\\n    def __init__(\\n        self,\\n        encoding_name: str = \"gpt2\",\\n        model_name: Optional[str] = None,\\n        allowed_special: Union[Literal[\"all\"], AbstractSet[str]] = set(),\\n        disallowed_special: Union[Literal[\"all\"], Collection[str]] = \"all\",\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(**kwargs)\\n        try:\\n            import tiktoken\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import tiktoken python package. \"\\n                \"This is needed in order to for TokenTextSplitter. \"\\n                \"Please install it with `pip install tiktoken`.\"\\n            )\\n\\n        if model_name is not None:\\n            enc = tiktoken.encoding_for_model(model_name)\\n        else:\\n            enc = tiktoken.get_encoding(encoding_name)\\n        self._tokenizer = enc\\n        self._allowed_special = allowed_special\\n        self._disallowed_special = disallowed_special\\n\\n    def split_text(self, text: str) -> List[str]:\\n        def _encode(_text: str) -> List[int]:\\n            return self._tokenizer.encode(\\n                _text,\\n                allowed_special=self._allowed_special,\\n                disallowed_special=self._disallowed_special,\\n            )\\n\\n        tokenizer = Tokenizer(\\n            chunk_overlap=self._chunk_overlap,\\n            tokens_per_chunk=self._chunk_size,\\n            decode=self._tokenizer.decode,\\n            encode=_encode,\\n        )\\n\\n        return split_text_on_tokens(text=text, tokenizer=tokenizer)',\n",
              "  'class Tokenizer:\\n    \"\"\"Tokenizer data class.\"\"\"\\n\\n    chunk_overlap: int\\n    \"\"\"Overlap in tokens between chunks\"\"\"\\n    tokens_per_chunk: int\\n    \"\"\"Maximum number of tokens per chunk\"\"\"\\n    decode: Callable[[List[int]], str]\\n    \"\"\" Function to decode a list of token ids to a string\"\"\"\\n    encode: Callable[[str], List[int]]\\n    \"\"\" Function to encode a string to a list of token ids\"\"\"\\n\\n\\ndef split_text_on_tokens(*, text: str, tokenizer: Tokenizer) -> List[str]:\\n    \"\"\"Split incoming text and return chunks using tokenizer.\"\"\"\\n    splits: List[str] = []\\n    input_ids = tokenizer.encode(text)\\n    start_idx = 0\\n    cur_idx = min(start_idx + tokenizer.tokens_per_chunk, len(input_ids))\\n    chunk_ids = input_ids[start_idx:cur_idx]\\n    while start_idx < len(input_ids):\\n        splits.append(tokenizer.decode(chunk_ids))\\n        if cur_idx == len(input_ids):\\n            break\\n        start_idx += tokenizer.tokens_per_chunk - tokenizer.chunk_overlap\\n        cur_idx = min(start_idx + tokenizer.tokens_per_chunk, len(input_ids))\\n        chunk_ids = input_ids[start_idx:cur_idx]\\n    return splits',\n",
              "  '# map xhtml namespace prefix\\n        ns_map = {\"h\": \"http://www.w3.org/1999/xhtml\"}\\n\\n        # build list of elements from DOM\\n        elements = []\\n        for element in result_dom.findall(\"*//*\", ns_map):\\n            if element.findall(\"*[@class=\\'headers\\']\") or element.findall(\\n                \"*[@class=\\'chunk\\']\"\\n            ):\\n                elements.append(\\n                    ElementType(\\n                        url=file,\\n                        xpath=\"\".join(\\n                            [\\n                                node.text\\n                                for node in element.findall(\"*[@class=\\'xpath\\']\", ns_map)\\n                            ]\\n                        ),\\n                        content=\"\".join(\\n                            [\\n                                node.text\\n                                for node in element.findall(\"*[@class=\\'chunk\\']\", ns_map)\\n                            ]\\n                        ),\\n                        metadata={\\n                            # Add text of specified headers to metadata using header\\n                            # mapping.\\n                            header_mapping[node.tag]: node.text\\n                            for node in filter(\\n                                lambda x: x.tag in header_filter,\\n                                element.findall(\"*[@class=\\'headers\\']/*\", ns_map),\\n                            )\\n                        },\\n                    )\\n                )\\n\\n        if not self.return_each_element:\\n            return self.aggregate_elements_to_chunks(elements)\\n        else:\\n            return [\\n                Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n                for chunk in elements\\n            ]\\n\\n\\n# should be in newer Python versions (3.10+)\\n# @dataclass(frozen=True, kw_only=True, slots=True)\\n@dataclass(frozen=True)',\n",
              "  'return [\\n            Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n            for chunk in aggregated_chunks\\n        ]\\n\\n    def split_text_from_url(self, url: str) -> List[Document]:\\n        \"\"\"Split HTML from web URL\\n\\n        Args:\\n            url: web URL\\n        \"\"\"\\n        r = requests.get(url)\\n        return self.split_text_from_file(BytesIO(r.content))\\n\\n    def split_text(self, text: str) -> List[Document]:\\n        \"\"\"Split HTML text string\\n\\n        Args:\\n            text: HTML text\\n        \"\"\"\\n        return self.split_text_from_file(StringIO(text))\\n\\n    def split_text_from_file(self, file: Any) -> List[Document]:\\n        \"\"\"Split HTML file\\n\\n        Args:\\n            file: HTML file\\n        \"\"\"\\n        try:\\n            from lxml import etree\\n        except ImportError as e:\\n            raise ImportError(\\n                \"Unable to import lxml, please install with `pip install lxml`.\"\\n            ) from e\\n        # use lxml library to parse html document and return xml ElementTree\\n        parser = etree.HTMLParser()\\n        tree = etree.parse(file, parser)\\n\\n        # document transformation for \"structure-aware\" chunking is handled with xsl.\\n        # see comments in html_chunks_with_headers.xslt for more detailed information.\\n        xslt_path = (\\n            pathlib.Path(__file__).parent\\n            / \"document_transformers/xsl/html_chunks_with_headers.xslt\"\\n        )\\n        xslt_tree = etree.parse(xslt_path)\\n        transform = etree.XSLT(xslt_tree)\\n        result = transform(tree)\\n        result_dom = etree.fromstring(str(result))\\n\\n        # create filter and mapping for header metadata\\n        header_filter = [header[0] for header in self.headers_to_split_on]\\n        header_mapping = dict(self.headers_to_split_on)\\n\\n        # map xhtml namespace prefix\\n        ns_map = {\"h\": \"http://www.w3.org/1999/xhtml\"}',\n",
              "  'else:\\n            raise ValueError(\\n                f\"Language {language} is not supported! \"\\n                f\"Please choose from {list(Language)}\"\\n            )',\n",
              "  'if issubclass(cls, TokenTextSplitter):\\n            extra_kwargs = {\\n                \"encoding_name\": encoding_name,\\n                \"model_name\": model_name,\\n                \"allowed_special\": allowed_special,\\n                \"disallowed_special\": disallowed_special,\\n            }\\n            kwargs = {**kwargs, **extra_kwargs}\\n\\n        return cls(length_function=_tiktoken_encoder, **kwargs)\\n\\n    def transform_documents(\\n        self, documents: Sequence[Document], **kwargs: Any\\n    ) -> Sequence[Document]:\\n        \"\"\"Transform sequence of documents by splitting them.\"\"\"\\n        return self.split_documents(list(documents))\\n\\n    async def atransform_documents(\\n        self, documents: Sequence[Document], **kwargs: Any\\n    ) -> Sequence[Document]:\\n        \"\"\"Asynchronously transform a sequence of documents by splitting them.\"\"\"\\n        return await asyncio.get_running_loop().run_in_executor(\\n            None, partial(self.transform_documents, **kwargs), documents\\n        )',\n",
              "  'if not isinstance(tokenizer, PreTrainedTokenizerBase):\\n                raise ValueError(\\n                    \"Tokenizer received was not an instance of PreTrainedTokenizerBase\"\\n                )\\n\\n            def _huggingface_tokenizer_length(text: str) -> int:\\n                return len(tokenizer.encode(text))\\n\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import transformers python package. \"\\n                \"Please install it with `pip install transformers`.\"\\n            )\\n        return cls(length_function=_huggingface_tokenizer_length, **kwargs)\\n\\n    @classmethod\\n    def from_tiktoken_encoder(\\n        cls: Type[TS],\\n        encoding_name: str = \"gpt2\",\\n        model_name: Optional[str] = None,\\n        allowed_special: Union[Literal[\"all\"], AbstractSet[str]] = set(),\\n        disallowed_special: Union[Literal[\"all\"], Collection[str]] = \"all\",\\n        **kwargs: Any,\\n    ) -> TS:\\n        \"\"\"Text splitter that uses tiktoken encoder to count length.\"\"\"\\n        try:\\n            import tiktoken\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import tiktoken python package. \"\\n                \"This is needed in order to calculate max_tokens_for_prompt. \"\\n                \"Please install it with `pip install tiktoken`.\"\\n            )\\n\\n        if model_name is not None:\\n            enc = tiktoken.encoding_for_model(model_name)\\n        else:\\n            enc = tiktoken.get_encoding(encoding_name)\\n\\n        def _tiktoken_encoder(text: str) -> int:\\n            return len(\\n                enc.encode(\\n                    text,\\n                    allowed_special=allowed_special,\\n                    disallowed_special=disallowed_special,\\n                )\\n            )',\n",
              "  'docs = []\\n        current_doc: List[str] = []\\n        total = 0\\n        for d in splits:\\n            _len = self._length_function(d)\\n            if (\\n                total + _len + (separator_len if len(current_doc) > 0 else 0)\\n                > self._chunk_size\\n            ):\\n                if total > self._chunk_size:\\n                    logger.warning(\\n                        f\"Created a chunk of size {total}, \"\\n                        f\"which is longer than the specified {self._chunk_size}\"\\n                    )\\n                if len(current_doc) > 0:\\n                    doc = self._join_docs(current_doc, separator)\\n                    if doc is not None:\\n                        docs.append(doc)\\n                    # Keep on popping if:\\n                    # - we have a larger chunk than in the chunk overlap\\n                    # - or if we still have any chunks and the length is long\\n                    while total > self._chunk_overlap or (\\n                        total + _len + (separator_len if len(current_doc) > 0 else 0)\\n                        > self._chunk_size\\n                        and total > 0\\n                    ):\\n                        total -= self._length_function(current_doc[0]) + (\\n                            separator_len if len(current_doc) > 1 else 0\\n                        )\\n                        current_doc = current_doc[1:]\\n            current_doc.append(d)\\n            total += _len + (separator_len if len(current_doc) > 1 else 0)\\n        doc = self._join_docs(current_doc, separator)\\n        if doc is not None:\\n            docs.append(doc)\\n        return docs\\n\\n    @classmethod\\n    def from_huggingface_tokenizer(cls, tokenizer: Any, **kwargs: Any) -> TextSplitter:\\n        \"\"\"Text splitter that uses HuggingFace tokenizer to count length.\"\"\"\\n        try:\\n            from transformers import PreTrainedTokenizerBase',\n",
              "  '@abstractmethod\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split text into multiple components.\"\"\"\\n\\n    def create_documents(\\n        self, texts: List[str], metadatas: Optional[List[dict]] = None\\n    ) -> List[Document]:\\n        \"\"\"Create documents from a list of texts.\"\"\"\\n        _metadatas = metadatas or [{}] * len(texts)\\n        documents = []\\n        for i, text in enumerate(texts):\\n            index = -1\\n            for chunk in self.split_text(text):\\n                metadata = copy.deepcopy(_metadatas[i])\\n                if self._add_start_index:\\n                    index = text.find(chunk, index + 1)\\n                    metadata[\"start_index\"] = index\\n                new_doc = Document(page_content=chunk, metadata=metadata)\\n                documents.append(new_doc)\\n        return documents\\n\\n    def split_documents(self, documents: Iterable[Document]) -> List[Document]:\\n        \"\"\"Split documents.\"\"\"\\n        texts, metadatas = [], []\\n        for doc in documents:\\n            texts.append(doc.page_content)\\n            metadatas.append(doc.metadata)\\n        return self.create_documents(texts, metadatas=metadatas)\\n\\n    def _join_docs(self, docs: List[str], separator: str) -> Optional[str]:\\n        text = separator.join(docs)\\n        if self._strip_whitespace:\\n            text = text.strip()\\n        if text == \"\":\\n            return None\\n        else:\\n            return text\\n\\n    def _merge_splits(self, splits: Iterable[str], separator: str) -> List[str]:\\n        # We now want to combine these smaller pieces into medium size\\n        # chunks to send to the LLM.\\n        separator_len = self._length_function(separator)',\n",
              "  'class TextSplitter(BaseDocumentTransformer, ABC):\\n    \"\"\"Interface for splitting text into chunks.\"\"\"\\n\\n    def __init__(\\n        self,\\n        chunk_size: int = 4000,\\n        chunk_overlap: int = 200,\\n        length_function: Callable[[str], int] = len,\\n        keep_separator: bool = False,\\n        add_start_index: bool = False,\\n        strip_whitespace: bool = True,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\\n\\n        Args:\\n            chunk_size: Maximum size of chunks to return\\n            chunk_overlap: Overlap in characters between chunks\\n            length_function: Function that measures the length of given chunks\\n            keep_separator: Whether to keep the separator in the chunks\\n            add_start_index: If `True`, includes chunk\\'s start index in metadata\\n            strip_whitespace: If `True`, strips whitespace from the start and end of\\n                              every document\\n        \"\"\"\\n        if chunk_overlap > chunk_size:\\n            raise ValueError(\\n                f\"Got a larger chunk overlap ({chunk_overlap}) than chunk size \"\\n                f\"({chunk_size}), should be smaller.\"\\n            )\\n        self._chunk_size = chunk_size\\n        self._chunk_overlap = chunk_overlap\\n        self._length_function = length_function\\n        self._keep_separator = keep_separator\\n        self._add_start_index = add_start_index\\n        self._strip_whitespace = strip_whitespace\\n\\n    @abstractmethod\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split text into multiple components.\"\"\"',\n",
              "  'def _split_text_with_regex(\\n    text: str, separator: str, keep_separator: bool\\n) -> List[str]:\\n    # Now that we have the separator, split the text\\n    if separator:\\n        if keep_separator:\\n            # The parentheses in the pattern keep the delimiters in the result.\\n            _splits = re.split(f\"({separator})\", text)\\n            splits = [_splits[i] + _splits[i + 1] for i in range(1, len(_splits), 2)]\\n            if len(_splits) % 2 == 0:\\n                splits += _splits[-1:]\\n            splits = [_splits[0]] + splits\\n        else:\\n            splits = re.split(separator, text)\\n    else:\\n        splits = list(text)\\n    return [s for s in splits if s != \"\"]',\n",
              "  '\"\"\"**Text Splitters** are classes for splitting text.\\n\\n\\n**Class hierarchy:**\\n\\n.. code-block::\\n\\n    BaseDocumentTransformer --> TextSplitter --> <name>TextSplitter  # Example: CharacterTextSplitter\\n                                                 RecursiveCharacterTextSplitter -->  <name>TextSplitter\\n\\nNote: **MarkdownHeaderTextSplitter** and **HTMLHeaderTextSplitter do not derive from TextSplitter.\\n\\n\\n**Main helpers:**\\n\\n.. code-block::\\n\\n    Document, Tokenizer, Language, LineType, HeaderType\\n\\n\"\"\"  # noqa: E501\\n\\nfrom __future__ import annotations\\n\\nimport asyncio\\nimport copy\\nimport logging\\nimport pathlib\\nimport re\\nfrom abc import ABC, abstractmethod\\nfrom dataclasses import dataclass\\nfrom enum import Enum\\nfrom functools import partial\\nfrom io import BytesIO, StringIO\\nfrom typing import (\\n    AbstractSet,\\n    Any,\\n    Callable,\\n    Collection,\\n    Dict,\\n    Iterable,\\n    List,\\n    Literal,\\n    Optional,\\n    Sequence,\\n    Tuple,\\n    Type,\\n    TypedDict,\\n    TypeVar,\\n    Union,\\n    cast,\\n)\\n\\nimport requests\\nfrom langchain_core.documents import BaseDocumentTransformer, Document\\n\\nlogger = logging.getLogger(__name__)\\n\\nTS = TypeVar(\"TS\", bound=\"TextSplitter\")\\n\\n\\ndef _make_spacy_pipeline_for_splitting(\\n    pipeline: str, *, max_length: int = 1_000_000\\n) -> Any:  # avoid importing spacy\\n    try:\\n        import spacy\\n    except ImportError:\\n        raise ImportError(\\n            \"Spacy is not installed, please install it with `pip install spacy`.\"\\n        )\\n    if pipeline == \"sentencizer\":\\n        from spacy.lang.en import English\\n\\n        sentencizer = English()\\n        sentencizer.add_pipe(\"sentencizer\")\\n    else:\\n        sentencizer = spacy.load(pipeline, exclude=[\"ner\", \"tagger\"])\\n        sentencizer.max_length = max_length\\n    return sentencizer',\n",
              "  'class PythonCodeTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Python syntax.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a PythonCodeTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.PYTHON)\\n        super().__init__(separators=separators, **kwargs)\\n\\n\\nclass MarkdownTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Markdown-formatted headings.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a MarkdownTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.MARKDOWN)\\n        super().__init__(separators=separators, **kwargs)\\n\\n\\nclass LatexTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Latex-formatted layout elements.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a LatexTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.LATEX)\\n        super().__init__(separators=separators, **kwargs)',\n",
              "  'class NLTKTextSplitter(TextSplitter):\\n    \"\"\"Splitting text using NLTK package.\"\"\"\\n\\n    def __init__(\\n        self, separator: str = \"\\\\n\\\\n\", language: str = \"english\", **kwargs: Any\\n    ) -> None:\\n        \"\"\"Initialize the NLTK splitter.\"\"\"\\n        super().__init__(**kwargs)\\n        try:\\n            from nltk.tokenize import sent_tokenize\\n\\n            self._tokenizer = sent_tokenize\\n        except ImportError:\\n            raise ImportError(\\n                \"NLTK is not installed, please install it with `pip install nltk`.\"\\n            )\\n        self._separator = separator\\n        self._language = language\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        # First we naively split the large input into a bunch of smaller ones.\\n        splits = self._tokenizer(text, language=self._language)\\n        return self._merge_splits(splits, self._separator)\\n\\n\\nclass SpacyTextSplitter(TextSplitter):\\n    \"\"\"Splitting text using Spacy package.\\n\\n\\n    Per default, Spacy\\'s `en_core_web_sm` model is used and\\n    its default max_length is 1000000 (it is the length of maximum character\\n    this model takes which can be increased for large files). For a faster, but\\n    potentially less accurate splitting, you can use `pipeline=\\'sentencizer\\'`.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        separator: str = \"\\\\n\\\\n\",\\n        pipeline: str = \"en_core_web_sm\",\\n        max_length: int = 1_000_000,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Initialize the spacy text splitter.\"\"\"\\n        super().__init__(**kwargs)\\n        self._tokenizer = _make_spacy_pipeline_for_splitting(\\n            pipeline, max_length=max_length\\n        )\\n        self._separator = separator\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        splits = (s.text for s in self._tokenizer(text).sents)\\n        return self._merge_splits(splits, self._separator)\\n\\n\\n# For backwards compatibility',\n",
              "  'class CharacterTextSplitter(TextSplitter):\\n    \"\"\"Splitting text that looks at characters.\"\"\"\\n\\n    def __init__(\\n        self, separator: str = \"\\\\n\\\\n\", is_separator_regex: bool = False, **kwargs: Any\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(**kwargs)\\n        self._separator = separator\\n        self._is_separator_regex = is_separator_regex\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        # First we naively split the large input into a bunch of smaller ones.\\n        separator = (\\n            self._separator if self._is_separator_regex else re.escape(self._separator)\\n        )\\n        splits = _split_text_with_regex(text, separator, self._keep_separator)\\n        _separator = \"\" if self._keep_separator else self._separator\\n        return self._merge_splits(splits, _separator)\\n\\n\\nclass LineType(TypedDict):\\n    \"\"\"Line type as typed dict.\"\"\"\\n\\n    metadata: Dict[str, str]\\n    content: str\\n\\n\\nclass HeaderType(TypedDict):\\n    \"\"\"Header type as typed dict.\"\"\"\\n\\n    level: int\\n    name: str\\n    data: str',\n",
              "  '\"\\\\nevent \",\\n                \"\\\\nmodifier \",\\n                \"\\\\nerror \",\\n                \"\\\\nstruct \",\\n                \"\\\\nenum \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo while \",\\n                \"\\\\nassembly \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.COBOL:\\n            return [\\n                # Split along divisions\\n                \"\\\\nIDENTIFICATION DIVISION.\",\\n                \"\\\\nENVIRONMENT DIVISION.\",\\n                \"\\\\nDATA DIVISION.\",\\n                \"\\\\nPROCEDURE DIVISION.\",\\n                # Split along sections within DATA DIVISION\\n                \"\\\\nWORKING-STORAGE SECTION.\",\\n                \"\\\\nLINKAGE SECTION.\",\\n                \"\\\\nFILE SECTION.\",\\n                # Split along sections within PROCEDURE DIVISION\\n                \"\\\\nINPUT-OUTPUT SECTION.\",\\n                # Split along paragraphs and common statements\\n                \"\\\\nOPEN \",\\n                \"\\\\nCLOSE \",\\n                \"\\\\nREAD \",\\n                \"\\\\nWRITE \",\\n                \"\\\\nIF \",\\n                \"\\\\nELSE \",\\n                \"\\\\nMOVE \",\\n                \"\\\\nPERFORM \",\\n                \"\\\\nUNTIL \",\\n                \"\\\\nVARYING \",\\n                \"\\\\nACCEPT \",\\n                \"\\\\nDISPLAY \",\\n                \"\\\\nSTOP RUN.\",\\n                # Split by the normal type of lines\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]',\n",
              "  '\"<nav\",\\n                # Head\\n                \"<head\",\\n                \"<style\",\\n                \"<script\",\\n                \"<meta\",\\n                \"<title\",\\n                \"\",\\n            ]\\n        elif language == Language.CSHARP:\\n            return [\\n                \"\\\\ninterface \",\\n                \"\\\\nenum \",\\n                \"\\\\nimplements \",\\n                \"\\\\ndelegate \",\\n                \"\\\\nevent \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nabstract \",\\n                # Split along method definitions\\n                \"\\\\npublic \",\\n                \"\\\\nprotected \",\\n                \"\\\\nprivate \",\\n                \"\\\\nstatic \",\\n                \"\\\\nreturn \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\ncontinue \",\\n                \"\\\\nfor \",\\n                \"\\\\nforeach \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\nbreak \",\\n                \"\\\\ncase \",\\n                \"\\\\nelse \",\\n                # Split by exceptions\\n                \"\\\\ntry \",\\n                \"\\\\nthrow \",\\n                \"\\\\nfinally \",\\n                \"\\\\ncatch \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SOL:\\n            return [\\n                # Split along compiler information definitions\\n                \"\\\\npragma \",\\n                \"\\\\nusing \",\\n                # Split along contract definitions\\n                \"\\\\ncontract \",\\n                \"\\\\ninterface \",\\n                \"\\\\nlibrary \",\\n                # Split along method definitions\\n                \"\\\\nconstructor \",\\n                \"\\\\ntype \",\\n                \"\\\\nfunction \",\\n                \"\\\\nevent \",\\n                \"\\\\nmodifier \",\\n                \"\\\\nerror \",\\n                \"\\\\nstruct \",\\n                \"\\\\nenum \",\\n                # Split along control flow statements',\n",
              "  '\"\\\\n\\\\\\\\*\\\\\\\\*\\\\\\\\*+\\\\n\",\\n                \"\\\\n---+\\\\n\",\\n                \"\\\\n___+\\\\n\",\\n                # Note that this splitter doesn\\'t handle horizontal lines defined\\n                # by *three or more* of ***, ---, or ___, but this is not handled\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.LATEX:\\n            return [\\n                # First, try to split along Latex sections\\n                \"\\\\n\\\\\\\\\\\\\\\\chapter{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\section{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\subsection{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\subsubsection{\",\\n                # Now split by environments\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{enumerate}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{itemize}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{description}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{list}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{quote}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{quotation}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{verse}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{verbatim}\",\\n                # Now split by math environments\\n                \"\\\\n\\\\\\\\\\\\begin{align}\",\\n                \"$$\",\\n                \"$\",\\n                # Now split by the normal type of lines\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.HTML:\\n            return [\\n                # First, try to split along HTML tags\\n                \"<body\",\\n                \"<div\",\\n                \"<p\",\\n                \"<br\",\\n                \"<li\",\\n                \"<h1\",\\n                \"<h2\",\\n                \"<h3\",\\n                \"<h4\",\\n                \"<h5\",\\n                \"<h6\",\\n                \"<span\",\\n                \"<table\",\\n                \"<tr\",\\n                \"<td\",\\n                \"<th\",\\n                \"<ul\",\\n                \"<ol\",\\n                \"<header\",\\n                \"<footer\",\\n                \"<nav\",\\n                # Head\\n                \"<head\",\\n                \"<style\",\\n                \"<script\",\\n                \"<meta\",\\n                \"<title\",',\n",
              "  '\" \",\\n                \"\",\\n            ]\\n        elif language == Language.SCALA:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nobject \",\\n                # Split along method definitions\\n                \"\\\\ndef \",\\n                \"\\\\nval \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nmatch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SWIFT:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunc \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nstruct \",\\n                \"\\\\nenum \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.MARKDOWN:\\n            return [\\n                # First, try to split along Markdown headings (starting with level 2)\\n                \"\\\\n#{1,6} \",\\n                # Note the alternative syntax for headings (below) is not handled here\\n                # Heading level 2\\n                # ---------------\\n                # End of code block\\n                \"```\\\\n\",\\n                # Horizontal lines\\n                \"\\\\n\\\\\\\\*\\\\\\\\*\\\\\\\\*+\\\\n\",\\n                \"\\\\n---+\\\\n\",\\n                \"\\\\n___+\\\\n\",\\n                # Note that this splitter doesn\\'t handle horizontal lines defined',\n",
              "  'return [\\n                # First, try to split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\ndef \",\\n                \"\\\\n\\\\tdef \",\\n                # Now split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.RST:\\n            return [\\n                # Split along section titles\\n                \"\\\\n=+\\\\n\",\\n                \"\\\\n-+\\\\n\",\\n                \"\\\\n\\\\\\\\*+\\\\n\",\\n                # Split along directive markers\\n                \"\\\\n\\\\n.. *\\\\n\\\\n\",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.RUBY:\\n            return [\\n                # Split along method definitions\\n                \"\\\\ndef \",\\n                \"\\\\nclass \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nunless \",\\n                \"\\\\nwhile \",\\n                \"\\\\nfor \",\\n                \"\\\\ndo \",\\n                \"\\\\nbegin \",\\n                \"\\\\nrescue \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.RUST:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfn \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nwhile \",\\n                \"\\\\nfor \",\\n                \"\\\\nloop \",\\n                \"\\\\nmatch \",\\n                \"\\\\nconst \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SCALA:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",',\n",
              "  '\"\\\\nfunction \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                \"\\\\ndefault \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.PHP:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunction \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nforeach \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.PROTO:\\n            return [\\n                # Split along message definitions\\n                \"\\\\nmessage \",\\n                # Split along service definitions\\n                \"\\\\nservice \",\\n                # Split along enum definitions\\n                \"\\\\nenum \",\\n                # Split along option definitions\\n                \"\\\\noption \",\\n                # Split along import statements\\n                \"\\\\nimport \",\\n                # Split along syntax declarations\\n                \"\\\\nsyntax \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.PYTHON:\\n            return [\\n                # First, try to split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\ndef \",\\n                \"\\\\n\\\\tdef \",',\n",
              "  '\"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.KOTLIN:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along method definitions\\n                \"\\\\npublic \",\\n                \"\\\\nprotected \",\\n                \"\\\\nprivate \",\\n                \"\\\\ninternal \",\\n                \"\\\\ncompanion \",\\n                \"\\\\nfun \",\\n                \"\\\\nval \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nwhen \",\\n                \"\\\\ncase \",\\n                \"\\\\nelse \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.JS:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunction \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                \"\\\\nvar \",\\n                \"\\\\nclass \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                \"\\\\ndefault \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.TS:\\n            return [\\n                \"\\\\nenum \",\\n                \"\\\\ninterface \",\\n                \"\\\\nnamespace \",\\n                \"\\\\ntype \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along function definitions\\n                \"\\\\nfunction \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements',\n",
              "  '@staticmethod\\n    def get_separators_for_language(language: Language) -> List[str]:\\n        if language == Language.CPP:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along function definitions\\n                \"\\\\nvoid \",\\n                \"\\\\nint \",\\n                \"\\\\nfloat \",\\n                \"\\\\ndouble \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.GO:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunc \",\\n                \"\\\\nvar \",\\n                \"\\\\nconst \",\\n                \"\\\\ntype \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.JAVA:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along method definitions\\n                \"\\\\npublic \",\\n                \"\\\\nprotected \",\\n                \"\\\\nprivate \",\\n                \"\\\\nstatic \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.KOTLIN:\\n            return [',\n",
              "  '# map xhtml namespace prefix\\n        ns_map = {\"h\": \"http://www.w3.org/1999/xhtml\"}\\n\\n        # build list of elements from DOM\\n        elements = []\\n        for element in result_dom.findall(\"*//*\", ns_map):\\n            if element.findall(\"*[@class=\\'headers\\']\") or element.findall(\\n                \"*[@class=\\'chunk\\']\"\\n            ):\\n                elements.append(\\n                    ElementType(\\n                        url=file,\\n                        xpath=\"\".join(\\n                            [\\n                                node.text\\n                                for node in element.findall(\"*[@class=\\'xpath\\']\", ns_map)\\n                            ]\\n                        ),\\n                        content=\"\".join(\\n                            [\\n                                node.text\\n                                for node in element.findall(\"*[@class=\\'chunk\\']\", ns_map)\\n                            ]\\n                        ),\\n                        metadata={\\n                            # Add text of specified headers to metadata using header\\n                            # mapping.\\n                            header_mapping[node.tag]: node.text\\n                            for node in filter(\\n                                lambda x: x.tag in header_filter,\\n                                element.findall(\"*[@class=\\'headers\\']/*\", ns_map),\\n                            )\\n                        },\\n                    )\\n                )\\n\\n        if not self.return_each_element:\\n            return self.aggregate_elements_to_chunks(elements)\\n        else:\\n            return [\\n                Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n                for chunk in elements\\n            ]\\n\\n\\n# should be in newer Python versions (3.10+)\\n# @dataclass(frozen=True, kw_only=True, slots=True)\\n@dataclass(frozen=True)',\n",
              "  '\"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.KOTLIN:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along method definitions\\n                \"\\\\npublic \",\\n                \"\\\\nprotected \",\\n                \"\\\\nprivate \",\\n                \"\\\\ninternal \",\\n                \"\\\\ncompanion \",\\n                \"\\\\nfun \",\\n                \"\\\\nval \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nwhen \",\\n                \"\\\\ncase \",\\n                \"\\\\nelse \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.JS:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunction \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                \"\\\\nvar \",\\n                \"\\\\nclass \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                \"\\\\ndefault \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.TS:\\n            return [\\n                \"\\\\nenum \",\\n                \"\\\\ninterface \",\\n                \"\\\\nnamespace \",\\n                \"\\\\ntype \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along function definitions\\n                \"\\\\nfunction \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements',\n",
              "  '@staticmethod\\n    def get_separators_for_language(language: Language) -> List[str]:\\n        if language == Language.CPP:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along function definitions\\n                \"\\\\nvoid \",\\n                \"\\\\nint \",\\n                \"\\\\nfloat \",\\n                \"\\\\ndouble \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.GO:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunc \",\\n                \"\\\\nvar \",\\n                \"\\\\nconst \",\\n                \"\\\\ntype \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.JAVA:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along method definitions\\n                \"\\\\npublic \",\\n                \"\\\\nprotected \",\\n                \"\\\\nprivate \",\\n                \"\\\\nstatic \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.KOTLIN:\\n            return [',\n",
              "  '_separator = separator if self._is_separator_regex else re.escape(separator)\\n        splits = _split_text_with_regex(text, _separator, self._keep_separator)\\n\\n        # Now go merging things, recursively splitting longer texts.\\n        _good_splits = []\\n        _separator = \"\" if self._keep_separator else separator\\n        for s in splits:\\n            if self._length_function(s) < self._chunk_size:\\n                _good_splits.append(s)\\n            else:\\n                if _good_splits:\\n                    merged_text = self._merge_splits(_good_splits, _separator)\\n                    final_chunks.extend(merged_text)\\n                    _good_splits = []\\n                if not new_separators:\\n                    final_chunks.append(s)\\n                else:\\n                    other_info = self._split_text(s, new_separators)\\n                    final_chunks.extend(other_info)\\n        if _good_splits:\\n            merged_text = self._merge_splits(_good_splits, _separator)\\n            final_chunks.extend(merged_text)\\n        return final_chunks\\n\\n    def split_text(self, text: str) -> List[str]:\\n        return self._split_text(text, self._separators)\\n\\n    @classmethod\\n    def from_language(\\n        cls, language: Language, **kwargs: Any\\n    ) -> RecursiveCharacterTextSplitter:\\n        separators = cls.get_separators_for_language(language)\\n        return cls(separators=separators, is_separator_regex=True, **kwargs)',\n",
              "  'class RecursiveCharacterTextSplitter(TextSplitter):\\n    \"\"\"Splitting text by recursively look at characters.\\n\\n    Recursively tries to split by different characters to find one\\n    that works.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        separators: Optional[List[str]] = None,\\n        keep_separator: bool = True,\\n        is_separator_regex: bool = False,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(keep_separator=keep_separator, **kwargs)\\n        self._separators = separators or [\"\\\\n\\\\n\", \"\\\\n\", \" \", \"\"]\\n        self._is_separator_regex = is_separator_regex\\n\\n    def _split_text(self, text: str, separators: List[str]) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        final_chunks = []\\n        # Get appropriate separator to use\\n        separator = separators[-1]\\n        new_separators = []\\n        for i, _s in enumerate(separators):\\n            _separator = _s if self._is_separator_regex else re.escape(_s)\\n            if _s == \"\":\\n                separator = _s\\n                break\\n            if re.search(_separator, text):\\n                separator = _s\\n                new_separators = separators[i + 1 :]\\n                break\\n\\n        _separator = separator if self._is_separator_regex else re.escape(separator)\\n        splits = _split_text_with_regex(text, _separator, self._keep_separator)',\n",
              "  'class Language(str, Enum):\\n    \"\"\"Enum of the programming languages.\"\"\"\\n\\n    CPP = \"cpp\"\\n    GO = \"go\"\\n    JAVA = \"java\"\\n    KOTLIN = \"kotlin\"\\n    JS = \"js\"\\n    TS = \"ts\"\\n    PHP = \"php\"\\n    PROTO = \"proto\"\\n    PYTHON = \"python\"\\n    RST = \"rst\"\\n    RUBY = \"ruby\"\\n    RUST = \"rust\"\\n    SCALA = \"scala\"\\n    SWIFT = \"swift\"\\n    MARKDOWN = \"markdown\"\\n    LATEX = \"latex\"\\n    HTML = \"html\"\\n    SOL = \"sol\"\\n    CSHARP = \"csharp\"\\n    COBOL = \"cobol\"',\n",
              "  'def split_text(self, text: str) -> List[str]:\\n        def encode_strip_start_and_stop_token_ids(text: str) -> List[int]:\\n            return self._encode(text)[1:-1]\\n\\n        tokenizer = Tokenizer(\\n            chunk_overlap=self._chunk_overlap,\\n            tokens_per_chunk=self.tokens_per_chunk,\\n            decode=self.tokenizer.decode,\\n            encode=encode_strip_start_and_stop_token_ids,\\n        )\\n\\n        return split_text_on_tokens(text=text, tokenizer=tokenizer)\\n\\n    def count_tokens(self, *, text: str) -> int:\\n        return len(self._encode(text))\\n\\n    _max_length_equal_32_bit_integer: int = 2**32\\n\\n    def _encode(self, text: str) -> List[int]:\\n        token_ids_with_start_and_end_token_ids = self.tokenizer.encode(\\n            text,\\n            max_length=self._max_length_equal_32_bit_integer,\\n            truncation=\"do_not_truncate\",\\n        )\\n        return token_ids_with_start_and_end_token_ids',\n",
              "  'class SentenceTransformersTokenTextSplitter(TextSplitter):\\n    \"\"\"Splitting text to tokens using sentence model tokenizer.\"\"\"\\n\\n    def __init__(\\n        self,\\n        chunk_overlap: int = 50,\\n        model_name: str = \"sentence-transformers/all-mpnet-base-v2\",\\n        tokens_per_chunk: Optional[int] = None,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(**kwargs, chunk_overlap=chunk_overlap)\\n\\n        try:\\n            from sentence_transformers import SentenceTransformer\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import sentence_transformer python package. \"\\n                \"This is needed in order to for SentenceTransformersTokenTextSplitter. \"\\n                \"Please install it with `pip install sentence-transformers`.\"\\n            )\\n\\n        self.model_name = model_name\\n        self._model = SentenceTransformer(self.model_name)\\n        self.tokenizer = self._model.tokenizer\\n        self._initialize_chunk_configuration(tokens_per_chunk=tokens_per_chunk)\\n\\n    def _initialize_chunk_configuration(\\n        self, *, tokens_per_chunk: Optional[int]\\n    ) -> None:\\n        self.maximum_tokens_per_chunk = cast(int, self._model.max_seq_length)\\n\\n        if tokens_per_chunk is None:\\n            self.tokens_per_chunk = self.maximum_tokens_per_chunk\\n        else:\\n            self.tokens_per_chunk = tokens_per_chunk\\n\\n        if self.tokens_per_chunk > self.maximum_tokens_per_chunk:\\n            raise ValueError(\\n                f\"The token limit of the models \\'{self.model_name}\\'\"\\n                f\" is: {self.maximum_tokens_per_chunk}.\"\\n                f\" Argument tokens_per_chunk={self.tokens_per_chunk}\"\\n                f\" > maximum token limit.\"\\n            )\\n\\n    def split_text(self, text: str) -> List[str]:\\n        def encode_strip_start_and_stop_token_ids(text: str) -> List[int]:\\n            return self._encode(text)[1:-1]',\n",
              "  'class TokenTextSplitter(TextSplitter):\\n    \"\"\"Splitting text to tokens using model tokenizer.\"\"\"\\n\\n    def __init__(\\n        self,\\n        encoding_name: str = \"gpt2\",\\n        model_name: Optional[str] = None,\\n        allowed_special: Union[Literal[\"all\"], AbstractSet[str]] = set(),\\n        disallowed_special: Union[Literal[\"all\"], Collection[str]] = \"all\",\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(**kwargs)\\n        try:\\n            import tiktoken\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import tiktoken python package. \"\\n                \"This is needed in order to for TokenTextSplitter. \"\\n                \"Please install it with `pip install tiktoken`.\"\\n            )\\n\\n        if model_name is not None:\\n            enc = tiktoken.encoding_for_model(model_name)\\n        else:\\n            enc = tiktoken.get_encoding(encoding_name)\\n        self._tokenizer = enc\\n        self._allowed_special = allowed_special\\n        self._disallowed_special = disallowed_special\\n\\n    def split_text(self, text: str) -> List[str]:\\n        def _encode(_text: str) -> List[int]:\\n            return self._tokenizer.encode(\\n                _text,\\n                allowed_special=self._allowed_special,\\n                disallowed_special=self._disallowed_special,\\n            )\\n\\n        tokenizer = Tokenizer(\\n            chunk_overlap=self._chunk_overlap,\\n            tokens_per_chunk=self._chunk_size,\\n            decode=self._tokenizer.decode,\\n            encode=_encode,\\n        )\\n\\n        return split_text_on_tokens(text=text, tokenizer=tokenizer)',\n",
              "  'class Tokenizer:\\n    \"\"\"Tokenizer data class.\"\"\"\\n\\n    chunk_overlap: int\\n    \"\"\"Overlap in tokens between chunks\"\"\"\\n    tokens_per_chunk: int\\n    \"\"\"Maximum number of tokens per chunk\"\"\"\\n    decode: Callable[[List[int]], str]\\n    \"\"\" Function to decode a list of token ids to a string\"\"\"\\n    encode: Callable[[str], List[int]]\\n    \"\"\" Function to encode a string to a list of token ids\"\"\"\\n\\n\\ndef split_text_on_tokens(*, text: str, tokenizer: Tokenizer) -> List[str]:\\n    \"\"\"Split incoming text and return chunks using tokenizer.\"\"\"\\n    splits: List[str] = []\\n    input_ids = tokenizer.encode(text)\\n    start_idx = 0\\n    cur_idx = min(start_idx + tokenizer.tokens_per_chunk, len(input_ids))\\n    chunk_ids = input_ids[start_idx:cur_idx]\\n    while start_idx < len(input_ids):\\n        splits.append(tokenizer.decode(chunk_ids))\\n        if cur_idx == len(input_ids):\\n            break\\n        start_idx += tokenizer.tokens_per_chunk - tokenizer.chunk_overlap\\n        cur_idx = min(start_idx + tokenizer.tokens_per_chunk, len(input_ids))\\n        chunk_ids = input_ids[start_idx:cur_idx]\\n    return splits',\n",
              "  '_separator = separator if self._is_separator_regex else re.escape(separator)\\n        splits = _split_text_with_regex(text, _separator, self._keep_separator)\\n\\n        # Now go merging things, recursively splitting longer texts.\\n        _good_splits = []\\n        _separator = \"\" if self._keep_separator else separator\\n        for s in splits:\\n            if self._length_function(s) < self._chunk_size:\\n                _good_splits.append(s)\\n            else:\\n                if _good_splits:\\n                    merged_text = self._merge_splits(_good_splits, _separator)\\n                    final_chunks.extend(merged_text)\\n                    _good_splits = []\\n                if not new_separators:\\n                    final_chunks.append(s)\\n                else:\\n                    other_info = self._split_text(s, new_separators)\\n                    final_chunks.extend(other_info)\\n        if _good_splits:\\n            merged_text = self._merge_splits(_good_splits, _separator)\\n            final_chunks.extend(merged_text)\\n        return final_chunks\\n\\n    def split_text(self, text: str) -> List[str]:\\n        return self._split_text(text, self._separators)\\n\\n    @classmethod\\n    def from_language(\\n        cls, language: Language, **kwargs: Any\\n    ) -> RecursiveCharacterTextSplitter:\\n        separators = cls.get_separators_for_language(language)\\n        return cls(separators=separators, is_separator_regex=True, **kwargs)',\n",
              "  'return [\\n            Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n            for chunk in aggregated_chunks\\n        ]\\n\\n    def split_text_from_url(self, url: str) -> List[Document]:\\n        \"\"\"Split HTML from web URL\\n\\n        Args:\\n            url: web URL\\n        \"\"\"\\n        r = requests.get(url)\\n        return self.split_text_from_file(BytesIO(r.content))\\n\\n    def split_text(self, text: str) -> List[Document]:\\n        \"\"\"Split HTML text string\\n\\n        Args:\\n            text: HTML text\\n        \"\"\"\\n        return self.split_text_from_file(StringIO(text))\\n\\n    def split_text_from_file(self, file: Any) -> List[Document]:\\n        \"\"\"Split HTML file\\n\\n        Args:\\n            file: HTML file\\n        \"\"\"\\n        try:\\n            from lxml import etree\\n        except ImportError as e:\\n            raise ImportError(\\n                \"Unable to import lxml, please install with `pip install lxml`.\"\\n            ) from e\\n        # use lxml library to parse html document and return xml ElementTree\\n        parser = etree.HTMLParser()\\n        tree = etree.parse(file, parser)\\n\\n        # document transformation for \"structure-aware\" chunking is handled with xsl.\\n        # see comments in html_chunks_with_headers.xslt for more detailed information.\\n        xslt_path = (\\n            pathlib.Path(__file__).parent\\n            / \"document_transformers/xsl/html_chunks_with_headers.xslt\"\\n        )\\n        xslt_tree = etree.parse(xslt_path)\\n        transform = etree.XSLT(xslt_tree)\\n        result = transform(tree)\\n        result_dom = etree.fromstring(str(result))\\n\\n        # create filter and mapping for header metadata\\n        header_filter = [header[0] for header in self.headers_to_split_on]\\n        header_mapping = dict(self.headers_to_split_on)\\n\\n        # map xhtml namespace prefix\\n        ns_map = {\"h\": \"http://www.w3.org/1999/xhtml\"}',\n",
              "  'class HTMLHeaderTextSplitter:\\n    \"\"\"\\n    Splitting HTML files based on specified headers.\\n    Requires lxml package.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        headers_to_split_on: List[Tuple[str, str]],\\n        return_each_element: bool = False,\\n    ):\\n        \"\"\"Create a new HTMLHeaderTextSplitter.\\n\\n        Args:\\n            headers_to_split_on: list of tuples of headers we want to track mapped to\\n                (arbitrary) keys for metadata. Allowed header values: h1, h2, h3, h4,\\n                h5, h6 e.g. [(\"h1\", \"Header 1\"), (\"h2\", \"Header 2)].\\n            return_each_element: Return each element w/ associated headers.\\n        \"\"\"\\n        # Output element-by-element or aggregated into chunks w/ common headers\\n        self.return_each_element = return_each_element\\n        self.headers_to_split_on = sorted(headers_to_split_on)\\n\\n    def aggregate_elements_to_chunks(\\n        self, elements: List[ElementType]\\n    ) -> List[Document]:\\n        \"\"\"Combine elements with common metadata into chunks\\n\\n        Args:\\n            elements: HTML element content with associated identifying info and metadata\\n        \"\"\"\\n        aggregated_chunks: List[ElementType] = []\\n\\n        for element in elements:\\n            if (\\n                aggregated_chunks\\n                and aggregated_chunks[-1][\"metadata\"] == element[\"metadata\"]\\n            ):\\n                # If the last element in the aggregated list\\n                # has the same metadata as the current element,\\n                # append the current content to the last element\\'s content\\n                aggregated_chunks[-1][\"content\"] += \"  \\\\n\" + element[\"content\"]\\n            else:\\n                # Otherwise, append the current element to the aggregated list\\n                aggregated_chunks.append(element)\\n\\n        return [\\n            Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n            for chunk in aggregated_chunks\\n        ]',\n",
              "  'class ElementType(TypedDict):\\n    \"\"\"Element type as typed dict.\"\"\"\\n\\n    url: str\\n    xpath: str\\n    content: str\\n    metadata: Dict[str, str]',\n",
              "  'if current_content:\\n            lines_with_metadata.append(\\n                {\"content\": \"\\\\n\".join(current_content), \"metadata\": current_metadata}\\n            )\\n\\n        # lines_with_metadata has each line with associated header metadata\\n        # aggregate these into chunks based on common metadata\\n        if not self.return_each_line:\\n            return self.aggregate_lines_to_chunks(lines_with_metadata)\\n        else:\\n            return [\\n                Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n                for chunk in lines_with_metadata\\n            ]',\n",
              "  '# Push the current header to the stack\\n                        header: HeaderType = {\\n                            \"level\": current_header_level,\\n                            \"name\": name,\\n                            \"data\": stripped_line[len(sep) :].strip(),\\n                        }\\n                        header_stack.append(header)\\n                        # Update initial_metadata with the current header\\n                        initial_metadata[name] = header[\"data\"]\\n\\n                    # Add the previous line to the lines_with_metadata\\n                    # only if current_content is not empty\\n                    if current_content:\\n                        lines_with_metadata.append(\\n                            {\\n                                \"content\": \"\\\\n\".join(current_content),\\n                                \"metadata\": current_metadata.copy(),\\n                            }\\n                        )\\n                        current_content.clear()\\n\\n                    break\\n            else:\\n                if stripped_line:\\n                    current_content.append(stripped_line)\\n                elif current_content:\\n                    lines_with_metadata.append(\\n                        {\\n                            \"content\": \"\\\\n\".join(current_content),\\n                            \"metadata\": current_metadata.copy(),\\n                        }\\n                    )\\n                    current_content.clear()\\n\\n            current_metadata = initial_metadata.copy()\\n\\n        if current_content:\\n            lines_with_metadata.append(\\n                {\"content\": \"\\\\n\".join(current_content), \"metadata\": current_metadata}\\n            )',\n",
              "  'if in_code_block:\\n                current_content.append(stripped_line)\\n                continue\\n\\n            # Check each line against each of the header types (e.g., #, ##)\\n            for sep, name in self.headers_to_split_on:\\n                # Check if line starts with a header that we intend to split on\\n                if stripped_line.startswith(sep) and (\\n                    # Header with no text OR header is followed by space\\n                    # Both are valid conditions that sep is being used a header\\n                    len(stripped_line) == len(sep) or stripped_line[len(sep)] == \" \"\\n                ):\\n                    # Ensure we are tracking the header as metadata\\n                    if name is not None:\\n                        # Get the current header level\\n                        current_header_level = sep.count(\"#\")\\n\\n                        # Pop out headers of lower or same level from the stack\\n                        while (\\n                            header_stack\\n                            and header_stack[-1][\"level\"] >= current_header_level\\n                        ):\\n                            # We have encountered a new header\\n                            # at the same or higher level\\n                            popped_header = header_stack.pop()\\n                            # Clear the metadata for the\\n                            # popped header in initial_metadata\\n                            if popped_header[\"name\"] in initial_metadata:\\n                                initial_metadata.pop(popped_header[\"name\"])',\n",
              "  'def split_text(self, text: str) -> List[Document]:\\n        \"\"\"Split markdown file\\n        Args:\\n            text: Markdown file\"\"\"\\n\\n        # Split the input text by newline character (\"\\\\n\").\\n        lines = text.split(\"\\\\n\")\\n        # Final output\\n        lines_with_metadata: List[LineType] = []\\n        # Content and metadata of the chunk currently being processed\\n        current_content: List[str] = []\\n        current_metadata: Dict[str, str] = {}\\n        # Keep track of the nested header structure\\n        # header_stack: List[Dict[str, Union[int, str]]] = []\\n        header_stack: List[HeaderType] = []\\n        initial_metadata: Dict[str, str] = {}\\n\\n        in_code_block = False\\n        opening_fence = \"\"\\n\\n        for line in lines:\\n            stripped_line = line.strip()\\n\\n            if not in_code_block:\\n                # Exclude inline code spans\\n                if stripped_line.startswith(\"```\") and stripped_line.count(\"```\") == 1:\\n                    in_code_block = True\\n                    opening_fence = \"```\"\\n                elif stripped_line.startswith(\"~~~\"):\\n                    in_code_block = True\\n                    opening_fence = \"~~~\"\\n            else:\\n                if stripped_line.startswith(opening_fence):\\n                    in_code_block = False\\n                    opening_fence = \"\"\\n\\n            if in_code_block:\\n                current_content.append(stripped_line)\\n                continue',\n",
              "  'class MarkdownHeaderTextSplitter:\\n    \"\"\"Splitting markdown files based on specified headers.\"\"\"\\n\\n    def __init__(\\n        self, headers_to_split_on: List[Tuple[str, str]], return_each_line: bool = False\\n    ):\\n        \"\"\"Create a new MarkdownHeaderTextSplitter.\\n\\n        Args:\\n            headers_to_split_on: Headers we want to track\\n            return_each_line: Return each line w/ associated headers\\n        \"\"\"\\n        # Output line-by-line or aggregated into chunks w/ common headers\\n        self.return_each_line = return_each_line\\n        # Given the headers we want to split on,\\n        # (e.g., \"#, ##, etc\") order by length\\n        self.headers_to_split_on = sorted(\\n            headers_to_split_on, key=lambda split: len(split[0]), reverse=True\\n        )\\n\\n    def aggregate_lines_to_chunks(self, lines: List[LineType]) -> List[Document]:\\n        \"\"\"Combine lines with common metadata into chunks\\n        Args:\\n            lines: Line of text / associated header metadata\\n        \"\"\"\\n        aggregated_chunks: List[LineType] = []\\n\\n        for line in lines:\\n            if (\\n                aggregated_chunks\\n                and aggregated_chunks[-1][\"metadata\"] == line[\"metadata\"]\\n            ):\\n                # If the last line in the aggregated list\\n                # has the same metadata as the current line,\\n                # append the current content to the last lines\\'s content\\n                aggregated_chunks[-1][\"content\"] += \"  \\\\n\" + line[\"content\"]\\n            else:\\n                # Otherwise, append the current line to the aggregated list\\n                aggregated_chunks.append(line)\\n\\n        return [\\n            Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n            for chunk in aggregated_chunks\\n        ]\\n\\n    def split_text(self, text: str) -> List[Document]:\\n        \"\"\"Split markdown file\\n        Args:\\n            text: Markdown file\"\"\"',\n",
              "  '\"<nav\",\\n                # Head\\n                \"<head\",\\n                \"<style\",\\n                \"<script\",\\n                \"<meta\",\\n                \"<title\",\\n                \"\",\\n            ]\\n        elif language == Language.CSHARP:\\n            return [\\n                \"\\\\ninterface \",\\n                \"\\\\nenum \",\\n                \"\\\\nimplements \",\\n                \"\\\\ndelegate \",\\n                \"\\\\nevent \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nabstract \",\\n                # Split along method definitions\\n                \"\\\\npublic \",\\n                \"\\\\nprotected \",\\n                \"\\\\nprivate \",\\n                \"\\\\nstatic \",\\n                \"\\\\nreturn \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\ncontinue \",\\n                \"\\\\nfor \",\\n                \"\\\\nforeach \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\nbreak \",\\n                \"\\\\ncase \",\\n                \"\\\\nelse \",\\n                # Split by exceptions\\n                \"\\\\ntry \",\\n                \"\\\\nthrow \",\\n                \"\\\\nfinally \",\\n                \"\\\\ncatch \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SOL:\\n            return [\\n                # Split along compiler information definitions\\n                \"\\\\npragma \",\\n                \"\\\\nusing \",\\n                # Split along contract definitions\\n                \"\\\\ncontract \",\\n                \"\\\\ninterface \",\\n                \"\\\\nlibrary \",\\n                # Split along method definitions\\n                \"\\\\nconstructor \",\\n                \"\\\\ntype \",\\n                \"\\\\nfunction \",\\n                \"\\\\nevent \",\\n                \"\\\\nmodifier \",\\n                \"\\\\nerror \",\\n                \"\\\\nstruct \",\\n                \"\\\\nenum \",\\n                # Split along control flow statements',\n",
              "  'docs = []\\n        current_doc: List[str] = []\\n        total = 0\\n        for d in splits:\\n            _len = self._length_function(d)\\n            if (\\n                total + _len + (separator_len if len(current_doc) > 0 else 0)\\n                > self._chunk_size\\n            ):\\n                if total > self._chunk_size:\\n                    logger.warning(\\n                        f\"Created a chunk of size {total}, \"\\n                        f\"which is longer than the specified {self._chunk_size}\"\\n                    )\\n                if len(current_doc) > 0:\\n                    doc = self._join_docs(current_doc, separator)\\n                    if doc is not None:\\n                        docs.append(doc)\\n                    # Keep on popping if:\\n                    # - we have a larger chunk than in the chunk overlap\\n                    # - or if we still have any chunks and the length is long\\n                    while total > self._chunk_overlap or (\\n                        total + _len + (separator_len if len(current_doc) > 0 else 0)\\n                        > self._chunk_size\\n                        and total > 0\\n                    ):\\n                        total -= self._length_function(current_doc[0]) + (\\n                            separator_len if len(current_doc) > 1 else 0\\n                        )\\n                        current_doc = current_doc[1:]\\n            current_doc.append(d)\\n            total += _len + (separator_len if len(current_doc) > 1 else 0)\\n        doc = self._join_docs(current_doc, separator)\\n        if doc is not None:\\n            docs.append(doc)\\n        return docs\\n\\n    @classmethod\\n    def from_huggingface_tokenizer(cls, tokenizer: Any, **kwargs: Any) -> TextSplitter:\\n        \"\"\"Text splitter that uses HuggingFace tokenizer to count length.\"\"\"\\n        try:\\n            from transformers import PreTrainedTokenizerBase',\n",
              "  '@abstractmethod\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split text into multiple components.\"\"\"\\n\\n    def create_documents(\\n        self, texts: List[str], metadatas: Optional[List[dict]] = None\\n    ) -> List[Document]:\\n        \"\"\"Create documents from a list of texts.\"\"\"\\n        _metadatas = metadatas or [{}] * len(texts)\\n        documents = []\\n        for i, text in enumerate(texts):\\n            index = -1\\n            for chunk in self.split_text(text):\\n                metadata = copy.deepcopy(_metadatas[i])\\n                if self._add_start_index:\\n                    index = text.find(chunk, index + 1)\\n                    metadata[\"start_index\"] = index\\n                new_doc = Document(page_content=chunk, metadata=metadata)\\n                documents.append(new_doc)\\n        return documents\\n\\n    def split_documents(self, documents: Iterable[Document]) -> List[Document]:\\n        \"\"\"Split documents.\"\"\"\\n        texts, metadatas = [], []\\n        for doc in documents:\\n            texts.append(doc.page_content)\\n            metadatas.append(doc.metadata)\\n        return self.create_documents(texts, metadatas=metadatas)\\n\\n    def _join_docs(self, docs: List[str], separator: str) -> Optional[str]:\\n        text = separator.join(docs)\\n        if self._strip_whitespace:\\n            text = text.strip()\\n        if text == \"\":\\n            return None\\n        else:\\n            return text\\n\\n    def _merge_splits(self, splits: Iterable[str], separator: str) -> List[str]:\\n        # We now want to combine these smaller pieces into medium size\\n        # chunks to send to the LLM.\\n        separator_len = self._length_function(separator)',\n",
              "  'class TextSplitter(BaseDocumentTransformer, ABC):\\n    \"\"\"Interface for splitting text into chunks.\"\"\"\\n\\n    def __init__(\\n        self,\\n        chunk_size: int = 4000,\\n        chunk_overlap: int = 200,\\n        length_function: Callable[[str], int] = len,\\n        keep_separator: bool = False,\\n        add_start_index: bool = False,\\n        strip_whitespace: bool = True,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\\n\\n        Args:\\n            chunk_size: Maximum size of chunks to return\\n            chunk_overlap: Overlap in characters between chunks\\n            length_function: Function that measures the length of given chunks\\n            keep_separator: Whether to keep the separator in the chunks\\n            add_start_index: If `True`, includes chunk\\'s start index in metadata\\n            strip_whitespace: If `True`, strips whitespace from the start and end of\\n                              every document\\n        \"\"\"\\n        if chunk_overlap > chunk_size:\\n            raise ValueError(\\n                f\"Got a larger chunk overlap ({chunk_overlap}) than chunk size \"\\n                f\"({chunk_size}), should be smaller.\"\\n            )\\n        self._chunk_size = chunk_size\\n        self._chunk_overlap = chunk_overlap\\n        self._length_function = length_function\\n        self._keep_separator = keep_separator\\n        self._add_start_index = add_start_index\\n        self._strip_whitespace = strip_whitespace\\n\\n    @abstractmethod\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split text into multiple components.\"\"\"',\n",
              "  'def _split_text_with_regex(\\n    text: str, separator: str, keep_separator: bool\\n) -> List[str]:\\n    # Now that we have the separator, split the text\\n    if separator:\\n        if keep_separator:\\n            # The parentheses in the pattern keep the delimiters in the result.\\n            _splits = re.split(f\"({separator})\", text)\\n            splits = [_splits[i] + _splits[i + 1] for i in range(1, len(_splits), 2)]\\n            if len(_splits) % 2 == 0:\\n                splits += _splits[-1:]\\n            splits = [_splits[0]] + splits\\n        else:\\n            splits = re.split(separator, text)\\n    else:\\n        splits = list(text)\\n    return [s for s in splits if s != \"\"]',\n",
              "  '\"\"\"**Text Splitters** are classes for splitting text.\\n\\n\\n**Class hierarchy:**\\n\\n.. code-block::\\n\\n    BaseDocumentTransformer --> TextSplitter --> <name>TextSplitter  # Example: CharacterTextSplitter\\n                                                 RecursiveCharacterTextSplitter -->  <name>TextSplitter\\n\\nNote: **MarkdownHeaderTextSplitter** and **HTMLHeaderTextSplitter do not derive from TextSplitter.\\n\\n\\n**Main helpers:**\\n\\n.. code-block::\\n\\n    Document, Tokenizer, Language, LineType, HeaderType\\n\\n\"\"\"  # noqa: E501\\n\\nfrom __future__ import annotations\\n\\nimport asyncio\\nimport copy\\nimport logging\\nimport pathlib\\nimport re\\nfrom abc import ABC, abstractmethod\\nfrom dataclasses import dataclass\\nfrom enum import Enum\\nfrom functools import partial\\nfrom io import BytesIO, StringIO\\nfrom typing import (\\n    AbstractSet,\\n    Any,\\n    Callable,\\n    Collection,\\n    Dict,\\n    Iterable,\\n    List,\\n    Literal,\\n    Optional,\\n    Sequence,\\n    Tuple,\\n    Type,\\n    TypedDict,\\n    TypeVar,\\n    Union,\\n    cast,\\n)\\n\\nimport requests\\nfrom langchain_core.documents import BaseDocumentTransformer, Document\\n\\nlogger = logging.getLogger(__name__)\\n\\nTS = TypeVar(\"TS\", bound=\"TextSplitter\")\\n\\n\\ndef _make_spacy_pipeline_for_splitting(\\n    pipeline: str, *, max_length: int = 1_000_000\\n) -> Any:  # avoid importing spacy\\n    try:\\n        import spacy\\n    except ImportError:\\n        raise ImportError(\\n            \"Spacy is not installed, please install it with `pip install spacy`.\"\\n        )\\n    if pipeline == \"sentencizer\":\\n        from spacy.lang.en import English\\n\\n        sentencizer = English()\\n        sentencizer.add_pipe(\"sentencizer\")\\n    else:\\n        sentencizer = spacy.load(pipeline, exclude=[\"ner\", \"tagger\"])\\n        sentencizer.max_length = max_length\\n    return sentencizer',\n",
              "  'class PythonCodeTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Python syntax.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a PythonCodeTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.PYTHON)\\n        super().__init__(separators=separators, **kwargs)\\n\\n\\nclass MarkdownTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Markdown-formatted headings.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a MarkdownTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.MARKDOWN)\\n        super().__init__(separators=separators, **kwargs)\\n\\n\\nclass LatexTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Latex-formatted layout elements.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a LatexTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.LATEX)\\n        super().__init__(separators=separators, **kwargs)',\n",
              "  'class NLTKTextSplitter(TextSplitter):\\n    \"\"\"Splitting text using NLTK package.\"\"\"\\n\\n    def __init__(\\n        self, separator: str = \"\\\\n\\\\n\", language: str = \"english\", **kwargs: Any\\n    ) -> None:\\n        \"\"\"Initialize the NLTK splitter.\"\"\"\\n        super().__init__(**kwargs)\\n        try:\\n            from nltk.tokenize import sent_tokenize\\n\\n            self._tokenizer = sent_tokenize\\n        except ImportError:\\n            raise ImportError(\\n                \"NLTK is not installed, please install it with `pip install nltk`.\"\\n            )\\n        self._separator = separator\\n        self._language = language\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        # First we naively split the large input into a bunch of smaller ones.\\n        splits = self._tokenizer(text, language=self._language)\\n        return self._merge_splits(splits, self._separator)\\n\\n\\nclass SpacyTextSplitter(TextSplitter):\\n    \"\"\"Splitting text using Spacy package.\\n\\n\\n    Per default, Spacy\\'s `en_core_web_sm` model is used and\\n    its default max_length is 1000000 (it is the length of maximum character\\n    this model takes which can be increased for large files). For a faster, but\\n    potentially less accurate splitting, you can use `pipeline=\\'sentencizer\\'`.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        separator: str = \"\\\\n\\\\n\",\\n        pipeline: str = \"en_core_web_sm\",\\n        max_length: int = 1_000_000,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Initialize the spacy text splitter.\"\"\"\\n        super().__init__(**kwargs)\\n        self._tokenizer = _make_spacy_pipeline_for_splitting(\\n            pipeline, max_length=max_length\\n        )\\n        self._separator = separator\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        splits = (s.text for s in self._tokenizer(text).sents)\\n        return self._merge_splits(splits, self._separator)\\n\\n\\n# For backwards compatibility',\n",
              "  'else:\\n            raise ValueError(\\n                f\"Language {language} is not supported! \"\\n                f\"Please choose from {list(Language)}\"\\n            )',\n",
              "  '\"\\\\nevent \",\\n                \"\\\\nmodifier \",\\n                \"\\\\nerror \",\\n                \"\\\\nstruct \",\\n                \"\\\\nenum \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo while \",\\n                \"\\\\nassembly \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.COBOL:\\n            return [\\n                # Split along divisions\\n                \"\\\\nIDENTIFICATION DIVISION.\",\\n                \"\\\\nENVIRONMENT DIVISION.\",\\n                \"\\\\nDATA DIVISION.\",\\n                \"\\\\nPROCEDURE DIVISION.\",\\n                # Split along sections within DATA DIVISION\\n                \"\\\\nWORKING-STORAGE SECTION.\",\\n                \"\\\\nLINKAGE SECTION.\",\\n                \"\\\\nFILE SECTION.\",\\n                # Split along sections within PROCEDURE DIVISION\\n                \"\\\\nINPUT-OUTPUT SECTION.\",\\n                # Split along paragraphs and common statements\\n                \"\\\\nOPEN \",\\n                \"\\\\nCLOSE \",\\n                \"\\\\nREAD \",\\n                \"\\\\nWRITE \",\\n                \"\\\\nIF \",\\n                \"\\\\nELSE \",\\n                \"\\\\nMOVE \",\\n                \"\\\\nPERFORM \",\\n                \"\\\\nUNTIL \",\\n                \"\\\\nVARYING \",\\n                \"\\\\nACCEPT \",\\n                \"\\\\nDISPLAY \",\\n                \"\\\\nSTOP RUN.\",\\n                # Split by the normal type of lines\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]',\n",
              "  'if not isinstance(tokenizer, PreTrainedTokenizerBase):\\n                raise ValueError(\\n                    \"Tokenizer received was not an instance of PreTrainedTokenizerBase\"\\n                )\\n\\n            def _huggingface_tokenizer_length(text: str) -> int:\\n                return len(tokenizer.encode(text))\\n\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import transformers python package. \"\\n                \"Please install it with `pip install transformers`.\"\\n            )\\n        return cls(length_function=_huggingface_tokenizer_length, **kwargs)\\n\\n    @classmethod\\n    def from_tiktoken_encoder(\\n        cls: Type[TS],\\n        encoding_name: str = \"gpt2\",\\n        model_name: Optional[str] = None,\\n        allowed_special: Union[Literal[\"all\"], AbstractSet[str]] = set(),\\n        disallowed_special: Union[Literal[\"all\"], Collection[str]] = \"all\",\\n        **kwargs: Any,\\n    ) -> TS:\\n        \"\"\"Text splitter that uses tiktoken encoder to count length.\"\"\"\\n        try:\\n            import tiktoken\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import tiktoken python package. \"\\n                \"This is needed in order to calculate max_tokens_for_prompt. \"\\n                \"Please install it with `pip install tiktoken`.\"\\n            )\\n\\n        if model_name is not None:\\n            enc = tiktoken.encoding_for_model(model_name)\\n        else:\\n            enc = tiktoken.get_encoding(encoding_name)\\n\\n        def _tiktoken_encoder(text: str) -> int:\\n            return len(\\n                enc.encode(\\n                    text,\\n                    allowed_special=allowed_special,\\n                    disallowed_special=disallowed_special,\\n                )\\n            )',\n",
              "  '\"\\\\n\\\\\\\\*\\\\\\\\*\\\\\\\\*+\\\\n\",\\n                \"\\\\n---+\\\\n\",\\n                \"\\\\n___+\\\\n\",\\n                # Note that this splitter doesn\\'t handle horizontal lines defined\\n                # by *three or more* of ***, ---, or ___, but this is not handled\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.LATEX:\\n            return [\\n                # First, try to split along Latex sections\\n                \"\\\\n\\\\\\\\\\\\\\\\chapter{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\section{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\subsection{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\subsubsection{\",\\n                # Now split by environments\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{enumerate}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{itemize}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{description}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{list}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{quote}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{quotation}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{verse}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{verbatim}\",\\n                # Now split by math environments\\n                \"\\\\n\\\\\\\\\\\\begin{align}\",\\n                \"$$\",\\n                \"$\",\\n                # Now split by the normal type of lines\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.HTML:\\n            return [\\n                # First, try to split along HTML tags\\n                \"<body\",\\n                \"<div\",\\n                \"<p\",\\n                \"<br\",\\n                \"<li\",\\n                \"<h1\",\\n                \"<h2\",\\n                \"<h3\",\\n                \"<h4\",\\n                \"<h5\",\\n                \"<h6\",\\n                \"<span\",\\n                \"<table\",\\n                \"<tr\",\\n                \"<td\",\\n                \"<th\",\\n                \"<ul\",\\n                \"<ol\",\\n                \"<header\",\\n                \"<footer\",\\n                \"<nav\",\\n                # Head\\n                \"<head\",\\n                \"<style\",\\n                \"<script\",\\n                \"<meta\",\\n                \"<title\",',\n",
              "  '\" \",\\n                \"\",\\n            ]\\n        elif language == Language.SCALA:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nobject \",\\n                # Split along method definitions\\n                \"\\\\ndef \",\\n                \"\\\\nval \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nmatch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SWIFT:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunc \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nstruct \",\\n                \"\\\\nenum \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.MARKDOWN:\\n            return [\\n                # First, try to split along Markdown headings (starting with level 2)\\n                \"\\\\n#{1,6} \",\\n                # Note the alternative syntax for headings (below) is not handled here\\n                # Heading level 2\\n                # ---------------\\n                # End of code block\\n                \"```\\\\n\",\\n                # Horizontal lines\\n                \"\\\\n\\\\\\\\*\\\\\\\\*\\\\\\\\*+\\\\n\",\\n                \"\\\\n---+\\\\n\",\\n                \"\\\\n___+\\\\n\",\\n                # Note that this splitter doesn\\'t handle horizontal lines defined',\n",
              "  'return [\\n                # First, try to split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\ndef \",\\n                \"\\\\n\\\\tdef \",\\n                # Now split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.RST:\\n            return [\\n                # Split along section titles\\n                \"\\\\n=+\\\\n\",\\n                \"\\\\n-+\\\\n\",\\n                \"\\\\n\\\\\\\\*+\\\\n\",\\n                # Split along directive markers\\n                \"\\\\n\\\\n.. *\\\\n\\\\n\",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.RUBY:\\n            return [\\n                # Split along method definitions\\n                \"\\\\ndef \",\\n                \"\\\\nclass \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nunless \",\\n                \"\\\\nwhile \",\\n                \"\\\\nfor \",\\n                \"\\\\ndo \",\\n                \"\\\\nbegin \",\\n                \"\\\\nrescue \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.RUST:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfn \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nwhile \",\\n                \"\\\\nfor \",\\n                \"\\\\nloop \",\\n                \"\\\\nmatch \",\\n                \"\\\\nconst \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SCALA:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",',\n",
              "  '\"\\\\nfunction \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                \"\\\\ndefault \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.PHP:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunction \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nforeach \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.PROTO:\\n            return [\\n                # Split along message definitions\\n                \"\\\\nmessage \",\\n                # Split along service definitions\\n                \"\\\\nservice \",\\n                # Split along enum definitions\\n                \"\\\\nenum \",\\n                # Split along option definitions\\n                \"\\\\noption \",\\n                # Split along import statements\\n                \"\\\\nimport \",\\n                # Split along syntax declarations\\n                \"\\\\nsyntax \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.PYTHON:\\n            return [\\n                # First, try to split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\ndef \",\\n                \"\\\\n\\\\tdef \",',\n",
              "  '\"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.KOTLIN:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along method definitions\\n                \"\\\\npublic \",\\n                \"\\\\nprotected \",\\n                \"\\\\nprivate \",\\n                \"\\\\ninternal \",\\n                \"\\\\ncompanion \",\\n                \"\\\\nfun \",\\n                \"\\\\nval \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nwhen \",\\n                \"\\\\ncase \",\\n                \"\\\\nelse \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.JS:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunction \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                \"\\\\nvar \",\\n                \"\\\\nclass \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                \"\\\\ndefault \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.TS:\\n            return [\\n                \"\\\\nenum \",\\n                \"\\\\ninterface \",\\n                \"\\\\nnamespace \",\\n                \"\\\\ntype \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along function definitions\\n                \"\\\\nfunction \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements',\n",
              "  '@staticmethod\\n    def get_separators_for_language(language: Language) -> List[str]:\\n        if language == Language.CPP:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along function definitions\\n                \"\\\\nvoid \",\\n                \"\\\\nint \",\\n                \"\\\\nfloat \",\\n                \"\\\\ndouble \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.GO:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunc \",\\n                \"\\\\nvar \",\\n                \"\\\\nconst \",\\n                \"\\\\ntype \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.JAVA:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along method definitions\\n                \"\\\\npublic \",\\n                \"\\\\nprotected \",\\n                \"\\\\nprivate \",\\n                \"\\\\nstatic \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.KOTLIN:\\n            return [',\n",
              "  '_separator = separator if self._is_separator_regex else re.escape(separator)\\n        splits = _split_text_with_regex(text, _separator, self._keep_separator)\\n\\n        # Now go merging things, recursively splitting longer texts.\\n        _good_splits = []\\n        _separator = \"\" if self._keep_separator else separator\\n        for s in splits:\\n            if self._length_function(s) < self._chunk_size:\\n                _good_splits.append(s)\\n            else:\\n                if _good_splits:\\n                    merged_text = self._merge_splits(_good_splits, _separator)\\n                    final_chunks.extend(merged_text)\\n                    _good_splits = []\\n                if not new_separators:\\n                    final_chunks.append(s)\\n                else:\\n                    other_info = self._split_text(s, new_separators)\\n                    final_chunks.extend(other_info)\\n        if _good_splits:\\n            merged_text = self._merge_splits(_good_splits, _separator)\\n            final_chunks.extend(merged_text)\\n        return final_chunks\\n\\n    def split_text(self, text: str) -> List[str]:\\n        return self._split_text(text, self._separators)\\n\\n    @classmethod\\n    def from_language(\\n        cls, language: Language, **kwargs: Any\\n    ) -> RecursiveCharacterTextSplitter:\\n        separators = cls.get_separators_for_language(language)\\n        return cls(separators=separators, is_separator_regex=True, **kwargs)',\n",
              "  'class RecursiveCharacterTextSplitter(TextSplitter):\\n    \"\"\"Splitting text by recursively look at characters.\\n\\n    Recursively tries to split by different characters to find one\\n    that works.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        separators: Optional[List[str]] = None,\\n        keep_separator: bool = True,\\n        is_separator_regex: bool = False,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(keep_separator=keep_separator, **kwargs)\\n        self._separators = separators or [\"\\\\n\\\\n\", \"\\\\n\", \" \", \"\"]\\n        self._is_separator_regex = is_separator_regex\\n\\n    def _split_text(self, text: str, separators: List[str]) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        final_chunks = []\\n        # Get appropriate separator to use\\n        separator = separators[-1]\\n        new_separators = []\\n        for i, _s in enumerate(separators):\\n            _separator = _s if self._is_separator_regex else re.escape(_s)\\n            if _s == \"\":\\n                separator = _s\\n                break\\n            if re.search(_separator, text):\\n                separator = _s\\n                new_separators = separators[i + 1 :]\\n                break\\n\\n        _separator = separator if self._is_separator_regex else re.escape(separator)\\n        splits = _split_text_with_regex(text, _separator, self._keep_separator)',\n",
              "  'class HTMLHeaderTextSplitter:\\n    \"\"\"\\n    Splitting HTML files based on specified headers.\\n    Requires lxml package.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        headers_to_split_on: List[Tuple[str, str]],\\n        return_each_element: bool = False,\\n    ):\\n        \"\"\"Create a new HTMLHeaderTextSplitter.\\n\\n        Args:\\n            headers_to_split_on: list of tuples of headers we want to track mapped to\\n                (arbitrary) keys for metadata. Allowed header values: h1, h2, h3, h4,\\n                h5, h6 e.g. [(\"h1\", \"Header 1\"), (\"h2\", \"Header 2)].\\n            return_each_element: Return each element w/ associated headers.\\n        \"\"\"\\n        # Output element-by-element or aggregated into chunks w/ common headers\\n        self.return_each_element = return_each_element\\n        self.headers_to_split_on = sorted(headers_to_split_on)\\n\\n    def aggregate_elements_to_chunks(\\n        self, elements: List[ElementType]\\n    ) -> List[Document]:\\n        \"\"\"Combine elements with common metadata into chunks\\n\\n        Args:\\n            elements: HTML element content with associated identifying info and metadata\\n        \"\"\"\\n        aggregated_chunks: List[ElementType] = []\\n\\n        for element in elements:\\n            if (\\n                aggregated_chunks\\n                and aggregated_chunks[-1][\"metadata\"] == element[\"metadata\"]\\n            ):\\n                # If the last element in the aggregated list\\n                # has the same metadata as the current element,\\n                # append the current content to the last element\\'s content\\n                aggregated_chunks[-1][\"content\"] += \"  \\\\n\" + element[\"content\"]\\n            else:\\n                # Otherwise, append the current element to the aggregated list\\n                aggregated_chunks.append(element)\\n\\n        return [\\n            Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n            for chunk in aggregated_chunks\\n        ]',\n",
              "  'class RecursiveCharacterTextSplitter(TextSplitter):\\n    \"\"\"Splitting text by recursively look at characters.\\n\\n    Recursively tries to split by different characters to find one\\n    that works.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        separators: Optional[List[str]] = None,\\n        keep_separator: bool = True,\\n        is_separator_regex: bool = False,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(keep_separator=keep_separator, **kwargs)\\n        self._separators = separators or [\"\\\\n\\\\n\", \"\\\\n\", \" \", \"\"]\\n        self._is_separator_regex = is_separator_regex\\n\\n    def _split_text(self, text: str, separators: List[str]) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        final_chunks = []\\n        # Get appropriate separator to use\\n        separator = separators[-1]\\n        new_separators = []\\n        for i, _s in enumerate(separators):\\n            _separator = _s if self._is_separator_regex else re.escape(_s)\\n            if _s == \"\":\\n                separator = _s\\n                break\\n            if re.search(_separator, text):\\n                separator = _s\\n                new_separators = separators[i + 1 :]\\n                break\\n\\n        _separator = separator if self._is_separator_regex else re.escape(separator)\\n        splits = _split_text_with_regex(text, _separator, self._keep_separator)',\n",
              "  'class Language(str, Enum):\\n    \"\"\"Enum of the programming languages.\"\"\"\\n\\n    CPP = \"cpp\"\\n    GO = \"go\"\\n    JAVA = \"java\"\\n    KOTLIN = \"kotlin\"\\n    JS = \"js\"\\n    TS = \"ts\"\\n    PHP = \"php\"\\n    PROTO = \"proto\"\\n    PYTHON = \"python\"\\n    RST = \"rst\"\\n    RUBY = \"ruby\"\\n    RUST = \"rust\"\\n    SCALA = \"scala\"\\n    SWIFT = \"swift\"\\n    MARKDOWN = \"markdown\"\\n    LATEX = \"latex\"\\n    HTML = \"html\"\\n    SOL = \"sol\"\\n    CSHARP = \"csharp\"\\n    COBOL = \"cobol\"',\n",
              "  'def split_text(self, text: str) -> List[str]:\\n        def encode_strip_start_and_stop_token_ids(text: str) -> List[int]:\\n            return self._encode(text)[1:-1]\\n\\n        tokenizer = Tokenizer(\\n            chunk_overlap=self._chunk_overlap,\\n            tokens_per_chunk=self.tokens_per_chunk,\\n            decode=self.tokenizer.decode,\\n            encode=encode_strip_start_and_stop_token_ids,\\n        )\\n\\n        return split_text_on_tokens(text=text, tokenizer=tokenizer)\\n\\n    def count_tokens(self, *, text: str) -> int:\\n        return len(self._encode(text))\\n\\n    _max_length_equal_32_bit_integer: int = 2**32\\n\\n    def _encode(self, text: str) -> List[int]:\\n        token_ids_with_start_and_end_token_ids = self.tokenizer.encode(\\n            text,\\n            max_length=self._max_length_equal_32_bit_integer,\\n            truncation=\"do_not_truncate\",\\n        )\\n        return token_ids_with_start_and_end_token_ids',\n",
              "  'class SentenceTransformersTokenTextSplitter(TextSplitter):\\n    \"\"\"Splitting text to tokens using sentence model tokenizer.\"\"\"\\n\\n    def __init__(\\n        self,\\n        chunk_overlap: int = 50,\\n        model_name: str = \"sentence-transformers/all-mpnet-base-v2\",\\n        tokens_per_chunk: Optional[int] = None,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(**kwargs, chunk_overlap=chunk_overlap)\\n\\n        try:\\n            from sentence_transformers import SentenceTransformer\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import sentence_transformer python package. \"\\n                \"This is needed in order to for SentenceTransformersTokenTextSplitter. \"\\n                \"Please install it with `pip install sentence-transformers`.\"\\n            )\\n\\n        self.model_name = model_name\\n        self._model = SentenceTransformer(self.model_name)\\n        self.tokenizer = self._model.tokenizer\\n        self._initialize_chunk_configuration(tokens_per_chunk=tokens_per_chunk)\\n\\n    def _initialize_chunk_configuration(\\n        self, *, tokens_per_chunk: Optional[int]\\n    ) -> None:\\n        self.maximum_tokens_per_chunk = cast(int, self._model.max_seq_length)\\n\\n        if tokens_per_chunk is None:\\n            self.tokens_per_chunk = self.maximum_tokens_per_chunk\\n        else:\\n            self.tokens_per_chunk = tokens_per_chunk\\n\\n        if self.tokens_per_chunk > self.maximum_tokens_per_chunk:\\n            raise ValueError(\\n                f\"The token limit of the models \\'{self.model_name}\\'\"\\n                f\" is: {self.maximum_tokens_per_chunk}.\"\\n                f\" Argument tokens_per_chunk={self.tokens_per_chunk}\"\\n                f\" > maximum token limit.\"\\n            )\\n\\n    def split_text(self, text: str) -> List[str]:\\n        def encode_strip_start_and_stop_token_ids(text: str) -> List[int]:\\n            return self._encode(text)[1:-1]',\n",
              "  'class TokenTextSplitter(TextSplitter):\\n    \"\"\"Splitting text to tokens using model tokenizer.\"\"\"\\n\\n    def __init__(\\n        self,\\n        encoding_name: str = \"gpt2\",\\n        model_name: Optional[str] = None,\\n        allowed_special: Union[Literal[\"all\"], AbstractSet[str]] = set(),\\n        disallowed_special: Union[Literal[\"all\"], Collection[str]] = \"all\",\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(**kwargs)\\n        try:\\n            import tiktoken\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import tiktoken python package. \"\\n                \"This is needed in order to for TokenTextSplitter. \"\\n                \"Please install it with `pip install tiktoken`.\"\\n            )\\n\\n        if model_name is not None:\\n            enc = tiktoken.encoding_for_model(model_name)\\n        else:\\n            enc = tiktoken.get_encoding(encoding_name)\\n        self._tokenizer = enc\\n        self._allowed_special = allowed_special\\n        self._disallowed_special = disallowed_special\\n\\n    def split_text(self, text: str) -> List[str]:\\n        def _encode(_text: str) -> List[int]:\\n            return self._tokenizer.encode(\\n                _text,\\n                allowed_special=self._allowed_special,\\n                disallowed_special=self._disallowed_special,\\n            )\\n\\n        tokenizer = Tokenizer(\\n            chunk_overlap=self._chunk_overlap,\\n            tokens_per_chunk=self._chunk_size,\\n            decode=self._tokenizer.decode,\\n            encode=_encode,\\n        )\\n\\n        return split_text_on_tokens(text=text, tokenizer=tokenizer)',\n",
              "  'class Tokenizer:\\n    \"\"\"Tokenizer data class.\"\"\"\\n\\n    chunk_overlap: int\\n    \"\"\"Overlap in tokens between chunks\"\"\"\\n    tokens_per_chunk: int\\n    \"\"\"Maximum number of tokens per chunk\"\"\"\\n    decode: Callable[[List[int]], str]\\n    \"\"\" Function to decode a list of token ids to a string\"\"\"\\n    encode: Callable[[str], List[int]]\\n    \"\"\" Function to encode a string to a list of token ids\"\"\"\\n\\n\\ndef split_text_on_tokens(*, text: str, tokenizer: Tokenizer) -> List[str]:\\n    \"\"\"Split incoming text and return chunks using tokenizer.\"\"\"\\n    splits: List[str] = []\\n    input_ids = tokenizer.encode(text)\\n    start_idx = 0\\n    cur_idx = min(start_idx + tokenizer.tokens_per_chunk, len(input_ids))\\n    chunk_ids = input_ids[start_idx:cur_idx]\\n    while start_idx < len(input_ids):\\n        splits.append(tokenizer.decode(chunk_ids))\\n        if cur_idx == len(input_ids):\\n            break\\n        start_idx += tokenizer.tokens_per_chunk - tokenizer.chunk_overlap\\n        cur_idx = min(start_idx + tokenizer.tokens_per_chunk, len(input_ids))\\n        chunk_ids = input_ids[start_idx:cur_idx]\\n    return splits',\n",
              "  '# map xhtml namespace prefix\\n        ns_map = {\"h\": \"http://www.w3.org/1999/xhtml\"}\\n\\n        # build list of elements from DOM\\n        elements = []\\n        for element in result_dom.findall(\"*//*\", ns_map):\\n            if element.findall(\"*[@class=\\'headers\\']\") or element.findall(\\n                \"*[@class=\\'chunk\\']\"\\n            ):\\n                elements.append(\\n                    ElementType(\\n                        url=file,\\n                        xpath=\"\".join(\\n                            [\\n                                node.text\\n                                for node in element.findall(\"*[@class=\\'xpath\\']\", ns_map)\\n                            ]\\n                        ),\\n                        content=\"\".join(\\n                            [\\n                                node.text\\n                                for node in element.findall(\"*[@class=\\'chunk\\']\", ns_map)\\n                            ]\\n                        ),\\n                        metadata={\\n                            # Add text of specified headers to metadata using header\\n                            # mapping.\\n                            header_mapping[node.tag]: node.text\\n                            for node in filter(\\n                                lambda x: x.tag in header_filter,\\n                                element.findall(\"*[@class=\\'headers\\']/*\", ns_map),\\n                            )\\n                        },\\n                    )\\n                )\\n\\n        if not self.return_each_element:\\n            return self.aggregate_elements_to_chunks(elements)\\n        else:\\n            return [\\n                Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n                for chunk in elements\\n            ]\\n\\n\\n# should be in newer Python versions (3.10+)\\n# @dataclass(frozen=True, kw_only=True, slots=True)\\n@dataclass(frozen=True)',\n",
              "  'return [\\n            Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n            for chunk in aggregated_chunks\\n        ]\\n\\n    def split_text_from_url(self, url: str) -> List[Document]:\\n        \"\"\"Split HTML from web URL\\n\\n        Args:\\n            url: web URL\\n        \"\"\"\\n        r = requests.get(url)\\n        return self.split_text_from_file(BytesIO(r.content))\\n\\n    def split_text(self, text: str) -> List[Document]:\\n        \"\"\"Split HTML text string\\n\\n        Args:\\n            text: HTML text\\n        \"\"\"\\n        return self.split_text_from_file(StringIO(text))\\n\\n    def split_text_from_file(self, file: Any) -> List[Document]:\\n        \"\"\"Split HTML file\\n\\n        Args:\\n            file: HTML file\\n        \"\"\"\\n        try:\\n            from lxml import etree\\n        except ImportError as e:\\n            raise ImportError(\\n                \"Unable to import lxml, please install with `pip install lxml`.\"\\n            ) from e\\n        # use lxml library to parse html document and return xml ElementTree\\n        parser = etree.HTMLParser()\\n        tree = etree.parse(file, parser)\\n\\n        # document transformation for \"structure-aware\" chunking is handled with xsl.\\n        # see comments in html_chunks_with_headers.xslt for more detailed information.\\n        xslt_path = (\\n            pathlib.Path(__file__).parent\\n            / \"document_transformers/xsl/html_chunks_with_headers.xslt\"\\n        )\\n        xslt_tree = etree.parse(xslt_path)\\n        transform = etree.XSLT(xslt_tree)\\n        result = transform(tree)\\n        result_dom = etree.fromstring(str(result))\\n\\n        # create filter and mapping for header metadata\\n        header_filter = [header[0] for header in self.headers_to_split_on]\\n        header_mapping = dict(self.headers_to_split_on)\\n\\n        # map xhtml namespace prefix\\n        ns_map = {\"h\": \"http://www.w3.org/1999/xhtml\"}',\n",
              "  '\"\\\\nfunction \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                \"\\\\ndefault \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.PHP:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunction \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nforeach \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.PROTO:\\n            return [\\n                # Split along message definitions\\n                \"\\\\nmessage \",\\n                # Split along service definitions\\n                \"\\\\nservice \",\\n                # Split along enum definitions\\n                \"\\\\nenum \",\\n                # Split along option definitions\\n                \"\\\\noption \",\\n                # Split along import statements\\n                \"\\\\nimport \",\\n                # Split along syntax declarations\\n                \"\\\\nsyntax \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.PYTHON:\\n            return [\\n                # First, try to split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\ndef \",\\n                \"\\\\n\\\\tdef \",',\n",
              "  'class ElementType(TypedDict):\\n    \"\"\"Element type as typed dict.\"\"\"\\n\\n    url: str\\n    xpath: str\\n    content: str\\n    metadata: Dict[str, str]',\n",
              "  'if current_content:\\n            lines_with_metadata.append(\\n                {\"content\": \"\\\\n\".join(current_content), \"metadata\": current_metadata}\\n            )\\n\\n        # lines_with_metadata has each line with associated header metadata\\n        # aggregate these into chunks based on common metadata\\n        if not self.return_each_line:\\n            return self.aggregate_lines_to_chunks(lines_with_metadata)\\n        else:\\n            return [\\n                Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n                for chunk in lines_with_metadata\\n            ]',\n",
              "  '# Push the current header to the stack\\n                        header: HeaderType = {\\n                            \"level\": current_header_level,\\n                            \"name\": name,\\n                            \"data\": stripped_line[len(sep) :].strip(),\\n                        }\\n                        header_stack.append(header)\\n                        # Update initial_metadata with the current header\\n                        initial_metadata[name] = header[\"data\"]\\n\\n                    # Add the previous line to the lines_with_metadata\\n                    # only if current_content is not empty\\n                    if current_content:\\n                        lines_with_metadata.append(\\n                            {\\n                                \"content\": \"\\\\n\".join(current_content),\\n                                \"metadata\": current_metadata.copy(),\\n                            }\\n                        )\\n                        current_content.clear()\\n\\n                    break\\n            else:\\n                if stripped_line:\\n                    current_content.append(stripped_line)\\n                elif current_content:\\n                    lines_with_metadata.append(\\n                        {\\n                            \"content\": \"\\\\n\".join(current_content),\\n                            \"metadata\": current_metadata.copy(),\\n                        }\\n                    )\\n                    current_content.clear()\\n\\n            current_metadata = initial_metadata.copy()\\n\\n        if current_content:\\n            lines_with_metadata.append(\\n                {\"content\": \"\\\\n\".join(current_content), \"metadata\": current_metadata}\\n            )',\n",
              "  'if in_code_block:\\n                current_content.append(stripped_line)\\n                continue\\n\\n            # Check each line against each of the header types (e.g., #, ##)\\n            for sep, name in self.headers_to_split_on:\\n                # Check if line starts with a header that we intend to split on\\n                if stripped_line.startswith(sep) and (\\n                    # Header with no text OR header is followed by space\\n                    # Both are valid conditions that sep is being used a header\\n                    len(stripped_line) == len(sep) or stripped_line[len(sep)] == \" \"\\n                ):\\n                    # Ensure we are tracking the header as metadata\\n                    if name is not None:\\n                        # Get the current header level\\n                        current_header_level = sep.count(\"#\")\\n\\n                        # Pop out headers of lower or same level from the stack\\n                        while (\\n                            header_stack\\n                            and header_stack[-1][\"level\"] >= current_header_level\\n                        ):\\n                            # We have encountered a new header\\n                            # at the same or higher level\\n                            popped_header = header_stack.pop()\\n                            # Clear the metadata for the\\n                            # popped header in initial_metadata\\n                            if popped_header[\"name\"] in initial_metadata:\\n                                initial_metadata.pop(popped_header[\"name\"])',\n",
              "  'def split_text(self, text: str) -> List[Document]:\\n        \"\"\"Split markdown file\\n        Args:\\n            text: Markdown file\"\"\"\\n\\n        # Split the input text by newline character (\"\\\\n\").\\n        lines = text.split(\"\\\\n\")\\n        # Final output\\n        lines_with_metadata: List[LineType] = []\\n        # Content and metadata of the chunk currently being processed\\n        current_content: List[str] = []\\n        current_metadata: Dict[str, str] = {}\\n        # Keep track of the nested header structure\\n        # header_stack: List[Dict[str, Union[int, str]]] = []\\n        header_stack: List[HeaderType] = []\\n        initial_metadata: Dict[str, str] = {}\\n\\n        in_code_block = False\\n        opening_fence = \"\"\\n\\n        for line in lines:\\n            stripped_line = line.strip()\\n\\n            if not in_code_block:\\n                # Exclude inline code spans\\n                if stripped_line.startswith(\"```\") and stripped_line.count(\"```\") == 1:\\n                    in_code_block = True\\n                    opening_fence = \"```\"\\n                elif stripped_line.startswith(\"~~~\"):\\n                    in_code_block = True\\n                    opening_fence = \"~~~\"\\n            else:\\n                if stripped_line.startswith(opening_fence):\\n                    in_code_block = False\\n                    opening_fence = \"\"\\n\\n            if in_code_block:\\n                current_content.append(stripped_line)\\n                continue',\n",
              "  'class MarkdownHeaderTextSplitter:\\n    \"\"\"Splitting markdown files based on specified headers.\"\"\"\\n\\n    def __init__(\\n        self, headers_to_split_on: List[Tuple[str, str]], return_each_line: bool = False\\n    ):\\n        \"\"\"Create a new MarkdownHeaderTextSplitter.\\n\\n        Args:\\n            headers_to_split_on: Headers we want to track\\n            return_each_line: Return each line w/ associated headers\\n        \"\"\"\\n        # Output line-by-line or aggregated into chunks w/ common headers\\n        self.return_each_line = return_each_line\\n        # Given the headers we want to split on,\\n        # (e.g., \"#, ##, etc\") order by length\\n        self.headers_to_split_on = sorted(\\n            headers_to_split_on, key=lambda split: len(split[0]), reverse=True\\n        )\\n\\n    def aggregate_lines_to_chunks(self, lines: List[LineType]) -> List[Document]:\\n        \"\"\"Combine lines with common metadata into chunks\\n        Args:\\n            lines: Line of text / associated header metadata\\n        \"\"\"\\n        aggregated_chunks: List[LineType] = []\\n\\n        for line in lines:\\n            if (\\n                aggregated_chunks\\n                and aggregated_chunks[-1][\"metadata\"] == line[\"metadata\"]\\n            ):\\n                # If the last line in the aggregated list\\n                # has the same metadata as the current line,\\n                # append the current content to the last lines\\'s content\\n                aggregated_chunks[-1][\"content\"] += \"  \\\\n\" + line[\"content\"]\\n            else:\\n                # Otherwise, append the current line to the aggregated list\\n                aggregated_chunks.append(line)\\n\\n        return [\\n            Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n            for chunk in aggregated_chunks\\n        ]\\n\\n    def split_text(self, text: str) -> List[Document]:\\n        \"\"\"Split markdown file\\n        Args:\\n            text: Markdown file\"\"\"',\n",
              "  'class CharacterTextSplitter(TextSplitter):\\n    \"\"\"Splitting text that looks at characters.\"\"\"\\n\\n    def __init__(\\n        self, separator: str = \"\\\\n\\\\n\", is_separator_regex: bool = False, **kwargs: Any\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(**kwargs)\\n        self._separator = separator\\n        self._is_separator_regex = is_separator_regex\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        # First we naively split the large input into a bunch of smaller ones.\\n        separator = (\\n            self._separator if self._is_separator_regex else re.escape(self._separator)\\n        )\\n        splits = _split_text_with_regex(text, separator, self._keep_separator)\\n        _separator = \"\" if self._keep_separator else self._separator\\n        return self._merge_splits(splits, _separator)\\n\\n\\nclass LineType(TypedDict):\\n    \"\"\"Line type as typed dict.\"\"\"\\n\\n    metadata: Dict[str, str]\\n    content: str\\n\\n\\nclass HeaderType(TypedDict):\\n    \"\"\"Header type as typed dict.\"\"\"\\n\\n    level: int\\n    name: str\\n    data: str',\n",
              "  'if issubclass(cls, TokenTextSplitter):\\n            extra_kwargs = {\\n                \"encoding_name\": encoding_name,\\n                \"model_name\": model_name,\\n                \"allowed_special\": allowed_special,\\n                \"disallowed_special\": disallowed_special,\\n            }\\n            kwargs = {**kwargs, **extra_kwargs}\\n\\n        return cls(length_function=_tiktoken_encoder, **kwargs)\\n\\n    def transform_documents(\\n        self, documents: Sequence[Document], **kwargs: Any\\n    ) -> Sequence[Document]:\\n        \"\"\"Transform sequence of documents by splitting them.\"\"\"\\n        return self.split_documents(list(documents))\\n\\n    async def atransform_documents(\\n        self, documents: Sequence[Document], **kwargs: Any\\n    ) -> Sequence[Document]:\\n        \"\"\"Asynchronously transform a sequence of documents by splitting them.\"\"\"\\n        return await asyncio.get_running_loop().run_in_executor(\\n            None, partial(self.transform_documents, **kwargs), documents\\n        )',\n",
              "  'return ForefrontAI\\n    elif name == \"GooseAI\":\\n        from langchain.llms import GooseAI\\n\\n        _warn_on_import(name, replacement=\"langchain.llms.GooseAI\")\\n\\n        return GooseAI\\n    elif name == \"HuggingFaceHub\":\\n        from langchain.llms import HuggingFaceHub\\n\\n        _warn_on_import(name, replacement=\"langchain.llms.HuggingFaceHub\")\\n\\n        return HuggingFaceHub\\n    elif name == \"HuggingFaceTextGenInference\":\\n        from langchain.llms import HuggingFaceTextGenInference\\n\\n        _warn_on_import(name, replacement=\"langchain.llms.HuggingFaceTextGenInference\")\\n\\n        return HuggingFaceTextGenInference\\n    elif name == \"LlamaCpp\":\\n        from langchain.llms import LlamaCpp\\n\\n        _warn_on_import(name, replacement=\"langchain.llms.LlamaCpp\")\\n\\n        return LlamaCpp\\n    elif name == \"Modal\":\\n        from langchain.llms import Modal\\n\\n        _warn_on_import(name, replacement=\"langchain.llms.Modal\")\\n\\n        return Modal\\n    elif name == \"OpenAI\":\\n        from langchain.llms import OpenAI\\n\\n        _warn_on_import(name, replacement=\"langchain.llms.OpenAI\")\\n\\n        return OpenAI\\n    elif name == \"Petals\":\\n        from langchain.llms import Petals\\n\\n        _warn_on_import(name, replacement=\"langchain.llms.Petals\")\\n\\n        return Petals\\n    elif name == \"PipelineAI\":\\n        from langchain.llms import PipelineAI\\n\\n        _warn_on_import(name, replacement=\"langchain.llms.PipelineAI\")\\n\\n        return PipelineAI\\n    elif name == \"SagemakerEndpoint\":\\n        from langchain.llms import SagemakerEndpoint\\n\\n        _warn_on_import(name, replacement=\"langchain.llms.SagemakerEndpoint\")\\n\\n        return SagemakerEndpoint\\n    elif name == \"StochasticAI\":\\n        from langchain.llms import StochasticAI\\n\\n        _warn_on_import(name, replacement=\"langchain.llms.StochasticAI\")\\n\\n        return StochasticAI\\n    elif name == \"Writer\":\\n        from langchain.llms import Writer\\n\\n        _warn_on_import(name, replacement=\"langchain.llms.Writer\")',\n",
              "  'return StochasticAI\\n    elif name == \"Writer\":\\n        from langchain.llms import Writer\\n\\n        _warn_on_import(name, replacement=\"langchain.llms.Writer\")\\n\\n        return Writer\\n    elif name == \"HuggingFacePipeline\":\\n        from langchain.llms.huggingface_pipeline import HuggingFacePipeline\\n\\n        _warn_on_import(\\n            name, replacement=\"langchain.llms.huggingface_pipeline.HuggingFacePipeline\"\\n        )\\n\\n        return HuggingFacePipeline\\n    elif name == \"FewShotPromptTemplate\":\\n        from langchain_core.prompts import FewShotPromptTemplate\\n\\n        _warn_on_import(name, replacement=\"langchain.prompts.FewShotPromptTemplate\")\\n\\n        return FewShotPromptTemplate\\n    elif name == \"Prompt\":\\n        from langchain.prompts import Prompt\\n\\n        _warn_on_import(name, replacement=\"langchain.prompts.Prompt\")\\n\\n        return Prompt\\n    elif name == \"PromptTemplate\":\\n        from langchain_core.prompts import PromptTemplate\\n\\n        _warn_on_import(name, replacement=\"langchain.prompts.PromptTemplate\")\\n\\n        return PromptTemplate\\n    elif name == \"BasePromptTemplate\":\\n        from langchain_core.prompts import BasePromptTemplate\\n\\n        _warn_on_import(\\n            name, replacement=\"langchain.schema.prompt_template.BasePromptTemplate\"\\n        )\\n\\n        return BasePromptTemplate\\n    elif name == \"ArxivAPIWrapper\":\\n        from langchain.utilities import ArxivAPIWrapper\\n\\n        _warn_on_import(name, replacement=\"langchain.utilities.ArxivAPIWrapper\")\\n\\n        return ArxivAPIWrapper\\n    elif name == \"GoldenQueryAPIWrapper\":\\n        from langchain.utilities import GoldenQueryAPIWrapper\\n\\n        _warn_on_import(name, replacement=\"langchain.utilities.GoldenQueryAPIWrapper\")\\n\\n        return GoldenQueryAPIWrapper\\n    elif name == \"GoogleSearchAPIWrapper\":\\n        from langchain.utilities import GoogleSearchAPIWrapper\\n\\n        _warn_on_import(name, replacement=\"langchain.utilities.GoogleSearchAPIWrapper\")',\n",
              "  'return ForefrontAI\\n    elif name == \"GooseAI\":\\n        from langchain.llms import GooseAI\\n\\n        _warn_on_import(name, replacement=\"langchain.llms.GooseAI\")\\n\\n        return GooseAI\\n    elif name == \"HuggingFaceHub\":\\n        from langchain.llms import HuggingFaceHub\\n\\n        _warn_on_import(name, replacement=\"langchain.llms.HuggingFaceHub\")\\n\\n        return HuggingFaceHub\\n    elif name == \"HuggingFaceTextGenInference\":\\n        from langchain.llms import HuggingFaceTextGenInference\\n\\n        _warn_on_import(name, replacement=\"langchain.llms.HuggingFaceTextGenInference\")\\n\\n        return HuggingFaceTextGenInference\\n    elif name == \"LlamaCpp\":\\n        from langchain.llms import LlamaCpp\\n\\n        _warn_on_import(name, replacement=\"langchain.llms.LlamaCpp\")\\n\\n        return LlamaCpp\\n    elif name == \"Modal\":\\n        from langchain.llms import Modal\\n\\n        _warn_on_import(name, replacement=\"langchain.llms.Modal\")\\n\\n        return Modal\\n    elif name == \"OpenAI\":\\n        from langchain.llms import OpenAI\\n\\n        _warn_on_import(name, replacement=\"langchain.llms.OpenAI\")\\n\\n        return OpenAI\\n    elif name == \"Petals\":\\n        from langchain.llms import Petals\\n\\n        _warn_on_import(name, replacement=\"langchain.llms.Petals\")\\n\\n        return Petals\\n    elif name == \"PipelineAI\":\\n        from langchain.llms import PipelineAI\\n\\n        _warn_on_import(name, replacement=\"langchain.llms.PipelineAI\")\\n\\n        return PipelineAI\\n    elif name == \"SagemakerEndpoint\":\\n        from langchain.llms import SagemakerEndpoint\\n\\n        _warn_on_import(name, replacement=\"langchain.llms.SagemakerEndpoint\")\\n\\n        return SagemakerEndpoint\\n    elif name == \"StochasticAI\":\\n        from langchain.llms import StochasticAI\\n\\n        _warn_on_import(name, replacement=\"langchain.llms.StochasticAI\")\\n\\n        return StochasticAI\\n    elif name == \"Writer\":\\n        from langchain.llms import Writer\\n\\n        _warn_on_import(name, replacement=\"langchain.llms.Writer\")',\n",
              "  '_warn_on_import(name, replacement=\"langchain.chains.QAWithSourcesChain\")\\n\\n        return QAWithSourcesChain\\n    elif name == \"VectorDBQA\":\\n        from langchain.chains import VectorDBQA\\n\\n        _warn_on_import(name, replacement=\"langchain.chains.VectorDBQA\")\\n\\n        return VectorDBQA\\n    elif name == \"VectorDBQAWithSourcesChain\":\\n        from langchain.chains import VectorDBQAWithSourcesChain\\n\\n        _warn_on_import(name, replacement=\"langchain.chains.VectorDBQAWithSourcesChain\")\\n\\n        return VectorDBQAWithSourcesChain\\n    elif name == \"InMemoryDocstore\":\\n        from langchain.docstore import InMemoryDocstore\\n\\n        _warn_on_import(name, replacement=\"langchain.docstore.InMemoryDocstore\")\\n\\n        return InMemoryDocstore\\n    elif name == \"Wikipedia\":\\n        from langchain.docstore import Wikipedia\\n\\n        _warn_on_import(name, replacement=\"langchain.docstore.Wikipedia\")\\n\\n        return Wikipedia\\n    elif name == \"Anthropic\":\\n        from langchain.llms import Anthropic\\n\\n        _warn_on_import(name, replacement=\"langchain.llms.Anthropic\")\\n\\n        return Anthropic\\n    elif name == \"Banana\":\\n        from langchain.llms import Banana\\n\\n        _warn_on_import(name, replacement=\"langchain.llms.Banana\")\\n\\n        return Banana\\n    elif name == \"CerebriumAI\":\\n        from langchain.llms import CerebriumAI\\n\\n        _warn_on_import(name, replacement=\"langchain.llms.CerebriumAI\")\\n\\n        return CerebriumAI\\n    elif name == \"Cohere\":\\n        from langchain.llms import Cohere\\n\\n        _warn_on_import(name, replacement=\"langchain.llms.Cohere\")\\n\\n        return Cohere\\n    elif name == \"ForefrontAI\":\\n        from langchain.llms import ForefrontAI\\n\\n        _warn_on_import(name, replacement=\"langchain.llms.ForefrontAI\")\\n\\n        return ForefrontAI\\n    elif name == \"GooseAI\":\\n        from langchain.llms import GooseAI\\n\\n        _warn_on_import(name, replacement=\"langchain.llms.GooseAI\")',\n",
              "  'def __getattr__(name: str) -> Any:\\n    if name == \"MRKLChain\":\\n        from langchain.agents import MRKLChain\\n\\n        _warn_on_import(name, replacement=\"langchain.agents.MRKLChain\")\\n\\n        return MRKLChain\\n    elif name == \"ReActChain\":\\n        from langchain.agents import ReActChain\\n\\n        _warn_on_import(name, replacement=\"langchain.agents.ReActChain\")\\n\\n        return ReActChain\\n    elif name == \"SelfAskWithSearchChain\":\\n        from langchain.agents import SelfAskWithSearchChain\\n\\n        _warn_on_import(name, replacement=\"langchain.agents.SelfAskWithSearchChain\")\\n\\n        return SelfAskWithSearchChain\\n    elif name == \"ConversationChain\":\\n        from langchain.chains import ConversationChain\\n\\n        _warn_on_import(name, replacement=\"langchain.chains.ConversationChain\")\\n\\n        return ConversationChain\\n    elif name == \"LLMBashChain\":\\n        raise ImportError(\\n            \"This module has been moved to langchain-experimental. \"\\n            \"For more details: \"\\n            \"https://github.com/langchain-ai/langchain/discussions/11352.\"\\n            \"To access this code, install it with `pip install langchain-experimental`.\"\\n            \"`from langchain_experimental.llm_bash.base \"\\n            \"import LLMBashChain`\"\\n        )\\n\\n    elif name == \"LLMChain\":\\n        from langchain.chains import LLMChain\\n\\n        _warn_on_import(name, replacement=\"langchain.chains.LLMChain\")\\n\\n        return LLMChain\\n    elif name == \"LLMCheckerChain\":\\n        from langchain.chains import LLMCheckerChain\\n\\n        _warn_on_import(name, replacement=\"langchain.chains.LLMCheckerChain\")\\n\\n        return LLMCheckerChain\\n    elif name == \"LLMMathChain\":\\n        from langchain.chains import LLMMathChain\\n\\n        _warn_on_import(name, replacement=\"langchain.chains.LLMMathChain\")\\n\\n        return LLMMathChain\\n    elif name == \"QAWithSourcesChain\":\\n        from langchain.chains import QAWithSourcesChain\\n\\n        _warn_on_import(name, replacement=\"langchain.chains.QAWithSourcesChain\")',\n",
              "  '# ruff: noqa: E402\\n\"\"\"Main entrypoint into package.\"\"\"\\nimport warnings\\nfrom importlib import metadata\\nfrom typing import Any, Optional\\n\\nfrom langchain_core._api.deprecation import surface_langchain_deprecation_warnings\\n\\ntry:\\n    __version__ = metadata.version(__package__)\\nexcept metadata.PackageNotFoundError:\\n    # Case where package metadata is not available.\\n    __version__ = \"\"\\ndel metadata  # optional, avoids polluting the results of dir(__package__)\\n\\n\\ndef _is_interactive_env() -> bool:\\n    \"\"\"Determine if running within IPython or Jupyter.\"\"\"\\n    import sys\\n\\n    return hasattr(sys, \"ps2\")\\n\\n\\ndef _warn_on_import(name: str, replacement: Optional[str] = None) -> None:\\n    \"\"\"Warn on import of deprecated module.\"\"\"\\n    if _is_interactive_env():\\n        # No warnings for interactive environments.\\n        # This is done to avoid polluting the output of interactive environments\\n        # where users rely on auto-complete and may trigger this warning\\n        # even if they are not using any deprecated modules\\n        return\\n\\n    if replacement:\\n        warnings.warn(\\n            f\"Importing {name} from langchain root module is no longer supported. \"\\n            f\"Please use {replacement} instead.\"\\n        )\\n    else:\\n        warnings.warn(\\n            f\"Importing {name} from langchain root module is no longer supported.\"\\n        )\\n\\n\\n# Surfaces Deprecation and Pending Deprecation warnings from langchain.\\nsurface_langchain_deprecation_warnings()',\n",
              "  'return _llm_cache\\n    else:\\n        raise AttributeError(f\"Could not find: {name}\")\\n\\n\\n__all__ = [\\n    \"LLMChain\",\\n    \"LLMCheckerChain\",\\n    \"LLMMathChain\",\\n    \"ArxivAPIWrapper\",\\n    \"GoldenQueryAPIWrapper\",\\n    \"SelfAskWithSearchChain\",\\n    \"SerpAPIWrapper\",\\n    \"SerpAPIChain\",\\n    \"SearxSearchWrapper\",\\n    \"GoogleSearchAPIWrapper\",\\n    \"GoogleSerperAPIWrapper\",\\n    \"WolframAlphaAPIWrapper\",\\n    \"WikipediaAPIWrapper\",\\n    \"Anthropic\",\\n    \"Banana\",\\n    \"CerebriumAI\",\\n    \"Cohere\",\\n    \"ForefrontAI\",\\n    \"GooseAI\",\\n    \"Modal\",\\n    \"OpenAI\",\\n    \"Petals\",\\n    \"PipelineAI\",\\n    \"StochasticAI\",\\n    \"Writer\",\\n    \"BasePromptTemplate\",\\n    \"Prompt\",\\n    \"FewShotPromptTemplate\",\\n    \"PromptTemplate\",\\n    \"ReActChain\",\\n    \"Wikipedia\",\\n    \"HuggingFaceHub\",\\n    \"SagemakerEndpoint\",\\n    \"HuggingFacePipeline\",\\n    \"SQLDatabase\",\\n    \"PowerBIDataset\",\\n    \"FAISS\",\\n    \"MRKLChain\",\\n    \"VectorDBQA\",\\n    \"ElasticVectorSearch\",\\n    \"InMemoryDocstore\",\\n    \"ConversationChain\",\\n    \"VectorDBQAWithSourcesChain\",\\n    \"QAWithSourcesChain\",\\n    \"LlamaCpp\",\\n    \"HuggingFaceTextGenInference\",\\n]',\n",
              "  'return ElasticVectorSearch\\n    # For backwards compatibility\\n    elif name == \"SerpAPIChain\" or name == \"SerpAPIWrapper\":\\n        from langchain.utilities import SerpAPIWrapper\\n\\n        _warn_on_import(name, replacement=\"langchain.utilities.SerpAPIWrapper\")\\n\\n        return SerpAPIWrapper\\n    elif name == \"verbose\":\\n        from langchain.globals import _verbose\\n\\n        _warn_on_import(\\n            name,\\n            replacement=(\\n                \"langchain.globals.set_verbose() / langchain.globals.get_verbose()\"\\n            ),\\n        )\\n\\n        return _verbose\\n    elif name == \"debug\":\\n        from langchain.globals import _debug\\n\\n        _warn_on_import(\\n            name,\\n            replacement=(\\n                \"langchain.globals.set_debug() / langchain.globals.get_debug()\"\\n            ),\\n        )\\n\\n        return _debug\\n    elif name == \"llm_cache\":\\n        from langchain.globals import _llm_cache\\n\\n        _warn_on_import(\\n            name,\\n            replacement=(\\n                \"langchain.globals.set_llm_cache() / langchain.globals.get_llm_cache()\"\\n            ),\\n        )\\n\\n        return _llm_cache\\n    else:\\n        raise AttributeError(f\"Could not find: {name}\")',\n",
              "  '_warn_on_import(name, replacement=\"langchain.utilities.GoogleSearchAPIWrapper\")\\n\\n        return GoogleSearchAPIWrapper\\n    elif name == \"GoogleSerperAPIWrapper\":\\n        from langchain.utilities import GoogleSerperAPIWrapper\\n\\n        _warn_on_import(name, replacement=\"langchain.utilities.GoogleSerperAPIWrapper\")\\n\\n        return GoogleSerperAPIWrapper\\n    elif name == \"PowerBIDataset\":\\n        from langchain.utilities import PowerBIDataset\\n\\n        _warn_on_import(name, replacement=\"langchain.utilities.PowerBIDataset\")\\n\\n        return PowerBIDataset\\n    elif name == \"SearxSearchWrapper\":\\n        from langchain.utilities import SearxSearchWrapper\\n\\n        _warn_on_import(name, replacement=\"langchain.utilities.SearxSearchWrapper\")\\n\\n        return SearxSearchWrapper\\n    elif name == \"WikipediaAPIWrapper\":\\n        from langchain.utilities import WikipediaAPIWrapper\\n\\n        _warn_on_import(name, replacement=\"langchain.utilities.WikipediaAPIWrapper\")\\n\\n        return WikipediaAPIWrapper\\n    elif name == \"WolframAlphaAPIWrapper\":\\n        from langchain.utilities import WolframAlphaAPIWrapper\\n\\n        _warn_on_import(name, replacement=\"langchain.utilities.WolframAlphaAPIWrapper\")\\n\\n        return WolframAlphaAPIWrapper\\n    elif name == \"SQLDatabase\":\\n        from langchain.utilities import SQLDatabase\\n\\n        _warn_on_import(name, replacement=\"langchain.utilities.SQLDatabase\")\\n\\n        return SQLDatabase\\n    elif name == \"FAISS\":\\n        from langchain.vectorstores import FAISS\\n\\n        _warn_on_import(name, replacement=\"langchain.vectorstores.FAISS\")\\n\\n        return FAISS\\n    elif name == \"ElasticVectorSearch\":\\n        from langchain.vectorstores import ElasticVectorSearch\\n\\n        _warn_on_import(name, replacement=\"langchain.vectorstores.ElasticVectorSearch\")\\n\\n        return ElasticVectorSearch\\n    # For backwards compatibility\\n    elif name == \"SerpAPIChain\" or name == \"SerpAPIWrapper\":\\n        from langchain.utilities import SerpAPIWrapper',\n",
              "  'return StochasticAI\\n    elif name == \"Writer\":\\n        from langchain.llms import Writer\\n\\n        _warn_on_import(name, replacement=\"langchain.llms.Writer\")\\n\\n        return Writer\\n    elif name == \"HuggingFacePipeline\":\\n        from langchain.llms.huggingface_pipeline import HuggingFacePipeline\\n\\n        _warn_on_import(\\n            name, replacement=\"langchain.llms.huggingface_pipeline.HuggingFacePipeline\"\\n        )\\n\\n        return HuggingFacePipeline\\n    elif name == \"FewShotPromptTemplate\":\\n        from langchain_core.prompts import FewShotPromptTemplate\\n\\n        _warn_on_import(name, replacement=\"langchain.prompts.FewShotPromptTemplate\")\\n\\n        return FewShotPromptTemplate\\n    elif name == \"Prompt\":\\n        from langchain.prompts import Prompt\\n\\n        _warn_on_import(name, replacement=\"langchain.prompts.Prompt\")\\n\\n        return Prompt\\n    elif name == \"PromptTemplate\":\\n        from langchain_core.prompts import PromptTemplate\\n\\n        _warn_on_import(name, replacement=\"langchain.prompts.PromptTemplate\")\\n\\n        return PromptTemplate\\n    elif name == \"BasePromptTemplate\":\\n        from langchain_core.prompts import BasePromptTemplate\\n\\n        _warn_on_import(\\n            name, replacement=\"langchain.schema.prompt_template.BasePromptTemplate\"\\n        )\\n\\n        return BasePromptTemplate\\n    elif name == \"ArxivAPIWrapper\":\\n        from langchain.utilities import ArxivAPIWrapper\\n\\n        _warn_on_import(name, replacement=\"langchain.utilities.ArxivAPIWrapper\")\\n\\n        return ArxivAPIWrapper\\n    elif name == \"GoldenQueryAPIWrapper\":\\n        from langchain.utilities import GoldenQueryAPIWrapper\\n\\n        _warn_on_import(name, replacement=\"langchain.utilities.GoldenQueryAPIWrapper\")\\n\\n        return GoldenQueryAPIWrapper\\n    elif name == \"GoogleSearchAPIWrapper\":\\n        from langchain.utilities import GoogleSearchAPIWrapper\\n\\n        _warn_on_import(name, replacement=\"langchain.utilities.GoogleSearchAPIWrapper\")',\n",
              "  '_warn_on_import(name, replacement=\"langchain.utilities.GoogleSearchAPIWrapper\")\\n\\n        return GoogleSearchAPIWrapper\\n    elif name == \"GoogleSerperAPIWrapper\":\\n        from langchain.utilities import GoogleSerperAPIWrapper\\n\\n        _warn_on_import(name, replacement=\"langchain.utilities.GoogleSerperAPIWrapper\")\\n\\n        return GoogleSerperAPIWrapper\\n    elif name == \"PowerBIDataset\":\\n        from langchain.utilities import PowerBIDataset\\n\\n        _warn_on_import(name, replacement=\"langchain.utilities.PowerBIDataset\")\\n\\n        return PowerBIDataset\\n    elif name == \"SearxSearchWrapper\":\\n        from langchain.utilities import SearxSearchWrapper\\n\\n        _warn_on_import(name, replacement=\"langchain.utilities.SearxSearchWrapper\")\\n\\n        return SearxSearchWrapper\\n    elif name == \"WikipediaAPIWrapper\":\\n        from langchain.utilities import WikipediaAPIWrapper\\n\\n        _warn_on_import(name, replacement=\"langchain.utilities.WikipediaAPIWrapper\")\\n\\n        return WikipediaAPIWrapper\\n    elif name == \"WolframAlphaAPIWrapper\":\\n        from langchain.utilities import WolframAlphaAPIWrapper\\n\\n        _warn_on_import(name, replacement=\"langchain.utilities.WolframAlphaAPIWrapper\")\\n\\n        return WolframAlphaAPIWrapper\\n    elif name == \"SQLDatabase\":\\n        from langchain.utilities import SQLDatabase\\n\\n        _warn_on_import(name, replacement=\"langchain.utilities.SQLDatabase\")\\n\\n        return SQLDatabase\\n    elif name == \"FAISS\":\\n        from langchain.vectorstores import FAISS\\n\\n        _warn_on_import(name, replacement=\"langchain.vectorstores.FAISS\")\\n\\n        return FAISS\\n    elif name == \"ElasticVectorSearch\":\\n        from langchain.vectorstores import ElasticVectorSearch\\n\\n        _warn_on_import(name, replacement=\"langchain.vectorstores.ElasticVectorSearch\")\\n\\n        return ElasticVectorSearch\\n    # For backwards compatibility\\n    elif name == \"SerpAPIChain\" or name == \"SerpAPIWrapper\":\\n        from langchain.utilities import SerpAPIWrapper',\n",
              "  '_warn_on_import(name, replacement=\"langchain.chains.QAWithSourcesChain\")\\n\\n        return QAWithSourcesChain\\n    elif name == \"VectorDBQA\":\\n        from langchain.chains import VectorDBQA\\n\\n        _warn_on_import(name, replacement=\"langchain.chains.VectorDBQA\")\\n\\n        return VectorDBQA\\n    elif name == \"VectorDBQAWithSourcesChain\":\\n        from langchain.chains import VectorDBQAWithSourcesChain\\n\\n        _warn_on_import(name, replacement=\"langchain.chains.VectorDBQAWithSourcesChain\")\\n\\n        return VectorDBQAWithSourcesChain\\n    elif name == \"InMemoryDocstore\":\\n        from langchain.docstore import InMemoryDocstore\\n\\n        _warn_on_import(name, replacement=\"langchain.docstore.InMemoryDocstore\")\\n\\n        return InMemoryDocstore\\n    elif name == \"Wikipedia\":\\n        from langchain.docstore import Wikipedia\\n\\n        _warn_on_import(name, replacement=\"langchain.docstore.Wikipedia\")\\n\\n        return Wikipedia\\n    elif name == \"Anthropic\":\\n        from langchain.llms import Anthropic\\n\\n        _warn_on_import(name, replacement=\"langchain.llms.Anthropic\")\\n\\n        return Anthropic\\n    elif name == \"Banana\":\\n        from langchain.llms import Banana\\n\\n        _warn_on_import(name, replacement=\"langchain.llms.Banana\")\\n\\n        return Banana\\n    elif name == \"CerebriumAI\":\\n        from langchain.llms import CerebriumAI\\n\\n        _warn_on_import(name, replacement=\"langchain.llms.CerebriumAI\")\\n\\n        return CerebriumAI\\n    elif name == \"Cohere\":\\n        from langchain.llms import Cohere\\n\\n        _warn_on_import(name, replacement=\"langchain.llms.Cohere\")\\n\\n        return Cohere\\n    elif name == \"ForefrontAI\":\\n        from langchain.llms import ForefrontAI\\n\\n        _warn_on_import(name, replacement=\"langchain.llms.ForefrontAI\")\\n\\n        return ForefrontAI\\n    elif name == \"GooseAI\":\\n        from langchain.llms import GooseAI\\n\\n        _warn_on_import(name, replacement=\"langchain.llms.GooseAI\")',\n",
              "  'def __getattr__(name: str) -> Any:\\n    if name == \"MRKLChain\":\\n        from langchain.agents import MRKLChain\\n\\n        _warn_on_import(name, replacement=\"langchain.agents.MRKLChain\")\\n\\n        return MRKLChain\\n    elif name == \"ReActChain\":\\n        from langchain.agents import ReActChain\\n\\n        _warn_on_import(name, replacement=\"langchain.agents.ReActChain\")\\n\\n        return ReActChain\\n    elif name == \"SelfAskWithSearchChain\":\\n        from langchain.agents import SelfAskWithSearchChain\\n\\n        _warn_on_import(name, replacement=\"langchain.agents.SelfAskWithSearchChain\")\\n\\n        return SelfAskWithSearchChain\\n    elif name == \"ConversationChain\":\\n        from langchain.chains import ConversationChain\\n\\n        _warn_on_import(name, replacement=\"langchain.chains.ConversationChain\")\\n\\n        return ConversationChain\\n    elif name == \"LLMBashChain\":\\n        raise ImportError(\\n            \"This module has been moved to langchain-experimental. \"\\n            \"For more details: \"\\n            \"https://github.com/langchain-ai/langchain/discussions/11352.\"\\n            \"To access this code, install it with `pip install langchain-experimental`.\"\\n            \"`from langchain_experimental.llm_bash.base \"\\n            \"import LLMBashChain`\"\\n        )\\n\\n    elif name == \"LLMChain\":\\n        from langchain.chains import LLMChain\\n\\n        _warn_on_import(name, replacement=\"langchain.chains.LLMChain\")\\n\\n        return LLMChain\\n    elif name == \"LLMCheckerChain\":\\n        from langchain.chains import LLMCheckerChain\\n\\n        _warn_on_import(name, replacement=\"langchain.chains.LLMCheckerChain\")\\n\\n        return LLMCheckerChain\\n    elif name == \"LLMMathChain\":\\n        from langchain.chains import LLMMathChain\\n\\n        _warn_on_import(name, replacement=\"langchain.chains.LLMMathChain\")\\n\\n        return LLMMathChain\\n    elif name == \"QAWithSourcesChain\":\\n        from langchain.chains import QAWithSourcesChain\\n\\n        _warn_on_import(name, replacement=\"langchain.chains.QAWithSourcesChain\")',\n",
              "  '# ruff: noqa: E402\\n\"\"\"Main entrypoint into package.\"\"\"\\nimport warnings\\nfrom importlib import metadata\\nfrom typing import Any, Optional\\n\\nfrom langchain_core._api.deprecation import surface_langchain_deprecation_warnings\\n\\ntry:\\n    __version__ = metadata.version(__package__)\\nexcept metadata.PackageNotFoundError:\\n    # Case where package metadata is not available.\\n    __version__ = \"\"\\ndel metadata  # optional, avoids polluting the results of dir(__package__)\\n\\n\\ndef _is_interactive_env() -> bool:\\n    \"\"\"Determine if running within IPython or Jupyter.\"\"\"\\n    import sys\\n\\n    return hasattr(sys, \"ps2\")\\n\\n\\ndef _warn_on_import(name: str, replacement: Optional[str] = None) -> None:\\n    \"\"\"Warn on import of deprecated module.\"\"\"\\n    if _is_interactive_env():\\n        # No warnings for interactive environments.\\n        # This is done to avoid polluting the output of interactive environments\\n        # where users rely on auto-complete and may trigger this warning\\n        # even if they are not using any deprecated modules\\n        return\\n\\n    if replacement:\\n        warnings.warn(\\n            f\"Importing {name} from langchain root module is no longer supported. \"\\n            f\"Please use {replacement} instead.\"\\n        )\\n    else:\\n        warnings.warn(\\n            f\"Importing {name} from langchain root module is no longer supported.\"\\n        )\\n\\n\\n# Surfaces Deprecation and Pending Deprecation warnings from langchain.\\nsurface_langchain_deprecation_warnings()',\n",
              "  'return _llm_cache\\n    else:\\n        raise AttributeError(f\"Could not find: {name}\")\\n\\n\\n__all__ = [\\n    \"LLMChain\",\\n    \"LLMCheckerChain\",\\n    \"LLMMathChain\",\\n    \"ArxivAPIWrapper\",\\n    \"GoldenQueryAPIWrapper\",\\n    \"SelfAskWithSearchChain\",\\n    \"SerpAPIWrapper\",\\n    \"SerpAPIChain\",\\n    \"SearxSearchWrapper\",\\n    \"GoogleSearchAPIWrapper\",\\n    \"GoogleSerperAPIWrapper\",\\n    \"WolframAlphaAPIWrapper\",\\n    \"WikipediaAPIWrapper\",\\n    \"Anthropic\",\\n    \"Banana\",\\n    \"CerebriumAI\",\\n    \"Cohere\",\\n    \"ForefrontAI\",\\n    \"GooseAI\",\\n    \"Modal\",\\n    \"OpenAI\",\\n    \"Petals\",\\n    \"PipelineAI\",\\n    \"StochasticAI\",\\n    \"Writer\",\\n    \"BasePromptTemplate\",\\n    \"Prompt\",\\n    \"FewShotPromptTemplate\",\\n    \"PromptTemplate\",\\n    \"ReActChain\",\\n    \"Wikipedia\",\\n    \"HuggingFaceHub\",\\n    \"SagemakerEndpoint\",\\n    \"HuggingFacePipeline\",\\n    \"SQLDatabase\",\\n    \"PowerBIDataset\",\\n    \"FAISS\",\\n    \"MRKLChain\",\\n    \"VectorDBQA\",\\n    \"ElasticVectorSearch\",\\n    \"InMemoryDocstore\",\\n    \"ConversationChain\",\\n    \"VectorDBQAWithSourcesChain\",\\n    \"QAWithSourcesChain\",\\n    \"LlamaCpp\",\\n    \"HuggingFaceTextGenInference\",\\n]',\n",
              "  'return ElasticVectorSearch\\n    # For backwards compatibility\\n    elif name == \"SerpAPIChain\" or name == \"SerpAPIWrapper\":\\n        from langchain.utilities import SerpAPIWrapper\\n\\n        _warn_on_import(name, replacement=\"langchain.utilities.SerpAPIWrapper\")\\n\\n        return SerpAPIWrapper\\n    elif name == \"verbose\":\\n        from langchain.globals import _verbose\\n\\n        _warn_on_import(\\n            name,\\n            replacement=(\\n                \"langchain.globals.set_verbose() / langchain.globals.get_verbose()\"\\n            ),\\n        )\\n\\n        return _verbose\\n    elif name == \"debug\":\\n        from langchain.globals import _debug\\n\\n        _warn_on_import(\\n            name,\\n            replacement=(\\n                \"langchain.globals.set_debug() / langchain.globals.get_debug()\"\\n            ),\\n        )\\n\\n        return _debug\\n    elif name == \"llm_cache\":\\n        from langchain.globals import _llm_cache\\n\\n        _warn_on_import(\\n            name,\\n            replacement=(\\n                \"langchain.globals.set_llm_cache() / langchain.globals.get_llm_cache()\"\\n            ),\\n        )\\n\\n        return _llm_cache\\n    else:\\n        raise AttributeError(f\"Could not find: {name}\")',\n",
              "  '_warn_on_import(name, replacement=\"langchain.utilities.GoogleSearchAPIWrapper\")\\n\\n        return GoogleSearchAPIWrapper\\n    elif name == \"GoogleSerperAPIWrapper\":\\n        from langchain.utilities import GoogleSerperAPIWrapper\\n\\n        _warn_on_import(name, replacement=\"langchain.utilities.GoogleSerperAPIWrapper\")\\n\\n        return GoogleSerperAPIWrapper\\n    elif name == \"PowerBIDataset\":\\n        from langchain.utilities import PowerBIDataset\\n\\n        _warn_on_import(name, replacement=\"langchain.utilities.PowerBIDataset\")\\n\\n        return PowerBIDataset\\n    elif name == \"SearxSearchWrapper\":\\n        from langchain.utilities import SearxSearchWrapper\\n\\n        _warn_on_import(name, replacement=\"langchain.utilities.SearxSearchWrapper\")\\n\\n        return SearxSearchWrapper\\n    elif name == \"WikipediaAPIWrapper\":\\n        from langchain.utilities import WikipediaAPIWrapper\\n\\n        _warn_on_import(name, replacement=\"langchain.utilities.WikipediaAPIWrapper\")\\n\\n        return WikipediaAPIWrapper\\n    elif name == \"WolframAlphaAPIWrapper\":\\n        from langchain.utilities import WolframAlphaAPIWrapper\\n\\n        _warn_on_import(name, replacement=\"langchain.utilities.WolframAlphaAPIWrapper\")\\n\\n        return WolframAlphaAPIWrapper\\n    elif name == \"SQLDatabase\":\\n        from langchain.utilities import SQLDatabase\\n\\n        _warn_on_import(name, replacement=\"langchain.utilities.SQLDatabase\")\\n\\n        return SQLDatabase\\n    elif name == \"FAISS\":\\n        from langchain.vectorstores import FAISS\\n\\n        _warn_on_import(name, replacement=\"langchain.vectorstores.FAISS\")\\n\\n        return FAISS\\n    elif name == \"ElasticVectorSearch\":\\n        from langchain.vectorstores import ElasticVectorSearch\\n\\n        _warn_on_import(name, replacement=\"langchain.vectorstores.ElasticVectorSearch\")\\n\\n        return ElasticVectorSearch\\n    # For backwards compatibility\\n    elif name == \"SerpAPIChain\" or name == \"SerpAPIWrapper\":\\n        from langchain.utilities import SerpAPIWrapper',\n",
              "  'return StochasticAI\\n    elif name == \"Writer\":\\n        from langchain.llms import Writer\\n\\n        _warn_on_import(name, replacement=\"langchain.llms.Writer\")\\n\\n        return Writer\\n    elif name == \"HuggingFacePipeline\":\\n        from langchain.llms.huggingface_pipeline import HuggingFacePipeline\\n\\n        _warn_on_import(\\n            name, replacement=\"langchain.llms.huggingface_pipeline.HuggingFacePipeline\"\\n        )\\n\\n        return HuggingFacePipeline\\n    elif name == \"FewShotPromptTemplate\":\\n        from langchain_core.prompts import FewShotPromptTemplate\\n\\n        _warn_on_import(name, replacement=\"langchain.prompts.FewShotPromptTemplate\")\\n\\n        return FewShotPromptTemplate\\n    elif name == \"Prompt\":\\n        from langchain.prompts import Prompt\\n\\n        _warn_on_import(name, replacement=\"langchain.prompts.Prompt\")\\n\\n        return Prompt\\n    elif name == \"PromptTemplate\":\\n        from langchain_core.prompts import PromptTemplate\\n\\n        _warn_on_import(name, replacement=\"langchain.prompts.PromptTemplate\")\\n\\n        return PromptTemplate\\n    elif name == \"BasePromptTemplate\":\\n        from langchain_core.prompts import BasePromptTemplate\\n\\n        _warn_on_import(\\n            name, replacement=\"langchain.schema.prompt_template.BasePromptTemplate\"\\n        )\\n\\n        return BasePromptTemplate\\n    elif name == \"ArxivAPIWrapper\":\\n        from langchain.utilities import ArxivAPIWrapper\\n\\n        _warn_on_import(name, replacement=\"langchain.utilities.ArxivAPIWrapper\")\\n\\n        return ArxivAPIWrapper\\n    elif name == \"GoldenQueryAPIWrapper\":\\n        from langchain.utilities import GoldenQueryAPIWrapper\\n\\n        _warn_on_import(name, replacement=\"langchain.utilities.GoldenQueryAPIWrapper\")\\n\\n        return GoldenQueryAPIWrapper\\n    elif name == \"GoogleSearchAPIWrapper\":\\n        from langchain.utilities import GoogleSearchAPIWrapper\\n\\n        _warn_on_import(name, replacement=\"langchain.utilities.GoogleSearchAPIWrapper\")',\n",
              "  'return ForefrontAI\\n    elif name == \"GooseAI\":\\n        from langchain.llms import GooseAI\\n\\n        _warn_on_import(name, replacement=\"langchain.llms.GooseAI\")\\n\\n        return GooseAI\\n    elif name == \"HuggingFaceHub\":\\n        from langchain.llms import HuggingFaceHub\\n\\n        _warn_on_import(name, replacement=\"langchain.llms.HuggingFaceHub\")\\n\\n        return HuggingFaceHub\\n    elif name == \"HuggingFaceTextGenInference\":\\n        from langchain.llms import HuggingFaceTextGenInference\\n\\n        _warn_on_import(name, replacement=\"langchain.llms.HuggingFaceTextGenInference\")\\n\\n        return HuggingFaceTextGenInference\\n    elif name == \"LlamaCpp\":\\n        from langchain.llms import LlamaCpp\\n\\n        _warn_on_import(name, replacement=\"langchain.llms.LlamaCpp\")\\n\\n        return LlamaCpp\\n    elif name == \"Modal\":\\n        from langchain.llms import Modal\\n\\n        _warn_on_import(name, replacement=\"langchain.llms.Modal\")\\n\\n        return Modal\\n    elif name == \"OpenAI\":\\n        from langchain.llms import OpenAI\\n\\n        _warn_on_import(name, replacement=\"langchain.llms.OpenAI\")\\n\\n        return OpenAI\\n    elif name == \"Petals\":\\n        from langchain.llms import Petals\\n\\n        _warn_on_import(name, replacement=\"langchain.llms.Petals\")\\n\\n        return Petals\\n    elif name == \"PipelineAI\":\\n        from langchain.llms import PipelineAI\\n\\n        _warn_on_import(name, replacement=\"langchain.llms.PipelineAI\")\\n\\n        return PipelineAI\\n    elif name == \"SagemakerEndpoint\":\\n        from langchain.llms import SagemakerEndpoint\\n\\n        _warn_on_import(name, replacement=\"langchain.llms.SagemakerEndpoint\")\\n\\n        return SagemakerEndpoint\\n    elif name == \"StochasticAI\":\\n        from langchain.llms import StochasticAI\\n\\n        _warn_on_import(name, replacement=\"langchain.llms.StochasticAI\")\\n\\n        return StochasticAI\\n    elif name == \"Writer\":\\n        from langchain.llms import Writer\\n\\n        _warn_on_import(name, replacement=\"langchain.llms.Writer\")',\n",
              "  'def _get_open_meteo_api(llm: BaseLanguageModel) -> BaseTool:\\n    chain = APIChain.from_llm_and_api_docs(\\n        llm,\\n        open_meteo_docs.OPEN_METEO_DOCS,\\n        limit_to_domains=[\"https://api.open-meteo.com/\"],\\n    )\\n    return Tool(\\n        name=\"Open-Meteo-API\",\\n        description=\"Useful for when you want to get weather information from the OpenMeteo API. The input should be a question in natural language that this API can answer.\",\\n        func=chain.run,\\n    )\\n\\n\\n_LLM_TOOLS: Dict[str, Callable[[BaseLanguageModel], BaseTool]] = {\\n    \"llm-math\": _get_llm_math,\\n    \"open-meteo-api\": _get_open_meteo_api,\\n}\\n\\n\\ndef _get_news_api(llm: BaseLanguageModel, **kwargs: Any) -> BaseTool:\\n    news_api_key = kwargs[\"news_api_key\"]\\n    chain = APIChain.from_llm_and_api_docs(\\n        llm,\\n        news_docs.NEWS_DOCS,\\n        headers={\"X-Api-Key\": news_api_key},\\n        limit_to_domains=[\"https://newsapi.org/\"],\\n    )\\n    return Tool(\\n        name=\"News-API\",\\n        description=\"Use this when you want to get information about the top headlines of current news stories. The input should be a question in natural language that this API can answer.\",\\n        func=chain.run,\\n    )\\n\\n\\ndef _get_tmdb_api(llm: BaseLanguageModel, **kwargs: Any) -> BaseTool:\\n    tmdb_bearer_token = kwargs[\"tmdb_bearer_token\"]\\n    chain = APIChain.from_llm_and_api_docs(\\n        llm,\\n        tmdb_docs.TMDB_DOCS,\\n        headers={\"Authorization\": f\"Bearer {tmdb_bearer_token}\"},\\n        limit_to_domains=[\"https://api.themoviedb.org/\"],\\n    )\\n    return Tool(\\n        name=\"TMDB-API\",\\n        description=\"Useful for when you want to get information from The Movie Database. The input should be a question in natural language that this API can answer.\",\\n        func=chain.run,\\n    )',\n",
              "  'def load_tools(\\n    tool_names: List[str],\\n    llm: Optional[BaseLanguageModel] = None,\\n    callbacks: Callbacks = None,\\n    **kwargs: Any,\\n) -> List[BaseTool]:\\n    \"\"\"Load tools based on their name.\\n\\n    Tools allow agents to interact with various resources and services like\\n    APIs, databases, file systems, etc.\\n\\n    Please scope the permissions of each tools to the minimum required for the\\n    application.\\n\\n    For example, if an application only needs to read from a database,\\n    the database tool should not be given write permissions. Moreover\\n    consider scoping the permissions to only allow accessing specific\\n    tables and impose user-level quota for limiting resource usage.\\n\\n    Please read the APIs of the individual tools to determine which configuration\\n    they support.\\n\\n    See [Security](https://python.langchain.com/docs/security) for more information.\\n\\n    Args:\\n        tool_names: name of tools to load.\\n        llm: An optional language model, may be needed to initialize certain tools.\\n        callbacks: Optional callback manager or list of callback handlers.\\n            If not provided, default global callback manager will be used.',\n",
              "  'def _handle_callbacks(\\n    callback_manager: Optional[BaseCallbackManager], callbacks: Callbacks\\n) -> Callbacks:\\n    if callback_manager is not None:\\n        warnings.warn(\\n            \"callback_manager is deprecated. Please use callbacks instead.\",\\n            DeprecationWarning,\\n        )\\n        if callbacks is not None:\\n            raise ValueError(\\n                \"Cannot specify both callback_manager and callbacks arguments.\"\\n            )\\n        return callback_manager\\n    return callbacks\\n\\n\\ndef load_huggingface_tool(\\n    task_or_repo_id: str,\\n    model_repo_id: Optional[str] = None,\\n    token: Optional[str] = None,\\n    remote: bool = False,\\n    **kwargs: Any,\\n) -> BaseTool:\\n    \"\"\"Loads a tool from the HuggingFace Hub.\\n\\n    Args:\\n        task_or_repo_id: Task or model repo id.\\n        model_repo_id: Optional model repo id.\\n        token: Optional token.\\n        remote: Optional remote. Defaults to False.\\n        **kwargs:\\n\\n    Returns:\\n        A tool.\\n    \"\"\"\\n    try:\\n        from transformers import load_tool\\n    except ImportError:\\n        raise ImportError(\\n            \"HuggingFace tools require the libraries `transformers>=4.29.0`\"\\n            \" and `huggingface_hub>=0.14.1` to be installed.\"\\n            \" Please install it with\"\\n            \" `pip install --upgrade transformers huggingface_hub`.\"\\n        )\\n    hf_tool = load_tool(\\n        task_or_repo_id,\\n        model_repo_id=model_repo_id,\\n        token=token,\\n        remote=remote,\\n        **kwargs,\\n    )\\n    outputs = hf_tool.outputs\\n    if set(outputs) != {\"text\"}:\\n        raise NotImplementedError(\"Multimodal outputs not supported yet.\")\\n    inputs = hf_tool.inputs\\n    if set(inputs) != {\"text\"}:\\n        raise NotImplementedError(\"Multimodal inputs not supported yet.\")\\n    return Tool.from_function(\\n        hf_tool.__call__, name=hf_tool.name, description=hf_tool.description\\n    )',\n",
              "  '\"searchapi-results-json\": (\\n        _get_searchapi_results_json,\\n        [\"searchapi_api_key\", \"aiosession\"],\\n    ),\\n    \"serpapi\": (_get_serpapi, [\"serpapi_api_key\", \"aiosession\"]),\\n    \"dalle-image-generator\": (_get_dalle_image_generator, [\"openai_api_key\"]),\\n    \"twilio\": (_get_twilio, [\"account_sid\", \"auth_token\", \"from_number\"]),\\n    \"searx-search\": (_get_searx_search, [\"searx_host\", \"engines\", \"aiosession\"]),\\n    \"merriam-webster\": (_get_merriam_webster, [\"merriam_webster_api_key\"]),\\n    \"wikipedia\": (_get_wikipedia, [\"top_k_results\", \"lang\"]),\\n    \"arxiv\": (\\n        _get_arxiv,\\n        [\"top_k_results\", \"load_max_docs\", \"load_all_available_meta\"],\\n    ),\\n    \"golden-query\": (_get_golden_query, [\"golden_api_key\"]),\\n    \"pubmed\": (_get_pubmed, [\"top_k_results\"]),\\n    \"human\": (_get_human_tool, [\"prompt_func\", \"input_func\"]),\\n    \"awslambda\": (\\n        _get_lambda_api,\\n        [\"awslambda_tool_name\", \"awslambda_tool_description\", \"function_name\"],\\n    ),\\n    \"stackexchange\": (_get_stackexchange, []),\\n    \"sceneXplain\": (_get_scenexplain, []),\\n    \"graphql\": (_get_graphql_tool, [\"graphql_endpoint\"]),\\n    \"openweathermap-api\": (_get_openweathermap, [\"openweathermap_api_key\"]),\\n    \"dataforseo-api-search\": (\\n        _get_dataforseo_api_search,\\n        [\"api_login\", \"api_password\", \"aiosession\"],\\n    ),\\n    \"dataforseo-api-search-json\": (\\n        _get_dataforseo_api_search_json,\\n        [\"api_login\", \"api_password\", \"aiosession\"],\\n    ),\\n    \"eleven_labs_text2speech\": (_get_eleven_labs_text2speech, [\"eleven_api_key\"]),\\n    \"google_cloud_texttospeech\": (_get_google_cloud_texttospeech, []),\\n    \"reddit_search\": (\\n        _get_reddit_search,\\n        [\"reddit_client_id\", \"reddit_client_secret\", \"reddit_user_agent\"],\\n    ),\\n}',\n",
              "  '_EXTRA_LLM_TOOLS: Dict[\\n    str,\\n    Tuple[Callable[[Arg(BaseLanguageModel, \"llm\"), KwArg(Any)], BaseTool], List[str]],\\n] = {\\n    \"news-api\": (_get_news_api, [\"news_api_key\"]),\\n    \"tmdb-api\": (_get_tmdb_api, [\"tmdb_bearer_token\"]),\\n    \"podcast-api\": (_get_podcast_api, [\"listen_api_key\"]),\\n    \"memorize\": (_get_memorize, []),\\n}\\n_EXTRA_OPTIONAL_TOOLS: Dict[str, Tuple[Callable[[KwArg(Any)], BaseTool], List[str]]] = {\\n    \"wolfram-alpha\": (_get_wolfram_alpha, [\"wolfram_alpha_appid\"]),\\n    \"google-search\": (_get_google_search, [\"google_api_key\", \"google_cse_id\"]),\\n    \"google-search-results-json\": (\\n        _get_google_search_results_json,\\n        [\"google_api_key\", \"google_cse_id\", \"num_results\"],\\n    ),\\n    \"searx-search-results-json\": (\\n        _get_searx_search_results_json,\\n        [\"searx_host\", \"engines\", \"num_results\", \"aiosession\"],\\n    ),\\n    \"bing-search\": (_get_bing_search, [\"bing_subscription_key\", \"bing_search_url\"]),\\n    \"metaphor-search\": (_get_metaphor_search, [\"metaphor_api_key\"]),\\n    \"ddg-search\": (_get_ddg_search, []),\\n    \"google-lens\": (_get_google_lens, [\"serp_api_key\"]),\\n    \"google-serper\": (_get_google_serper, [\"serper_api_key\", \"aiosession\"]),\\n    \"google-scholar\": (\\n        _get_google_scholar,\\n        [\"top_k_results\", \"hl\", \"lr\", \"serp_api_key\"],\\n    ),\\n    \"google-finance\": (\\n        _get_google_finance,\\n        [\"serp_api_key\"],\\n    ),\\n    \"google-trends\": (\\n        _get_google_trends,\\n        [\"serp_api_key\"],\\n    ),\\n    \"google-jobs\": (\\n        _get_google_jobs,\\n        [\"serp_api_key\"],\\n    ),\\n    \"google-serper-results-json\": (\\n        _get_google_serper_results_json,\\n        [\"serper_api_key\", \"aiosession\"],\\n    ),\\n    \"searchapi\": (_get_searchapi, [\"searchapi_api_key\", \"aiosession\"]),\\n    \"searchapi-results-json\": (\\n        _get_searchapi_results_json,\\n        [\"searchapi_api_key\", \"aiosession\"],\\n    ),\\n    \"serpapi\": (_get_serpapi, [\"serpapi_api_key\", \"aiosession\"]),',\n",
              "  'def _get_reddit_search(**kwargs: Any) -> BaseTool:\\n    return RedditSearchRun(api_wrapper=RedditSearchAPIWrapper(**kwargs))',\n",
              "  'def _get_eleven_labs_text2speech(**kwargs: Any) -> BaseTool:\\n    return ElevenLabsText2SpeechTool(**kwargs)\\n\\n\\ndef _get_memorize(llm: BaseLanguageModel, **kwargs: Any) -> BaseTool:\\n    return Memorize(llm=llm)\\n\\n\\ndef _get_google_cloud_texttospeech(**kwargs: Any) -> BaseTool:\\n    return GoogleCloudTextToSpeechTool(**kwargs)',\n",
              "  'def _get_twilio(**kwargs: Any) -> BaseTool:\\n    return Tool(\\n        name=\"Text-Message\",\\n        description=\"Useful for when you need to send a text message to a provided phone number.\",\\n        func=TwilioAPIWrapper(**kwargs).run,\\n    )\\n\\n\\ndef _get_searx_search(**kwargs: Any) -> BaseTool:\\n    return SearxSearchRun(wrapper=SearxSearchWrapper(**kwargs))\\n\\n\\ndef _get_searx_search_results_json(**kwargs: Any) -> BaseTool:\\n    wrapper_kwargs = {k: v for k, v in kwargs.items() if k != \"num_results\"}\\n    return SearxSearchResults(wrapper=SearxSearchWrapper(**wrapper_kwargs), **kwargs)\\n\\n\\ndef _get_bing_search(**kwargs: Any) -> BaseTool:\\n    return BingSearchRun(api_wrapper=BingSearchAPIWrapper(**kwargs))\\n\\n\\ndef _get_metaphor_search(**kwargs: Any) -> BaseTool:\\n    return MetaphorSearchResults(api_wrapper=MetaphorSearchAPIWrapper(**kwargs))\\n\\n\\ndef _get_ddg_search(**kwargs: Any) -> BaseTool:\\n    return DuckDuckGoSearchRun(api_wrapper=DuckDuckGoSearchAPIWrapper(**kwargs))\\n\\n\\ndef _get_human_tool(**kwargs: Any) -> BaseTool:\\n    return HumanInputRun(**kwargs)\\n\\n\\ndef _get_scenexplain(**kwargs: Any) -> BaseTool:\\n    return SceneXplainTool(**kwargs)\\n\\n\\ndef _get_graphql_tool(**kwargs: Any) -> BaseTool:\\n    graphql_endpoint = kwargs[\"graphql_endpoint\"]\\n    wrapper = GraphQLAPIWrapper(graphql_endpoint=graphql_endpoint)\\n    return BaseGraphQLTool(graphql_wrapper=wrapper)\\n\\n\\ndef _get_openweathermap(**kwargs: Any) -> BaseTool:\\n    return OpenWeatherMapQueryRun(api_wrapper=OpenWeatherMapAPIWrapper(**kwargs))\\n\\n\\ndef _get_dataforseo_api_search(**kwargs: Any) -> BaseTool:\\n    return DataForSeoAPISearchRun(api_wrapper=DataForSeoAPIWrapper(**kwargs))\\n\\n\\ndef _get_dataforseo_api_search_json(**kwargs: Any) -> BaseTool:\\n    return DataForSeoAPISearchResults(api_wrapper=DataForSeoAPIWrapper(**kwargs))\\n\\n\\ndef _get_eleven_labs_text2speech(**kwargs: Any) -> BaseTool:\\n    return ElevenLabsText2SpeechTool(**kwargs)',\n",
              "  'def _get_google_lens(**kwargs: Any) -> BaseTool:\\n    return GoogleLensQueryRun(api_wrapper=GoogleLensAPIWrapper(**kwargs))\\n\\n\\ndef _get_google_serper(**kwargs: Any) -> BaseTool:\\n    return GoogleSerperRun(api_wrapper=GoogleSerperAPIWrapper(**kwargs))\\n\\n\\ndef _get_google_scholar(**kwargs: Any) -> BaseTool:\\n    return GoogleScholarQueryRun(api_wrapper=GoogleScholarAPIWrapper(**kwargs))\\n\\n\\ndef _get_google_finance(**kwargs: Any) -> BaseTool:\\n    return GoogleFinanceQueryRun(api_wrapper=GoogleFinanceAPIWrapper(**kwargs))\\n\\n\\ndef _get_google_trends(**kwargs: Any) -> BaseTool:\\n    return GoogleTrendsQueryRun(api_wrapper=GoogleTrendsAPIWrapper(**kwargs))\\n\\n\\ndef _get_google_serper_results_json(**kwargs: Any) -> BaseTool:\\n    return GoogleSerperResults(api_wrapper=GoogleSerperAPIWrapper(**kwargs))\\n\\n\\ndef _get_google_search_results_json(**kwargs: Any) -> BaseTool:\\n    return GoogleSearchResults(api_wrapper=GoogleSearchAPIWrapper(**kwargs))\\n\\n\\ndef _get_searchapi(**kwargs: Any) -> BaseTool:\\n    return SearchAPIRun(api_wrapper=SearchApiAPIWrapper(**kwargs))\\n\\n\\ndef _get_searchapi_results_json(**kwargs: Any) -> BaseTool:\\n    return SearchAPIResults(api_wrapper=SearchApiAPIWrapper(**kwargs))\\n\\n\\ndef _get_serpapi(**kwargs: Any) -> BaseTool:\\n    return Tool(\\n        name=\"Search\",\\n        description=\"A search engine. Useful for when you need to answer questions about current events. Input should be a search query.\",\\n        func=SerpAPIWrapper(**kwargs).run,\\n        coroutine=SerpAPIWrapper(**kwargs).arun,\\n    )\\n\\n\\ndef _get_stackexchange(**kwargs: Any) -> BaseTool:\\n    return StackExchangeTool(api_wrapper=StackExchangeAPIWrapper(**kwargs))\\n\\n\\ndef _get_dalle_image_generator(**kwargs: Any) -> Tool:\\n    return Tool(\\n        \"Dall-E-Image-Generator\",\\n        DallEAPIWrapper(**kwargs).run,\\n        \"A wrapper around OpenAI DALL-E API. Useful for when you need to generate images from a text description. Input should be an image description.\",\\n    )',\n",
              "  'def _get_podcast_api(llm: BaseLanguageModel, **kwargs: Any) -> BaseTool:\\n    listen_api_key = kwargs[\"listen_api_key\"]\\n    chain = APIChain.from_llm_and_api_docs(\\n        llm,\\n        podcast_docs.PODCAST_DOCS,\\n        headers={\"X-ListenAPI-Key\": listen_api_key},\\n        limit_to_domains=[\"https://listen-api.listennotes.com/\"],\\n    )\\n    return Tool(\\n        name=\"Podcast-API\",\\n        description=\"Use the Listen Notes Podcast API to search all podcasts or episodes. The input should be a question in natural language that this API can answer.\",\\n        func=chain.run,\\n    )\\n\\n\\ndef _get_lambda_api(**kwargs: Any) -> BaseTool:\\n    return Tool(\\n        name=kwargs[\"awslambda_tool_name\"],\\n        description=kwargs[\"awslambda_tool_description\"],\\n        func=LambdaWrapper(**kwargs).run,\\n    )\\n\\n\\ndef _get_wolfram_alpha(**kwargs: Any) -> BaseTool:\\n    return WolframAlphaQueryRun(api_wrapper=WolframAlphaAPIWrapper(**kwargs))\\n\\n\\ndef _get_google_search(**kwargs: Any) -> BaseTool:\\n    return GoogleSearchRun(api_wrapper=GoogleSearchAPIWrapper(**kwargs))\\n\\n\\ndef _get_merriam_webster(**kwargs: Any) -> BaseTool:\\n    return MerriamWebsterQueryRun(api_wrapper=MerriamWebsterAPIWrapper(**kwargs))\\n\\n\\ndef _get_wikipedia(**kwargs: Any) -> BaseTool:\\n    return WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(**kwargs))\\n\\n\\ndef _get_arxiv(**kwargs: Any) -> BaseTool:\\n    return ArxivQueryRun(api_wrapper=ArxivAPIWrapper(**kwargs))\\n\\n\\ndef _get_golden_query(**kwargs: Any) -> BaseTool:\\n    return GoldenQueryRun(api_wrapper=GoldenQueryAPIWrapper(**kwargs))\\n\\n\\ndef _get_pubmed(**kwargs: Any) -> BaseTool:\\n    return PubmedQueryRun(api_wrapper=PubMedAPIWrapper(**kwargs))\\n\\n\\ndef _get_google_jobs(**kwargs: Any) -> BaseTool:\\n    return GoogleJobsQueryRun(api_wrapper=GoogleJobsAPIWrapper(**kwargs))\\n\\n\\ndef _get_google_lens(**kwargs: Any) -> BaseTool:\\n    return GoogleLensQueryRun(api_wrapper=GoogleLensAPIWrapper(**kwargs))',\n",
              "  '_warn_on_import(name, replacement=\"langchain.chains.QAWithSourcesChain\")\\n\\n        return QAWithSourcesChain\\n    elif name == \"VectorDBQA\":\\n        from langchain.chains import VectorDBQA\\n\\n        _warn_on_import(name, replacement=\"langchain.chains.VectorDBQA\")\\n\\n        return VectorDBQA\\n    elif name == \"VectorDBQAWithSourcesChain\":\\n        from langchain.chains import VectorDBQAWithSourcesChain\\n\\n        _warn_on_import(name, replacement=\"langchain.chains.VectorDBQAWithSourcesChain\")\\n\\n        return VectorDBQAWithSourcesChain\\n    elif name == \"InMemoryDocstore\":\\n        from langchain.docstore import InMemoryDocstore\\n\\n        _warn_on_import(name, replacement=\"langchain.docstore.InMemoryDocstore\")\\n\\n        return InMemoryDocstore\\n    elif name == \"Wikipedia\":\\n        from langchain.docstore import Wikipedia\\n\\n        _warn_on_import(name, replacement=\"langchain.docstore.Wikipedia\")\\n\\n        return Wikipedia\\n    elif name == \"Anthropic\":\\n        from langchain.llms import Anthropic\\n\\n        _warn_on_import(name, replacement=\"langchain.llms.Anthropic\")\\n\\n        return Anthropic\\n    elif name == \"Banana\":\\n        from langchain.llms import Banana\\n\\n        _warn_on_import(name, replacement=\"langchain.llms.Banana\")\\n\\n        return Banana\\n    elif name == \"CerebriumAI\":\\n        from langchain.llms import CerebriumAI\\n\\n        _warn_on_import(name, replacement=\"langchain.llms.CerebriumAI\")\\n\\n        return CerebriumAI\\n    elif name == \"Cohere\":\\n        from langchain.llms import Cohere\\n\\n        _warn_on_import(name, replacement=\"langchain.llms.Cohere\")\\n\\n        return Cohere\\n    elif name == \"ForefrontAI\":\\n        from langchain.llms import ForefrontAI\\n\\n        _warn_on_import(name, replacement=\"langchain.llms.ForefrontAI\")\\n\\n        return ForefrontAI\\n    elif name == \"GooseAI\":\\n        from langchain.llms import GooseAI\\n\\n        _warn_on_import(name, replacement=\"langchain.llms.GooseAI\")',\n",
              "  'def _get_python_repl() -> BaseTool:\\n    raise ImportError(\\n        \"This tool has been moved to langchain experiment. \"\\n        \"This tool has access to a python REPL. \"\\n        \"For best practices make sure to sandbox this tool. \"\\n        \"Read https://github.com/langchain-ai/langchain/blob/master/SECURITY.md \"\\n        \"To keep using this code as is, install langchain experimental and \"\\n        \"update relevant imports replacing \\'langchain\\' with \\'langchain_experimental\\'\"\\n    )\\n\\n\\ndef _get_tools_requests_get() -> BaseTool:\\n    return RequestsGetTool(requests_wrapper=TextRequestsWrapper())\\n\\n\\ndef _get_tools_requests_post() -> BaseTool:\\n    return RequestsPostTool(requests_wrapper=TextRequestsWrapper())\\n\\n\\ndef _get_tools_requests_patch() -> BaseTool:\\n    return RequestsPatchTool(requests_wrapper=TextRequestsWrapper())\\n\\n\\ndef _get_tools_requests_put() -> BaseTool:\\n    return RequestsPutTool(requests_wrapper=TextRequestsWrapper())\\n\\n\\ndef _get_tools_requests_delete() -> BaseTool:\\n    return RequestsDeleteTool(requests_wrapper=TextRequestsWrapper())\\n\\n\\ndef _get_terminal() -> BaseTool:\\n    return ShellTool()\\n\\n\\ndef _get_sleep() -> BaseTool:\\n    return SleepTool()\\n\\n\\n_BASE_TOOLS: Dict[str, Callable[[], BaseTool]] = {\\n    \"requests\": _get_tools_requests_get,  # preserved for backwards compatibility\\n    \"requests_get\": _get_tools_requests_get,\\n    \"requests_post\": _get_tools_requests_post,\\n    \"requests_patch\": _get_tools_requests_patch,\\n    \"requests_put\": _get_tools_requests_put,\\n    \"requests_delete\": _get_tools_requests_delete,\\n    \"terminal\": _get_terminal,\\n    \"sleep\": _get_sleep,\\n}\\n\\n\\ndef _get_llm_math(llm: BaseLanguageModel) -> BaseTool:\\n    return Tool(\\n        name=\"Calculator\",\\n        description=\"Useful for when you need to answer questions about math.\",\\n        func=LLMMathChain.from_llm(llm=llm).run,\\n        coroutine=LLMMathChain.from_llm(llm=llm).arun,\\n    )',\n",
              "  'from langchain.utilities.metaphor_search import MetaphorSearchAPIWrapper\\nfrom langchain.utilities.awslambda import LambdaWrapper\\nfrom langchain.utilities.graphql import GraphQLAPIWrapper\\nfrom langchain.utilities.searchapi import SearchApiAPIWrapper\\nfrom langchain.utilities.searx_search import SearxSearchWrapper\\nfrom langchain.utilities.serpapi import SerpAPIWrapper\\nfrom langchain.utilities.stackexchange import StackExchangeAPIWrapper\\nfrom langchain.utilities.twilio import TwilioAPIWrapper\\nfrom langchain.utilities.merriam_webster import MerriamWebsterAPIWrapper\\nfrom langchain.utilities.wikipedia import WikipediaAPIWrapper\\nfrom langchain.utilities.wolfram_alpha import WolframAlphaAPIWrapper\\nfrom langchain.utilities.openweathermap import OpenWeatherMapAPIWrapper\\nfrom langchain.utilities.dataforseo_api_search import DataForSeoAPIWrapper\\nfrom langchain.utilities.reddit_search import RedditSearchAPIWrapper',\n",
              "  'RequestsPostTool,\\n    RequestsPutTool,\\n)\\nfrom langchain.tools.eleven_labs.text2speech import ElevenLabsText2SpeechTool\\nfrom langchain.tools.scenexplain.tool import SceneXplainTool\\nfrom langchain.tools.searx_search.tool import SearxSearchResults, SearxSearchRun\\nfrom langchain.tools.shell.tool import ShellTool\\nfrom langchain.tools.sleep.tool import SleepTool\\nfrom langchain.tools.stackexchange.tool import StackExchangeTool\\nfrom langchain.tools.merriam_webster.tool import MerriamWebsterQueryRun\\nfrom langchain.tools.wikipedia.tool import WikipediaQueryRun\\nfrom langchain.tools.wolfram_alpha.tool import WolframAlphaQueryRun\\nfrom langchain.tools.openweathermap.tool import OpenWeatherMapQueryRun\\nfrom langchain.tools.dataforseo_api_search import DataForSeoAPISearchRun\\nfrom langchain.tools.dataforseo_api_search import DataForSeoAPISearchResults\\nfrom langchain.tools.memorize.tool import Memorize\\nfrom langchain.tools.reddit_search.tool import RedditSearchRun\\nfrom langchain.utilities.arxiv import ArxivAPIWrapper\\nfrom langchain.utilities.golden_query import GoldenQueryAPIWrapper\\nfrom langchain.utilities.pubmed import PubMedAPIWrapper\\nfrom langchain.utilities.bing_search import BingSearchAPIWrapper\\nfrom langchain.utilities.duckduckgo_search import DuckDuckGoSearchAPIWrapper\\nfrom langchain.utilities.google_lens import GoogleLensAPIWrapper\\nfrom langchain.utilities.google_jobs import GoogleJobsAPIWrapper\\nfrom langchain.utilities.google_search import GoogleSearchAPIWrapper\\nfrom langchain.utilities.google_serper import GoogleSerperAPIWrapper\\nfrom langchain.utilities.google_scholar import GoogleScholarAPIWrapper\\nfrom langchain.utilities.google_finance import GoogleFinanceAPIWrapper\\nfrom langchain.utilities.google_trends import GoogleTrendsAPIWrapper\\nfrom langchain.utilities.metaphor_search import MetaphorSearchAPIWrapper\\nfrom langchain.utilities.awslambda import LambdaWrapper\\nfrom langchain.utilities.graphql import GraphQLAPIWrapper',\n",
              "  'from langchain.agents.tools import Tool\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain.callbacks.base import BaseCallbackManager\\nfrom langchain.callbacks.manager import Callbacks\\nfrom langchain.chains.api import news_docs, open_meteo_docs, podcast_docs, tmdb_docs\\nfrom langchain.chains.api.base import APIChain\\nfrom langchain.chains.llm_math.base import LLMMathChain\\nfrom langchain.utilities.dalle_image_generator import DallEAPIWrapper\\nfrom langchain.utilities.requests import TextRequestsWrapper\\nfrom langchain.tools.arxiv.tool import ArxivQueryRun\\nfrom langchain.tools.golden_query.tool import GoldenQueryRun\\nfrom langchain.tools.pubmed.tool import PubmedQueryRun\\nfrom langchain_core.tools import BaseTool\\nfrom langchain.tools.bing_search.tool import BingSearchRun\\nfrom langchain.tools.ddg_search.tool import DuckDuckGoSearchRun\\nfrom langchain.tools.google_cloud.texttospeech import GoogleCloudTextToSpeechTool\\nfrom langchain.tools.google_lens.tool import GoogleLensQueryRun\\nfrom langchain.tools.google_search.tool import GoogleSearchResults, GoogleSearchRun\\nfrom langchain.tools.google_scholar.tool import GoogleScholarQueryRun\\nfrom langchain.tools.google_finance.tool import GoogleFinanceQueryRun\\nfrom langchain.tools.google_trends.tool import GoogleTrendsQueryRun\\nfrom langchain.tools.metaphor_search.tool import MetaphorSearchResults\\nfrom langchain.tools.google_jobs.tool import GoogleJobsQueryRun\\nfrom langchain.tools.google_serper.tool import GoogleSerperResults, GoogleSerperRun\\nfrom langchain.tools.searchapi.tool import SearchAPIResults, SearchAPIRun\\nfrom langchain.tools.graphql.tool import BaseGraphQLTool\\nfrom langchain.tools.human.tool import HumanInputRun\\nfrom langchain.tools.requests.tool import (\\n    RequestsDeleteTool,\\n    RequestsGetTool,\\n    RequestsPatchTool,\\n    RequestsPostTool,\\n    RequestsPutTool,\\n)\\nfrom langchain.tools.eleven_labs.text2speech import ElevenLabsText2SpeechTool\\nfrom langchain.tools.scenexplain.tool import SceneXplainTool',\n",
              "  '# flake8: noqa\\n\"\"\"Tools provide access to various resources and services.\\n\\nLangChain has a large ecosystem of integrations with various external resources\\nlike local and remote file systems, APIs and databases.\\n\\nThese integrations allow developers to create versatile applications that combine the\\npower of LLMs with the ability to access, interact with and manipulate external\\nresources.\\n\\nWhen developing an application, developers should inspect the capabilities and\\npermissions of the tools that underlie the given agent toolkit, and determine\\nwhether permissions of the given toolkit are appropriate for the application.\\n\\nSee [Security](https://python.langchain.com/docs/security) for more information.\\n\"\"\"\\nimport warnings\\nfrom typing import Any, Dict, List, Optional, Callable, Tuple\\nfrom mypy_extensions import Arg, KwArg',\n",
              "  '\"\"\"Keep here for backwards compatibility.\"\"\"\\nfrom langchain.chains.example_generator import generate_example\\n\\n__all__ = [\"generate_example\"]',\n",
              "  'return _llm_cache\\n    else:\\n        raise AttributeError(f\"Could not find: {name}\")\\n\\n\\n__all__ = [\\n    \"LLMChain\",\\n    \"LLMCheckerChain\",\\n    \"LLMMathChain\",\\n    \"ArxivAPIWrapper\",\\n    \"GoldenQueryAPIWrapper\",\\n    \"SelfAskWithSearchChain\",\\n    \"SerpAPIWrapper\",\\n    \"SerpAPIChain\",\\n    \"SearxSearchWrapper\",\\n    \"GoogleSearchAPIWrapper\",\\n    \"GoogleSerperAPIWrapper\",\\n    \"WolframAlphaAPIWrapper\",\\n    \"WikipediaAPIWrapper\",\\n    \"Anthropic\",\\n    \"Banana\",\\n    \"CerebriumAI\",\\n    \"Cohere\",\\n    \"ForefrontAI\",\\n    \"GooseAI\",\\n    \"Modal\",\\n    \"OpenAI\",\\n    \"Petals\",\\n    \"PipelineAI\",\\n    \"StochasticAI\",\\n    \"Writer\",\\n    \"BasePromptTemplate\",\\n    \"Prompt\",\\n    \"FewShotPromptTemplate\",\\n    \"PromptTemplate\",\\n    \"ReActChain\",\\n    \"Wikipedia\",\\n    \"HuggingFaceHub\",\\n    \"SagemakerEndpoint\",\\n    \"HuggingFacePipeline\",\\n    \"SQLDatabase\",\\n    \"PowerBIDataset\",\\n    \"FAISS\",\\n    \"MRKLChain\",\\n    \"VectorDBQA\",\\n    \"ElasticVectorSearch\",\\n    \"InMemoryDocstore\",\\n    \"ConversationChain\",\\n    \"VectorDBQAWithSourcesChain\",\\n    \"QAWithSourcesChain\",\\n    \"LlamaCpp\",\\n    \"HuggingFaceTextGenInference\",\\n]',\n",
              "  'return ElasticVectorSearch\\n    # For backwards compatibility\\n    elif name == \"SerpAPIChain\" or name == \"SerpAPIWrapper\":\\n        from langchain.utilities import SerpAPIWrapper\\n\\n        _warn_on_import(name, replacement=\"langchain.utilities.SerpAPIWrapper\")\\n\\n        return SerpAPIWrapper\\n    elif name == \"verbose\":\\n        from langchain.globals import _verbose\\n\\n        _warn_on_import(\\n            name,\\n            replacement=(\\n                \"langchain.globals.set_verbose() / langchain.globals.get_verbose()\"\\n            ),\\n        )\\n\\n        return _verbose\\n    elif name == \"debug\":\\n        from langchain.globals import _debug\\n\\n        _warn_on_import(\\n            name,\\n            replacement=(\\n                \"langchain.globals.set_debug() / langchain.globals.get_debug()\"\\n            ),\\n        )\\n\\n        return _debug\\n    elif name == \"llm_cache\":\\n        from langchain.globals import _llm_cache\\n\\n        _warn_on_import(\\n            name,\\n            replacement=(\\n                \"langchain.globals.set_llm_cache() / langchain.globals.get_llm_cache()\"\\n            ),\\n        )\\n\\n        return _llm_cache\\n    else:\\n        raise AttributeError(f\"Could not find: {name}\")',\n",
              "  '# flake8: noqa\\n\"\"\"Tools provide access to various resources and services.\\n\\nLangChain has a large ecosystem of integrations with various external resources\\nlike local and remote file systems, APIs and databases.\\n\\nThese integrations allow developers to create versatile applications that combine the\\npower of LLMs with the ability to access, interact with and manipulate external\\nresources.\\n\\nWhen developing an application, developers should inspect the capabilities and\\npermissions of the tools that underlie the given agent toolkit, and determine\\nwhether permissions of the given toolkit are appropriate for the application.\\n\\nSee [Security](https://python.langchain.com/docs/security) for more information.\\n\"\"\"\\nimport warnings\\nfrom typing import Any, Dict, List, Optional, Callable, Tuple\\nfrom mypy_extensions import Arg, KwArg',\n",
              "  'def _get_eleven_labs_text2speech(**kwargs: Any) -> BaseTool:\\n    return ElevenLabsText2SpeechTool(**kwargs)\\n\\n\\ndef _get_memorize(llm: BaseLanguageModel, **kwargs: Any) -> BaseTool:\\n    return Memorize(llm=llm)\\n\\n\\ndef _get_google_cloud_texttospeech(**kwargs: Any) -> BaseTool:\\n    return GoogleCloudTextToSpeechTool(**kwargs)',\n",
              "  'def _get_twilio(**kwargs: Any) -> BaseTool:\\n    return Tool(\\n        name=\"Text-Message\",\\n        description=\"Useful for when you need to send a text message to a provided phone number.\",\\n        func=TwilioAPIWrapper(**kwargs).run,\\n    )\\n\\n\\ndef _get_searx_search(**kwargs: Any) -> BaseTool:\\n    return SearxSearchRun(wrapper=SearxSearchWrapper(**kwargs))\\n\\n\\ndef _get_searx_search_results_json(**kwargs: Any) -> BaseTool:\\n    wrapper_kwargs = {k: v for k, v in kwargs.items() if k != \"num_results\"}\\n    return SearxSearchResults(wrapper=SearxSearchWrapper(**wrapper_kwargs), **kwargs)\\n\\n\\ndef _get_bing_search(**kwargs: Any) -> BaseTool:\\n    return BingSearchRun(api_wrapper=BingSearchAPIWrapper(**kwargs))\\n\\n\\ndef _get_metaphor_search(**kwargs: Any) -> BaseTool:\\n    return MetaphorSearchResults(api_wrapper=MetaphorSearchAPIWrapper(**kwargs))\\n\\n\\ndef _get_ddg_search(**kwargs: Any) -> BaseTool:\\n    return DuckDuckGoSearchRun(api_wrapper=DuckDuckGoSearchAPIWrapper(**kwargs))\\n\\n\\ndef _get_human_tool(**kwargs: Any) -> BaseTool:\\n    return HumanInputRun(**kwargs)\\n\\n\\ndef _get_scenexplain(**kwargs: Any) -> BaseTool:\\n    return SceneXplainTool(**kwargs)\\n\\n\\ndef _get_graphql_tool(**kwargs: Any) -> BaseTool:\\n    graphql_endpoint = kwargs[\"graphql_endpoint\"]\\n    wrapper = GraphQLAPIWrapper(graphql_endpoint=graphql_endpoint)\\n    return BaseGraphQLTool(graphql_wrapper=wrapper)\\n\\n\\ndef _get_openweathermap(**kwargs: Any) -> BaseTool:\\n    return OpenWeatherMapQueryRun(api_wrapper=OpenWeatherMapAPIWrapper(**kwargs))\\n\\n\\ndef _get_dataforseo_api_search(**kwargs: Any) -> BaseTool:\\n    return DataForSeoAPISearchRun(api_wrapper=DataForSeoAPIWrapper(**kwargs))\\n\\n\\ndef _get_dataforseo_api_search_json(**kwargs: Any) -> BaseTool:\\n    return DataForSeoAPISearchResults(api_wrapper=DataForSeoAPIWrapper(**kwargs))\\n\\n\\ndef _get_eleven_labs_text2speech(**kwargs: Any) -> BaseTool:\\n    return ElevenLabsText2SpeechTool(**kwargs)',\n",
              "  'def _get_google_lens(**kwargs: Any) -> BaseTool:\\n    return GoogleLensQueryRun(api_wrapper=GoogleLensAPIWrapper(**kwargs))\\n\\n\\ndef _get_google_serper(**kwargs: Any) -> BaseTool:\\n    return GoogleSerperRun(api_wrapper=GoogleSerperAPIWrapper(**kwargs))\\n\\n\\ndef _get_google_scholar(**kwargs: Any) -> BaseTool:\\n    return GoogleScholarQueryRun(api_wrapper=GoogleScholarAPIWrapper(**kwargs))\\n\\n\\ndef _get_google_finance(**kwargs: Any) -> BaseTool:\\n    return GoogleFinanceQueryRun(api_wrapper=GoogleFinanceAPIWrapper(**kwargs))\\n\\n\\ndef _get_google_trends(**kwargs: Any) -> BaseTool:\\n    return GoogleTrendsQueryRun(api_wrapper=GoogleTrendsAPIWrapper(**kwargs))\\n\\n\\ndef _get_google_serper_results_json(**kwargs: Any) -> BaseTool:\\n    return GoogleSerperResults(api_wrapper=GoogleSerperAPIWrapper(**kwargs))\\n\\n\\ndef _get_google_search_results_json(**kwargs: Any) -> BaseTool:\\n    return GoogleSearchResults(api_wrapper=GoogleSearchAPIWrapper(**kwargs))\\n\\n\\ndef _get_searchapi(**kwargs: Any) -> BaseTool:\\n    return SearchAPIRun(api_wrapper=SearchApiAPIWrapper(**kwargs))\\n\\n\\ndef _get_searchapi_results_json(**kwargs: Any) -> BaseTool:\\n    return SearchAPIResults(api_wrapper=SearchApiAPIWrapper(**kwargs))\\n\\n\\ndef _get_serpapi(**kwargs: Any) -> BaseTool:\\n    return Tool(\\n        name=\"Search\",\\n        description=\"A search engine. Useful for when you need to answer questions about current events. Input should be a search query.\",\\n        func=SerpAPIWrapper(**kwargs).run,\\n        coroutine=SerpAPIWrapper(**kwargs).arun,\\n    )\\n\\n\\ndef _get_stackexchange(**kwargs: Any) -> BaseTool:\\n    return StackExchangeTool(api_wrapper=StackExchangeAPIWrapper(**kwargs))\\n\\n\\ndef _get_dalle_image_generator(**kwargs: Any) -> Tool:\\n    return Tool(\\n        \"Dall-E-Image-Generator\",\\n        DallEAPIWrapper(**kwargs).run,\\n        \"A wrapper around OpenAI DALL-E API. Useful for when you need to generate images from a text description. Input should be an image description.\",\\n    )',\n",
              "  'def _get_podcast_api(llm: BaseLanguageModel, **kwargs: Any) -> BaseTool:\\n    listen_api_key = kwargs[\"listen_api_key\"]\\n    chain = APIChain.from_llm_and_api_docs(\\n        llm,\\n        podcast_docs.PODCAST_DOCS,\\n        headers={\"X-ListenAPI-Key\": listen_api_key},\\n        limit_to_domains=[\"https://listen-api.listennotes.com/\"],\\n    )\\n    return Tool(\\n        name=\"Podcast-API\",\\n        description=\"Use the Listen Notes Podcast API to search all podcasts or episodes. The input should be a question in natural language that this API can answer.\",\\n        func=chain.run,\\n    )\\n\\n\\ndef _get_lambda_api(**kwargs: Any) -> BaseTool:\\n    return Tool(\\n        name=kwargs[\"awslambda_tool_name\"],\\n        description=kwargs[\"awslambda_tool_description\"],\\n        func=LambdaWrapper(**kwargs).run,\\n    )\\n\\n\\ndef _get_wolfram_alpha(**kwargs: Any) -> BaseTool:\\n    return WolframAlphaQueryRun(api_wrapper=WolframAlphaAPIWrapper(**kwargs))\\n\\n\\ndef _get_google_search(**kwargs: Any) -> BaseTool:\\n    return GoogleSearchRun(api_wrapper=GoogleSearchAPIWrapper(**kwargs))\\n\\n\\ndef _get_merriam_webster(**kwargs: Any) -> BaseTool:\\n    return MerriamWebsterQueryRun(api_wrapper=MerriamWebsterAPIWrapper(**kwargs))\\n\\n\\ndef _get_wikipedia(**kwargs: Any) -> BaseTool:\\n    return WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(**kwargs))\\n\\n\\ndef _get_arxiv(**kwargs: Any) -> BaseTool:\\n    return ArxivQueryRun(api_wrapper=ArxivAPIWrapper(**kwargs))\\n\\n\\ndef _get_golden_query(**kwargs: Any) -> BaseTool:\\n    return GoldenQueryRun(api_wrapper=GoldenQueryAPIWrapper(**kwargs))\\n\\n\\ndef _get_pubmed(**kwargs: Any) -> BaseTool:\\n    return PubmedQueryRun(api_wrapper=PubMedAPIWrapper(**kwargs))\\n\\n\\ndef _get_google_jobs(**kwargs: Any) -> BaseTool:\\n    return GoogleJobsQueryRun(api_wrapper=GoogleJobsAPIWrapper(**kwargs))\\n\\n\\ndef _get_google_lens(**kwargs: Any) -> BaseTool:\\n    return GoogleLensQueryRun(api_wrapper=GoogleLensAPIWrapper(**kwargs))',\n",
              "  'def _get_open_meteo_api(llm: BaseLanguageModel) -> BaseTool:\\n    chain = APIChain.from_llm_and_api_docs(\\n        llm,\\n        open_meteo_docs.OPEN_METEO_DOCS,\\n        limit_to_domains=[\"https://api.open-meteo.com/\"],\\n    )\\n    return Tool(\\n        name=\"Open-Meteo-API\",\\n        description=\"Useful for when you want to get weather information from the OpenMeteo API. The input should be a question in natural language that this API can answer.\",\\n        func=chain.run,\\n    )\\n\\n\\n_LLM_TOOLS: Dict[str, Callable[[BaseLanguageModel], BaseTool]] = {\\n    \"llm-math\": _get_llm_math,\\n    \"open-meteo-api\": _get_open_meteo_api,\\n}\\n\\n\\ndef _get_news_api(llm: BaseLanguageModel, **kwargs: Any) -> BaseTool:\\n    news_api_key = kwargs[\"news_api_key\"]\\n    chain = APIChain.from_llm_and_api_docs(\\n        llm,\\n        news_docs.NEWS_DOCS,\\n        headers={\"X-Api-Key\": news_api_key},\\n        limit_to_domains=[\"https://newsapi.org/\"],\\n    )\\n    return Tool(\\n        name=\"News-API\",\\n        description=\"Use this when you want to get information about the top headlines of current news stories. The input should be a question in natural language that this API can answer.\",\\n        func=chain.run,\\n    )\\n\\n\\ndef _get_tmdb_api(llm: BaseLanguageModel, **kwargs: Any) -> BaseTool:\\n    tmdb_bearer_token = kwargs[\"tmdb_bearer_token\"]\\n    chain = APIChain.from_llm_and_api_docs(\\n        llm,\\n        tmdb_docs.TMDB_DOCS,\\n        headers={\"Authorization\": f\"Bearer {tmdb_bearer_token}\"},\\n        limit_to_domains=[\"https://api.themoviedb.org/\"],\\n    )\\n    return Tool(\\n        name=\"TMDB-API\",\\n        description=\"Useful for when you want to get information from The Movie Database. The input should be a question in natural language that this API can answer.\",\\n        func=chain.run,\\n    )',\n",
              "  'def _get_python_repl() -> BaseTool:\\n    raise ImportError(\\n        \"This tool has been moved to langchain experiment. \"\\n        \"This tool has access to a python REPL. \"\\n        \"For best practices make sure to sandbox this tool. \"\\n        \"Read https://github.com/langchain-ai/langchain/blob/master/SECURITY.md \"\\n        \"To keep using this code as is, install langchain experimental and \"\\n        \"update relevant imports replacing \\'langchain\\' with \\'langchain_experimental\\'\"\\n    )\\n\\n\\ndef _get_tools_requests_get() -> BaseTool:\\n    return RequestsGetTool(requests_wrapper=TextRequestsWrapper())\\n\\n\\ndef _get_tools_requests_post() -> BaseTool:\\n    return RequestsPostTool(requests_wrapper=TextRequestsWrapper())\\n\\n\\ndef _get_tools_requests_patch() -> BaseTool:\\n    return RequestsPatchTool(requests_wrapper=TextRequestsWrapper())\\n\\n\\ndef _get_tools_requests_put() -> BaseTool:\\n    return RequestsPutTool(requests_wrapper=TextRequestsWrapper())\\n\\n\\ndef _get_tools_requests_delete() -> BaseTool:\\n    return RequestsDeleteTool(requests_wrapper=TextRequestsWrapper())\\n\\n\\ndef _get_terminal() -> BaseTool:\\n    return ShellTool()\\n\\n\\ndef _get_sleep() -> BaseTool:\\n    return SleepTool()\\n\\n\\n_BASE_TOOLS: Dict[str, Callable[[], BaseTool]] = {\\n    \"requests\": _get_tools_requests_get,  # preserved for backwards compatibility\\n    \"requests_get\": _get_tools_requests_get,\\n    \"requests_post\": _get_tools_requests_post,\\n    \"requests_patch\": _get_tools_requests_patch,\\n    \"requests_put\": _get_tools_requests_put,\\n    \"requests_delete\": _get_tools_requests_delete,\\n    \"terminal\": _get_terminal,\\n    \"sleep\": _get_sleep,\\n}\\n\\n\\ndef _get_llm_math(llm: BaseLanguageModel) -> BaseTool:\\n    return Tool(\\n        name=\"Calculator\",\\n        description=\"Useful for when you need to answer questions about math.\",\\n        func=LLMMathChain.from_llm(llm=llm).run,\\n        coroutine=LLMMathChain.from_llm(llm=llm).arun,\\n    )',\n",
              "  'from langchain.utilities.metaphor_search import MetaphorSearchAPIWrapper\\nfrom langchain.utilities.awslambda import LambdaWrapper\\nfrom langchain.utilities.graphql import GraphQLAPIWrapper\\nfrom langchain.utilities.searchapi import SearchApiAPIWrapper\\nfrom langchain.utilities.searx_search import SearxSearchWrapper\\nfrom langchain.utilities.serpapi import SerpAPIWrapper\\nfrom langchain.utilities.stackexchange import StackExchangeAPIWrapper\\nfrom langchain.utilities.twilio import TwilioAPIWrapper\\nfrom langchain.utilities.merriam_webster import MerriamWebsterAPIWrapper\\nfrom langchain.utilities.wikipedia import WikipediaAPIWrapper\\nfrom langchain.utilities.wolfram_alpha import WolframAlphaAPIWrapper\\nfrom langchain.utilities.openweathermap import OpenWeatherMapAPIWrapper\\nfrom langchain.utilities.dataforseo_api_search import DataForSeoAPIWrapper\\nfrom langchain.utilities.reddit_search import RedditSearchAPIWrapper',\n",
              "  'RequestsPostTool,\\n    RequestsPutTool,\\n)\\nfrom langchain.tools.eleven_labs.text2speech import ElevenLabsText2SpeechTool\\nfrom langchain.tools.scenexplain.tool import SceneXplainTool\\nfrom langchain.tools.searx_search.tool import SearxSearchResults, SearxSearchRun\\nfrom langchain.tools.shell.tool import ShellTool\\nfrom langchain.tools.sleep.tool import SleepTool\\nfrom langchain.tools.stackexchange.tool import StackExchangeTool\\nfrom langchain.tools.merriam_webster.tool import MerriamWebsterQueryRun\\nfrom langchain.tools.wikipedia.tool import WikipediaQueryRun\\nfrom langchain.tools.wolfram_alpha.tool import WolframAlphaQueryRun\\nfrom langchain.tools.openweathermap.tool import OpenWeatherMapQueryRun\\nfrom langchain.tools.dataforseo_api_search import DataForSeoAPISearchRun\\nfrom langchain.tools.dataforseo_api_search import DataForSeoAPISearchResults\\nfrom langchain.tools.memorize.tool import Memorize\\nfrom langchain.tools.reddit_search.tool import RedditSearchRun\\nfrom langchain.utilities.arxiv import ArxivAPIWrapper\\nfrom langchain.utilities.golden_query import GoldenQueryAPIWrapper\\nfrom langchain.utilities.pubmed import PubMedAPIWrapper\\nfrom langchain.utilities.bing_search import BingSearchAPIWrapper\\nfrom langchain.utilities.duckduckgo_search import DuckDuckGoSearchAPIWrapper\\nfrom langchain.utilities.google_lens import GoogleLensAPIWrapper\\nfrom langchain.utilities.google_jobs import GoogleJobsAPIWrapper\\nfrom langchain.utilities.google_search import GoogleSearchAPIWrapper\\nfrom langchain.utilities.google_serper import GoogleSerperAPIWrapper\\nfrom langchain.utilities.google_scholar import GoogleScholarAPIWrapper\\nfrom langchain.utilities.google_finance import GoogleFinanceAPIWrapper\\nfrom langchain.utilities.google_trends import GoogleTrendsAPIWrapper\\nfrom langchain.utilities.metaphor_search import MetaphorSearchAPIWrapper\\nfrom langchain.utilities.awslambda import LambdaWrapper\\nfrom langchain.utilities.graphql import GraphQLAPIWrapper',\n",
              "  'from langchain.agents.tools import Tool\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain.callbacks.base import BaseCallbackManager\\nfrom langchain.callbacks.manager import Callbacks\\nfrom langchain.chains.api import news_docs, open_meteo_docs, podcast_docs, tmdb_docs\\nfrom langchain.chains.api.base import APIChain\\nfrom langchain.chains.llm_math.base import LLMMathChain\\nfrom langchain.utilities.dalle_image_generator import DallEAPIWrapper\\nfrom langchain.utilities.requests import TextRequestsWrapper\\nfrom langchain.tools.arxiv.tool import ArxivQueryRun\\nfrom langchain.tools.golden_query.tool import GoldenQueryRun\\nfrom langchain.tools.pubmed.tool import PubmedQueryRun\\nfrom langchain_core.tools import BaseTool\\nfrom langchain.tools.bing_search.tool import BingSearchRun\\nfrom langchain.tools.ddg_search.tool import DuckDuckGoSearchRun\\nfrom langchain.tools.google_cloud.texttospeech import GoogleCloudTextToSpeechTool\\nfrom langchain.tools.google_lens.tool import GoogleLensQueryRun\\nfrom langchain.tools.google_search.tool import GoogleSearchResults, GoogleSearchRun\\nfrom langchain.tools.google_scholar.tool import GoogleScholarQueryRun\\nfrom langchain.tools.google_finance.tool import GoogleFinanceQueryRun\\nfrom langchain.tools.google_trends.tool import GoogleTrendsQueryRun\\nfrom langchain.tools.metaphor_search.tool import MetaphorSearchResults\\nfrom langchain.tools.google_jobs.tool import GoogleJobsQueryRun\\nfrom langchain.tools.google_serper.tool import GoogleSerperResults, GoogleSerperRun\\nfrom langchain.tools.searchapi.tool import SearchAPIResults, SearchAPIRun\\nfrom langchain.tools.graphql.tool import BaseGraphQLTool\\nfrom langchain.tools.human.tool import HumanInputRun\\nfrom langchain.tools.requests.tool import (\\n    RequestsDeleteTool,\\n    RequestsGetTool,\\n    RequestsPatchTool,\\n    RequestsPostTool,\\n    RequestsPutTool,\\n)\\nfrom langchain.tools.eleven_labs.text2speech import ElevenLabsText2SpeechTool\\nfrom langchain.tools.scenexplain.tool import SceneXplainTool',\n",
              "  'def _get_reddit_search(**kwargs: Any) -> BaseTool:\\n    return RedditSearchRun(api_wrapper=RedditSearchAPIWrapper(**kwargs))',\n",
              "  'class PythonCodeTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Python syntax.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a PythonCodeTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.PYTHON)\\n        super().__init__(separators=separators, **kwargs)\\n\\n\\nclass MarkdownTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Markdown-formatted headings.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a MarkdownTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.MARKDOWN)\\n        super().__init__(separators=separators, **kwargs)\\n\\n\\nclass LatexTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Latex-formatted layout elements.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a LatexTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.LATEX)\\n        super().__init__(separators=separators, **kwargs)',\n",
              "  'class NLTKTextSplitter(TextSplitter):\\n    \"\"\"Splitting text using NLTK package.\"\"\"\\n\\n    def __init__(\\n        self, separator: str = \"\\\\n\\\\n\", language: str = \"english\", **kwargs: Any\\n    ) -> None:\\n        \"\"\"Initialize the NLTK splitter.\"\"\"\\n        super().__init__(**kwargs)\\n        try:\\n            from nltk.tokenize import sent_tokenize\\n\\n            self._tokenizer = sent_tokenize\\n        except ImportError:\\n            raise ImportError(\\n                \"NLTK is not installed, please install it with `pip install nltk`.\"\\n            )\\n        self._separator = separator\\n        self._language = language\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        # First we naively split the large input into a bunch of smaller ones.\\n        splits = self._tokenizer(text, language=self._language)\\n        return self._merge_splits(splits, self._separator)\\n\\n\\nclass SpacyTextSplitter(TextSplitter):\\n    \"\"\"Splitting text using Spacy package.\\n\\n\\n    Per default, Spacy\\'s `en_core_web_sm` model is used and\\n    its default max_length is 1000000 (it is the length of maximum character\\n    this model takes which can be increased for large files). For a faster, but\\n    potentially less accurate splitting, you can use `pipeline=\\'sentencizer\\'`.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        separator: str = \"\\\\n\\\\n\",\\n        pipeline: str = \"en_core_web_sm\",\\n        max_length: int = 1_000_000,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Initialize the spacy text splitter.\"\"\"\\n        super().__init__(**kwargs)\\n        self._tokenizer = _make_spacy_pipeline_for_splitting(\\n            pipeline, max_length=max_length\\n        )\\n        self._separator = separator\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        splits = (s.text for s in self._tokenizer(text).sents)\\n        return self._merge_splits(splits, self._separator)\\n\\n\\n# For backwards compatibility',\n",
              "  'else:\\n            raise ValueError(\\n                f\"Language {language} is not supported! \"\\n                f\"Please choose from {list(Language)}\"\\n            )',\n",
              "  '\"\\\\nevent \",\\n                \"\\\\nmodifier \",\\n                \"\\\\nerror \",\\n                \"\\\\nstruct \",\\n                \"\\\\nenum \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo while \",\\n                \"\\\\nassembly \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.COBOL:\\n            return [\\n                # Split along divisions\\n                \"\\\\nIDENTIFICATION DIVISION.\",\\n                \"\\\\nENVIRONMENT DIVISION.\",\\n                \"\\\\nDATA DIVISION.\",\\n                \"\\\\nPROCEDURE DIVISION.\",\\n                # Split along sections within DATA DIVISION\\n                \"\\\\nWORKING-STORAGE SECTION.\",\\n                \"\\\\nLINKAGE SECTION.\",\\n                \"\\\\nFILE SECTION.\",\\n                # Split along sections within PROCEDURE DIVISION\\n                \"\\\\nINPUT-OUTPUT SECTION.\",\\n                # Split along paragraphs and common statements\\n                \"\\\\nOPEN \",\\n                \"\\\\nCLOSE \",\\n                \"\\\\nREAD \",\\n                \"\\\\nWRITE \",\\n                \"\\\\nIF \",\\n                \"\\\\nELSE \",\\n                \"\\\\nMOVE \",\\n                \"\\\\nPERFORM \",\\n                \"\\\\nUNTIL \",\\n                \"\\\\nVARYING \",\\n                \"\\\\nACCEPT \",\\n                \"\\\\nDISPLAY \",\\n                \"\\\\nSTOP RUN.\",\\n                # Split by the normal type of lines\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]',\n",
              "  '\"<nav\",\\n                # Head\\n                \"<head\",\\n                \"<style\",\\n                \"<script\",\\n                \"<meta\",\\n                \"<title\",\\n                \"\",\\n            ]\\n        elif language == Language.CSHARP:\\n            return [\\n                \"\\\\ninterface \",\\n                \"\\\\nenum \",\\n                \"\\\\nimplements \",\\n                \"\\\\ndelegate \",\\n                \"\\\\nevent \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nabstract \",\\n                # Split along method definitions\\n                \"\\\\npublic \",\\n                \"\\\\nprotected \",\\n                \"\\\\nprivate \",\\n                \"\\\\nstatic \",\\n                \"\\\\nreturn \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\ncontinue \",\\n                \"\\\\nfor \",\\n                \"\\\\nforeach \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\nbreak \",\\n                \"\\\\ncase \",\\n                \"\\\\nelse \",\\n                # Split by exceptions\\n                \"\\\\ntry \",\\n                \"\\\\nthrow \",\\n                \"\\\\nfinally \",\\n                \"\\\\ncatch \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SOL:\\n            return [\\n                # Split along compiler information definitions\\n                \"\\\\npragma \",\\n                \"\\\\nusing \",\\n                # Split along contract definitions\\n                \"\\\\ncontract \",\\n                \"\\\\ninterface \",\\n                \"\\\\nlibrary \",\\n                # Split along method definitions\\n                \"\\\\nconstructor \",\\n                \"\\\\ntype \",\\n                \"\\\\nfunction \",\\n                \"\\\\nevent \",\\n                \"\\\\nmodifier \",\\n                \"\\\\nerror \",\\n                \"\\\\nstruct \",\\n                \"\\\\nenum \",\\n                # Split along control flow statements',\n",
              "  '\"\\\\n\\\\\\\\*\\\\\\\\*\\\\\\\\*+\\\\n\",\\n                \"\\\\n---+\\\\n\",\\n                \"\\\\n___+\\\\n\",\\n                # Note that this splitter doesn\\'t handle horizontal lines defined\\n                # by *three or more* of ***, ---, or ___, but this is not handled\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.LATEX:\\n            return [\\n                # First, try to split along Latex sections\\n                \"\\\\n\\\\\\\\\\\\\\\\chapter{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\section{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\subsection{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\subsubsection{\",\\n                # Now split by environments\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{enumerate}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{itemize}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{description}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{list}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{quote}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{quotation}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{verse}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{verbatim}\",\\n                # Now split by math environments\\n                \"\\\\n\\\\\\\\\\\\begin{align}\",\\n                \"$$\",\\n                \"$\",\\n                # Now split by the normal type of lines\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.HTML:\\n            return [\\n                # First, try to split along HTML tags\\n                \"<body\",\\n                \"<div\",\\n                \"<p\",\\n                \"<br\",\\n                \"<li\",\\n                \"<h1\",\\n                \"<h2\",\\n                \"<h3\",\\n                \"<h4\",\\n                \"<h5\",\\n                \"<h6\",\\n                \"<span\",\\n                \"<table\",\\n                \"<tr\",\\n                \"<td\",\\n                \"<th\",\\n                \"<ul\",\\n                \"<ol\",\\n                \"<header\",\\n                \"<footer\",\\n                \"<nav\",\\n                # Head\\n                \"<head\",\\n                \"<style\",\\n                \"<script\",\\n                \"<meta\",\\n                \"<title\",',\n",
              "  '\" \",\\n                \"\",\\n            ]\\n        elif language == Language.SCALA:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nobject \",\\n                # Split along method definitions\\n                \"\\\\ndef \",\\n                \"\\\\nval \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nmatch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SWIFT:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunc \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nstruct \",\\n                \"\\\\nenum \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.MARKDOWN:\\n            return [\\n                # First, try to split along Markdown headings (starting with level 2)\\n                \"\\\\n#{1,6} \",\\n                # Note the alternative syntax for headings (below) is not handled here\\n                # Heading level 2\\n                # ---------------\\n                # End of code block\\n                \"```\\\\n\",\\n                # Horizontal lines\\n                \"\\\\n\\\\\\\\*\\\\\\\\*\\\\\\\\*+\\\\n\",\\n                \"\\\\n---+\\\\n\",\\n                \"\\\\n___+\\\\n\",\\n                # Note that this splitter doesn\\'t handle horizontal lines defined',\n",
              "  'return [\\n                # First, try to split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\ndef \",\\n                \"\\\\n\\\\tdef \",\\n                # Now split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.RST:\\n            return [\\n                # Split along section titles\\n                \"\\\\n=+\\\\n\",\\n                \"\\\\n-+\\\\n\",\\n                \"\\\\n\\\\\\\\*+\\\\n\",\\n                # Split along directive markers\\n                \"\\\\n\\\\n.. *\\\\n\\\\n\",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.RUBY:\\n            return [\\n                # Split along method definitions\\n                \"\\\\ndef \",\\n                \"\\\\nclass \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nunless \",\\n                \"\\\\nwhile \",\\n                \"\\\\nfor \",\\n                \"\\\\ndo \",\\n                \"\\\\nbegin \",\\n                \"\\\\nrescue \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.RUST:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfn \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nwhile \",\\n                \"\\\\nfor \",\\n                \"\\\\nloop \",\\n                \"\\\\nmatch \",\\n                \"\\\\nconst \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SCALA:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",',\n",
              "  'def __getattr__(name: str) -> Any:\\n    if name == \"MRKLChain\":\\n        from langchain.agents import MRKLChain\\n\\n        _warn_on_import(name, replacement=\"langchain.agents.MRKLChain\")\\n\\n        return MRKLChain\\n    elif name == \"ReActChain\":\\n        from langchain.agents import ReActChain\\n\\n        _warn_on_import(name, replacement=\"langchain.agents.ReActChain\")\\n\\n        return ReActChain\\n    elif name == \"SelfAskWithSearchChain\":\\n        from langchain.agents import SelfAskWithSearchChain\\n\\n        _warn_on_import(name, replacement=\"langchain.agents.SelfAskWithSearchChain\")\\n\\n        return SelfAskWithSearchChain\\n    elif name == \"ConversationChain\":\\n        from langchain.chains import ConversationChain\\n\\n        _warn_on_import(name, replacement=\"langchain.chains.ConversationChain\")\\n\\n        return ConversationChain\\n    elif name == \"LLMBashChain\":\\n        raise ImportError(\\n            \"This module has been moved to langchain-experimental. \"\\n            \"For more details: \"\\n            \"https://github.com/langchain-ai/langchain/discussions/11352.\"\\n            \"To access this code, install it with `pip install langchain-experimental`.\"\\n            \"`from langchain_experimental.llm_bash.base \"\\n            \"import LLMBashChain`\"\\n        )\\n\\n    elif name == \"LLMChain\":\\n        from langchain.chains import LLMChain\\n\\n        _warn_on_import(name, replacement=\"langchain.chains.LLMChain\")\\n\\n        return LLMChain\\n    elif name == \"LLMCheckerChain\":\\n        from langchain.chains import LLMCheckerChain\\n\\n        _warn_on_import(name, replacement=\"langchain.chains.LLMCheckerChain\")\\n\\n        return LLMCheckerChain\\n    elif name == \"LLMMathChain\":\\n        from langchain.chains import LLMMathChain\\n\\n        _warn_on_import(name, replacement=\"langchain.chains.LLMMathChain\")\\n\\n        return LLMMathChain\\n    elif name == \"QAWithSourcesChain\":\\n        from langchain.chains import QAWithSourcesChain\\n\\n        _warn_on_import(name, replacement=\"langchain.chains.QAWithSourcesChain\")',\n",
              "  'def __getattr__(name: str) -> Any:\\n    if name == \"MRKLChain\":\\n        from langchain.agents import MRKLChain\\n\\n        _warn_on_import(name, replacement=\"langchain.agents.MRKLChain\")\\n\\n        return MRKLChain\\n    elif name == \"ReActChain\":\\n        from langchain.agents import ReActChain\\n\\n        _warn_on_import(name, replacement=\"langchain.agents.ReActChain\")\\n\\n        return ReActChain\\n    elif name == \"SelfAskWithSearchChain\":\\n        from langchain.agents import SelfAskWithSearchChain\\n\\n        _warn_on_import(name, replacement=\"langchain.agents.SelfAskWithSearchChain\")\\n\\n        return SelfAskWithSearchChain\\n    elif name == \"ConversationChain\":\\n        from langchain.chains import ConversationChain\\n\\n        _warn_on_import(name, replacement=\"langchain.chains.ConversationChain\")\\n\\n        return ConversationChain\\n    elif name == \"LLMBashChain\":\\n        raise ImportError(\\n            \"This module has been moved to langchain-experimental. \"\\n            \"For more details: \"\\n            \"https://github.com/langchain-ai/langchain/discussions/11352.\"\\n            \"To access this code, install it with `pip install langchain-experimental`.\"\\n            \"`from langchain_experimental.llm_bash.base \"\\n            \"import LLMBashChain`\"\\n        )\\n\\n    elif name == \"LLMChain\":\\n        from langchain.chains import LLMChain\\n\\n        _warn_on_import(name, replacement=\"langchain.chains.LLMChain\")\\n\\n        return LLMChain\\n    elif name == \"LLMCheckerChain\":\\n        from langchain.chains import LLMCheckerChain\\n\\n        _warn_on_import(name, replacement=\"langchain.chains.LLMCheckerChain\")\\n\\n        return LLMCheckerChain\\n    elif name == \"LLMMathChain\":\\n        from langchain.chains import LLMMathChain\\n\\n        _warn_on_import(name, replacement=\"langchain.chains.LLMMathChain\")\\n\\n        return LLMMathChain\\n    elif name == \"QAWithSourcesChain\":\\n        from langchain.chains import QAWithSourcesChain\\n\\n        _warn_on_import(name, replacement=\"langchain.chains.QAWithSourcesChain\")',\n",
              "  '# ruff: noqa: E402\\n\"\"\"Main entrypoint into package.\"\"\"\\nimport warnings\\nfrom importlib import metadata\\nfrom typing import Any, Optional\\n\\nfrom langchain_core._api.deprecation import surface_langchain_deprecation_warnings\\n\\ntry:\\n    __version__ = metadata.version(__package__)\\nexcept metadata.PackageNotFoundError:\\n    # Case where package metadata is not available.\\n    __version__ = \"\"\\ndel metadata  # optional, avoids polluting the results of dir(__package__)\\n\\n\\ndef _is_interactive_env() -> bool:\\n    \"\"\"Determine if running within IPython or Jupyter.\"\"\"\\n    import sys\\n\\n    return hasattr(sys, \"ps2\")\\n\\n\\ndef _warn_on_import(name: str, replacement: Optional[str] = None) -> None:\\n    \"\"\"Warn on import of deprecated module.\"\"\"\\n    if _is_interactive_env():\\n        # No warnings for interactive environments.\\n        # This is done to avoid polluting the output of interactive environments\\n        # where users rely on auto-complete and may trigger this warning\\n        # even if they are not using any deprecated modules\\n        return\\n\\n    if replacement:\\n        warnings.warn(\\n            f\"Importing {name} from langchain root module is no longer supported. \"\\n            f\"Please use {replacement} instead.\"\\n        )\\n    else:\\n        warnings.warn(\\n            f\"Importing {name} from langchain root module is no longer supported.\"\\n        )\\n\\n\\n# Surfaces Deprecation and Pending Deprecation warnings from langchain.\\nsurface_langchain_deprecation_warnings()',\n",
              "  'return _llm_cache\\n    else:\\n        raise AttributeError(f\"Could not find: {name}\")\\n\\n\\n__all__ = [\\n    \"LLMChain\",\\n    \"LLMCheckerChain\",\\n    \"LLMMathChain\",\\n    \"ArxivAPIWrapper\",\\n    \"GoldenQueryAPIWrapper\",\\n    \"SelfAskWithSearchChain\",\\n    \"SerpAPIWrapper\",\\n    \"SerpAPIChain\",\\n    \"SearxSearchWrapper\",\\n    \"GoogleSearchAPIWrapper\",\\n    \"GoogleSerperAPIWrapper\",\\n    \"WolframAlphaAPIWrapper\",\\n    \"WikipediaAPIWrapper\",\\n    \"Anthropic\",\\n    \"Banana\",\\n    \"CerebriumAI\",\\n    \"Cohere\",\\n    \"ForefrontAI\",\\n    \"GooseAI\",\\n    \"Modal\",\\n    \"OpenAI\",\\n    \"Petals\",\\n    \"PipelineAI\",\\n    \"StochasticAI\",\\n    \"Writer\",\\n    \"BasePromptTemplate\",\\n    \"Prompt\",\\n    \"FewShotPromptTemplate\",\\n    \"PromptTemplate\",\\n    \"ReActChain\",\\n    \"Wikipedia\",\\n    \"HuggingFaceHub\",\\n    \"SagemakerEndpoint\",\\n    \"HuggingFacePipeline\",\\n    \"SQLDatabase\",\\n    \"PowerBIDataset\",\\n    \"FAISS\",\\n    \"MRKLChain\",\\n    \"VectorDBQA\",\\n    \"ElasticVectorSearch\",\\n    \"InMemoryDocstore\",\\n    \"ConversationChain\",\\n    \"VectorDBQAWithSourcesChain\",\\n    \"QAWithSourcesChain\",\\n    \"LlamaCpp\",\\n    \"HuggingFaceTextGenInference\",\\n]',\n",
              "  'return ElasticVectorSearch\\n    # For backwards compatibility\\n    elif name == \"SerpAPIChain\" or name == \"SerpAPIWrapper\":\\n        from langchain.utilities import SerpAPIWrapper\\n\\n        _warn_on_import(name, replacement=\"langchain.utilities.SerpAPIWrapper\")\\n\\n        return SerpAPIWrapper\\n    elif name == \"verbose\":\\n        from langchain.globals import _verbose\\n\\n        _warn_on_import(\\n            name,\\n            replacement=(\\n                \"langchain.globals.set_verbose() / langchain.globals.get_verbose()\"\\n            ),\\n        )\\n\\n        return _verbose\\n    elif name == \"debug\":\\n        from langchain.globals import _debug\\n\\n        _warn_on_import(\\n            name,\\n            replacement=(\\n                \"langchain.globals.set_debug() / langchain.globals.get_debug()\"\\n            ),\\n        )\\n\\n        return _debug\\n    elif name == \"llm_cache\":\\n        from langchain.globals import _llm_cache\\n\\n        _warn_on_import(\\n            name,\\n            replacement=(\\n                \"langchain.globals.set_llm_cache() / langchain.globals.get_llm_cache()\"\\n            ),\\n        )\\n\\n        return _llm_cache\\n    else:\\n        raise AttributeError(f\"Could not find: {name}\")',\n",
              "  '_warn_on_import(name, replacement=\"langchain.utilities.GoogleSearchAPIWrapper\")\\n\\n        return GoogleSearchAPIWrapper\\n    elif name == \"GoogleSerperAPIWrapper\":\\n        from langchain.utilities import GoogleSerperAPIWrapper\\n\\n        _warn_on_import(name, replacement=\"langchain.utilities.GoogleSerperAPIWrapper\")\\n\\n        return GoogleSerperAPIWrapper\\n    elif name == \"PowerBIDataset\":\\n        from langchain.utilities import PowerBIDataset\\n\\n        _warn_on_import(name, replacement=\"langchain.utilities.PowerBIDataset\")\\n\\n        return PowerBIDataset\\n    elif name == \"SearxSearchWrapper\":\\n        from langchain.utilities import SearxSearchWrapper\\n\\n        _warn_on_import(name, replacement=\"langchain.utilities.SearxSearchWrapper\")\\n\\n        return SearxSearchWrapper\\n    elif name == \"WikipediaAPIWrapper\":\\n        from langchain.utilities import WikipediaAPIWrapper\\n\\n        _warn_on_import(name, replacement=\"langchain.utilities.WikipediaAPIWrapper\")\\n\\n        return WikipediaAPIWrapper\\n    elif name == \"WolframAlphaAPIWrapper\":\\n        from langchain.utilities import WolframAlphaAPIWrapper\\n\\n        _warn_on_import(name, replacement=\"langchain.utilities.WolframAlphaAPIWrapper\")\\n\\n        return WolframAlphaAPIWrapper\\n    elif name == \"SQLDatabase\":\\n        from langchain.utilities import SQLDatabase\\n\\n        _warn_on_import(name, replacement=\"langchain.utilities.SQLDatabase\")\\n\\n        return SQLDatabase\\n    elif name == \"FAISS\":\\n        from langchain.vectorstores import FAISS\\n\\n        _warn_on_import(name, replacement=\"langchain.vectorstores.FAISS\")\\n\\n        return FAISS\\n    elif name == \"ElasticVectorSearch\":\\n        from langchain.vectorstores import ElasticVectorSearch\\n\\n        _warn_on_import(name, replacement=\"langchain.vectorstores.ElasticVectorSearch\")\\n\\n        return ElasticVectorSearch\\n    # For backwards compatibility\\n    elif name == \"SerpAPIChain\" or name == \"SerpAPIWrapper\":\\n        from langchain.utilities import SerpAPIWrapper',\n",
              "  'return StochasticAI\\n    elif name == \"Writer\":\\n        from langchain.llms import Writer\\n\\n        _warn_on_import(name, replacement=\"langchain.llms.Writer\")\\n\\n        return Writer\\n    elif name == \"HuggingFacePipeline\":\\n        from langchain.llms.huggingface_pipeline import HuggingFacePipeline\\n\\n        _warn_on_import(\\n            name, replacement=\"langchain.llms.huggingface_pipeline.HuggingFacePipeline\"\\n        )\\n\\n        return HuggingFacePipeline\\n    elif name == \"FewShotPromptTemplate\":\\n        from langchain_core.prompts import FewShotPromptTemplate\\n\\n        _warn_on_import(name, replacement=\"langchain.prompts.FewShotPromptTemplate\")\\n\\n        return FewShotPromptTemplate\\n    elif name == \"Prompt\":\\n        from langchain.prompts import Prompt\\n\\n        _warn_on_import(name, replacement=\"langchain.prompts.Prompt\")\\n\\n        return Prompt\\n    elif name == \"PromptTemplate\":\\n        from langchain_core.prompts import PromptTemplate\\n\\n        _warn_on_import(name, replacement=\"langchain.prompts.PromptTemplate\")\\n\\n        return PromptTemplate\\n    elif name == \"BasePromptTemplate\":\\n        from langchain_core.prompts import BasePromptTemplate\\n\\n        _warn_on_import(\\n            name, replacement=\"langchain.schema.prompt_template.BasePromptTemplate\"\\n        )\\n\\n        return BasePromptTemplate\\n    elif name == \"ArxivAPIWrapper\":\\n        from langchain.utilities import ArxivAPIWrapper\\n\\n        _warn_on_import(name, replacement=\"langchain.utilities.ArxivAPIWrapper\")\\n\\n        return ArxivAPIWrapper\\n    elif name == \"GoldenQueryAPIWrapper\":\\n        from langchain.utilities import GoldenQueryAPIWrapper\\n\\n        _warn_on_import(name, replacement=\"langchain.utilities.GoldenQueryAPIWrapper\")\\n\\n        return GoldenQueryAPIWrapper\\n    elif name == \"GoogleSearchAPIWrapper\":\\n        from langchain.utilities import GoogleSearchAPIWrapper\\n\\n        _warn_on_import(name, replacement=\"langchain.utilities.GoogleSearchAPIWrapper\")',\n",
              "  'return ForefrontAI\\n    elif name == \"GooseAI\":\\n        from langchain.llms import GooseAI\\n\\n        _warn_on_import(name, replacement=\"langchain.llms.GooseAI\")\\n\\n        return GooseAI\\n    elif name == \"HuggingFaceHub\":\\n        from langchain.llms import HuggingFaceHub\\n\\n        _warn_on_import(name, replacement=\"langchain.llms.HuggingFaceHub\")\\n\\n        return HuggingFaceHub\\n    elif name == \"HuggingFaceTextGenInference\":\\n        from langchain.llms import HuggingFaceTextGenInference\\n\\n        _warn_on_import(name, replacement=\"langchain.llms.HuggingFaceTextGenInference\")\\n\\n        return HuggingFaceTextGenInference\\n    elif name == \"LlamaCpp\":\\n        from langchain.llms import LlamaCpp\\n\\n        _warn_on_import(name, replacement=\"langchain.llms.LlamaCpp\")\\n\\n        return LlamaCpp\\n    elif name == \"Modal\":\\n        from langchain.llms import Modal\\n\\n        _warn_on_import(name, replacement=\"langchain.llms.Modal\")\\n\\n        return Modal\\n    elif name == \"OpenAI\":\\n        from langchain.llms import OpenAI\\n\\n        _warn_on_import(name, replacement=\"langchain.llms.OpenAI\")\\n\\n        return OpenAI\\n    elif name == \"Petals\":\\n        from langchain.llms import Petals\\n\\n        _warn_on_import(name, replacement=\"langchain.llms.Petals\")\\n\\n        return Petals\\n    elif name == \"PipelineAI\":\\n        from langchain.llms import PipelineAI\\n\\n        _warn_on_import(name, replacement=\"langchain.llms.PipelineAI\")\\n\\n        return PipelineAI\\n    elif name == \"SagemakerEndpoint\":\\n        from langchain.llms import SagemakerEndpoint\\n\\n        _warn_on_import(name, replacement=\"langchain.llms.SagemakerEndpoint\")\\n\\n        return SagemakerEndpoint\\n    elif name == \"StochasticAI\":\\n        from langchain.llms import StochasticAI\\n\\n        _warn_on_import(name, replacement=\"langchain.llms.StochasticAI\")\\n\\n        return StochasticAI\\n    elif name == \"Writer\":\\n        from langchain.llms import Writer\\n\\n        _warn_on_import(name, replacement=\"langchain.llms.Writer\")',\n",
              "  '_warn_on_import(name, replacement=\"langchain.chains.QAWithSourcesChain\")\\n\\n        return QAWithSourcesChain\\n    elif name == \"VectorDBQA\":\\n        from langchain.chains import VectorDBQA\\n\\n        _warn_on_import(name, replacement=\"langchain.chains.VectorDBQA\")\\n\\n        return VectorDBQA\\n    elif name == \"VectorDBQAWithSourcesChain\":\\n        from langchain.chains import VectorDBQAWithSourcesChain\\n\\n        _warn_on_import(name, replacement=\"langchain.chains.VectorDBQAWithSourcesChain\")\\n\\n        return VectorDBQAWithSourcesChain\\n    elif name == \"InMemoryDocstore\":\\n        from langchain.docstore import InMemoryDocstore\\n\\n        _warn_on_import(name, replacement=\"langchain.docstore.InMemoryDocstore\")\\n\\n        return InMemoryDocstore\\n    elif name == \"Wikipedia\":\\n        from langchain.docstore import Wikipedia\\n\\n        _warn_on_import(name, replacement=\"langchain.docstore.Wikipedia\")\\n\\n        return Wikipedia\\n    elif name == \"Anthropic\":\\n        from langchain.llms import Anthropic\\n\\n        _warn_on_import(name, replacement=\"langchain.llms.Anthropic\")\\n\\n        return Anthropic\\n    elif name == \"Banana\":\\n        from langchain.llms import Banana\\n\\n        _warn_on_import(name, replacement=\"langchain.llms.Banana\")\\n\\n        return Banana\\n    elif name == \"CerebriumAI\":\\n        from langchain.llms import CerebriumAI\\n\\n        _warn_on_import(name, replacement=\"langchain.llms.CerebriumAI\")\\n\\n        return CerebriumAI\\n    elif name == \"Cohere\":\\n        from langchain.llms import Cohere\\n\\n        _warn_on_import(name, replacement=\"langchain.llms.Cohere\")\\n\\n        return Cohere\\n    elif name == \"ForefrontAI\":\\n        from langchain.llms import ForefrontAI\\n\\n        _warn_on_import(name, replacement=\"langchain.llms.ForefrontAI\")\\n\\n        return ForefrontAI\\n    elif name == \"GooseAI\":\\n        from langchain.llms import GooseAI\\n\\n        _warn_on_import(name, replacement=\"langchain.llms.GooseAI\")',\n",
              "  'class Language(str, Enum):\\n    \"\"\"Enum of the programming languages.\"\"\"\\n\\n    CPP = \"cpp\"\\n    GO = \"go\"\\n    JAVA = \"java\"\\n    KOTLIN = \"kotlin\"\\n    JS = \"js\"\\n    TS = \"ts\"\\n    PHP = \"php\"\\n    PROTO = \"proto\"\\n    PYTHON = \"python\"\\n    RST = \"rst\"\\n    RUBY = \"ruby\"\\n    RUST = \"rust\"\\n    SCALA = \"scala\"\\n    SWIFT = \"swift\"\\n    MARKDOWN = \"markdown\"\\n    LATEX = \"latex\"\\n    HTML = \"html\"\\n    SOL = \"sol\"\\n    CSHARP = \"csharp\"\\n    COBOL = \"cobol\"',\n",
              "  '# ruff: noqa: E402\\n\"\"\"Main entrypoint into package.\"\"\"\\nimport warnings\\nfrom importlib import metadata\\nfrom typing import Any, Optional\\n\\nfrom langchain_core._api.deprecation import surface_langchain_deprecation_warnings\\n\\ntry:\\n    __version__ = metadata.version(__package__)\\nexcept metadata.PackageNotFoundError:\\n    # Case where package metadata is not available.\\n    __version__ = \"\"\\ndel metadata  # optional, avoids polluting the results of dir(__package__)\\n\\n\\ndef _is_interactive_env() -> bool:\\n    \"\"\"Determine if running within IPython or Jupyter.\"\"\"\\n    import sys\\n\\n    return hasattr(sys, \"ps2\")\\n\\n\\ndef _warn_on_import(name: str, replacement: Optional[str] = None) -> None:\\n    \"\"\"Warn on import of deprecated module.\"\"\"\\n    if _is_interactive_env():\\n        # No warnings for interactive environments.\\n        # This is done to avoid polluting the output of interactive environments\\n        # where users rely on auto-complete and may trigger this warning\\n        # even if they are not using any deprecated modules\\n        return\\n\\n    if replacement:\\n        warnings.warn(\\n            f\"Importing {name} from langchain root module is no longer supported. \"\\n            f\"Please use {replacement} instead.\"\\n        )\\n    else:\\n        warnings.warn(\\n            f\"Importing {name} from langchain root module is no longer supported.\"\\n        )\\n\\n\\n# Surfaces Deprecation and Pending Deprecation warnings from langchain.\\nsurface_langchain_deprecation_warnings()',\n",
              "  'def get_all_tool_names() -> List[str]:\\n    \"\"\"Get a list of all possible tool names.\"\"\"\\n    return (\\n        list(_BASE_TOOLS)\\n        + list(_EXTRA_OPTIONAL_TOOLS)\\n        + list(_EXTRA_LLM_TOOLS)\\n        + list(_LLM_TOOLS)\\n    )',\n",
              "  'tool = _get_tool_func(**sub_kwargs)\\n            tools.append(tool)\\n        else:\\n            raise ValueError(f\"Got unknown tool {name}\")\\n    if callbacks is not None:\\n        for tool in tools:\\n            tool.callbacks = callbacks\\n    return tools',\n",
              "  'Returns:\\n        List of tools.\\n    \"\"\"\\n    tools = []\\n    callbacks = _handle_callbacks(\\n        callback_manager=kwargs.get(\"callback_manager\"), callbacks=callbacks\\n    )\\n    # print(_BASE_TOOLS)\\n    # print(1)\\n    for name in tool_names:\\n        if name == \"requests\":\\n            warnings.warn(\\n                \"tool name `requests` is deprecated - \"\\n                \"please use `requests_all` or specify the requests method\"\\n            )\\n        if name == \"requests_all\":\\n            # expand requests into various methods\\n            requests_method_tools = [\\n                _tool for _tool in _BASE_TOOLS if _tool.startswith(\"requests_\")\\n            ]\\n            tool_names.extend(requests_method_tools)\\n        elif name in _BASE_TOOLS:\\n            tools.append(_BASE_TOOLS[name]())\\n        elif name in _LLM_TOOLS:\\n            if llm is None:\\n                raise ValueError(f\"Tool {name} requires an LLM to be provided\")\\n            tool = _LLM_TOOLS[name](llm)\\n            tools.append(tool)\\n        elif name in _EXTRA_LLM_TOOLS:\\n            if llm is None:\\n                raise ValueError(f\"Tool {name} requires an LLM to be provided\")\\n            _get_llm_tool_func, extra_keys = _EXTRA_LLM_TOOLS[name]\\n            missing_keys = set(extra_keys).difference(kwargs)\\n            if missing_keys:\\n                raise ValueError(\\n                    f\"Tool {name} requires some parameters that were not \"\\n                    f\"provided: {missing_keys}\"\\n                )\\n            sub_kwargs = {k: kwargs[k] for k in extra_keys}\\n            tool = _get_llm_tool_func(llm=llm, **sub_kwargs)\\n            tools.append(tool)\\n        elif name in _EXTRA_OPTIONAL_TOOLS:\\n            _get_tool_func, extra_keys = _EXTRA_OPTIONAL_TOOLS[name]\\n            sub_kwargs = {k: kwargs[k] for k in extra_keys if k in kwargs}\\n            tool = _get_tool_func(**sub_kwargs)\\n            tools.append(tool)\\n        else:\\n            raise ValueError(f\"Got unknown tool {name}\")',\n",
              "  'def load_tools(\\n    tool_names: List[str],\\n    llm: Optional[BaseLanguageModel] = None,\\n    callbacks: Callbacks = None,\\n    **kwargs: Any,\\n) -> List[BaseTool]:\\n    \"\"\"Load tools based on their name.\\n\\n    Tools allow agents to interact with various resources and services like\\n    APIs, databases, file systems, etc.\\n\\n    Please scope the permissions of each tools to the minimum required for the\\n    application.\\n\\n    For example, if an application only needs to read from a database,\\n    the database tool should not be given write permissions. Moreover\\n    consider scoping the permissions to only allow accessing specific\\n    tables and impose user-level quota for limiting resource usage.\\n\\n    Please read the APIs of the individual tools to determine which configuration\\n    they support.\\n\\n    See [Security](https://python.langchain.com/docs/security) for more information.\\n\\n    Args:\\n        tool_names: name of tools to load.\\n        llm: An optional language model, may be needed to initialize certain tools.\\n        callbacks: Optional callback manager or list of callback handlers.\\n            If not provided, default global callback manager will be used.',\n",
              "  'def _handle_callbacks(\\n    callback_manager: Optional[BaseCallbackManager], callbacks: Callbacks\\n) -> Callbacks:\\n    if callback_manager is not None:\\n        warnings.warn(\\n            \"callback_manager is deprecated. Please use callbacks instead.\",\\n            DeprecationWarning,\\n        )\\n        if callbacks is not None:\\n            raise ValueError(\\n                \"Cannot specify both callback_manager and callbacks arguments.\"\\n            )\\n        return callback_manager\\n    return callbacks\\n\\n\\ndef load_huggingface_tool(\\n    task_or_repo_id: str,\\n    model_repo_id: Optional[str] = None,\\n    token: Optional[str] = None,\\n    remote: bool = False,\\n    **kwargs: Any,\\n) -> BaseTool:\\n    \"\"\"Loads a tool from the HuggingFace Hub.\\n\\n    Args:\\n        task_or_repo_id: Task or model repo id.\\n        model_repo_id: Optional model repo id.\\n        token: Optional token.\\n        remote: Optional remote. Defaults to False.\\n        **kwargs:\\n\\n    Returns:\\n        A tool.\\n    \"\"\"\\n    try:\\n        from transformers import load_tool\\n    except ImportError:\\n        raise ImportError(\\n            \"HuggingFace tools require the libraries `transformers>=4.29.0`\"\\n            \" and `huggingface_hub>=0.14.1` to be installed.\"\\n            \" Please install it with\"\\n            \" `pip install --upgrade transformers huggingface_hub`.\"\\n        )\\n    hf_tool = load_tool(\\n        task_or_repo_id,\\n        model_repo_id=model_repo_id,\\n        token=token,\\n        remote=remote,\\n        **kwargs,\\n    )\\n    outputs = hf_tool.outputs\\n    if set(outputs) != {\"text\"}:\\n        raise NotImplementedError(\"Multimodal outputs not supported yet.\")\\n    inputs = hf_tool.inputs\\n    if set(inputs) != {\"text\"}:\\n        raise NotImplementedError(\"Multimodal inputs not supported yet.\")\\n    return Tool.from_function(\\n        hf_tool.__call__, name=hf_tool.name, description=hf_tool.description\\n    )',\n",
              "  '\"searchapi-results-json\": (\\n        _get_searchapi_results_json,\\n        [\"searchapi_api_key\", \"aiosession\"],\\n    ),\\n    \"serpapi\": (_get_serpapi, [\"serpapi_api_key\", \"aiosession\"]),\\n    \"dalle-image-generator\": (_get_dalle_image_generator, [\"openai_api_key\"]),\\n    \"twilio\": (_get_twilio, [\"account_sid\", \"auth_token\", \"from_number\"]),\\n    \"searx-search\": (_get_searx_search, [\"searx_host\", \"engines\", \"aiosession\"]),\\n    \"merriam-webster\": (_get_merriam_webster, [\"merriam_webster_api_key\"]),\\n    \"wikipedia\": (_get_wikipedia, [\"top_k_results\", \"lang\"]),\\n    \"arxiv\": (\\n        _get_arxiv,\\n        [\"top_k_results\", \"load_max_docs\", \"load_all_available_meta\"],\\n    ),\\n    \"golden-query\": (_get_golden_query, [\"golden_api_key\"]),\\n    \"pubmed\": (_get_pubmed, [\"top_k_results\"]),\\n    \"human\": (_get_human_tool, [\"prompt_func\", \"input_func\"]),\\n    \"awslambda\": (\\n        _get_lambda_api,\\n        [\"awslambda_tool_name\", \"awslambda_tool_description\", \"function_name\"],\\n    ),\\n    \"stackexchange\": (_get_stackexchange, []),\\n    \"sceneXplain\": (_get_scenexplain, []),\\n    \"graphql\": (_get_graphql_tool, [\"graphql_endpoint\"]),\\n    \"openweathermap-api\": (_get_openweathermap, [\"openweathermap_api_key\"]),\\n    \"dataforseo-api-search\": (\\n        _get_dataforseo_api_search,\\n        [\"api_login\", \"api_password\", \"aiosession\"],\\n    ),\\n    \"dataforseo-api-search-json\": (\\n        _get_dataforseo_api_search_json,\\n        [\"api_login\", \"api_password\", \"aiosession\"],\\n    ),\\n    \"eleven_labs_text2speech\": (_get_eleven_labs_text2speech, [\"eleven_api_key\"]),\\n    \"google_cloud_texttospeech\": (_get_google_cloud_texttospeech, []),\\n    \"reddit_search\": (\\n        _get_reddit_search,\\n        [\"reddit_client_id\", \"reddit_client_secret\", \"reddit_user_agent\"],\\n    ),\\n}',\n",
              "  '_EXTRA_LLM_TOOLS: Dict[\\n    str,\\n    Tuple[Callable[[Arg(BaseLanguageModel, \"llm\"), KwArg(Any)], BaseTool], List[str]],\\n] = {\\n    \"news-api\": (_get_news_api, [\"news_api_key\"]),\\n    \"tmdb-api\": (_get_tmdb_api, [\"tmdb_bearer_token\"]),\\n    \"podcast-api\": (_get_podcast_api, [\"listen_api_key\"]),\\n    \"memorize\": (_get_memorize, []),\\n}\\n_EXTRA_OPTIONAL_TOOLS: Dict[str, Tuple[Callable[[KwArg(Any)], BaseTool], List[str]]] = {\\n    \"wolfram-alpha\": (_get_wolfram_alpha, [\"wolfram_alpha_appid\"]),\\n    \"google-search\": (_get_google_search, [\"google_api_key\", \"google_cse_id\"]),\\n    \"google-search-results-json\": (\\n        _get_google_search_results_json,\\n        [\"google_api_key\", \"google_cse_id\", \"num_results\"],\\n    ),\\n    \"searx-search-results-json\": (\\n        _get_searx_search_results_json,\\n        [\"searx_host\", \"engines\", \"num_results\", \"aiosession\"],\\n    ),\\n    \"bing-search\": (_get_bing_search, [\"bing_subscription_key\", \"bing_search_url\"]),\\n    \"metaphor-search\": (_get_metaphor_search, [\"metaphor_api_key\"]),\\n    \"ddg-search\": (_get_ddg_search, []),\\n    \"google-lens\": (_get_google_lens, [\"serp_api_key\"]),\\n    \"google-serper\": (_get_google_serper, [\"serper_api_key\", \"aiosession\"]),\\n    \"google-scholar\": (\\n        _get_google_scholar,\\n        [\"top_k_results\", \"hl\", \"lr\", \"serp_api_key\"],\\n    ),\\n    \"google-finance\": (\\n        _get_google_finance,\\n        [\"serp_api_key\"],\\n    ),\\n    \"google-trends\": (\\n        _get_google_trends,\\n        [\"serp_api_key\"],\\n    ),\\n    \"google-jobs\": (\\n        _get_google_jobs,\\n        [\"serp_api_key\"],\\n    ),\\n    \"google-serper-results-json\": (\\n        _get_google_serper_results_json,\\n        [\"serper_api_key\", \"aiosession\"],\\n    ),\\n    \"searchapi\": (_get_searchapi, [\"searchapi_api_key\", \"aiosession\"]),\\n    \"searchapi-results-json\": (\\n        _get_searchapi_results_json,\\n        [\"searchapi_api_key\", \"aiosession\"],\\n    ),\\n    \"serpapi\": (_get_serpapi, [\"serpapi_api_key\", \"aiosession\"]),',\n",
              "  'class RecursiveCharacterTextSplitter(TextSplitter):\\n    \"\"\"Splitting text by recursively look at characters.\\n\\n    Recursively tries to split by different characters to find one\\n    that works.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        separators: Optional[List[str]] = None,\\n        keep_separator: bool = True,\\n        is_separator_regex: bool = False,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(keep_separator=keep_separator, **kwargs)\\n        self._separators = separators or [\"\\\\n\\\\n\", \"\\\\n\", \" \", \"\"]\\n        self._is_separator_regex = is_separator_regex\\n\\n    def _split_text(self, text: str, separators: List[str]) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        final_chunks = []\\n        # Get appropriate separator to use\\n        separator = separators[-1]\\n        new_separators = []\\n        for i, _s in enumerate(separators):\\n            _separator = _s if self._is_separator_regex else re.escape(_s)\\n            if _s == \"\":\\n                separator = _s\\n                break\\n            if re.search(_separator, text):\\n                separator = _s\\n                new_separators = separators[i + 1 :]\\n                break\\n\\n        _separator = separator if self._is_separator_regex else re.escape(separator)\\n        splits = _split_text_with_regex(text, _separator, self._keep_separator)',\n",
              "  '\"\\\\nevent \",\\n                \"\\\\nmodifier \",\\n                \"\\\\nerror \",\\n                \"\\\\nstruct \",\\n                \"\\\\nenum \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo while \",\\n                \"\\\\nassembly \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.COBOL:\\n            return [\\n                # Split along divisions\\n                \"\\\\nIDENTIFICATION DIVISION.\",\\n                \"\\\\nENVIRONMENT DIVISION.\",\\n                \"\\\\nDATA DIVISION.\",\\n                \"\\\\nPROCEDURE DIVISION.\",\\n                # Split along sections within DATA DIVISION\\n                \"\\\\nWORKING-STORAGE SECTION.\",\\n                \"\\\\nLINKAGE SECTION.\",\\n                \"\\\\nFILE SECTION.\",\\n                # Split along sections within PROCEDURE DIVISION\\n                \"\\\\nINPUT-OUTPUT SECTION.\",\\n                # Split along paragraphs and common statements\\n                \"\\\\nOPEN \",\\n                \"\\\\nCLOSE \",\\n                \"\\\\nREAD \",\\n                \"\\\\nWRITE \",\\n                \"\\\\nIF \",\\n                \"\\\\nELSE \",\\n                \"\\\\nMOVE \",\\n                \"\\\\nPERFORM \",\\n                \"\\\\nUNTIL \",\\n                \"\\\\nVARYING \",\\n                \"\\\\nACCEPT \",\\n                \"\\\\nDISPLAY \",\\n                \"\\\\nSTOP RUN.\",\\n                # Split by the normal type of lines\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]',\n",
              "  '\"<nav\",\\n                # Head\\n                \"<head\",\\n                \"<style\",\\n                \"<script\",\\n                \"<meta\",\\n                \"<title\",\\n                \"\",\\n            ]\\n        elif language == Language.CSHARP:\\n            return [\\n                \"\\\\ninterface \",\\n                \"\\\\nenum \",\\n                \"\\\\nimplements \",\\n                \"\\\\ndelegate \",\\n                \"\\\\nevent \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nabstract \",\\n                # Split along method definitions\\n                \"\\\\npublic \",\\n                \"\\\\nprotected \",\\n                \"\\\\nprivate \",\\n                \"\\\\nstatic \",\\n                \"\\\\nreturn \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\ncontinue \",\\n                \"\\\\nfor \",\\n                \"\\\\nforeach \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\nbreak \",\\n                \"\\\\ncase \",\\n                \"\\\\nelse \",\\n                # Split by exceptions\\n                \"\\\\ntry \",\\n                \"\\\\nthrow \",\\n                \"\\\\nfinally \",\\n                \"\\\\ncatch \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SOL:\\n            return [\\n                # Split along compiler information definitions\\n                \"\\\\npragma \",\\n                \"\\\\nusing \",\\n                # Split along contract definitions\\n                \"\\\\ncontract \",\\n                \"\\\\ninterface \",\\n                \"\\\\nlibrary \",\\n                # Split along method definitions\\n                \"\\\\nconstructor \",\\n                \"\\\\ntype \",\\n                \"\\\\nfunction \",\\n                \"\\\\nevent \",\\n                \"\\\\nmodifier \",\\n                \"\\\\nerror \",\\n                \"\\\\nstruct \",\\n                \"\\\\nenum \",\\n                # Split along control flow statements',\n",
              "  '\"\\\\n\\\\\\\\*\\\\\\\\*\\\\\\\\*+\\\\n\",\\n                \"\\\\n---+\\\\n\",\\n                \"\\\\n___+\\\\n\",\\n                # Note that this splitter doesn\\'t handle horizontal lines defined\\n                # by *three or more* of ***, ---, or ___, but this is not handled\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.LATEX:\\n            return [\\n                # First, try to split along Latex sections\\n                \"\\\\n\\\\\\\\\\\\\\\\chapter{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\section{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\subsection{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\subsubsection{\",\\n                # Now split by environments\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{enumerate}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{itemize}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{description}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{list}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{quote}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{quotation}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{verse}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{verbatim}\",\\n                # Now split by math environments\\n                \"\\\\n\\\\\\\\\\\\begin{align}\",\\n                \"$$\",\\n                \"$\",\\n                # Now split by the normal type of lines\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.HTML:\\n            return [\\n                # First, try to split along HTML tags\\n                \"<body\",\\n                \"<div\",\\n                \"<p\",\\n                \"<br\",\\n                \"<li\",\\n                \"<h1\",\\n                \"<h2\",\\n                \"<h3\",\\n                \"<h4\",\\n                \"<h5\",\\n                \"<h6\",\\n                \"<span\",\\n                \"<table\",\\n                \"<tr\",\\n                \"<td\",\\n                \"<th\",\\n                \"<ul\",\\n                \"<ol\",\\n                \"<header\",\\n                \"<footer\",\\n                \"<nav\",\\n                # Head\\n                \"<head\",\\n                \"<style\",\\n                \"<script\",\\n                \"<meta\",\\n                \"<title\",',\n",
              "  '\" \",\\n                \"\",\\n            ]\\n        elif language == Language.SCALA:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nobject \",\\n                # Split along method definitions\\n                \"\\\\ndef \",\\n                \"\\\\nval \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nmatch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SWIFT:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunc \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nstruct \",\\n                \"\\\\nenum \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.MARKDOWN:\\n            return [\\n                # First, try to split along Markdown headings (starting with level 2)\\n                \"\\\\n#{1,6} \",\\n                # Note the alternative syntax for headings (below) is not handled here\\n                # Heading level 2\\n                # ---------------\\n                # End of code block\\n                \"```\\\\n\",\\n                # Horizontal lines\\n                \"\\\\n\\\\\\\\*\\\\\\\\*\\\\\\\\*+\\\\n\",\\n                \"\\\\n---+\\\\n\",\\n                \"\\\\n___+\\\\n\",\\n                # Note that this splitter doesn\\'t handle horizontal lines defined',\n",
              "  'return [\\n                # First, try to split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\ndef \",\\n                \"\\\\n\\\\tdef \",\\n                # Now split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.RST:\\n            return [\\n                # Split along section titles\\n                \"\\\\n=+\\\\n\",\\n                \"\\\\n-+\\\\n\",\\n                \"\\\\n\\\\\\\\*+\\\\n\",\\n                # Split along directive markers\\n                \"\\\\n\\\\n.. *\\\\n\\\\n\",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.RUBY:\\n            return [\\n                # Split along method definitions\\n                \"\\\\ndef \",\\n                \"\\\\nclass \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nunless \",\\n                \"\\\\nwhile \",\\n                \"\\\\nfor \",\\n                \"\\\\ndo \",\\n                \"\\\\nbegin \",\\n                \"\\\\nrescue \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.RUST:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfn \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nwhile \",\\n                \"\\\\nfor \",\\n                \"\\\\nloop \",\\n                \"\\\\nmatch \",\\n                \"\\\\nconst \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SCALA:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",',\n",
              "  '\"\\\\nfunction \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                \"\\\\ndefault \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.PHP:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunction \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nforeach \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.PROTO:\\n            return [\\n                # Split along message definitions\\n                \"\\\\nmessage \",\\n                # Split along service definitions\\n                \"\\\\nservice \",\\n                # Split along enum definitions\\n                \"\\\\nenum \",\\n                # Split along option definitions\\n                \"\\\\noption \",\\n                # Split along import statements\\n                \"\\\\nimport \",\\n                # Split along syntax declarations\\n                \"\\\\nsyntax \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.PYTHON:\\n            return [\\n                # First, try to split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\ndef \",\\n                \"\\\\n\\\\tdef \",',\n",
              "  '\"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.KOTLIN:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along method definitions\\n                \"\\\\npublic \",\\n                \"\\\\nprotected \",\\n                \"\\\\nprivate \",\\n                \"\\\\ninternal \",\\n                \"\\\\ncompanion \",\\n                \"\\\\nfun \",\\n                \"\\\\nval \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nwhen \",\\n                \"\\\\ncase \",\\n                \"\\\\nelse \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.JS:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunction \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                \"\\\\nvar \",\\n                \"\\\\nclass \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                \"\\\\ndefault \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.TS:\\n            return [\\n                \"\\\\nenum \",\\n                \"\\\\ninterface \",\\n                \"\\\\nnamespace \",\\n                \"\\\\ntype \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along function definitions\\n                \"\\\\nfunction \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements',\n",
              "  '@staticmethod\\n    def get_separators_for_language(language: Language) -> List[str]:\\n        if language == Language.CPP:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along function definitions\\n                \"\\\\nvoid \",\\n                \"\\\\nint \",\\n                \"\\\\nfloat \",\\n                \"\\\\ndouble \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.GO:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunc \",\\n                \"\\\\nvar \",\\n                \"\\\\nconst \",\\n                \"\\\\ntype \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.JAVA:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along method definitions\\n                \"\\\\npublic \",\\n                \"\\\\nprotected \",\\n                \"\\\\nprivate \",\\n                \"\\\\nstatic \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.KOTLIN:\\n            return [',\n",
              "  '_separator = separator if self._is_separator_regex else re.escape(separator)\\n        splits = _split_text_with_regex(text, _separator, self._keep_separator)\\n\\n        # Now go merging things, recursively splitting longer texts.\\n        _good_splits = []\\n        _separator = \"\" if self._keep_separator else separator\\n        for s in splits:\\n            if self._length_function(s) < self._chunk_size:\\n                _good_splits.append(s)\\n            else:\\n                if _good_splits:\\n                    merged_text = self._merge_splits(_good_splits, _separator)\\n                    final_chunks.extend(merged_text)\\n                    _good_splits = []\\n                if not new_separators:\\n                    final_chunks.append(s)\\n                else:\\n                    other_info = self._split_text(s, new_separators)\\n                    final_chunks.extend(other_info)\\n        if _good_splits:\\n            merged_text = self._merge_splits(_good_splits, _separator)\\n            final_chunks.extend(merged_text)\\n        return final_chunks\\n\\n    def split_text(self, text: str) -> List[str]:\\n        return self._split_text(text, self._separators)\\n\\n    @classmethod\\n    def from_language(\\n        cls, language: Language, **kwargs: Any\\n    ) -> RecursiveCharacterTextSplitter:\\n        separators = cls.get_separators_for_language(language)\\n        return cls(separators=separators, is_separator_regex=True, **kwargs)',\n",
              "  'else:\\n            raise ValueError(\\n                f\"Language {language} is not supported! \"\\n                f\"Please choose from {list(Language)}\"\\n            )',\n",
              "  'class Language(str, Enum):\\n    \"\"\"Enum of the programming languages.\"\"\"\\n\\n    CPP = \"cpp\"\\n    GO = \"go\"\\n    JAVA = \"java\"\\n    KOTLIN = \"kotlin\"\\n    JS = \"js\"\\n    TS = \"ts\"\\n    PHP = \"php\"\\n    PROTO = \"proto\"\\n    PYTHON = \"python\"\\n    RST = \"rst\"\\n    RUBY = \"ruby\"\\n    RUST = \"rust\"\\n    SCALA = \"scala\"\\n    SWIFT = \"swift\"\\n    MARKDOWN = \"markdown\"\\n    LATEX = \"latex\"\\n    HTML = \"html\"\\n    SOL = \"sol\"\\n    CSHARP = \"csharp\"\\n    COBOL = \"cobol\"',\n",
              "  'def split_text(self, text: str) -> List[str]:\\n        def encode_strip_start_and_stop_token_ids(text: str) -> List[int]:\\n            return self._encode(text)[1:-1]\\n\\n        tokenizer = Tokenizer(\\n            chunk_overlap=self._chunk_overlap,\\n            tokens_per_chunk=self.tokens_per_chunk,\\n            decode=self.tokenizer.decode,\\n            encode=encode_strip_start_and_stop_token_ids,\\n        )\\n\\n        return split_text_on_tokens(text=text, tokenizer=tokenizer)\\n\\n    def count_tokens(self, *, text: str) -> int:\\n        return len(self._encode(text))\\n\\n    _max_length_equal_32_bit_integer: int = 2**32\\n\\n    def _encode(self, text: str) -> List[int]:\\n        token_ids_with_start_and_end_token_ids = self.tokenizer.encode(\\n            text,\\n            max_length=self._max_length_equal_32_bit_integer,\\n            truncation=\"do_not_truncate\",\\n        )\\n        return token_ids_with_start_and_end_token_ids',\n",
              "  'class SentenceTransformersTokenTextSplitter(TextSplitter):\\n    \"\"\"Splitting text to tokens using sentence model tokenizer.\"\"\"\\n\\n    def __init__(\\n        self,\\n        chunk_overlap: int = 50,\\n        model_name: str = \"sentence-transformers/all-mpnet-base-v2\",\\n        tokens_per_chunk: Optional[int] = None,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(**kwargs, chunk_overlap=chunk_overlap)\\n\\n        try:\\n            from sentence_transformers import SentenceTransformer\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import sentence_transformer python package. \"\\n                \"This is needed in order to for SentenceTransformersTokenTextSplitter. \"\\n                \"Please install it with `pip install sentence-transformers`.\"\\n            )\\n\\n        self.model_name = model_name\\n        self._model = SentenceTransformer(self.model_name)\\n        self.tokenizer = self._model.tokenizer\\n        self._initialize_chunk_configuration(tokens_per_chunk=tokens_per_chunk)\\n\\n    def _initialize_chunk_configuration(\\n        self, *, tokens_per_chunk: Optional[int]\\n    ) -> None:\\n        self.maximum_tokens_per_chunk = cast(int, self._model.max_seq_length)\\n\\n        if tokens_per_chunk is None:\\n            self.tokens_per_chunk = self.maximum_tokens_per_chunk\\n        else:\\n            self.tokens_per_chunk = tokens_per_chunk\\n\\n        if self.tokens_per_chunk > self.maximum_tokens_per_chunk:\\n            raise ValueError(\\n                f\"The token limit of the models \\'{self.model_name}\\'\"\\n                f\" is: {self.maximum_tokens_per_chunk}.\"\\n                f\" Argument tokens_per_chunk={self.tokens_per_chunk}\"\\n                f\" > maximum token limit.\"\\n            )\\n\\n    def split_text(self, text: str) -> List[str]:\\n        def encode_strip_start_and_stop_token_ids(text: str) -> List[int]:\\n            return self._encode(text)[1:-1]',\n",
              "  'class TokenTextSplitter(TextSplitter):\\n    \"\"\"Splitting text to tokens using model tokenizer.\"\"\"\\n\\n    def __init__(\\n        self,\\n        encoding_name: str = \"gpt2\",\\n        model_name: Optional[str] = None,\\n        allowed_special: Union[Literal[\"all\"], AbstractSet[str]] = set(),\\n        disallowed_special: Union[Literal[\"all\"], Collection[str]] = \"all\",\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(**kwargs)\\n        try:\\n            import tiktoken\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import tiktoken python package. \"\\n                \"This is needed in order to for TokenTextSplitter. \"\\n                \"Please install it with `pip install tiktoken`.\"\\n            )\\n\\n        if model_name is not None:\\n            enc = tiktoken.encoding_for_model(model_name)\\n        else:\\n            enc = tiktoken.get_encoding(encoding_name)\\n        self._tokenizer = enc\\n        self._allowed_special = allowed_special\\n        self._disallowed_special = disallowed_special\\n\\n    def split_text(self, text: str) -> List[str]:\\n        def _encode(_text: str) -> List[int]:\\n            return self._tokenizer.encode(\\n                _text,\\n                allowed_special=self._allowed_special,\\n                disallowed_special=self._disallowed_special,\\n            )\\n\\n        tokenizer = Tokenizer(\\n            chunk_overlap=self._chunk_overlap,\\n            tokens_per_chunk=self._chunk_size,\\n            decode=self._tokenizer.decode,\\n            encode=_encode,\\n        )\\n\\n        return split_text_on_tokens(text=text, tokenizer=tokenizer)',\n",
              "  'class Tokenizer:\\n    \"\"\"Tokenizer data class.\"\"\"\\n\\n    chunk_overlap: int\\n    \"\"\"Overlap in tokens between chunks\"\"\"\\n    tokens_per_chunk: int\\n    \"\"\"Maximum number of tokens per chunk\"\"\"\\n    decode: Callable[[List[int]], str]\\n    \"\"\" Function to decode a list of token ids to a string\"\"\"\\n    encode: Callable[[str], List[int]]\\n    \"\"\" Function to encode a string to a list of token ids\"\"\"\\n\\n\\ndef split_text_on_tokens(*, text: str, tokenizer: Tokenizer) -> List[str]:\\n    \"\"\"Split incoming text and return chunks using tokenizer.\"\"\"\\n    splits: List[str] = []\\n    input_ids = tokenizer.encode(text)\\n    start_idx = 0\\n    cur_idx = min(start_idx + tokenizer.tokens_per_chunk, len(input_ids))\\n    chunk_ids = input_ids[start_idx:cur_idx]\\n    while start_idx < len(input_ids):\\n        splits.append(tokenizer.decode(chunk_ids))\\n        if cur_idx == len(input_ids):\\n            break\\n        start_idx += tokenizer.tokens_per_chunk - tokenizer.chunk_overlap\\n        cur_idx = min(start_idx + tokenizer.tokens_per_chunk, len(input_ids))\\n        chunk_ids = input_ids[start_idx:cur_idx]\\n    return splits',\n",
              "  '# map xhtml namespace prefix\\n        ns_map = {\"h\": \"http://www.w3.org/1999/xhtml\"}\\n\\n        # build list of elements from DOM\\n        elements = []\\n        for element in result_dom.findall(\"*//*\", ns_map):\\n            if element.findall(\"*[@class=\\'headers\\']\") or element.findall(\\n                \"*[@class=\\'chunk\\']\"\\n            ):\\n                elements.append(\\n                    ElementType(\\n                        url=file,\\n                        xpath=\"\".join(\\n                            [\\n                                node.text\\n                                for node in element.findall(\"*[@class=\\'xpath\\']\", ns_map)\\n                            ]\\n                        ),\\n                        content=\"\".join(\\n                            [\\n                                node.text\\n                                for node in element.findall(\"*[@class=\\'chunk\\']\", ns_map)\\n                            ]\\n                        ),\\n                        metadata={\\n                            # Add text of specified headers to metadata using header\\n                            # mapping.\\n                            header_mapping[node.tag]: node.text\\n                            for node in filter(\\n                                lambda x: x.tag in header_filter,\\n                                element.findall(\"*[@class=\\'headers\\']/*\", ns_map),\\n                            )\\n                        },\\n                    )\\n                )\\n\\n        if not self.return_each_element:\\n            return self.aggregate_elements_to_chunks(elements)\\n        else:\\n            return [\\n                Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n                for chunk in elements\\n            ]\\n\\n\\n# should be in newer Python versions (3.10+)\\n# @dataclass(frozen=True, kw_only=True, slots=True)\\n@dataclass(frozen=True)',\n",
              "  'return [\\n            Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n            for chunk in aggregated_chunks\\n        ]\\n\\n    def split_text_from_url(self, url: str) -> List[Document]:\\n        \"\"\"Split HTML from web URL\\n\\n        Args:\\n            url: web URL\\n        \"\"\"\\n        r = requests.get(url)\\n        return self.split_text_from_file(BytesIO(r.content))\\n\\n    def split_text(self, text: str) -> List[Document]:\\n        \"\"\"Split HTML text string\\n\\n        Args:\\n            text: HTML text\\n        \"\"\"\\n        return self.split_text_from_file(StringIO(text))\\n\\n    def split_text_from_file(self, file: Any) -> List[Document]:\\n        \"\"\"Split HTML file\\n\\n        Args:\\n            file: HTML file\\n        \"\"\"\\n        try:\\n            from lxml import etree\\n        except ImportError as e:\\n            raise ImportError(\\n                \"Unable to import lxml, please install with `pip install lxml`.\"\\n            ) from e\\n        # use lxml library to parse html document and return xml ElementTree\\n        parser = etree.HTMLParser()\\n        tree = etree.parse(file, parser)\\n\\n        # document transformation for \"structure-aware\" chunking is handled with xsl.\\n        # see comments in html_chunks_with_headers.xslt for more detailed information.\\n        xslt_path = (\\n            pathlib.Path(__file__).parent\\n            / \"document_transformers/xsl/html_chunks_with_headers.xslt\"\\n        )\\n        xslt_tree = etree.parse(xslt_path)\\n        transform = etree.XSLT(xslt_tree)\\n        result = transform(tree)\\n        result_dom = etree.fromstring(str(result))\\n\\n        # create filter and mapping for header metadata\\n        header_filter = [header[0] for header in self.headers_to_split_on]\\n        header_mapping = dict(self.headers_to_split_on)\\n\\n        # map xhtml namespace prefix\\n        ns_map = {\"h\": \"http://www.w3.org/1999/xhtml\"}',\n",
              "  'class HTMLHeaderTextSplitter:\\n    \"\"\"\\n    Splitting HTML files based on specified headers.\\n    Requires lxml package.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        headers_to_split_on: List[Tuple[str, str]],\\n        return_each_element: bool = False,\\n    ):\\n        \"\"\"Create a new HTMLHeaderTextSplitter.\\n\\n        Args:\\n            headers_to_split_on: list of tuples of headers we want to track mapped to\\n                (arbitrary) keys for metadata. Allowed header values: h1, h2, h3, h4,\\n                h5, h6 e.g. [(\"h1\", \"Header 1\"), (\"h2\", \"Header 2)].\\n            return_each_element: Return each element w/ associated headers.\\n        \"\"\"\\n        # Output element-by-element or aggregated into chunks w/ common headers\\n        self.return_each_element = return_each_element\\n        self.headers_to_split_on = sorted(headers_to_split_on)\\n\\n    def aggregate_elements_to_chunks(\\n        self, elements: List[ElementType]\\n    ) -> List[Document]:\\n        \"\"\"Combine elements with common metadata into chunks\\n\\n        Args:\\n            elements: HTML element content with associated identifying info and metadata\\n        \"\"\"\\n        aggregated_chunks: List[ElementType] = []\\n\\n        for element in elements:\\n            if (\\n                aggregated_chunks\\n                and aggregated_chunks[-1][\"metadata\"] == element[\"metadata\"]\\n            ):\\n                # If the last element in the aggregated list\\n                # has the same metadata as the current element,\\n                # append the current content to the last element\\'s content\\n                aggregated_chunks[-1][\"content\"] += \"  \\\\n\" + element[\"content\"]\\n            else:\\n                # Otherwise, append the current element to the aggregated list\\n                aggregated_chunks.append(element)\\n\\n        return [\\n            Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n            for chunk in aggregated_chunks\\n        ]',\n",
              "  'if issubclass(cls, TokenTextSplitter):\\n            extra_kwargs = {\\n                \"encoding_name\": encoding_name,\\n                \"model_name\": model_name,\\n                \"allowed_special\": allowed_special,\\n                \"disallowed_special\": disallowed_special,\\n            }\\n            kwargs = {**kwargs, **extra_kwargs}\\n\\n        return cls(length_function=_tiktoken_encoder, **kwargs)\\n\\n    def transform_documents(\\n        self, documents: Sequence[Document], **kwargs: Any\\n    ) -> Sequence[Document]:\\n        \"\"\"Transform sequence of documents by splitting them.\"\"\"\\n        return self.split_documents(list(documents))\\n\\n    async def atransform_documents(\\n        self, documents: Sequence[Document], **kwargs: Any\\n    ) -> Sequence[Document]:\\n        \"\"\"Asynchronously transform a sequence of documents by splitting them.\"\"\"\\n        return await asyncio.get_running_loop().run_in_executor(\\n            None, partial(self.transform_documents, **kwargs), documents\\n        )',\n",
              "  'return [\\n            Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n            for chunk in aggregated_chunks\\n        ]\\n\\n    def split_text_from_url(self, url: str) -> List[Document]:\\n        \"\"\"Split HTML from web URL\\n\\n        Args:\\n            url: web URL\\n        \"\"\"\\n        r = requests.get(url)\\n        return self.split_text_from_file(BytesIO(r.content))\\n\\n    def split_text(self, text: str) -> List[Document]:\\n        \"\"\"Split HTML text string\\n\\n        Args:\\n            text: HTML text\\n        \"\"\"\\n        return self.split_text_from_file(StringIO(text))\\n\\n    def split_text_from_file(self, file: Any) -> List[Document]:\\n        \"\"\"Split HTML file\\n\\n        Args:\\n            file: HTML file\\n        \"\"\"\\n        try:\\n            from lxml import etree\\n        except ImportError as e:\\n            raise ImportError(\\n                \"Unable to import lxml, please install with `pip install lxml`.\"\\n            ) from e\\n        # use lxml library to parse html document and return xml ElementTree\\n        parser = etree.HTMLParser()\\n        tree = etree.parse(file, parser)\\n\\n        # document transformation for \"structure-aware\" chunking is handled with xsl.\\n        # see comments in html_chunks_with_headers.xslt for more detailed information.\\n        xslt_path = (\\n            pathlib.Path(__file__).parent\\n            / \"document_transformers/xsl/html_chunks_with_headers.xslt\"\\n        )\\n        xslt_tree = etree.parse(xslt_path)\\n        transform = etree.XSLT(xslt_tree)\\n        result = transform(tree)\\n        result_dom = etree.fromstring(str(result))\\n\\n        # create filter and mapping for header metadata\\n        header_filter = [header[0] for header in self.headers_to_split_on]\\n        header_mapping = dict(self.headers_to_split_on)\\n\\n        # map xhtml namespace prefix\\n        ns_map = {\"h\": \"http://www.w3.org/1999/xhtml\"}',\n",
              "  'class HTMLHeaderTextSplitter:\\n    \"\"\"\\n    Splitting HTML files based on specified headers.\\n    Requires lxml package.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        headers_to_split_on: List[Tuple[str, str]],\\n        return_each_element: bool = False,\\n    ):\\n        \"\"\"Create a new HTMLHeaderTextSplitter.\\n\\n        Args:\\n            headers_to_split_on: list of tuples of headers we want to track mapped to\\n                (arbitrary) keys for metadata. Allowed header values: h1, h2, h3, h4,\\n                h5, h6 e.g. [(\"h1\", \"Header 1\"), (\"h2\", \"Header 2)].\\n            return_each_element: Return each element w/ associated headers.\\n        \"\"\"\\n        # Output element-by-element or aggregated into chunks w/ common headers\\n        self.return_each_element = return_each_element\\n        self.headers_to_split_on = sorted(headers_to_split_on)\\n\\n    def aggregate_elements_to_chunks(\\n        self, elements: List[ElementType]\\n    ) -> List[Document]:\\n        \"\"\"Combine elements with common metadata into chunks\\n\\n        Args:\\n            elements: HTML element content with associated identifying info and metadata\\n        \"\"\"\\n        aggregated_chunks: List[ElementType] = []\\n\\n        for element in elements:\\n            if (\\n                aggregated_chunks\\n                and aggregated_chunks[-1][\"metadata\"] == element[\"metadata\"]\\n            ):\\n                # If the last element in the aggregated list\\n                # has the same metadata as the current element,\\n                # append the current content to the last element\\'s content\\n                aggregated_chunks[-1][\"content\"] += \"  \\\\n\" + element[\"content\"]\\n            else:\\n                # Otherwise, append the current element to the aggregated list\\n                aggregated_chunks.append(element)\\n\\n        return [\\n            Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n            for chunk in aggregated_chunks\\n        ]',\n",
              "  'class ElementType(TypedDict):\\n    \"\"\"Element type as typed dict.\"\"\"\\n\\n    url: str\\n    xpath: str\\n    content: str\\n    metadata: Dict[str, str]',\n",
              "  'if current_content:\\n            lines_with_metadata.append(\\n                {\"content\": \"\\\\n\".join(current_content), \"metadata\": current_metadata}\\n            )\\n\\n        # lines_with_metadata has each line with associated header metadata\\n        # aggregate these into chunks based on common metadata\\n        if not self.return_each_line:\\n            return self.aggregate_lines_to_chunks(lines_with_metadata)\\n        else:\\n            return [\\n                Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n                for chunk in lines_with_metadata\\n            ]',\n",
              "  '# Push the current header to the stack\\n                        header: HeaderType = {\\n                            \"level\": current_header_level,\\n                            \"name\": name,\\n                            \"data\": stripped_line[len(sep) :].strip(),\\n                        }\\n                        header_stack.append(header)\\n                        # Update initial_metadata with the current header\\n                        initial_metadata[name] = header[\"data\"]\\n\\n                    # Add the previous line to the lines_with_metadata\\n                    # only if current_content is not empty\\n                    if current_content:\\n                        lines_with_metadata.append(\\n                            {\\n                                \"content\": \"\\\\n\".join(current_content),\\n                                \"metadata\": current_metadata.copy(),\\n                            }\\n                        )\\n                        current_content.clear()\\n\\n                    break\\n            else:\\n                if stripped_line:\\n                    current_content.append(stripped_line)\\n                elif current_content:\\n                    lines_with_metadata.append(\\n                        {\\n                            \"content\": \"\\\\n\".join(current_content),\\n                            \"metadata\": current_metadata.copy(),\\n                        }\\n                    )\\n                    current_content.clear()\\n\\n            current_metadata = initial_metadata.copy()\\n\\n        if current_content:\\n            lines_with_metadata.append(\\n                {\"content\": \"\\\\n\".join(current_content), \"metadata\": current_metadata}\\n            )',\n",
              "  'if in_code_block:\\n                current_content.append(stripped_line)\\n                continue\\n\\n            # Check each line against each of the header types (e.g., #, ##)\\n            for sep, name in self.headers_to_split_on:\\n                # Check if line starts with a header that we intend to split on\\n                if stripped_line.startswith(sep) and (\\n                    # Header with no text OR header is followed by space\\n                    # Both are valid conditions that sep is being used a header\\n                    len(stripped_line) == len(sep) or stripped_line[len(sep)] == \" \"\\n                ):\\n                    # Ensure we are tracking the header as metadata\\n                    if name is not None:\\n                        # Get the current header level\\n                        current_header_level = sep.count(\"#\")\\n\\n                        # Pop out headers of lower or same level from the stack\\n                        while (\\n                            header_stack\\n                            and header_stack[-1][\"level\"] >= current_header_level\\n                        ):\\n                            # We have encountered a new header\\n                            # at the same or higher level\\n                            popped_header = header_stack.pop()\\n                            # Clear the metadata for the\\n                            # popped header in initial_metadata\\n                            if popped_header[\"name\"] in initial_metadata:\\n                                initial_metadata.pop(popped_header[\"name\"])',\n",
              "  'def split_text(self, text: str) -> List[Document]:\\n        \"\"\"Split markdown file\\n        Args:\\n            text: Markdown file\"\"\"\\n\\n        # Split the input text by newline character (\"\\\\n\").\\n        lines = text.split(\"\\\\n\")\\n        # Final output\\n        lines_with_metadata: List[LineType] = []\\n        # Content and metadata of the chunk currently being processed\\n        current_content: List[str] = []\\n        current_metadata: Dict[str, str] = {}\\n        # Keep track of the nested header structure\\n        # header_stack: List[Dict[str, Union[int, str]]] = []\\n        header_stack: List[HeaderType] = []\\n        initial_metadata: Dict[str, str] = {}\\n\\n        in_code_block = False\\n        opening_fence = \"\"\\n\\n        for line in lines:\\n            stripped_line = line.strip()\\n\\n            if not in_code_block:\\n                # Exclude inline code spans\\n                if stripped_line.startswith(\"```\") and stripped_line.count(\"```\") == 1:\\n                    in_code_block = True\\n                    opening_fence = \"```\"\\n                elif stripped_line.startswith(\"~~~\"):\\n                    in_code_block = True\\n                    opening_fence = \"~~~\"\\n            else:\\n                if stripped_line.startswith(opening_fence):\\n                    in_code_block = False\\n                    opening_fence = \"\"\\n\\n            if in_code_block:\\n                current_content.append(stripped_line)\\n                continue',\n",
              "  'class MarkdownHeaderTextSplitter:\\n    \"\"\"Splitting markdown files based on specified headers.\"\"\"\\n\\n    def __init__(\\n        self, headers_to_split_on: List[Tuple[str, str]], return_each_line: bool = False\\n    ):\\n        \"\"\"Create a new MarkdownHeaderTextSplitter.\\n\\n        Args:\\n            headers_to_split_on: Headers we want to track\\n            return_each_line: Return each line w/ associated headers\\n        \"\"\"\\n        # Output line-by-line or aggregated into chunks w/ common headers\\n        self.return_each_line = return_each_line\\n        # Given the headers we want to split on,\\n        # (e.g., \"#, ##, etc\") order by length\\n        self.headers_to_split_on = sorted(\\n            headers_to_split_on, key=lambda split: len(split[0]), reverse=True\\n        )\\n\\n    def aggregate_lines_to_chunks(self, lines: List[LineType]) -> List[Document]:\\n        \"\"\"Combine lines with common metadata into chunks\\n        Args:\\n            lines: Line of text / associated header metadata\\n        \"\"\"\\n        aggregated_chunks: List[LineType] = []\\n\\n        for line in lines:\\n            if (\\n                aggregated_chunks\\n                and aggregated_chunks[-1][\"metadata\"] == line[\"metadata\"]\\n            ):\\n                # If the last line in the aggregated list\\n                # has the same metadata as the current line,\\n                # append the current content to the last lines\\'s content\\n                aggregated_chunks[-1][\"content\"] += \"  \\\\n\" + line[\"content\"]\\n            else:\\n                # Otherwise, append the current line to the aggregated list\\n                aggregated_chunks.append(line)\\n\\n        return [\\n            Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n            for chunk in aggregated_chunks\\n        ]\\n\\n    def split_text(self, text: str) -> List[Document]:\\n        \"\"\"Split markdown file\\n        Args:\\n            text: Markdown file\"\"\"',\n",
              "  'class CharacterTextSplitter(TextSplitter):\\n    \"\"\"Splitting text that looks at characters.\"\"\"\\n\\n    def __init__(\\n        self, separator: str = \"\\\\n\\\\n\", is_separator_regex: bool = False, **kwargs: Any\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(**kwargs)\\n        self._separator = separator\\n        self._is_separator_regex = is_separator_regex\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        # First we naively split the large input into a bunch of smaller ones.\\n        separator = (\\n            self._separator if self._is_separator_regex else re.escape(self._separator)\\n        )\\n        splits = _split_text_with_regex(text, separator, self._keep_separator)\\n        _separator = \"\" if self._keep_separator else self._separator\\n        return self._merge_splits(splits, _separator)\\n\\n\\nclass LineType(TypedDict):\\n    \"\"\"Line type as typed dict.\"\"\"\\n\\n    metadata: Dict[str, str]\\n    content: str\\n\\n\\nclass HeaderType(TypedDict):\\n    \"\"\"Header type as typed dict.\"\"\"\\n\\n    level: int\\n    name: str\\n    data: str',\n",
              "  'class ElementType(TypedDict):\\n    \"\"\"Element type as typed dict.\"\"\"\\n\\n    url: str\\n    xpath: str\\n    content: str\\n    metadata: Dict[str, str]',\n",
              "  'if not isinstance(tokenizer, PreTrainedTokenizerBase):\\n                raise ValueError(\\n                    \"Tokenizer received was not an instance of PreTrainedTokenizerBase\"\\n                )\\n\\n            def _huggingface_tokenizer_length(text: str) -> int:\\n                return len(tokenizer.encode(text))\\n\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import transformers python package. \"\\n                \"Please install it with `pip install transformers`.\"\\n            )\\n        return cls(length_function=_huggingface_tokenizer_length, **kwargs)\\n\\n    @classmethod\\n    def from_tiktoken_encoder(\\n        cls: Type[TS],\\n        encoding_name: str = \"gpt2\",\\n        model_name: Optional[str] = None,\\n        allowed_special: Union[Literal[\"all\"], AbstractSet[str]] = set(),\\n        disallowed_special: Union[Literal[\"all\"], Collection[str]] = \"all\",\\n        **kwargs: Any,\\n    ) -> TS:\\n        \"\"\"Text splitter that uses tiktoken encoder to count length.\"\"\"\\n        try:\\n            import tiktoken\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import tiktoken python package. \"\\n                \"This is needed in order to calculate max_tokens_for_prompt. \"\\n                \"Please install it with `pip install tiktoken`.\"\\n            )\\n\\n        if model_name is not None:\\n            enc = tiktoken.encoding_for_model(model_name)\\n        else:\\n            enc = tiktoken.get_encoding(encoding_name)\\n\\n        def _tiktoken_encoder(text: str) -> int:\\n            return len(\\n                enc.encode(\\n                    text,\\n                    allowed_special=allowed_special,\\n                    disallowed_special=disallowed_special,\\n                )\\n            )',\n",
              "  'docs = []\\n        current_doc: List[str] = []\\n        total = 0\\n        for d in splits:\\n            _len = self._length_function(d)\\n            if (\\n                total + _len + (separator_len if len(current_doc) > 0 else 0)\\n                > self._chunk_size\\n            ):\\n                if total > self._chunk_size:\\n                    logger.warning(\\n                        f\"Created a chunk of size {total}, \"\\n                        f\"which is longer than the specified {self._chunk_size}\"\\n                    )\\n                if len(current_doc) > 0:\\n                    doc = self._join_docs(current_doc, separator)\\n                    if doc is not None:\\n                        docs.append(doc)\\n                    # Keep on popping if:\\n                    # - we have a larger chunk than in the chunk overlap\\n                    # - or if we still have any chunks and the length is long\\n                    while total > self._chunk_overlap or (\\n                        total + _len + (separator_len if len(current_doc) > 0 else 0)\\n                        > self._chunk_size\\n                        and total > 0\\n                    ):\\n                        total -= self._length_function(current_doc[0]) + (\\n                            separator_len if len(current_doc) > 1 else 0\\n                        )\\n                        current_doc = current_doc[1:]\\n            current_doc.append(d)\\n            total += _len + (separator_len if len(current_doc) > 1 else 0)\\n        doc = self._join_docs(current_doc, separator)\\n        if doc is not None:\\n            docs.append(doc)\\n        return docs\\n\\n    @classmethod\\n    def from_huggingface_tokenizer(cls, tokenizer: Any, **kwargs: Any) -> TextSplitter:\\n        \"\"\"Text splitter that uses HuggingFace tokenizer to count length.\"\"\"\\n        try:\\n            from transformers import PreTrainedTokenizerBase',\n",
              "  '@abstractmethod\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split text into multiple components.\"\"\"\\n\\n    def create_documents(\\n        self, texts: List[str], metadatas: Optional[List[dict]] = None\\n    ) -> List[Document]:\\n        \"\"\"Create documents from a list of texts.\"\"\"\\n        _metadatas = metadatas or [{}] * len(texts)\\n        documents = []\\n        for i, text in enumerate(texts):\\n            index = -1\\n            for chunk in self.split_text(text):\\n                metadata = copy.deepcopy(_metadatas[i])\\n                if self._add_start_index:\\n                    index = text.find(chunk, index + 1)\\n                    metadata[\"start_index\"] = index\\n                new_doc = Document(page_content=chunk, metadata=metadata)\\n                documents.append(new_doc)\\n        return documents\\n\\n    def split_documents(self, documents: Iterable[Document]) -> List[Document]:\\n        \"\"\"Split documents.\"\"\"\\n        texts, metadatas = [], []\\n        for doc in documents:\\n            texts.append(doc.page_content)\\n            metadatas.append(doc.metadata)\\n        return self.create_documents(texts, metadatas=metadatas)\\n\\n    def _join_docs(self, docs: List[str], separator: str) -> Optional[str]:\\n        text = separator.join(docs)\\n        if self._strip_whitespace:\\n            text = text.strip()\\n        if text == \"\":\\n            return None\\n        else:\\n            return text\\n\\n    def _merge_splits(self, splits: Iterable[str], separator: str) -> List[str]:\\n        # We now want to combine these smaller pieces into medium size\\n        # chunks to send to the LLM.\\n        separator_len = self._length_function(separator)',\n",
              "  'class TextSplitter(BaseDocumentTransformer, ABC):\\n    \"\"\"Interface for splitting text into chunks.\"\"\"\\n\\n    def __init__(\\n        self,\\n        chunk_size: int = 4000,\\n        chunk_overlap: int = 200,\\n        length_function: Callable[[str], int] = len,\\n        keep_separator: bool = False,\\n        add_start_index: bool = False,\\n        strip_whitespace: bool = True,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\\n\\n        Args:\\n            chunk_size: Maximum size of chunks to return\\n            chunk_overlap: Overlap in characters between chunks\\n            length_function: Function that measures the length of given chunks\\n            keep_separator: Whether to keep the separator in the chunks\\n            add_start_index: If `True`, includes chunk\\'s start index in metadata\\n            strip_whitespace: If `True`, strips whitespace from the start and end of\\n                              every document\\n        \"\"\"\\n        if chunk_overlap > chunk_size:\\n            raise ValueError(\\n                f\"Got a larger chunk overlap ({chunk_overlap}) than chunk size \"\\n                f\"({chunk_size}), should be smaller.\"\\n            )\\n        self._chunk_size = chunk_size\\n        self._chunk_overlap = chunk_overlap\\n        self._length_function = length_function\\n        self._keep_separator = keep_separator\\n        self._add_start_index = add_start_index\\n        self._strip_whitespace = strip_whitespace\\n\\n    @abstractmethod\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split text into multiple components.\"\"\"',\n",
              "  'def _split_text_with_regex(\\n    text: str, separator: str, keep_separator: bool\\n) -> List[str]:\\n    # Now that we have the separator, split the text\\n    if separator:\\n        if keep_separator:\\n            # The parentheses in the pattern keep the delimiters in the result.\\n            _splits = re.split(f\"({separator})\", text)\\n            splits = [_splits[i] + _splits[i + 1] for i in range(1, len(_splits), 2)]\\n            if len(_splits) % 2 == 0:\\n                splits += _splits[-1:]\\n            splits = [_splits[0]] + splits\\n        else:\\n            splits = re.split(separator, text)\\n    else:\\n        splits = list(text)\\n    return [s for s in splits if s != \"\"]',\n",
              "  '\"\"\"**Text Splitters** are classes for splitting text.\\n\\n\\n**Class hierarchy:**\\n\\n.. code-block::\\n\\n    BaseDocumentTransformer --> TextSplitter --> <name>TextSplitter  # Example: CharacterTextSplitter\\n                                                 RecursiveCharacterTextSplitter -->  <name>TextSplitter\\n\\nNote: **MarkdownHeaderTextSplitter** and **HTMLHeaderTextSplitter do not derive from TextSplitter.\\n\\n\\n**Main helpers:**\\n\\n.. code-block::\\n\\n    Document, Tokenizer, Language, LineType, HeaderType\\n\\n\"\"\"  # noqa: E501\\n\\nfrom __future__ import annotations\\n\\nimport asyncio\\nimport copy\\nimport logging\\nimport pathlib\\nimport re\\nfrom abc import ABC, abstractmethod\\nfrom dataclasses import dataclass\\nfrom enum import Enum\\nfrom functools import partial\\nfrom io import BytesIO, StringIO\\nfrom typing import (\\n    AbstractSet,\\n    Any,\\n    Callable,\\n    Collection,\\n    Dict,\\n    Iterable,\\n    List,\\n    Literal,\\n    Optional,\\n    Sequence,\\n    Tuple,\\n    Type,\\n    TypedDict,\\n    TypeVar,\\n    Union,\\n    cast,\\n)\\n\\nimport requests\\nfrom langchain_core.documents import BaseDocumentTransformer, Document\\n\\nlogger = logging.getLogger(__name__)\\n\\nTS = TypeVar(\"TS\", bound=\"TextSplitter\")\\n\\n\\ndef _make_spacy_pipeline_for_splitting(\\n    pipeline: str, *, max_length: int = 1_000_000\\n) -> Any:  # avoid importing spacy\\n    try:\\n        import spacy\\n    except ImportError:\\n        raise ImportError(\\n            \"Spacy is not installed, please install it with `pip install spacy`.\"\\n        )\\n    if pipeline == \"sentencizer\":\\n        from spacy.lang.en import English\\n\\n        sentencizer = English()\\n        sentencizer.add_pipe(\"sentencizer\")\\n    else:\\n        sentencizer = spacy.load(pipeline, exclude=[\"ner\", \"tagger\"])\\n        sentencizer.max_length = max_length\\n    return sentencizer',\n",
              "  'class PythonCodeTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Python syntax.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a PythonCodeTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.PYTHON)\\n        super().__init__(separators=separators, **kwargs)\\n\\n\\nclass MarkdownTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Markdown-formatted headings.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a MarkdownTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.MARKDOWN)\\n        super().__init__(separators=separators, **kwargs)\\n\\n\\nclass LatexTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Latex-formatted layout elements.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a LatexTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.LATEX)\\n        super().__init__(separators=separators, **kwargs)',\n",
              "  'class NLTKTextSplitter(TextSplitter):\\n    \"\"\"Splitting text using NLTK package.\"\"\"\\n\\n    def __init__(\\n        self, separator: str = \"\\\\n\\\\n\", language: str = \"english\", **kwargs: Any\\n    ) -> None:\\n        \"\"\"Initialize the NLTK splitter.\"\"\"\\n        super().__init__(**kwargs)\\n        try:\\n            from nltk.tokenize import sent_tokenize\\n\\n            self._tokenizer = sent_tokenize\\n        except ImportError:\\n            raise ImportError(\\n                \"NLTK is not installed, please install it with `pip install nltk`.\"\\n            )\\n        self._separator = separator\\n        self._language = language\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        # First we naively split the large input into a bunch of smaller ones.\\n        splits = self._tokenizer(text, language=self._language)\\n        return self._merge_splits(splits, self._separator)\\n\\n\\nclass SpacyTextSplitter(TextSplitter):\\n    \"\"\"Splitting text using Spacy package.\\n\\n\\n    Per default, Spacy\\'s `en_core_web_sm` model is used and\\n    its default max_length is 1000000 (it is the length of maximum character\\n    this model takes which can be increased for large files). For a faster, but\\n    potentially less accurate splitting, you can use `pipeline=\\'sentencizer\\'`.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        separator: str = \"\\\\n\\\\n\",\\n        pipeline: str = \"en_core_web_sm\",\\n        max_length: int = 1_000_000,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Initialize the spacy text splitter.\"\"\"\\n        super().__init__(**kwargs)\\n        self._tokenizer = _make_spacy_pipeline_for_splitting(\\n            pipeline, max_length=max_length\\n        )\\n        self._separator = separator\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        splits = (s.text for s in self._tokenizer(text).sents)\\n        return self._merge_splits(splits, self._separator)\\n\\n\\n# For backwards compatibility',\n",
              "  'def split_text(self, text: str) -> List[str]:\\n        def encode_strip_start_and_stop_token_ids(text: str) -> List[int]:\\n            return self._encode(text)[1:-1]\\n\\n        tokenizer = Tokenizer(\\n            chunk_overlap=self._chunk_overlap,\\n            tokens_per_chunk=self.tokens_per_chunk,\\n            decode=self.tokenizer.decode,\\n            encode=encode_strip_start_and_stop_token_ids,\\n        )\\n\\n        return split_text_on_tokens(text=text, tokenizer=tokenizer)\\n\\n    def count_tokens(self, *, text: str) -> int:\\n        return len(self._encode(text))\\n\\n    _max_length_equal_32_bit_integer: int = 2**32\\n\\n    def _encode(self, text: str) -> List[int]:\\n        token_ids_with_start_and_end_token_ids = self.tokenizer.encode(\\n            text,\\n            max_length=self._max_length_equal_32_bit_integer,\\n            truncation=\"do_not_truncate\",\\n        )\\n        return token_ids_with_start_and_end_token_ids',\n",
              "  '\"\\\\n\\\\\\\\*\\\\\\\\*\\\\\\\\*+\\\\n\",\\n                \"\\\\n---+\\\\n\",\\n                \"\\\\n___+\\\\n\",\\n                # Note that this splitter doesn\\'t handle horizontal lines defined\\n                # by *three or more* of ***, ---, or ___, but this is not handled\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.LATEX:\\n            return [\\n                # First, try to split along Latex sections\\n                \"\\\\n\\\\\\\\\\\\\\\\chapter{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\section{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\subsection{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\subsubsection{\",\\n                # Now split by environments\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{enumerate}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{itemize}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{description}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{list}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{quote}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{quotation}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{verse}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{verbatim}\",\\n                # Now split by math environments\\n                \"\\\\n\\\\\\\\\\\\begin{align}\",\\n                \"$$\",\\n                \"$\",\\n                # Now split by the normal type of lines\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.HTML:\\n            return [\\n                # First, try to split along HTML tags\\n                \"<body\",\\n                \"<div\",\\n                \"<p\",\\n                \"<br\",\\n                \"<li\",\\n                \"<h1\",\\n                \"<h2\",\\n                \"<h3\",\\n                \"<h4\",\\n                \"<h5\",\\n                \"<h6\",\\n                \"<span\",\\n                \"<table\",\\n                \"<tr\",\\n                \"<td\",\\n                \"<th\",\\n                \"<ul\",\\n                \"<ol\",\\n                \"<header\",\\n                \"<footer\",\\n                \"<nav\",\\n                # Head\\n                \"<head\",\\n                \"<style\",\\n                \"<script\",\\n                \"<meta\",\\n                \"<title\",',\n",
              "  '\" \",\\n                \"\",\\n            ]\\n        elif language == Language.SCALA:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nobject \",\\n                # Split along method definitions\\n                \"\\\\ndef \",\\n                \"\\\\nval \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nmatch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SWIFT:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunc \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nstruct \",\\n                \"\\\\nenum \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.MARKDOWN:\\n            return [\\n                # First, try to split along Markdown headings (starting with level 2)\\n                \"\\\\n#{1,6} \",\\n                # Note the alternative syntax for headings (below) is not handled here\\n                # Heading level 2\\n                # ---------------\\n                # End of code block\\n                \"```\\\\n\",\\n                # Horizontal lines\\n                \"\\\\n\\\\\\\\*\\\\\\\\*\\\\\\\\*+\\\\n\",\\n                \"\\\\n---+\\\\n\",\\n                \"\\\\n___+\\\\n\",\\n                # Note that this splitter doesn\\'t handle horizontal lines defined',\n",
              "  'return [\\n                # First, try to split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\ndef \",\\n                \"\\\\n\\\\tdef \",\\n                # Now split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.RST:\\n            return [\\n                # Split along section titles\\n                \"\\\\n=+\\\\n\",\\n                \"\\\\n-+\\\\n\",\\n                \"\\\\n\\\\\\\\*+\\\\n\",\\n                # Split along directive markers\\n                \"\\\\n\\\\n.. *\\\\n\\\\n\",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.RUBY:\\n            return [\\n                # Split along method definitions\\n                \"\\\\ndef \",\\n                \"\\\\nclass \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nunless \",\\n                \"\\\\nwhile \",\\n                \"\\\\nfor \",\\n                \"\\\\ndo \",\\n                \"\\\\nbegin \",\\n                \"\\\\nrescue \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.RUST:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfn \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nwhile \",\\n                \"\\\\nfor \",\\n                \"\\\\nloop \",\\n                \"\\\\nmatch \",\\n                \"\\\\nconst \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SCALA:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",',\n",
              "  '\"\\\\nfunction \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                \"\\\\ndefault \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.PHP:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunction \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nforeach \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.PROTO:\\n            return [\\n                # Split along message definitions\\n                \"\\\\nmessage \",\\n                # Split along service definitions\\n                \"\\\\nservice \",\\n                # Split along enum definitions\\n                \"\\\\nenum \",\\n                # Split along option definitions\\n                \"\\\\noption \",\\n                # Split along import statements\\n                \"\\\\nimport \",\\n                # Split along syntax declarations\\n                \"\\\\nsyntax \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.PYTHON:\\n            return [\\n                # First, try to split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\ndef \",\\n                \"\\\\n\\\\tdef \",',\n",
              "  '\"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.KOTLIN:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along method definitions\\n                \"\\\\npublic \",\\n                \"\\\\nprotected \",\\n                \"\\\\nprivate \",\\n                \"\\\\ninternal \",\\n                \"\\\\ncompanion \",\\n                \"\\\\nfun \",\\n                \"\\\\nval \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nwhen \",\\n                \"\\\\ncase \",\\n                \"\\\\nelse \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.JS:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunction \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                \"\\\\nvar \",\\n                \"\\\\nclass \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                \"\\\\ndefault \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.TS:\\n            return [\\n                \"\\\\nenum \",\\n                \"\\\\ninterface \",\\n                \"\\\\nnamespace \",\\n                \"\\\\ntype \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along function definitions\\n                \"\\\\nfunction \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements',\n",
              "  '@staticmethod\\n    def get_separators_for_language(language: Language) -> List[str]:\\n        if language == Language.CPP:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along function definitions\\n                \"\\\\nvoid \",\\n                \"\\\\nint \",\\n                \"\\\\nfloat \",\\n                \"\\\\ndouble \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.GO:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunc \",\\n                \"\\\\nvar \",\\n                \"\\\\nconst \",\\n                \"\\\\ntype \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.JAVA:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along method definitions\\n                \"\\\\npublic \",\\n                \"\\\\nprotected \",\\n                \"\\\\nprivate \",\\n                \"\\\\nstatic \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.KOTLIN:\\n            return [',\n",
              "  '_separator = separator if self._is_separator_regex else re.escape(separator)\\n        splits = _split_text_with_regex(text, _separator, self._keep_separator)\\n\\n        # Now go merging things, recursively splitting longer texts.\\n        _good_splits = []\\n        _separator = \"\" if self._keep_separator else separator\\n        for s in splits:\\n            if self._length_function(s) < self._chunk_size:\\n                _good_splits.append(s)\\n            else:\\n                if _good_splits:\\n                    merged_text = self._merge_splits(_good_splits, _separator)\\n                    final_chunks.extend(merged_text)\\n                    _good_splits = []\\n                if not new_separators:\\n                    final_chunks.append(s)\\n                else:\\n                    other_info = self._split_text(s, new_separators)\\n                    final_chunks.extend(other_info)\\n        if _good_splits:\\n            merged_text = self._merge_splits(_good_splits, _separator)\\n            final_chunks.extend(merged_text)\\n        return final_chunks\\n\\n    def split_text(self, text: str) -> List[str]:\\n        return self._split_text(text, self._separators)\\n\\n    @classmethod\\n    def from_language(\\n        cls, language: Language, **kwargs: Any\\n    ) -> RecursiveCharacterTextSplitter:\\n        separators = cls.get_separators_for_language(language)\\n        return cls(separators=separators, is_separator_regex=True, **kwargs)',\n",
              "  'class RecursiveCharacterTextSplitter(TextSplitter):\\n    \"\"\"Splitting text by recursively look at characters.\\n\\n    Recursively tries to split by different characters to find one\\n    that works.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        separators: Optional[List[str]] = None,\\n        keep_separator: bool = True,\\n        is_separator_regex: bool = False,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(keep_separator=keep_separator, **kwargs)\\n        self._separators = separators or [\"\\\\n\\\\n\", \"\\\\n\", \" \", \"\"]\\n        self._is_separator_regex = is_separator_regex\\n\\n    def _split_text(self, text: str, separators: List[str]) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        final_chunks = []\\n        # Get appropriate separator to use\\n        separator = separators[-1]\\n        new_separators = []\\n        for i, _s in enumerate(separators):\\n            _separator = _s if self._is_separator_regex else re.escape(_s)\\n            if _s == \"\":\\n                separator = _s\\n                break\\n            if re.search(_separator, text):\\n                separator = _s\\n                new_separators = separators[i + 1 :]\\n                break\\n\\n        _separator = separator if self._is_separator_regex else re.escape(separator)\\n        splits = _split_text_with_regex(text, _separator, self._keep_separator)',\n",
              "  'class Language(str, Enum):\\n    \"\"\"Enum of the programming languages.\"\"\"\\n\\n    CPP = \"cpp\"\\n    GO = \"go\"\\n    JAVA = \"java\"\\n    KOTLIN = \"kotlin\"\\n    JS = \"js\"\\n    TS = \"ts\"\\n    PHP = \"php\"\\n    PROTO = \"proto\"\\n    PYTHON = \"python\"\\n    RST = \"rst\"\\n    RUBY = \"ruby\"\\n    RUST = \"rust\"\\n    SCALA = \"scala\"\\n    SWIFT = \"swift\"\\n    MARKDOWN = \"markdown\"\\n    LATEX = \"latex\"\\n    HTML = \"html\"\\n    SOL = \"sol\"\\n    CSHARP = \"csharp\"\\n    COBOL = \"cobol\"',\n",
              "  '\"<nav\",\\n                # Head\\n                \"<head\",\\n                \"<style\",\\n                \"<script\",\\n                \"<meta\",\\n                \"<title\",\\n                \"\",\\n            ]\\n        elif language == Language.CSHARP:\\n            return [\\n                \"\\\\ninterface \",\\n                \"\\\\nenum \",\\n                \"\\\\nimplements \",\\n                \"\\\\ndelegate \",\\n                \"\\\\nevent \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nabstract \",\\n                # Split along method definitions\\n                \"\\\\npublic \",\\n                \"\\\\nprotected \",\\n                \"\\\\nprivate \",\\n                \"\\\\nstatic \",\\n                \"\\\\nreturn \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\ncontinue \",\\n                \"\\\\nfor \",\\n                \"\\\\nforeach \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\nbreak \",\\n                \"\\\\ncase \",\\n                \"\\\\nelse \",\\n                # Split by exceptions\\n                \"\\\\ntry \",\\n                \"\\\\nthrow \",\\n                \"\\\\nfinally \",\\n                \"\\\\ncatch \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SOL:\\n            return [\\n                # Split along compiler information definitions\\n                \"\\\\npragma \",\\n                \"\\\\nusing \",\\n                # Split along contract definitions\\n                \"\\\\ncontract \",\\n                \"\\\\ninterface \",\\n                \"\\\\nlibrary \",\\n                # Split along method definitions\\n                \"\\\\nconstructor \",\\n                \"\\\\ntype \",\\n                \"\\\\nfunction \",\\n                \"\\\\nevent \",\\n                \"\\\\nmodifier \",\\n                \"\\\\nerror \",\\n                \"\\\\nstruct \",\\n                \"\\\\nenum \",\\n                # Split along control flow statements',\n",
              "  'class SentenceTransformersTokenTextSplitter(TextSplitter):\\n    \"\"\"Splitting text to tokens using sentence model tokenizer.\"\"\"\\n\\n    def __init__(\\n        self,\\n        chunk_overlap: int = 50,\\n        model_name: str = \"sentence-transformers/all-mpnet-base-v2\",\\n        tokens_per_chunk: Optional[int] = None,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(**kwargs, chunk_overlap=chunk_overlap)\\n\\n        try:\\n            from sentence_transformers import SentenceTransformer\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import sentence_transformer python package. \"\\n                \"This is needed in order to for SentenceTransformersTokenTextSplitter. \"\\n                \"Please install it with `pip install sentence-transformers`.\"\\n            )\\n\\n        self.model_name = model_name\\n        self._model = SentenceTransformer(self.model_name)\\n        self.tokenizer = self._model.tokenizer\\n        self._initialize_chunk_configuration(tokens_per_chunk=tokens_per_chunk)\\n\\n    def _initialize_chunk_configuration(\\n        self, *, tokens_per_chunk: Optional[int]\\n    ) -> None:\\n        self.maximum_tokens_per_chunk = cast(int, self._model.max_seq_length)\\n\\n        if tokens_per_chunk is None:\\n            self.tokens_per_chunk = self.maximum_tokens_per_chunk\\n        else:\\n            self.tokens_per_chunk = tokens_per_chunk\\n\\n        if self.tokens_per_chunk > self.maximum_tokens_per_chunk:\\n            raise ValueError(\\n                f\"The token limit of the models \\'{self.model_name}\\'\"\\n                f\" is: {self.maximum_tokens_per_chunk}.\"\\n                f\" Argument tokens_per_chunk={self.tokens_per_chunk}\"\\n                f\" > maximum token limit.\"\\n            )\\n\\n    def split_text(self, text: str) -> List[str]:\\n        def encode_strip_start_and_stop_token_ids(text: str) -> List[int]:\\n            return self._encode(text)[1:-1]',\n",
              "  'class TokenTextSplitter(TextSplitter):\\n    \"\"\"Splitting text to tokens using model tokenizer.\"\"\"\\n\\n    def __init__(\\n        self,\\n        encoding_name: str = \"gpt2\",\\n        model_name: Optional[str] = None,\\n        allowed_special: Union[Literal[\"all\"], AbstractSet[str]] = set(),\\n        disallowed_special: Union[Literal[\"all\"], Collection[str]] = \"all\",\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(**kwargs)\\n        try:\\n            import tiktoken\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import tiktoken python package. \"\\n                \"This is needed in order to for TokenTextSplitter. \"\\n                \"Please install it with `pip install tiktoken`.\"\\n            )\\n\\n        if model_name is not None:\\n            enc = tiktoken.encoding_for_model(model_name)\\n        else:\\n            enc = tiktoken.get_encoding(encoding_name)\\n        self._tokenizer = enc\\n        self._allowed_special = allowed_special\\n        self._disallowed_special = disallowed_special\\n\\n    def split_text(self, text: str) -> List[str]:\\n        def _encode(_text: str) -> List[int]:\\n            return self._tokenizer.encode(\\n                _text,\\n                allowed_special=self._allowed_special,\\n                disallowed_special=self._disallowed_special,\\n            )\\n\\n        tokenizer = Tokenizer(\\n            chunk_overlap=self._chunk_overlap,\\n            tokens_per_chunk=self._chunk_size,\\n            decode=self._tokenizer.decode,\\n            encode=_encode,\\n        )\\n\\n        return split_text_on_tokens(text=text, tokenizer=tokenizer)',\n",
              "  'class Tokenizer:\\n    \"\"\"Tokenizer data class.\"\"\"\\n\\n    chunk_overlap: int\\n    \"\"\"Overlap in tokens between chunks\"\"\"\\n    tokens_per_chunk: int\\n    \"\"\"Maximum number of tokens per chunk\"\"\"\\n    decode: Callable[[List[int]], str]\\n    \"\"\" Function to decode a list of token ids to a string\"\"\"\\n    encode: Callable[[str], List[int]]\\n    \"\"\" Function to encode a string to a list of token ids\"\"\"\\n\\n\\ndef split_text_on_tokens(*, text: str, tokenizer: Tokenizer) -> List[str]:\\n    \"\"\"Split incoming text and return chunks using tokenizer.\"\"\"\\n    splits: List[str] = []\\n    input_ids = tokenizer.encode(text)\\n    start_idx = 0\\n    cur_idx = min(start_idx + tokenizer.tokens_per_chunk, len(input_ids))\\n    chunk_ids = input_ids[start_idx:cur_idx]\\n    while start_idx < len(input_ids):\\n        splits.append(tokenizer.decode(chunk_ids))\\n        if cur_idx == len(input_ids):\\n            break\\n        start_idx += tokenizer.tokens_per_chunk - tokenizer.chunk_overlap\\n        cur_idx = min(start_idx + tokenizer.tokens_per_chunk, len(input_ids))\\n        chunk_ids = input_ids[start_idx:cur_idx]\\n    return splits',\n",
              "  '# map xhtml namespace prefix\\n        ns_map = {\"h\": \"http://www.w3.org/1999/xhtml\"}\\n\\n        # build list of elements from DOM\\n        elements = []\\n        for element in result_dom.findall(\"*//*\", ns_map):\\n            if element.findall(\"*[@class=\\'headers\\']\") or element.findall(\\n                \"*[@class=\\'chunk\\']\"\\n            ):\\n                elements.append(\\n                    ElementType(\\n                        url=file,\\n                        xpath=\"\".join(\\n                            [\\n                                node.text\\n                                for node in element.findall(\"*[@class=\\'xpath\\']\", ns_map)\\n                            ]\\n                        ),\\n                        content=\"\".join(\\n                            [\\n                                node.text\\n                                for node in element.findall(\"*[@class=\\'chunk\\']\", ns_map)\\n                            ]\\n                        ),\\n                        metadata={\\n                            # Add text of specified headers to metadata using header\\n                            # mapping.\\n                            header_mapping[node.tag]: node.text\\n                            for node in filter(\\n                                lambda x: x.tag in header_filter,\\n                                element.findall(\"*[@class=\\'headers\\']/*\", ns_map),\\n                            )\\n                        },\\n                    )\\n                )\\n\\n        if not self.return_each_element:\\n            return self.aggregate_elements_to_chunks(elements)\\n        else:\\n            return [\\n                Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n                for chunk in elements\\n            ]\\n\\n\\n# should be in newer Python versions (3.10+)\\n# @dataclass(frozen=True, kw_only=True, slots=True)\\n@dataclass(frozen=True)',\n",
              "  'return [\\n            Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n            for chunk in aggregated_chunks\\n        ]\\n\\n    def split_text_from_url(self, url: str) -> List[Document]:\\n        \"\"\"Split HTML from web URL\\n\\n        Args:\\n            url: web URL\\n        \"\"\"\\n        r = requests.get(url)\\n        return self.split_text_from_file(BytesIO(r.content))\\n\\n    def split_text(self, text: str) -> List[Document]:\\n        \"\"\"Split HTML text string\\n\\n        Args:\\n            text: HTML text\\n        \"\"\"\\n        return self.split_text_from_file(StringIO(text))\\n\\n    def split_text_from_file(self, file: Any) -> List[Document]:\\n        \"\"\"Split HTML file\\n\\n        Args:\\n            file: HTML file\\n        \"\"\"\\n        try:\\n            from lxml import etree\\n        except ImportError as e:\\n            raise ImportError(\\n                \"Unable to import lxml, please install with `pip install lxml`.\"\\n            ) from e\\n        # use lxml library to parse html document and return xml ElementTree\\n        parser = etree.HTMLParser()\\n        tree = etree.parse(file, parser)\\n\\n        # document transformation for \"structure-aware\" chunking is handled with xsl.\\n        # see comments in html_chunks_with_headers.xslt for more detailed information.\\n        xslt_path = (\\n            pathlib.Path(__file__).parent\\n            / \"document_transformers/xsl/html_chunks_with_headers.xslt\"\\n        )\\n        xslt_tree = etree.parse(xslt_path)\\n        transform = etree.XSLT(xslt_tree)\\n        result = transform(tree)\\n        result_dom = etree.fromstring(str(result))\\n\\n        # create filter and mapping for header metadata\\n        header_filter = [header[0] for header in self.headers_to_split_on]\\n        header_mapping = dict(self.headers_to_split_on)\\n\\n        # map xhtml namespace prefix\\n        ns_map = {\"h\": \"http://www.w3.org/1999/xhtml\"}',\n",
              "  'class HTMLHeaderTextSplitter:\\n    \"\"\"\\n    Splitting HTML files based on specified headers.\\n    Requires lxml package.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        headers_to_split_on: List[Tuple[str, str]],\\n        return_each_element: bool = False,\\n    ):\\n        \"\"\"Create a new HTMLHeaderTextSplitter.\\n\\n        Args:\\n            headers_to_split_on: list of tuples of headers we want to track mapped to\\n                (arbitrary) keys for metadata. Allowed header values: h1, h2, h3, h4,\\n                h5, h6 e.g. [(\"h1\", \"Header 1\"), (\"h2\", \"Header 2)].\\n            return_each_element: Return each element w/ associated headers.\\n        \"\"\"\\n        # Output element-by-element or aggregated into chunks w/ common headers\\n        self.return_each_element = return_each_element\\n        self.headers_to_split_on = sorted(headers_to_split_on)\\n\\n    def aggregate_elements_to_chunks(\\n        self, elements: List[ElementType]\\n    ) -> List[Document]:\\n        \"\"\"Combine elements with common metadata into chunks\\n\\n        Args:\\n            elements: HTML element content with associated identifying info and metadata\\n        \"\"\"\\n        aggregated_chunks: List[ElementType] = []\\n\\n        for element in elements:\\n            if (\\n                aggregated_chunks\\n                and aggregated_chunks[-1][\"metadata\"] == element[\"metadata\"]\\n            ):\\n                # If the last element in the aggregated list\\n                # has the same metadata as the current element,\\n                # append the current content to the last element\\'s content\\n                aggregated_chunks[-1][\"content\"] += \"  \\\\n\" + element[\"content\"]\\n            else:\\n                # Otherwise, append the current element to the aggregated list\\n                aggregated_chunks.append(element)\\n\\n        return [\\n            Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n            for chunk in aggregated_chunks\\n        ]',\n",
              "  'class ElementType(TypedDict):\\n    \"\"\"Element type as typed dict.\"\"\"\\n\\n    url: str\\n    xpath: str\\n    content: str\\n    metadata: Dict[str, str]',\n",
              "  'if current_content:\\n            lines_with_metadata.append(\\n                {\"content\": \"\\\\n\".join(current_content), \"metadata\": current_metadata}\\n            )\\n\\n        # lines_with_metadata has each line with associated header metadata\\n        # aggregate these into chunks based on common metadata\\n        if not self.return_each_line:\\n            return self.aggregate_lines_to_chunks(lines_with_metadata)\\n        else:\\n            return [\\n                Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n                for chunk in lines_with_metadata\\n            ]',\n",
              "  'docs = []\\n        current_doc: List[str] = []\\n        total = 0\\n        for d in splits:\\n            _len = self._length_function(d)\\n            if (\\n                total + _len + (separator_len if len(current_doc) > 0 else 0)\\n                > self._chunk_size\\n            ):\\n                if total > self._chunk_size:\\n                    logger.warning(\\n                        f\"Created a chunk of size {total}, \"\\n                        f\"which is longer than the specified {self._chunk_size}\"\\n                    )\\n                if len(current_doc) > 0:\\n                    doc = self._join_docs(current_doc, separator)\\n                    if doc is not None:\\n                        docs.append(doc)\\n                    # Keep on popping if:\\n                    # - we have a larger chunk than in the chunk overlap\\n                    # - or if we still have any chunks and the length is long\\n                    while total > self._chunk_overlap or (\\n                        total + _len + (separator_len if len(current_doc) > 0 else 0)\\n                        > self._chunk_size\\n                        and total > 0\\n                    ):\\n                        total -= self._length_function(current_doc[0]) + (\\n                            separator_len if len(current_doc) > 1 else 0\\n                        )\\n                        current_doc = current_doc[1:]\\n            current_doc.append(d)\\n            total += _len + (separator_len if len(current_doc) > 1 else 0)\\n        doc = self._join_docs(current_doc, separator)\\n        if doc is not None:\\n            docs.append(doc)\\n        return docs\\n\\n    @classmethod\\n    def from_huggingface_tokenizer(cls, tokenizer: Any, **kwargs: Any) -> TextSplitter:\\n        \"\"\"Text splitter that uses HuggingFace tokenizer to count length.\"\"\"\\n        try:\\n            from transformers import PreTrainedTokenizerBase',\n",
              "  'if current_content:\\n            lines_with_metadata.append(\\n                {\"content\": \"\\\\n\".join(current_content), \"metadata\": current_metadata}\\n            )\\n\\n        # lines_with_metadata has each line with associated header metadata\\n        # aggregate these into chunks based on common metadata\\n        if not self.return_each_line:\\n            return self.aggregate_lines_to_chunks(lines_with_metadata)\\n        else:\\n            return [\\n                Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n                for chunk in lines_with_metadata\\n            ]',\n",
              "  '# Push the current header to the stack\\n                        header: HeaderType = {\\n                            \"level\": current_header_level,\\n                            \"name\": name,\\n                            \"data\": stripped_line[len(sep) :].strip(),\\n                        }\\n                        header_stack.append(header)\\n                        # Update initial_metadata with the current header\\n                        initial_metadata[name] = header[\"data\"]\\n\\n                    # Add the previous line to the lines_with_metadata\\n                    # only if current_content is not empty\\n                    if current_content:\\n                        lines_with_metadata.append(\\n                            {\\n                                \"content\": \"\\\\n\".join(current_content),\\n                                \"metadata\": current_metadata.copy(),\\n                            }\\n                        )\\n                        current_content.clear()\\n\\n                    break\\n            else:\\n                if stripped_line:\\n                    current_content.append(stripped_line)\\n                elif current_content:\\n                    lines_with_metadata.append(\\n                        {\\n                            \"content\": \"\\\\n\".join(current_content),\\n                            \"metadata\": current_metadata.copy(),\\n                        }\\n                    )\\n                    current_content.clear()\\n\\n            current_metadata = initial_metadata.copy()\\n\\n        if current_content:\\n            lines_with_metadata.append(\\n                {\"content\": \"\\\\n\".join(current_content), \"metadata\": current_metadata}\\n            )',\n",
              "  'if in_code_block:\\n                current_content.append(stripped_line)\\n                continue\\n\\n            # Check each line against each of the header types (e.g., #, ##)\\n            for sep, name in self.headers_to_split_on:\\n                # Check if line starts with a header that we intend to split on\\n                if stripped_line.startswith(sep) and (\\n                    # Header with no text OR header is followed by space\\n                    # Both are valid conditions that sep is being used a header\\n                    len(stripped_line) == len(sep) or stripped_line[len(sep)] == \" \"\\n                ):\\n                    # Ensure we are tracking the header as metadata\\n                    if name is not None:\\n                        # Get the current header level\\n                        current_header_level = sep.count(\"#\")\\n\\n                        # Pop out headers of lower or same level from the stack\\n                        while (\\n                            header_stack\\n                            and header_stack[-1][\"level\"] >= current_header_level\\n                        ):\\n                            # We have encountered a new header\\n                            # at the same or higher level\\n                            popped_header = header_stack.pop()\\n                            # Clear the metadata for the\\n                            # popped header in initial_metadata\\n                            if popped_header[\"name\"] in initial_metadata:\\n                                initial_metadata.pop(popped_header[\"name\"])',\n",
              "  'def split_text(self, text: str) -> List[Document]:\\n        \"\"\"Split markdown file\\n        Args:\\n            text: Markdown file\"\"\"\\n\\n        # Split the input text by newline character (\"\\\\n\").\\n        lines = text.split(\"\\\\n\")\\n        # Final output\\n        lines_with_metadata: List[LineType] = []\\n        # Content and metadata of the chunk currently being processed\\n        current_content: List[str] = []\\n        current_metadata: Dict[str, str] = {}\\n        # Keep track of the nested header structure\\n        # header_stack: List[Dict[str, Union[int, str]]] = []\\n        header_stack: List[HeaderType] = []\\n        initial_metadata: Dict[str, str] = {}\\n\\n        in_code_block = False\\n        opening_fence = \"\"\\n\\n        for line in lines:\\n            stripped_line = line.strip()\\n\\n            if not in_code_block:\\n                # Exclude inline code spans\\n                if stripped_line.startswith(\"```\") and stripped_line.count(\"```\") == 1:\\n                    in_code_block = True\\n                    opening_fence = \"```\"\\n                elif stripped_line.startswith(\"~~~\"):\\n                    in_code_block = True\\n                    opening_fence = \"~~~\"\\n            else:\\n                if stripped_line.startswith(opening_fence):\\n                    in_code_block = False\\n                    opening_fence = \"\"\\n\\n            if in_code_block:\\n                current_content.append(stripped_line)\\n                continue',\n",
              "  'class MarkdownHeaderTextSplitter:\\n    \"\"\"Splitting markdown files based on specified headers.\"\"\"\\n\\n    def __init__(\\n        self, headers_to_split_on: List[Tuple[str, str]], return_each_line: bool = False\\n    ):\\n        \"\"\"Create a new MarkdownHeaderTextSplitter.\\n\\n        Args:\\n            headers_to_split_on: Headers we want to track\\n            return_each_line: Return each line w/ associated headers\\n        \"\"\"\\n        # Output line-by-line or aggregated into chunks w/ common headers\\n        self.return_each_line = return_each_line\\n        # Given the headers we want to split on,\\n        # (e.g., \"#, ##, etc\") order by length\\n        self.headers_to_split_on = sorted(\\n            headers_to_split_on, key=lambda split: len(split[0]), reverse=True\\n        )\\n\\n    def aggregate_lines_to_chunks(self, lines: List[LineType]) -> List[Document]:\\n        \"\"\"Combine lines with common metadata into chunks\\n        Args:\\n            lines: Line of text / associated header metadata\\n        \"\"\"\\n        aggregated_chunks: List[LineType] = []\\n\\n        for line in lines:\\n            if (\\n                aggregated_chunks\\n                and aggregated_chunks[-1][\"metadata\"] == line[\"metadata\"]\\n            ):\\n                # If the last line in the aggregated list\\n                # has the same metadata as the current line,\\n                # append the current content to the last lines\\'s content\\n                aggregated_chunks[-1][\"content\"] += \"  \\\\n\" + line[\"content\"]\\n            else:\\n                # Otherwise, append the current line to the aggregated list\\n                aggregated_chunks.append(line)\\n\\n        return [\\n            Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n            for chunk in aggregated_chunks\\n        ]\\n\\n    def split_text(self, text: str) -> List[Document]:\\n        \"\"\"Split markdown file\\n        Args:\\n            text: Markdown file\"\"\"',\n",
              "  'class CharacterTextSplitter(TextSplitter):\\n    \"\"\"Splitting text that looks at characters.\"\"\"\\n\\n    def __init__(\\n        self, separator: str = \"\\\\n\\\\n\", is_separator_regex: bool = False, **kwargs: Any\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(**kwargs)\\n        self._separator = separator\\n        self._is_separator_regex = is_separator_regex\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        # First we naively split the large input into a bunch of smaller ones.\\n        separator = (\\n            self._separator if self._is_separator_regex else re.escape(self._separator)\\n        )\\n        splits = _split_text_with_regex(text, separator, self._keep_separator)\\n        _separator = \"\" if self._keep_separator else self._separator\\n        return self._merge_splits(splits, _separator)\\n\\n\\nclass LineType(TypedDict):\\n    \"\"\"Line type as typed dict.\"\"\"\\n\\n    metadata: Dict[str, str]\\n    content: str\\n\\n\\nclass HeaderType(TypedDict):\\n    \"\"\"Header type as typed dict.\"\"\"\\n\\n    level: int\\n    name: str\\n    data: str',\n",
              "  'if issubclass(cls, TokenTextSplitter):\\n            extra_kwargs = {\\n                \"encoding_name\": encoding_name,\\n                \"model_name\": model_name,\\n                \"allowed_special\": allowed_special,\\n                \"disallowed_special\": disallowed_special,\\n            }\\n            kwargs = {**kwargs, **extra_kwargs}\\n\\n        return cls(length_function=_tiktoken_encoder, **kwargs)\\n\\n    def transform_documents(\\n        self, documents: Sequence[Document], **kwargs: Any\\n    ) -> Sequence[Document]:\\n        \"\"\"Transform sequence of documents by splitting them.\"\"\"\\n        return self.split_documents(list(documents))\\n\\n    async def atransform_documents(\\n        self, documents: Sequence[Document], **kwargs: Any\\n    ) -> Sequence[Document]:\\n        \"\"\"Asynchronously transform a sequence of documents by splitting them.\"\"\"\\n        return await asyncio.get_running_loop().run_in_executor(\\n            None, partial(self.transform_documents, **kwargs), documents\\n        )',\n",
              "  'if not isinstance(tokenizer, PreTrainedTokenizerBase):\\n                raise ValueError(\\n                    \"Tokenizer received was not an instance of PreTrainedTokenizerBase\"\\n                )\\n\\n            def _huggingface_tokenizer_length(text: str) -> int:\\n                return len(tokenizer.encode(text))\\n\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import transformers python package. \"\\n                \"Please install it with `pip install transformers`.\"\\n            )\\n        return cls(length_function=_huggingface_tokenizer_length, **kwargs)\\n\\n    @classmethod\\n    def from_tiktoken_encoder(\\n        cls: Type[TS],\\n        encoding_name: str = \"gpt2\",\\n        model_name: Optional[str] = None,\\n        allowed_special: Union[Literal[\"all\"], AbstractSet[str]] = set(),\\n        disallowed_special: Union[Literal[\"all\"], Collection[str]] = \"all\",\\n        **kwargs: Any,\\n    ) -> TS:\\n        \"\"\"Text splitter that uses tiktoken encoder to count length.\"\"\"\\n        try:\\n            import tiktoken\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import tiktoken python package. \"\\n                \"This is needed in order to calculate max_tokens_for_prompt. \"\\n                \"Please install it with `pip install tiktoken`.\"\\n            )\\n\\n        if model_name is not None:\\n            enc = tiktoken.encoding_for_model(model_name)\\n        else:\\n            enc = tiktoken.get_encoding(encoding_name)\\n\\n        def _tiktoken_encoder(text: str) -> int:\\n            return len(\\n                enc.encode(\\n                    text,\\n                    allowed_special=allowed_special,\\n                    disallowed_special=disallowed_special,\\n                )\\n            )',\n",
              "  '# map xhtml namespace prefix\\n        ns_map = {\"h\": \"http://www.w3.org/1999/xhtml\"}\\n\\n        # build list of elements from DOM\\n        elements = []\\n        for element in result_dom.findall(\"*//*\", ns_map):\\n            if element.findall(\"*[@class=\\'headers\\']\") or element.findall(\\n                \"*[@class=\\'chunk\\']\"\\n            ):\\n                elements.append(\\n                    ElementType(\\n                        url=file,\\n                        xpath=\"\".join(\\n                            [\\n                                node.text\\n                                for node in element.findall(\"*[@class=\\'xpath\\']\", ns_map)\\n                            ]\\n                        ),\\n                        content=\"\".join(\\n                            [\\n                                node.text\\n                                for node in element.findall(\"*[@class=\\'chunk\\']\", ns_map)\\n                            ]\\n                        ),\\n                        metadata={\\n                            # Add text of specified headers to metadata using header\\n                            # mapping.\\n                            header_mapping[node.tag]: node.text\\n                            for node in filter(\\n                                lambda x: x.tag in header_filter,\\n                                element.findall(\"*[@class=\\'headers\\']/*\", ns_map),\\n                            )\\n                        },\\n                    )\\n                )\\n\\n        if not self.return_each_element:\\n            return self.aggregate_elements_to_chunks(elements)\\n        else:\\n            return [\\n                Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n                for chunk in elements\\n            ]\\n\\n\\n# should be in newer Python versions (3.10+)\\n# @dataclass(frozen=True, kw_only=True, slots=True)\\n@dataclass(frozen=True)',\n",
              "  '@abstractmethod\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split text into multiple components.\"\"\"\\n\\n    def create_documents(\\n        self, texts: List[str], metadatas: Optional[List[dict]] = None\\n    ) -> List[Document]:\\n        \"\"\"Create documents from a list of texts.\"\"\"\\n        _metadatas = metadatas or [{}] * len(texts)\\n        documents = []\\n        for i, text in enumerate(texts):\\n            index = -1\\n            for chunk in self.split_text(text):\\n                metadata = copy.deepcopy(_metadatas[i])\\n                if self._add_start_index:\\n                    index = text.find(chunk, index + 1)\\n                    metadata[\"start_index\"] = index\\n                new_doc = Document(page_content=chunk, metadata=metadata)\\n                documents.append(new_doc)\\n        return documents\\n\\n    def split_documents(self, documents: Iterable[Document]) -> List[Document]:\\n        \"\"\"Split documents.\"\"\"\\n        texts, metadatas = [], []\\n        for doc in documents:\\n            texts.append(doc.page_content)\\n            metadatas.append(doc.metadata)\\n        return self.create_documents(texts, metadatas=metadatas)\\n\\n    def _join_docs(self, docs: List[str], separator: str) -> Optional[str]:\\n        text = separator.join(docs)\\n        if self._strip_whitespace:\\n            text = text.strip()\\n        if text == \"\":\\n            return None\\n        else:\\n            return text\\n\\n    def _merge_splits(self, splits: Iterable[str], separator: str) -> List[str]:\\n        # We now want to combine these smaller pieces into medium size\\n        # chunks to send to the LLM.\\n        separator_len = self._length_function(separator)',\n",
              "  'class TextSplitter(BaseDocumentTransformer, ABC):\\n    \"\"\"Interface for splitting text into chunks.\"\"\"\\n\\n    def __init__(\\n        self,\\n        chunk_size: int = 4000,\\n        chunk_overlap: int = 200,\\n        length_function: Callable[[str], int] = len,\\n        keep_separator: bool = False,\\n        add_start_index: bool = False,\\n        strip_whitespace: bool = True,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\\n\\n        Args:\\n            chunk_size: Maximum size of chunks to return\\n            chunk_overlap: Overlap in characters between chunks\\n            length_function: Function that measures the length of given chunks\\n            keep_separator: Whether to keep the separator in the chunks\\n            add_start_index: If `True`, includes chunk\\'s start index in metadata\\n            strip_whitespace: If `True`, strips whitespace from the start and end of\\n                              every document\\n        \"\"\"\\n        if chunk_overlap > chunk_size:\\n            raise ValueError(\\n                f\"Got a larger chunk overlap ({chunk_overlap}) than chunk size \"\\n                f\"({chunk_size}), should be smaller.\"\\n            )\\n        self._chunk_size = chunk_size\\n        self._chunk_overlap = chunk_overlap\\n        self._length_function = length_function\\n        self._keep_separator = keep_separator\\n        self._add_start_index = add_start_index\\n        self._strip_whitespace = strip_whitespace\\n\\n    @abstractmethod\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split text into multiple components.\"\"\"',\n",
              "  'def _split_text_with_regex(\\n    text: str, separator: str, keep_separator: bool\\n) -> List[str]:\\n    # Now that we have the separator, split the text\\n    if separator:\\n        if keep_separator:\\n            # The parentheses in the pattern keep the delimiters in the result.\\n            _splits = re.split(f\"({separator})\", text)\\n            splits = [_splits[i] + _splits[i + 1] for i in range(1, len(_splits), 2)]\\n            if len(_splits) % 2 == 0:\\n                splits += _splits[-1:]\\n            splits = [_splits[0]] + splits\\n        else:\\n            splits = re.split(separator, text)\\n    else:\\n        splits = list(text)\\n    return [s for s in splits if s != \"\"]',\n",
              "  '\"\"\"**Text Splitters** are classes for splitting text.\\n\\n\\n**Class hierarchy:**\\n\\n.. code-block::\\n\\n    BaseDocumentTransformer --> TextSplitter --> <name>TextSplitter  # Example: CharacterTextSplitter\\n                                                 RecursiveCharacterTextSplitter -->  <name>TextSplitter\\n\\nNote: **MarkdownHeaderTextSplitter** and **HTMLHeaderTextSplitter do not derive from TextSplitter.\\n\\n\\n**Main helpers:**\\n\\n.. code-block::\\n\\n    Document, Tokenizer, Language, LineType, HeaderType\\n\\n\"\"\"  # noqa: E501\\n\\nfrom __future__ import annotations\\n\\nimport asyncio\\nimport copy\\nimport logging\\nimport pathlib\\nimport re\\nfrom abc import ABC, abstractmethod\\nfrom dataclasses import dataclass\\nfrom enum import Enum\\nfrom functools import partial\\nfrom io import BytesIO, StringIO\\nfrom typing import (\\n    AbstractSet,\\n    Any,\\n    Callable,\\n    Collection,\\n    Dict,\\n    Iterable,\\n    List,\\n    Literal,\\n    Optional,\\n    Sequence,\\n    Tuple,\\n    Type,\\n    TypedDict,\\n    TypeVar,\\n    Union,\\n    cast,\\n)\\n\\nimport requests\\nfrom langchain_core.documents import BaseDocumentTransformer, Document\\n\\nlogger = logging.getLogger(__name__)\\n\\nTS = TypeVar(\"TS\", bound=\"TextSplitter\")\\n\\n\\ndef _make_spacy_pipeline_for_splitting(\\n    pipeline: str, *, max_length: int = 1_000_000\\n) -> Any:  # avoid importing spacy\\n    try:\\n        import spacy\\n    except ImportError:\\n        raise ImportError(\\n            \"Spacy is not installed, please install it with `pip install spacy`.\"\\n        )\\n    if pipeline == \"sentencizer\":\\n        from spacy.lang.en import English\\n\\n        sentencizer = English()\\n        sentencizer.add_pipe(\"sentencizer\")\\n    else:\\n        sentencizer = spacy.load(pipeline, exclude=[\"ner\", \"tagger\"])\\n        sentencizer.max_length = max_length\\n    return sentencizer',\n",
              "  'class PythonCodeTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Python syntax.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a PythonCodeTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.PYTHON)\\n        super().__init__(separators=separators, **kwargs)\\n\\n\\nclass MarkdownTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Markdown-formatted headings.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a MarkdownTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.MARKDOWN)\\n        super().__init__(separators=separators, **kwargs)\\n\\n\\nclass LatexTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Latex-formatted layout elements.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a LatexTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.LATEX)\\n        super().__init__(separators=separators, **kwargs)',\n",
              "  'class NLTKTextSplitter(TextSplitter):\\n    \"\"\"Splitting text using NLTK package.\"\"\"\\n\\n    def __init__(\\n        self, separator: str = \"\\\\n\\\\n\", language: str = \"english\", **kwargs: Any\\n    ) -> None:\\n        \"\"\"Initialize the NLTK splitter.\"\"\"\\n        super().__init__(**kwargs)\\n        try:\\n            from nltk.tokenize import sent_tokenize\\n\\n            self._tokenizer = sent_tokenize\\n        except ImportError:\\n            raise ImportError(\\n                \"NLTK is not installed, please install it with `pip install nltk`.\"\\n            )\\n        self._separator = separator\\n        self._language = language\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        # First we naively split the large input into a bunch of smaller ones.\\n        splits = self._tokenizer(text, language=self._language)\\n        return self._merge_splits(splits, self._separator)\\n\\n\\nclass SpacyTextSplitter(TextSplitter):\\n    \"\"\"Splitting text using Spacy package.\\n\\n\\n    Per default, Spacy\\'s `en_core_web_sm` model is used and\\n    its default max_length is 1000000 (it is the length of maximum character\\n    this model takes which can be increased for large files). For a faster, but\\n    potentially less accurate splitting, you can use `pipeline=\\'sentencizer\\'`.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        separator: str = \"\\\\n\\\\n\",\\n        pipeline: str = \"en_core_web_sm\",\\n        max_length: int = 1_000_000,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Initialize the spacy text splitter.\"\"\"\\n        super().__init__(**kwargs)\\n        self._tokenizer = _make_spacy_pipeline_for_splitting(\\n            pipeline, max_length=max_length\\n        )\\n        self._separator = separator\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        splits = (s.text for s in self._tokenizer(text).sents)\\n        return self._merge_splits(splits, self._separator)\\n\\n\\n# For backwards compatibility',\n",
              "  'else:\\n            raise ValueError(\\n                f\"Language {language} is not supported! \"\\n                f\"Please choose from {list(Language)}\"\\n            )',\n",
              "  '\"\\\\nevent \",\\n                \"\\\\nmodifier \",\\n                \"\\\\nerror \",\\n                \"\\\\nstruct \",\\n                \"\\\\nenum \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo while \",\\n                \"\\\\nassembly \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.COBOL:\\n            return [\\n                # Split along divisions\\n                \"\\\\nIDENTIFICATION DIVISION.\",\\n                \"\\\\nENVIRONMENT DIVISION.\",\\n                \"\\\\nDATA DIVISION.\",\\n                \"\\\\nPROCEDURE DIVISION.\",\\n                # Split along sections within DATA DIVISION\\n                \"\\\\nWORKING-STORAGE SECTION.\",\\n                \"\\\\nLINKAGE SECTION.\",\\n                \"\\\\nFILE SECTION.\",\\n                # Split along sections within PROCEDURE DIVISION\\n                \"\\\\nINPUT-OUTPUT SECTION.\",\\n                # Split along paragraphs and common statements\\n                \"\\\\nOPEN \",\\n                \"\\\\nCLOSE \",\\n                \"\\\\nREAD \",\\n                \"\\\\nWRITE \",\\n                \"\\\\nIF \",\\n                \"\\\\nELSE \",\\n                \"\\\\nMOVE \",\\n                \"\\\\nPERFORM \",\\n                \"\\\\nUNTIL \",\\n                \"\\\\nVARYING \",\\n                \"\\\\nACCEPT \",\\n                \"\\\\nDISPLAY \",\\n                \"\\\\nSTOP RUN.\",\\n                # Split by the normal type of lines\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]',\n",
              "  'return [\\n                # First, try to split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\ndef \",\\n                \"\\\\n\\\\tdef \",\\n                # Now split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.RST:\\n            return [\\n                # Split along section titles\\n                \"\\\\n=+\\\\n\",\\n                \"\\\\n-+\\\\n\",\\n                \"\\\\n\\\\\\\\*+\\\\n\",\\n                # Split along directive markers\\n                \"\\\\n\\\\n.. *\\\\n\\\\n\",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.RUBY:\\n            return [\\n                # Split along method definitions\\n                \"\\\\ndef \",\\n                \"\\\\nclass \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nunless \",\\n                \"\\\\nwhile \",\\n                \"\\\\nfor \",\\n                \"\\\\ndo \",\\n                \"\\\\nbegin \",\\n                \"\\\\nrescue \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.RUST:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfn \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nwhile \",\\n                \"\\\\nfor \",\\n                \"\\\\nloop \",\\n                \"\\\\nmatch \",\\n                \"\\\\nconst \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SCALA:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",',\n",
              "  'def _split_text_with_regex(\\n    text: str, separator: str, keep_separator: bool\\n) -> List[str]:\\n    # Now that we have the separator, split the text\\n    if separator:\\n        if keep_separator:\\n            # The parentheses in the pattern keep the delimiters in the result.\\n            _splits = re.split(f\"({separator})\", text)\\n            splits = [_splits[i] + _splits[i + 1] for i in range(1, len(_splits), 2)]\\n            if len(_splits) % 2 == 0:\\n                splits += _splits[-1:]\\n            splits = [_splits[0]] + splits\\n        else:\\n            splits = re.split(separator, text)\\n    else:\\n        splits = list(text)\\n    return [s for s in splits if s != \"\"]',\n",
              "  '\"\"\"**Text Splitters** are classes for splitting text.\\n\\n\\n**Class hierarchy:**\\n\\n.. code-block::\\n\\n    BaseDocumentTransformer --> TextSplitter --> <name>TextSplitter  # Example: CharacterTextSplitter\\n                                                 RecursiveCharacterTextSplitter -->  <name>TextSplitter\\n\\nNote: **MarkdownHeaderTextSplitter** and **HTMLHeaderTextSplitter do not derive from TextSplitter.\\n\\n\\n**Main helpers:**\\n\\n.. code-block::\\n\\n    Document, Tokenizer, Language, LineType, HeaderType\\n\\n\"\"\"  # noqa: E501\\n\\nfrom __future__ import annotations\\n\\nimport asyncio\\nimport copy\\nimport logging\\nimport pathlib\\nimport re\\nfrom abc import ABC, abstractmethod\\nfrom dataclasses import dataclass\\nfrom enum import Enum\\nfrom functools import partial\\nfrom io import BytesIO, StringIO\\nfrom typing import (\\n    AbstractSet,\\n    Any,\\n    Callable,\\n    Collection,\\n    Dict,\\n    Iterable,\\n    List,\\n    Literal,\\n    Optional,\\n    Sequence,\\n    Tuple,\\n    Type,\\n    TypedDict,\\n    TypeVar,\\n    Union,\\n    cast,\\n)\\n\\nimport requests\\nfrom langchain_core.documents import BaseDocumentTransformer, Document\\n\\nlogger = logging.getLogger(__name__)\\n\\nTS = TypeVar(\"TS\", bound=\"TextSplitter\")\\n\\n\\ndef _make_spacy_pipeline_for_splitting(\\n    pipeline: str, *, max_length: int = 1_000_000\\n) -> Any:  # avoid importing spacy\\n    try:\\n        import spacy\\n    except ImportError:\\n        raise ImportError(\\n            \"Spacy is not installed, please install it with `pip install spacy`.\"\\n        )\\n    if pipeline == \"sentencizer\":\\n        from spacy.lang.en import English\\n\\n        sentencizer = English()\\n        sentencizer.add_pipe(\"sentencizer\")\\n    else:\\n        sentencizer = spacy.load(pipeline, exclude=[\"ner\", \"tagger\"])\\n        sentencizer.max_length = max_length\\n    return sentencizer',\n",
              "  'class PythonCodeTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Python syntax.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a PythonCodeTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.PYTHON)\\n        super().__init__(separators=separators, **kwargs)\\n\\n\\nclass MarkdownTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Markdown-formatted headings.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a MarkdownTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.MARKDOWN)\\n        super().__init__(separators=separators, **kwargs)\\n\\n\\nclass LatexTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Latex-formatted layout elements.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a LatexTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.LATEX)\\n        super().__init__(separators=separators, **kwargs)',\n",
              "  'class NLTKTextSplitter(TextSplitter):\\n    \"\"\"Splitting text using NLTK package.\"\"\"\\n\\n    def __init__(\\n        self, separator: str = \"\\\\n\\\\n\", language: str = \"english\", **kwargs: Any\\n    ) -> None:\\n        \"\"\"Initialize the NLTK splitter.\"\"\"\\n        super().__init__(**kwargs)\\n        try:\\n            from nltk.tokenize import sent_tokenize\\n\\n            self._tokenizer = sent_tokenize\\n        except ImportError:\\n            raise ImportError(\\n                \"NLTK is not installed, please install it with `pip install nltk`.\"\\n            )\\n        self._separator = separator\\n        self._language = language\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        # First we naively split the large input into a bunch of smaller ones.\\n        splits = self._tokenizer(text, language=self._language)\\n        return self._merge_splits(splits, self._separator)\\n\\n\\nclass SpacyTextSplitter(TextSplitter):\\n    \"\"\"Splitting text using Spacy package.\\n\\n\\n    Per default, Spacy\\'s `en_core_web_sm` model is used and\\n    its default max_length is 1000000 (it is the length of maximum character\\n    this model takes which can be increased for large files). For a faster, but\\n    potentially less accurate splitting, you can use `pipeline=\\'sentencizer\\'`.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        separator: str = \"\\\\n\\\\n\",\\n        pipeline: str = \"en_core_web_sm\",\\n        max_length: int = 1_000_000,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Initialize the spacy text splitter.\"\"\"\\n        super().__init__(**kwargs)\\n        self._tokenizer = _make_spacy_pipeline_for_splitting(\\n            pipeline, max_length=max_length\\n        )\\n        self._separator = separator\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        splits = (s.text for s in self._tokenizer(text).sents)\\n        return self._merge_splits(splits, self._separator)\\n\\n\\n# For backwards compatibility',\n",
              "  'else:\\n            raise ValueError(\\n                f\"Language {language} is not supported! \"\\n                f\"Please choose from {list(Language)}\"\\n            )',\n",
              "  '\"\\\\nevent \",\\n                \"\\\\nmodifier \",\\n                \"\\\\nerror \",\\n                \"\\\\nstruct \",\\n                \"\\\\nenum \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo while \",\\n                \"\\\\nassembly \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.COBOL:\\n            return [\\n                # Split along divisions\\n                \"\\\\nIDENTIFICATION DIVISION.\",\\n                \"\\\\nENVIRONMENT DIVISION.\",\\n                \"\\\\nDATA DIVISION.\",\\n                \"\\\\nPROCEDURE DIVISION.\",\\n                # Split along sections within DATA DIVISION\\n                \"\\\\nWORKING-STORAGE SECTION.\",\\n                \"\\\\nLINKAGE SECTION.\",\\n                \"\\\\nFILE SECTION.\",\\n                # Split along sections within PROCEDURE DIVISION\\n                \"\\\\nINPUT-OUTPUT SECTION.\",\\n                # Split along paragraphs and common statements\\n                \"\\\\nOPEN \",\\n                \"\\\\nCLOSE \",\\n                \"\\\\nREAD \",\\n                \"\\\\nWRITE \",\\n                \"\\\\nIF \",\\n                \"\\\\nELSE \",\\n                \"\\\\nMOVE \",\\n                \"\\\\nPERFORM \",\\n                \"\\\\nUNTIL \",\\n                \"\\\\nVARYING \",\\n                \"\\\\nACCEPT \",\\n                \"\\\\nDISPLAY \",\\n                \"\\\\nSTOP RUN.\",\\n                # Split by the normal type of lines\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]',\n",
              "  '\"<nav\",\\n                # Head\\n                \"<head\",\\n                \"<style\",\\n                \"<script\",\\n                \"<meta\",\\n                \"<title\",\\n                \"\",\\n            ]\\n        elif language == Language.CSHARP:\\n            return [\\n                \"\\\\ninterface \",\\n                \"\\\\nenum \",\\n                \"\\\\nimplements \",\\n                \"\\\\ndelegate \",\\n                \"\\\\nevent \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nabstract \",\\n                # Split along method definitions\\n                \"\\\\npublic \",\\n                \"\\\\nprotected \",\\n                \"\\\\nprivate \",\\n                \"\\\\nstatic \",\\n                \"\\\\nreturn \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\ncontinue \",\\n                \"\\\\nfor \",\\n                \"\\\\nforeach \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\nbreak \",\\n                \"\\\\ncase \",\\n                \"\\\\nelse \",\\n                # Split by exceptions\\n                \"\\\\ntry \",\\n                \"\\\\nthrow \",\\n                \"\\\\nfinally \",\\n                \"\\\\ncatch \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SOL:\\n            return [\\n                # Split along compiler information definitions\\n                \"\\\\npragma \",\\n                \"\\\\nusing \",\\n                # Split along contract definitions\\n                \"\\\\ncontract \",\\n                \"\\\\ninterface \",\\n                \"\\\\nlibrary \",\\n                # Split along method definitions\\n                \"\\\\nconstructor \",\\n                \"\\\\ntype \",\\n                \"\\\\nfunction \",\\n                \"\\\\nevent \",\\n                \"\\\\nmodifier \",\\n                \"\\\\nerror \",\\n                \"\\\\nstruct \",\\n                \"\\\\nenum \",\\n                # Split along control flow statements',\n",
              "  '\"\\\\n\\\\\\\\*\\\\\\\\*\\\\\\\\*+\\\\n\",\\n                \"\\\\n---+\\\\n\",\\n                \"\\\\n___+\\\\n\",\\n                # Note that this splitter doesn\\'t handle horizontal lines defined\\n                # by *three or more* of ***, ---, or ___, but this is not handled\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.LATEX:\\n            return [\\n                # First, try to split along Latex sections\\n                \"\\\\n\\\\\\\\\\\\\\\\chapter{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\section{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\subsection{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\subsubsection{\",\\n                # Now split by environments\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{enumerate}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{itemize}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{description}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{list}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{quote}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{quotation}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{verse}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{verbatim}\",\\n                # Now split by math environments\\n                \"\\\\n\\\\\\\\\\\\begin{align}\",\\n                \"$$\",\\n                \"$\",\\n                # Now split by the normal type of lines\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.HTML:\\n            return [\\n                # First, try to split along HTML tags\\n                \"<body\",\\n                \"<div\",\\n                \"<p\",\\n                \"<br\",\\n                \"<li\",\\n                \"<h1\",\\n                \"<h2\",\\n                \"<h3\",\\n                \"<h4\",\\n                \"<h5\",\\n                \"<h6\",\\n                \"<span\",\\n                \"<table\",\\n                \"<tr\",\\n                \"<td\",\\n                \"<th\",\\n                \"<ul\",\\n                \"<ol\",\\n                \"<header\",\\n                \"<footer\",\\n                \"<nav\",\\n                # Head\\n                \"<head\",\\n                \"<style\",\\n                \"<script\",\\n                \"<meta\",\\n                \"<title\",',\n",
              "  '\" \",\\n                \"\",\\n            ]\\n        elif language == Language.SCALA:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nobject \",\\n                # Split along method definitions\\n                \"\\\\ndef \",\\n                \"\\\\nval \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nmatch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SWIFT:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunc \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nstruct \",\\n                \"\\\\nenum \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.MARKDOWN:\\n            return [\\n                # First, try to split along Markdown headings (starting with level 2)\\n                \"\\\\n#{1,6} \",\\n                # Note the alternative syntax for headings (below) is not handled here\\n                # Heading level 2\\n                # ---------------\\n                # End of code block\\n                \"```\\\\n\",\\n                # Horizontal lines\\n                \"\\\\n\\\\\\\\*\\\\\\\\*\\\\\\\\*+\\\\n\",\\n                \"\\\\n---+\\\\n\",\\n                \"\\\\n___+\\\\n\",\\n                # Note that this splitter doesn\\'t handle horizontal lines defined',\n",
              "  'class TextSplitter(BaseDocumentTransformer, ABC):\\n    \"\"\"Interface for splitting text into chunks.\"\"\"\\n\\n    def __init__(\\n        self,\\n        chunk_size: int = 4000,\\n        chunk_overlap: int = 200,\\n        length_function: Callable[[str], int] = len,\\n        keep_separator: bool = False,\\n        add_start_index: bool = False,\\n        strip_whitespace: bool = True,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\\n\\n        Args:\\n            chunk_size: Maximum size of chunks to return\\n            chunk_overlap: Overlap in characters between chunks\\n            length_function: Function that measures the length of given chunks\\n            keep_separator: Whether to keep the separator in the chunks\\n            add_start_index: If `True`, includes chunk\\'s start index in metadata\\n            strip_whitespace: If `True`, strips whitespace from the start and end of\\n                              every document\\n        \"\"\"\\n        if chunk_overlap > chunk_size:\\n            raise ValueError(\\n                f\"Got a larger chunk overlap ({chunk_overlap}) than chunk size \"\\n                f\"({chunk_size}), should be smaller.\"\\n            )\\n        self._chunk_size = chunk_size\\n        self._chunk_overlap = chunk_overlap\\n        self._length_function = length_function\\n        self._keep_separator = keep_separator\\n        self._add_start_index = add_start_index\\n        self._strip_whitespace = strip_whitespace\\n\\n    @abstractmethod\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split text into multiple components.\"\"\"',\n",
              "  '\"\\\\nfunction \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                \"\\\\ndefault \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.PHP:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunction \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nforeach \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.PROTO:\\n            return [\\n                # Split along message definitions\\n                \"\\\\nmessage \",\\n                # Split along service definitions\\n                \"\\\\nservice \",\\n                # Split along enum definitions\\n                \"\\\\nenum \",\\n                # Split along option definitions\\n                \"\\\\noption \",\\n                # Split along import statements\\n                \"\\\\nimport \",\\n                # Split along syntax declarations\\n                \"\\\\nsyntax \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.PYTHON:\\n            return [\\n                # First, try to split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\ndef \",\\n                \"\\\\n\\\\tdef \",',\n",
              "  '\"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.KOTLIN:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along method definitions\\n                \"\\\\npublic \",\\n                \"\\\\nprotected \",\\n                \"\\\\nprivate \",\\n                \"\\\\ninternal \",\\n                \"\\\\ncompanion \",\\n                \"\\\\nfun \",\\n                \"\\\\nval \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nwhen \",\\n                \"\\\\ncase \",\\n                \"\\\\nelse \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.JS:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunction \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                \"\\\\nvar \",\\n                \"\\\\nclass \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                \"\\\\ndefault \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.TS:\\n            return [\\n                \"\\\\nenum \",\\n                \"\\\\ninterface \",\\n                \"\\\\nnamespace \",\\n                \"\\\\ntype \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along function definitions\\n                \"\\\\nfunction \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements',\n",
              "  '@staticmethod\\n    def get_separators_for_language(language: Language) -> List[str]:\\n        if language == Language.CPP:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along function definitions\\n                \"\\\\nvoid \",\\n                \"\\\\nint \",\\n                \"\\\\nfloat \",\\n                \"\\\\ndouble \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.GO:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunc \",\\n                \"\\\\nvar \",\\n                \"\\\\nconst \",\\n                \"\\\\ntype \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.JAVA:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along method definitions\\n                \"\\\\npublic \",\\n                \"\\\\nprotected \",\\n                \"\\\\nprivate \",\\n                \"\\\\nstatic \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.KOTLIN:\\n            return [',\n",
              "  '_separator = separator if self._is_separator_regex else re.escape(separator)\\n        splits = _split_text_with_regex(text, _separator, self._keep_separator)\\n\\n        # Now go merging things, recursively splitting longer texts.\\n        _good_splits = []\\n        _separator = \"\" if self._keep_separator else separator\\n        for s in splits:\\n            if self._length_function(s) < self._chunk_size:\\n                _good_splits.append(s)\\n            else:\\n                if _good_splits:\\n                    merged_text = self._merge_splits(_good_splits, _separator)\\n                    final_chunks.extend(merged_text)\\n                    _good_splits = []\\n                if not new_separators:\\n                    final_chunks.append(s)\\n                else:\\n                    other_info = self._split_text(s, new_separators)\\n                    final_chunks.extend(other_info)\\n        if _good_splits:\\n            merged_text = self._merge_splits(_good_splits, _separator)\\n            final_chunks.extend(merged_text)\\n        return final_chunks\\n\\n    def split_text(self, text: str) -> List[str]:\\n        return self._split_text(text, self._separators)\\n\\n    @classmethod\\n    def from_language(\\n        cls, language: Language, **kwargs: Any\\n    ) -> RecursiveCharacterTextSplitter:\\n        separators = cls.get_separators_for_language(language)\\n        return cls(separators=separators, is_separator_regex=True, **kwargs)',\n",
              "  'class RecursiveCharacterTextSplitter(TextSplitter):\\n    \"\"\"Splitting text by recursively look at characters.\\n\\n    Recursively tries to split by different characters to find one\\n    that works.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        separators: Optional[List[str]] = None,\\n        keep_separator: bool = True,\\n        is_separator_regex: bool = False,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(keep_separator=keep_separator, **kwargs)\\n        self._separators = separators or [\"\\\\n\\\\n\", \"\\\\n\", \" \", \"\"]\\n        self._is_separator_regex = is_separator_regex\\n\\n    def _split_text(self, text: str, separators: List[str]) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        final_chunks = []\\n        # Get appropriate separator to use\\n        separator = separators[-1]\\n        new_separators = []\\n        for i, _s in enumerate(separators):\\n            _separator = _s if self._is_separator_regex else re.escape(_s)\\n            if _s == \"\":\\n                separator = _s\\n                break\\n            if re.search(_separator, text):\\n                separator = _s\\n                new_separators = separators[i + 1 :]\\n                break\\n\\n        _separator = separator if self._is_separator_regex else re.escape(separator)\\n        splits = _split_text_with_regex(text, _separator, self._keep_separator)',\n",
              "  'class Language(str, Enum):\\n    \"\"\"Enum of the programming languages.\"\"\"\\n\\n    CPP = \"cpp\"\\n    GO = \"go\"\\n    JAVA = \"java\"\\n    KOTLIN = \"kotlin\"\\n    JS = \"js\"\\n    TS = \"ts\"\\n    PHP = \"php\"\\n    PROTO = \"proto\"\\n    PYTHON = \"python\"\\n    RST = \"rst\"\\n    RUBY = \"ruby\"\\n    RUST = \"rust\"\\n    SCALA = \"scala\"\\n    SWIFT = \"swift\"\\n    MARKDOWN = \"markdown\"\\n    LATEX = \"latex\"\\n    HTML = \"html\"\\n    SOL = \"sol\"\\n    CSHARP = \"csharp\"\\n    COBOL = \"cobol\"',\n",
              "  'def split_text(self, text: str) -> List[str]:\\n        def encode_strip_start_and_stop_token_ids(text: str) -> List[int]:\\n            return self._encode(text)[1:-1]\\n\\n        tokenizer = Tokenizer(\\n            chunk_overlap=self._chunk_overlap,\\n            tokens_per_chunk=self.tokens_per_chunk,\\n            decode=self.tokenizer.decode,\\n            encode=encode_strip_start_and_stop_token_ids,\\n        )\\n\\n        return split_text_on_tokens(text=text, tokenizer=tokenizer)\\n\\n    def count_tokens(self, *, text: str) -> int:\\n        return len(self._encode(text))\\n\\n    _max_length_equal_32_bit_integer: int = 2**32\\n\\n    def _encode(self, text: str) -> List[int]:\\n        token_ids_with_start_and_end_token_ids = self.tokenizer.encode(\\n            text,\\n            max_length=self._max_length_equal_32_bit_integer,\\n            truncation=\"do_not_truncate\",\\n        )\\n        return token_ids_with_start_and_end_token_ids',\n",
              "  'class SentenceTransformersTokenTextSplitter(TextSplitter):\\n    \"\"\"Splitting text to tokens using sentence model tokenizer.\"\"\"\\n\\n    def __init__(\\n        self,\\n        chunk_overlap: int = 50,\\n        model_name: str = \"sentence-transformers/all-mpnet-base-v2\",\\n        tokens_per_chunk: Optional[int] = None,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(**kwargs, chunk_overlap=chunk_overlap)\\n\\n        try:\\n            from sentence_transformers import SentenceTransformer\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import sentence_transformer python package. \"\\n                \"This is needed in order to for SentenceTransformersTokenTextSplitter. \"\\n                \"Please install it with `pip install sentence-transformers`.\"\\n            )\\n\\n        self.model_name = model_name\\n        self._model = SentenceTransformer(self.model_name)\\n        self.tokenizer = self._model.tokenizer\\n        self._initialize_chunk_configuration(tokens_per_chunk=tokens_per_chunk)\\n\\n    def _initialize_chunk_configuration(\\n        self, *, tokens_per_chunk: Optional[int]\\n    ) -> None:\\n        self.maximum_tokens_per_chunk = cast(int, self._model.max_seq_length)\\n\\n        if tokens_per_chunk is None:\\n            self.tokens_per_chunk = self.maximum_tokens_per_chunk\\n        else:\\n            self.tokens_per_chunk = tokens_per_chunk\\n\\n        if self.tokens_per_chunk > self.maximum_tokens_per_chunk:\\n            raise ValueError(\\n                f\"The token limit of the models \\'{self.model_name}\\'\"\\n                f\" is: {self.maximum_tokens_per_chunk}.\"\\n                f\" Argument tokens_per_chunk={self.tokens_per_chunk}\"\\n                f\" > maximum token limit.\"\\n            )\\n\\n    def split_text(self, text: str) -> List[str]:\\n        def encode_strip_start_and_stop_token_ids(text: str) -> List[int]:\\n            return self._encode(text)[1:-1]',\n",
              "  '# Push the current header to the stack\\n                        header: HeaderType = {\\n                            \"level\": current_header_level,\\n                            \"name\": name,\\n                            \"data\": stripped_line[len(sep) :].strip(),\\n                        }\\n                        header_stack.append(header)\\n                        # Update initial_metadata with the current header\\n                        initial_metadata[name] = header[\"data\"]\\n\\n                    # Add the previous line to the lines_with_metadata\\n                    # only if current_content is not empty\\n                    if current_content:\\n                        lines_with_metadata.append(\\n                            {\\n                                \"content\": \"\\\\n\".join(current_content),\\n                                \"metadata\": current_metadata.copy(),\\n                            }\\n                        )\\n                        current_content.clear()\\n\\n                    break\\n            else:\\n                if stripped_line:\\n                    current_content.append(stripped_line)\\n                elif current_content:\\n                    lines_with_metadata.append(\\n                        {\\n                            \"content\": \"\\\\n\".join(current_content),\\n                            \"metadata\": current_metadata.copy(),\\n                        }\\n                    )\\n                    current_content.clear()\\n\\n            current_metadata = initial_metadata.copy()\\n\\n        if current_content:\\n            lines_with_metadata.append(\\n                {\"content\": \"\\\\n\".join(current_content), \"metadata\": current_metadata}\\n            )',\n",
              "  'def split_text(self, text: str) -> List[str]:\\n        def encode_strip_start_and_stop_token_ids(text: str) -> List[int]:\\n            return self._encode(text)[1:-1]\\n\\n        tokenizer = Tokenizer(\\n            chunk_overlap=self._chunk_overlap,\\n            tokens_per_chunk=self.tokens_per_chunk,\\n            decode=self.tokenizer.decode,\\n            encode=encode_strip_start_and_stop_token_ids,\\n        )\\n\\n        return split_text_on_tokens(text=text, tokenizer=tokenizer)\\n\\n    def count_tokens(self, *, text: str) -> int:\\n        return len(self._encode(text))\\n\\n    _max_length_equal_32_bit_integer: int = 2**32\\n\\n    def _encode(self, text: str) -> List[int]:\\n        token_ids_with_start_and_end_token_ids = self.tokenizer.encode(\\n            text,\\n            max_length=self._max_length_equal_32_bit_integer,\\n            truncation=\"do_not_truncate\",\\n        )\\n        return token_ids_with_start_and_end_token_ids',\n",
              "  'class SentenceTransformersTokenTextSplitter(TextSplitter):\\n    \"\"\"Splitting text to tokens using sentence model tokenizer.\"\"\"\\n\\n    def __init__(\\n        self,\\n        chunk_overlap: int = 50,\\n        model_name: str = \"sentence-transformers/all-mpnet-base-v2\",\\n        tokens_per_chunk: Optional[int] = None,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(**kwargs, chunk_overlap=chunk_overlap)\\n\\n        try:\\n            from sentence_transformers import SentenceTransformer\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import sentence_transformer python package. \"\\n                \"This is needed in order to for SentenceTransformersTokenTextSplitter. \"\\n                \"Please install it with `pip install sentence-transformers`.\"\\n            )\\n\\n        self.model_name = model_name\\n        self._model = SentenceTransformer(self.model_name)\\n        self.tokenizer = self._model.tokenizer\\n        self._initialize_chunk_configuration(tokens_per_chunk=tokens_per_chunk)\\n\\n    def _initialize_chunk_configuration(\\n        self, *, tokens_per_chunk: Optional[int]\\n    ) -> None:\\n        self.maximum_tokens_per_chunk = cast(int, self._model.max_seq_length)\\n\\n        if tokens_per_chunk is None:\\n            self.tokens_per_chunk = self.maximum_tokens_per_chunk\\n        else:\\n            self.tokens_per_chunk = tokens_per_chunk\\n\\n        if self.tokens_per_chunk > self.maximum_tokens_per_chunk:\\n            raise ValueError(\\n                f\"The token limit of the models \\'{self.model_name}\\'\"\\n                f\" is: {self.maximum_tokens_per_chunk}.\"\\n                f\" Argument tokens_per_chunk={self.tokens_per_chunk}\"\\n                f\" > maximum token limit.\"\\n            )\\n\\n    def split_text(self, text: str) -> List[str]:\\n        def encode_strip_start_and_stop_token_ids(text: str) -> List[int]:\\n            return self._encode(text)[1:-1]',\n",
              "  'class TokenTextSplitter(TextSplitter):\\n    \"\"\"Splitting text to tokens using model tokenizer.\"\"\"\\n\\n    def __init__(\\n        self,\\n        encoding_name: str = \"gpt2\",\\n        model_name: Optional[str] = None,\\n        allowed_special: Union[Literal[\"all\"], AbstractSet[str]] = set(),\\n        disallowed_special: Union[Literal[\"all\"], Collection[str]] = \"all\",\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(**kwargs)\\n        try:\\n            import tiktoken\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import tiktoken python package. \"\\n                \"This is needed in order to for TokenTextSplitter. \"\\n                \"Please install it with `pip install tiktoken`.\"\\n            )\\n\\n        if model_name is not None:\\n            enc = tiktoken.encoding_for_model(model_name)\\n        else:\\n            enc = tiktoken.get_encoding(encoding_name)\\n        self._tokenizer = enc\\n        self._allowed_special = allowed_special\\n        self._disallowed_special = disallowed_special\\n\\n    def split_text(self, text: str) -> List[str]:\\n        def _encode(_text: str) -> List[int]:\\n            return self._tokenizer.encode(\\n                _text,\\n                allowed_special=self._allowed_special,\\n                disallowed_special=self._disallowed_special,\\n            )\\n\\n        tokenizer = Tokenizer(\\n            chunk_overlap=self._chunk_overlap,\\n            tokens_per_chunk=self._chunk_size,\\n            decode=self._tokenizer.decode,\\n            encode=_encode,\\n        )\\n\\n        return split_text_on_tokens(text=text, tokenizer=tokenizer)',\n",
              "  'class Tokenizer:\\n    \"\"\"Tokenizer data class.\"\"\"\\n\\n    chunk_overlap: int\\n    \"\"\"Overlap in tokens between chunks\"\"\"\\n    tokens_per_chunk: int\\n    \"\"\"Maximum number of tokens per chunk\"\"\"\\n    decode: Callable[[List[int]], str]\\n    \"\"\" Function to decode a list of token ids to a string\"\"\"\\n    encode: Callable[[str], List[int]]\\n    \"\"\" Function to encode a string to a list of token ids\"\"\"\\n\\n\\ndef split_text_on_tokens(*, text: str, tokenizer: Tokenizer) -> List[str]:\\n    \"\"\"Split incoming text and return chunks using tokenizer.\"\"\"\\n    splits: List[str] = []\\n    input_ids = tokenizer.encode(text)\\n    start_idx = 0\\n    cur_idx = min(start_idx + tokenizer.tokens_per_chunk, len(input_ids))\\n    chunk_ids = input_ids[start_idx:cur_idx]\\n    while start_idx < len(input_ids):\\n        splits.append(tokenizer.decode(chunk_ids))\\n        if cur_idx == len(input_ids):\\n            break\\n        start_idx += tokenizer.tokens_per_chunk - tokenizer.chunk_overlap\\n        cur_idx = min(start_idx + tokenizer.tokens_per_chunk, len(input_ids))\\n        chunk_ids = input_ids[start_idx:cur_idx]\\n    return splits',\n",
              "  '# map xhtml namespace prefix\\n        ns_map = {\"h\": \"http://www.w3.org/1999/xhtml\"}\\n\\n        # build list of elements from DOM\\n        elements = []\\n        for element in result_dom.findall(\"*//*\", ns_map):\\n            if element.findall(\"*[@class=\\'headers\\']\") or element.findall(\\n                \"*[@class=\\'chunk\\']\"\\n            ):\\n                elements.append(\\n                    ElementType(\\n                        url=file,\\n                        xpath=\"\".join(\\n                            [\\n                                node.text\\n                                for node in element.findall(\"*[@class=\\'xpath\\']\", ns_map)\\n                            ]\\n                        ),\\n                        content=\"\".join(\\n                            [\\n                                node.text\\n                                for node in element.findall(\"*[@class=\\'chunk\\']\", ns_map)\\n                            ]\\n                        ),\\n                        metadata={\\n                            # Add text of specified headers to metadata using header\\n                            # mapping.\\n                            header_mapping[node.tag]: node.text\\n                            for node in filter(\\n                                lambda x: x.tag in header_filter,\\n                                element.findall(\"*[@class=\\'headers\\']/*\", ns_map),\\n                            )\\n                        },\\n                    )\\n                )\\n\\n        if not self.return_each_element:\\n            return self.aggregate_elements_to_chunks(elements)\\n        else:\\n            return [\\n                Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n                for chunk in elements\\n            ]\\n\\n\\n# should be in newer Python versions (3.10+)\\n# @dataclass(frozen=True, kw_only=True, slots=True)\\n@dataclass(frozen=True)',\n",
              "  'return [\\n            Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n            for chunk in aggregated_chunks\\n        ]\\n\\n    def split_text_from_url(self, url: str) -> List[Document]:\\n        \"\"\"Split HTML from web URL\\n\\n        Args:\\n            url: web URL\\n        \"\"\"\\n        r = requests.get(url)\\n        return self.split_text_from_file(BytesIO(r.content))\\n\\n    def split_text(self, text: str) -> List[Document]:\\n        \"\"\"Split HTML text string\\n\\n        Args:\\n            text: HTML text\\n        \"\"\"\\n        return self.split_text_from_file(StringIO(text))\\n\\n    def split_text_from_file(self, file: Any) -> List[Document]:\\n        \"\"\"Split HTML file\\n\\n        Args:\\n            file: HTML file\\n        \"\"\"\\n        try:\\n            from lxml import etree\\n        except ImportError as e:\\n            raise ImportError(\\n                \"Unable to import lxml, please install with `pip install lxml`.\"\\n            ) from e\\n        # use lxml library to parse html document and return xml ElementTree\\n        parser = etree.HTMLParser()\\n        tree = etree.parse(file, parser)\\n\\n        # document transformation for \"structure-aware\" chunking is handled with xsl.\\n        # see comments in html_chunks_with_headers.xslt for more detailed information.\\n        xslt_path = (\\n            pathlib.Path(__file__).parent\\n            / \"document_transformers/xsl/html_chunks_with_headers.xslt\"\\n        )\\n        xslt_tree = etree.parse(xslt_path)\\n        transform = etree.XSLT(xslt_tree)\\n        result = transform(tree)\\n        result_dom = etree.fromstring(str(result))\\n\\n        # create filter and mapping for header metadata\\n        header_filter = [header[0] for header in self.headers_to_split_on]\\n        header_mapping = dict(self.headers_to_split_on)\\n\\n        # map xhtml namespace prefix\\n        ns_map = {\"h\": \"http://www.w3.org/1999/xhtml\"}',\n",
              "  'class HTMLHeaderTextSplitter:\\n    \"\"\"\\n    Splitting HTML files based on specified headers.\\n    Requires lxml package.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        headers_to_split_on: List[Tuple[str, str]],\\n        return_each_element: bool = False,\\n    ):\\n        \"\"\"Create a new HTMLHeaderTextSplitter.\\n\\n        Args:\\n            headers_to_split_on: list of tuples of headers we want to track mapped to\\n                (arbitrary) keys for metadata. Allowed header values: h1, h2, h3, h4,\\n                h5, h6 e.g. [(\"h1\", \"Header 1\"), (\"h2\", \"Header 2)].\\n            return_each_element: Return each element w/ associated headers.\\n        \"\"\"\\n        # Output element-by-element or aggregated into chunks w/ common headers\\n        self.return_each_element = return_each_element\\n        self.headers_to_split_on = sorted(headers_to_split_on)\\n\\n    def aggregate_elements_to_chunks(\\n        self, elements: List[ElementType]\\n    ) -> List[Document]:\\n        \"\"\"Combine elements with common metadata into chunks\\n\\n        Args:\\n            elements: HTML element content with associated identifying info and metadata\\n        \"\"\"\\n        aggregated_chunks: List[ElementType] = []\\n\\n        for element in elements:\\n            if (\\n                aggregated_chunks\\n                and aggregated_chunks[-1][\"metadata\"] == element[\"metadata\"]\\n            ):\\n                # If the last element in the aggregated list\\n                # has the same metadata as the current element,\\n                # append the current content to the last element\\'s content\\n                aggregated_chunks[-1][\"content\"] += \"  \\\\n\" + element[\"content\"]\\n            else:\\n                # Otherwise, append the current element to the aggregated list\\n                aggregated_chunks.append(element)\\n\\n        return [\\n            Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n            for chunk in aggregated_chunks\\n        ]',\n",
              "  'class ElementType(TypedDict):\\n    \"\"\"Element type as typed dict.\"\"\"\\n\\n    url: str\\n    xpath: str\\n    content: str\\n    metadata: Dict[str, str]',\n",
              "  'if current_content:\\n            lines_with_metadata.append(\\n                {\"content\": \"\\\\n\".join(current_content), \"metadata\": current_metadata}\\n            )\\n\\n        # lines_with_metadata has each line with associated header metadata\\n        # aggregate these into chunks based on common metadata\\n        if not self.return_each_line:\\n            return self.aggregate_lines_to_chunks(lines_with_metadata)\\n        else:\\n            return [\\n                Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n                for chunk in lines_with_metadata\\n            ]',\n",
              "  'class TokenTextSplitter(TextSplitter):\\n    \"\"\"Splitting text to tokens using model tokenizer.\"\"\"\\n\\n    def __init__(\\n        self,\\n        encoding_name: str = \"gpt2\",\\n        model_name: Optional[str] = None,\\n        allowed_special: Union[Literal[\"all\"], AbstractSet[str]] = set(),\\n        disallowed_special: Union[Literal[\"all\"], Collection[str]] = \"all\",\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(**kwargs)\\n        try:\\n            import tiktoken\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import tiktoken python package. \"\\n                \"This is needed in order to for TokenTextSplitter. \"\\n                \"Please install it with `pip install tiktoken`.\"\\n            )\\n\\n        if model_name is not None:\\n            enc = tiktoken.encoding_for_model(model_name)\\n        else:\\n            enc = tiktoken.get_encoding(encoding_name)\\n        self._tokenizer = enc\\n        self._allowed_special = allowed_special\\n        self._disallowed_special = disallowed_special\\n\\n    def split_text(self, text: str) -> List[str]:\\n        def _encode(_text: str) -> List[int]:\\n            return self._tokenizer.encode(\\n                _text,\\n                allowed_special=self._allowed_special,\\n                disallowed_special=self._disallowed_special,\\n            )\\n\\n        tokenizer = Tokenizer(\\n            chunk_overlap=self._chunk_overlap,\\n            tokens_per_chunk=self._chunk_size,\\n            decode=self._tokenizer.decode,\\n            encode=_encode,\\n        )\\n\\n        return split_text_on_tokens(text=text, tokenizer=tokenizer)',\n",
              "  'if in_code_block:\\n                current_content.append(stripped_line)\\n                continue\\n\\n            # Check each line against each of the header types (e.g., #, ##)\\n            for sep, name in self.headers_to_split_on:\\n                # Check if line starts with a header that we intend to split on\\n                if stripped_line.startswith(sep) and (\\n                    # Header with no text OR header is followed by space\\n                    # Both are valid conditions that sep is being used a header\\n                    len(stripped_line) == len(sep) or stripped_line[len(sep)] == \" \"\\n                ):\\n                    # Ensure we are tracking the header as metadata\\n                    if name is not None:\\n                        # Get the current header level\\n                        current_header_level = sep.count(\"#\")\\n\\n                        # Pop out headers of lower or same level from the stack\\n                        while (\\n                            header_stack\\n                            and header_stack[-1][\"level\"] >= current_header_level\\n                        ):\\n                            # We have encountered a new header\\n                            # at the same or higher level\\n                            popped_header = header_stack.pop()\\n                            # Clear the metadata for the\\n                            # popped header in initial_metadata\\n                            if popped_header[\"name\"] in initial_metadata:\\n                                initial_metadata.pop(popped_header[\"name\"])',\n",
              "  'def split_text(self, text: str) -> List[Document]:\\n        \"\"\"Split markdown file\\n        Args:\\n            text: Markdown file\"\"\"\\n\\n        # Split the input text by newline character (\"\\\\n\").\\n        lines = text.split(\"\\\\n\")\\n        # Final output\\n        lines_with_metadata: List[LineType] = []\\n        # Content and metadata of the chunk currently being processed\\n        current_content: List[str] = []\\n        current_metadata: Dict[str, str] = {}\\n        # Keep track of the nested header structure\\n        # header_stack: List[Dict[str, Union[int, str]]] = []\\n        header_stack: List[HeaderType] = []\\n        initial_metadata: Dict[str, str] = {}\\n\\n        in_code_block = False\\n        opening_fence = \"\"\\n\\n        for line in lines:\\n            stripped_line = line.strip()\\n\\n            if not in_code_block:\\n                # Exclude inline code spans\\n                if stripped_line.startswith(\"```\") and stripped_line.count(\"```\") == 1:\\n                    in_code_block = True\\n                    opening_fence = \"```\"\\n                elif stripped_line.startswith(\"~~~\"):\\n                    in_code_block = True\\n                    opening_fence = \"~~~\"\\n            else:\\n                if stripped_line.startswith(opening_fence):\\n                    in_code_block = False\\n                    opening_fence = \"\"\\n\\n            if in_code_block:\\n                current_content.append(stripped_line)\\n                continue',\n",
              "  'class MarkdownHeaderTextSplitter:\\n    \"\"\"Splitting markdown files based on specified headers.\"\"\"\\n\\n    def __init__(\\n        self, headers_to_split_on: List[Tuple[str, str]], return_each_line: bool = False\\n    ):\\n        \"\"\"Create a new MarkdownHeaderTextSplitter.\\n\\n        Args:\\n            headers_to_split_on: Headers we want to track\\n            return_each_line: Return each line w/ associated headers\\n        \"\"\"\\n        # Output line-by-line or aggregated into chunks w/ common headers\\n        self.return_each_line = return_each_line\\n        # Given the headers we want to split on,\\n        # (e.g., \"#, ##, etc\") order by length\\n        self.headers_to_split_on = sorted(\\n            headers_to_split_on, key=lambda split: len(split[0]), reverse=True\\n        )\\n\\n    def aggregate_lines_to_chunks(self, lines: List[LineType]) -> List[Document]:\\n        \"\"\"Combine lines with common metadata into chunks\\n        Args:\\n            lines: Line of text / associated header metadata\\n        \"\"\"\\n        aggregated_chunks: List[LineType] = []\\n\\n        for line in lines:\\n            if (\\n                aggregated_chunks\\n                and aggregated_chunks[-1][\"metadata\"] == line[\"metadata\"]\\n            ):\\n                # If the last line in the aggregated list\\n                # has the same metadata as the current line,\\n                # append the current content to the last lines\\'s content\\n                aggregated_chunks[-1][\"content\"] += \"  \\\\n\" + line[\"content\"]\\n            else:\\n                # Otherwise, append the current line to the aggregated list\\n                aggregated_chunks.append(line)\\n\\n        return [\\n            Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n            for chunk in aggregated_chunks\\n        ]\\n\\n    def split_text(self, text: str) -> List[Document]:\\n        \"\"\"Split markdown file\\n        Args:\\n            text: Markdown file\"\"\"',\n",
              "  'class CharacterTextSplitter(TextSplitter):\\n    \"\"\"Splitting text that looks at characters.\"\"\"\\n\\n    def __init__(\\n        self, separator: str = \"\\\\n\\\\n\", is_separator_regex: bool = False, **kwargs: Any\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(**kwargs)\\n        self._separator = separator\\n        self._is_separator_regex = is_separator_regex\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        # First we naively split the large input into a bunch of smaller ones.\\n        separator = (\\n            self._separator if self._is_separator_regex else re.escape(self._separator)\\n        )\\n        splits = _split_text_with_regex(text, separator, self._keep_separator)\\n        _separator = \"\" if self._keep_separator else self._separator\\n        return self._merge_splits(splits, _separator)\\n\\n\\nclass LineType(TypedDict):\\n    \"\"\"Line type as typed dict.\"\"\"\\n\\n    metadata: Dict[str, str]\\n    content: str\\n\\n\\nclass HeaderType(TypedDict):\\n    \"\"\"Header type as typed dict.\"\"\"\\n\\n    level: int\\n    name: str\\n    data: str',\n",
              "  'if issubclass(cls, TokenTextSplitter):\\n            extra_kwargs = {\\n                \"encoding_name\": encoding_name,\\n                \"model_name\": model_name,\\n                \"allowed_special\": allowed_special,\\n                \"disallowed_special\": disallowed_special,\\n            }\\n            kwargs = {**kwargs, **extra_kwargs}\\n\\n        return cls(length_function=_tiktoken_encoder, **kwargs)\\n\\n    def transform_documents(\\n        self, documents: Sequence[Document], **kwargs: Any\\n    ) -> Sequence[Document]:\\n        \"\"\"Transform sequence of documents by splitting them.\"\"\"\\n        return self.split_documents(list(documents))\\n\\n    async def atransform_documents(\\n        self, documents: Sequence[Document], **kwargs: Any\\n    ) -> Sequence[Document]:\\n        \"\"\"Asynchronously transform a sequence of documents by splitting them.\"\"\"\\n        return await asyncio.get_running_loop().run_in_executor(\\n            None, partial(self.transform_documents, **kwargs), documents\\n        )',\n",
              "  'if not isinstance(tokenizer, PreTrainedTokenizerBase):\\n                raise ValueError(\\n                    \"Tokenizer received was not an instance of PreTrainedTokenizerBase\"\\n                )\\n\\n            def _huggingface_tokenizer_length(text: str) -> int:\\n                return len(tokenizer.encode(text))\\n\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import transformers python package. \"\\n                \"Please install it with `pip install transformers`.\"\\n            )\\n        return cls(length_function=_huggingface_tokenizer_length, **kwargs)\\n\\n    @classmethod\\n    def from_tiktoken_encoder(\\n        cls: Type[TS],\\n        encoding_name: str = \"gpt2\",\\n        model_name: Optional[str] = None,\\n        allowed_special: Union[Literal[\"all\"], AbstractSet[str]] = set(),\\n        disallowed_special: Union[Literal[\"all\"], Collection[str]] = \"all\",\\n        **kwargs: Any,\\n    ) -> TS:\\n        \"\"\"Text splitter that uses tiktoken encoder to count length.\"\"\"\\n        try:\\n            import tiktoken\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import tiktoken python package. \"\\n                \"This is needed in order to calculate max_tokens_for_prompt. \"\\n                \"Please install it with `pip install tiktoken`.\"\\n            )\\n\\n        if model_name is not None:\\n            enc = tiktoken.encoding_for_model(model_name)\\n        else:\\n            enc = tiktoken.get_encoding(encoding_name)\\n\\n        def _tiktoken_encoder(text: str) -> int:\\n            return len(\\n                enc.encode(\\n                    text,\\n                    allowed_special=allowed_special,\\n                    disallowed_special=disallowed_special,\\n                )\\n            )',\n",
              "  'docs = []\\n        current_doc: List[str] = []\\n        total = 0\\n        for d in splits:\\n            _len = self._length_function(d)\\n            if (\\n                total + _len + (separator_len if len(current_doc) > 0 else 0)\\n                > self._chunk_size\\n            ):\\n                if total > self._chunk_size:\\n                    logger.warning(\\n                        f\"Created a chunk of size {total}, \"\\n                        f\"which is longer than the specified {self._chunk_size}\"\\n                    )\\n                if len(current_doc) > 0:\\n                    doc = self._join_docs(current_doc, separator)\\n                    if doc is not None:\\n                        docs.append(doc)\\n                    # Keep on popping if:\\n                    # - we have a larger chunk than in the chunk overlap\\n                    # - or if we still have any chunks and the length is long\\n                    while total > self._chunk_overlap or (\\n                        total + _len + (separator_len if len(current_doc) > 0 else 0)\\n                        > self._chunk_size\\n                        and total > 0\\n                    ):\\n                        total -= self._length_function(current_doc[0]) + (\\n                            separator_len if len(current_doc) > 1 else 0\\n                        )\\n                        current_doc = current_doc[1:]\\n            current_doc.append(d)\\n            total += _len + (separator_len if len(current_doc) > 1 else 0)\\n        doc = self._join_docs(current_doc, separator)\\n        if doc is not None:\\n            docs.append(doc)\\n        return docs\\n\\n    @classmethod\\n    def from_huggingface_tokenizer(cls, tokenizer: Any, **kwargs: Any) -> TextSplitter:\\n        \"\"\"Text splitter that uses HuggingFace tokenizer to count length.\"\"\"\\n        try:\\n            from transformers import PreTrainedTokenizerBase',\n",
              "  '@abstractmethod\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split text into multiple components.\"\"\"\\n\\n    def create_documents(\\n        self, texts: List[str], metadatas: Optional[List[dict]] = None\\n    ) -> List[Document]:\\n        \"\"\"Create documents from a list of texts.\"\"\"\\n        _metadatas = metadatas or [{}] * len(texts)\\n        documents = []\\n        for i, text in enumerate(texts):\\n            index = -1\\n            for chunk in self.split_text(text):\\n                metadata = copy.deepcopy(_metadatas[i])\\n                if self._add_start_index:\\n                    index = text.find(chunk, index + 1)\\n                    metadata[\"start_index\"] = index\\n                new_doc = Document(page_content=chunk, metadata=metadata)\\n                documents.append(new_doc)\\n        return documents\\n\\n    def split_documents(self, documents: Iterable[Document]) -> List[Document]:\\n        \"\"\"Split documents.\"\"\"\\n        texts, metadatas = [], []\\n        for doc in documents:\\n            texts.append(doc.page_content)\\n            metadatas.append(doc.metadata)\\n        return self.create_documents(texts, metadatas=metadatas)\\n\\n    def _join_docs(self, docs: List[str], separator: str) -> Optional[str]:\\n        text = separator.join(docs)\\n        if self._strip_whitespace:\\n            text = text.strip()\\n        if text == \"\":\\n            return None\\n        else:\\n            return text\\n\\n    def _merge_splits(self, splits: Iterable[str], separator: str) -> List[str]:\\n        # We now want to combine these smaller pieces into medium size\\n        # chunks to send to the LLM.\\n        separator_len = self._length_function(separator)',\n",
              "  '\"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.KOTLIN:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along method definitions\\n                \"\\\\npublic \",\\n                \"\\\\nprotected \",\\n                \"\\\\nprivate \",\\n                \"\\\\ninternal \",\\n                \"\\\\ncompanion \",\\n                \"\\\\nfun \",\\n                \"\\\\nval \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nwhen \",\\n                \"\\\\ncase \",\\n                \"\\\\nelse \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.JS:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunction \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                \"\\\\nvar \",\\n                \"\\\\nclass \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                \"\\\\ndefault \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.TS:\\n            return [\\n                \"\\\\nenum \",\\n                \"\\\\ninterface \",\\n                \"\\\\nnamespace \",\\n                \"\\\\ntype \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along function definitions\\n                \"\\\\nfunction \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements',\n",
              "  'class PythonCodeTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Python syntax.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a PythonCodeTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.PYTHON)\\n        super().__init__(separators=separators, **kwargs)\\n\\n\\nclass MarkdownTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Markdown-formatted headings.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a MarkdownTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.MARKDOWN)\\n        super().__init__(separators=separators, **kwargs)\\n\\n\\nclass LatexTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Latex-formatted layout elements.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a LatexTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.LATEX)\\n        super().__init__(separators=separators, **kwargs)',\n",
              "  'class NLTKTextSplitter(TextSplitter):\\n    \"\"\"Splitting text using NLTK package.\"\"\"\\n\\n    def __init__(\\n        self, separator: str = \"\\\\n\\\\n\", language: str = \"english\", **kwargs: Any\\n    ) -> None:\\n        \"\"\"Initialize the NLTK splitter.\"\"\"\\n        super().__init__(**kwargs)\\n        try:\\n            from nltk.tokenize import sent_tokenize\\n\\n            self._tokenizer = sent_tokenize\\n        except ImportError:\\n            raise ImportError(\\n                \"NLTK is not installed, please install it with `pip install nltk`.\"\\n            )\\n        self._separator = separator\\n        self._language = language\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        # First we naively split the large input into a bunch of smaller ones.\\n        splits = self._tokenizer(text, language=self._language)\\n        return self._merge_splits(splits, self._separator)\\n\\n\\nclass SpacyTextSplitter(TextSplitter):\\n    \"\"\"Splitting text using Spacy package.\\n\\n\\n    Per default, Spacy\\'s `en_core_web_sm` model is used and\\n    its default max_length is 1000000 (it is the length of maximum character\\n    this model takes which can be increased for large files). For a faster, but\\n    potentially less accurate splitting, you can use `pipeline=\\'sentencizer\\'`.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        separator: str = \"\\\\n\\\\n\",\\n        pipeline: str = \"en_core_web_sm\",\\n        max_length: int = 1_000_000,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Initialize the spacy text splitter.\"\"\"\\n        super().__init__(**kwargs)\\n        self._tokenizer = _make_spacy_pipeline_for_splitting(\\n            pipeline, max_length=max_length\\n        )\\n        self._separator = separator\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        splits = (s.text for s in self._tokenizer(text).sents)\\n        return self._merge_splits(splits, self._separator)\\n\\n\\n# For backwards compatibility',\n",
              "  'else:\\n            raise ValueError(\\n                f\"Language {language} is not supported! \"\\n                f\"Please choose from {list(Language)}\"\\n            )',\n",
              "  '\"\\\\nevent \",\\n                \"\\\\nmodifier \",\\n                \"\\\\nerror \",\\n                \"\\\\nstruct \",\\n                \"\\\\nenum \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo while \",\\n                \"\\\\nassembly \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.COBOL:\\n            return [\\n                # Split along divisions\\n                \"\\\\nIDENTIFICATION DIVISION.\",\\n                \"\\\\nENVIRONMENT DIVISION.\",\\n                \"\\\\nDATA DIVISION.\",\\n                \"\\\\nPROCEDURE DIVISION.\",\\n                # Split along sections within DATA DIVISION\\n                \"\\\\nWORKING-STORAGE SECTION.\",\\n                \"\\\\nLINKAGE SECTION.\",\\n                \"\\\\nFILE SECTION.\",\\n                # Split along sections within PROCEDURE DIVISION\\n                \"\\\\nINPUT-OUTPUT SECTION.\",\\n                # Split along paragraphs and common statements\\n                \"\\\\nOPEN \",\\n                \"\\\\nCLOSE \",\\n                \"\\\\nREAD \",\\n                \"\\\\nWRITE \",\\n                \"\\\\nIF \",\\n                \"\\\\nELSE \",\\n                \"\\\\nMOVE \",\\n                \"\\\\nPERFORM \",\\n                \"\\\\nUNTIL \",\\n                \"\\\\nVARYING \",\\n                \"\\\\nACCEPT \",\\n                \"\\\\nDISPLAY \",\\n                \"\\\\nSTOP RUN.\",\\n                # Split by the normal type of lines\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]',\n",
              "  '\"<nav\",\\n                # Head\\n                \"<head\",\\n                \"<style\",\\n                \"<script\",\\n                \"<meta\",\\n                \"<title\",\\n                \"\",\\n            ]\\n        elif language == Language.CSHARP:\\n            return [\\n                \"\\\\ninterface \",\\n                \"\\\\nenum \",\\n                \"\\\\nimplements \",\\n                \"\\\\ndelegate \",\\n                \"\\\\nevent \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nabstract \",\\n                # Split along method definitions\\n                \"\\\\npublic \",\\n                \"\\\\nprotected \",\\n                \"\\\\nprivate \",\\n                \"\\\\nstatic \",\\n                \"\\\\nreturn \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\ncontinue \",\\n                \"\\\\nfor \",\\n                \"\\\\nforeach \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\nbreak \",\\n                \"\\\\ncase \",\\n                \"\\\\nelse \",\\n                # Split by exceptions\\n                \"\\\\ntry \",\\n                \"\\\\nthrow \",\\n                \"\\\\nfinally \",\\n                \"\\\\ncatch \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SOL:\\n            return [\\n                # Split along compiler information definitions\\n                \"\\\\npragma \",\\n                \"\\\\nusing \",\\n                # Split along contract definitions\\n                \"\\\\ncontract \",\\n                \"\\\\ninterface \",\\n                \"\\\\nlibrary \",\\n                # Split along method definitions\\n                \"\\\\nconstructor \",\\n                \"\\\\ntype \",\\n                \"\\\\nfunction \",\\n                \"\\\\nevent \",\\n                \"\\\\nmodifier \",\\n                \"\\\\nerror \",\\n                \"\\\\nstruct \",\\n                \"\\\\nenum \",\\n                # Split along control flow statements',\n",
              "  '\"\\\\n\\\\\\\\*\\\\\\\\*\\\\\\\\*+\\\\n\",\\n                \"\\\\n---+\\\\n\",\\n                \"\\\\n___+\\\\n\",\\n                # Note that this splitter doesn\\'t handle horizontal lines defined\\n                # by *three or more* of ***, ---, or ___, but this is not handled\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.LATEX:\\n            return [\\n                # First, try to split along Latex sections\\n                \"\\\\n\\\\\\\\\\\\\\\\chapter{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\section{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\subsection{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\subsubsection{\",\\n                # Now split by environments\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{enumerate}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{itemize}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{description}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{list}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{quote}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{quotation}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{verse}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{verbatim}\",\\n                # Now split by math environments\\n                \"\\\\n\\\\\\\\\\\\begin{align}\",\\n                \"$$\",\\n                \"$\",\\n                # Now split by the normal type of lines\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.HTML:\\n            return [\\n                # First, try to split along HTML tags\\n                \"<body\",\\n                \"<div\",\\n                \"<p\",\\n                \"<br\",\\n                \"<li\",\\n                \"<h1\",\\n                \"<h2\",\\n                \"<h3\",\\n                \"<h4\",\\n                \"<h5\",\\n                \"<h6\",\\n                \"<span\",\\n                \"<table\",\\n                \"<tr\",\\n                \"<td\",\\n                \"<th\",\\n                \"<ul\",\\n                \"<ol\",\\n                \"<header\",\\n                \"<footer\",\\n                \"<nav\",\\n                # Head\\n                \"<head\",\\n                \"<style\",\\n                \"<script\",\\n                \"<meta\",\\n                \"<title\",',\n",
              "  '\" \",\\n                \"\",\\n            ]\\n        elif language == Language.SCALA:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nobject \",\\n                # Split along method definitions\\n                \"\\\\ndef \",\\n                \"\\\\nval \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nmatch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SWIFT:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunc \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nstruct \",\\n                \"\\\\nenum \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.MARKDOWN:\\n            return [\\n                # First, try to split along Markdown headings (starting with level 2)\\n                \"\\\\n#{1,6} \",\\n                # Note the alternative syntax for headings (below) is not handled here\\n                # Heading level 2\\n                # ---------------\\n                # End of code block\\n                \"```\\\\n\",\\n                # Horizontal lines\\n                \"\\\\n\\\\\\\\*\\\\\\\\*\\\\\\\\*+\\\\n\",\\n                \"\\\\n---+\\\\n\",\\n                \"\\\\n___+\\\\n\",\\n                # Note that this splitter doesn\\'t handle horizontal lines defined',\n",
              "  'return [\\n                # First, try to split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\ndef \",\\n                \"\\\\n\\\\tdef \",\\n                # Now split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.RST:\\n            return [\\n                # Split along section titles\\n                \"\\\\n=+\\\\n\",\\n                \"\\\\n-+\\\\n\",\\n                \"\\\\n\\\\\\\\*+\\\\n\",\\n                # Split along directive markers\\n                \"\\\\n\\\\n.. *\\\\n\\\\n\",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.RUBY:\\n            return [\\n                # Split along method definitions\\n                \"\\\\ndef \",\\n                \"\\\\nclass \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nunless \",\\n                \"\\\\nwhile \",\\n                \"\\\\nfor \",\\n                \"\\\\ndo \",\\n                \"\\\\nbegin \",\\n                \"\\\\nrescue \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.RUST:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfn \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nwhile \",\\n                \"\\\\nfor \",\\n                \"\\\\nloop \",\\n                \"\\\\nmatch \",\\n                \"\\\\nconst \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SCALA:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",',\n",
              "  '\"\\\\nfunction \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                \"\\\\ndefault \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.PHP:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunction \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nforeach \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.PROTO:\\n            return [\\n                # Split along message definitions\\n                \"\\\\nmessage \",\\n                # Split along service definitions\\n                \"\\\\nservice \",\\n                # Split along enum definitions\\n                \"\\\\nenum \",\\n                # Split along option definitions\\n                \"\\\\noption \",\\n                # Split along import statements\\n                \"\\\\nimport \",\\n                # Split along syntax declarations\\n                \"\\\\nsyntax \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.PYTHON:\\n            return [\\n                # First, try to split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\ndef \",\\n                \"\\\\n\\\\tdef \",',\n",
              "  '\"\"\"**Text Splitters** are classes for splitting text.\\n\\n\\n**Class hierarchy:**\\n\\n.. code-block::\\n\\n    BaseDocumentTransformer --> TextSplitter --> <name>TextSplitter  # Example: CharacterTextSplitter\\n                                                 RecursiveCharacterTextSplitter -->  <name>TextSplitter\\n\\nNote: **MarkdownHeaderTextSplitter** and **HTMLHeaderTextSplitter do not derive from TextSplitter.\\n\\n\\n**Main helpers:**\\n\\n.. code-block::\\n\\n    Document, Tokenizer, Language, LineType, HeaderType\\n\\n\"\"\"  # noqa: E501\\n\\nfrom __future__ import annotations\\n\\nimport asyncio\\nimport copy\\nimport logging\\nimport pathlib\\nimport re\\nfrom abc import ABC, abstractmethod\\nfrom dataclasses import dataclass\\nfrom enum import Enum\\nfrom functools import partial\\nfrom io import BytesIO, StringIO\\nfrom typing import (\\n    AbstractSet,\\n    Any,\\n    Callable,\\n    Collection,\\n    Dict,\\n    Iterable,\\n    List,\\n    Literal,\\n    Optional,\\n    Sequence,\\n    Tuple,\\n    Type,\\n    TypedDict,\\n    TypeVar,\\n    Union,\\n    cast,\\n)\\n\\nimport requests\\nfrom langchain_core.documents import BaseDocumentTransformer, Document\\n\\nlogger = logging.getLogger(__name__)\\n\\nTS = TypeVar(\"TS\", bound=\"TextSplitter\")\\n\\n\\ndef _make_spacy_pipeline_for_splitting(\\n    pipeline: str, *, max_length: int = 1_000_000\\n) -> Any:  # avoid importing spacy\\n    try:\\n        import spacy\\n    except ImportError:\\n        raise ImportError(\\n            \"Spacy is not installed, please install it with `pip install spacy`.\"\\n        )\\n    if pipeline == \"sentencizer\":\\n        from spacy.lang.en import English\\n\\n        sentencizer = English()\\n        sentencizer.add_pipe(\"sentencizer\")\\n    else:\\n        sentencizer = spacy.load(pipeline, exclude=[\"ner\", \"tagger\"])\\n        sentencizer.max_length = max_length\\n    return sentencizer',\n",
              "  '@staticmethod\\n    def get_separators_for_language(language: Language) -> List[str]:\\n        if language == Language.CPP:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along function definitions\\n                \"\\\\nvoid \",\\n                \"\\\\nint \",\\n                \"\\\\nfloat \",\\n                \"\\\\ndouble \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.GO:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunc \",\\n                \"\\\\nvar \",\\n                \"\\\\nconst \",\\n                \"\\\\ntype \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.JAVA:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along method definitions\\n                \"\\\\npublic \",\\n                \"\\\\nprotected \",\\n                \"\\\\nprivate \",\\n                \"\\\\nstatic \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.KOTLIN:\\n            return [',\n",
              "  '_separator = separator if self._is_separator_regex else re.escape(separator)\\n        splits = _split_text_with_regex(text, _separator, self._keep_separator)\\n\\n        # Now go merging things, recursively splitting longer texts.\\n        _good_splits = []\\n        _separator = \"\" if self._keep_separator else separator\\n        for s in splits:\\n            if self._length_function(s) < self._chunk_size:\\n                _good_splits.append(s)\\n            else:\\n                if _good_splits:\\n                    merged_text = self._merge_splits(_good_splits, _separator)\\n                    final_chunks.extend(merged_text)\\n                    _good_splits = []\\n                if not new_separators:\\n                    final_chunks.append(s)\\n                else:\\n                    other_info = self._split_text(s, new_separators)\\n                    final_chunks.extend(other_info)\\n        if _good_splits:\\n            merged_text = self._merge_splits(_good_splits, _separator)\\n            final_chunks.extend(merged_text)\\n        return final_chunks\\n\\n    def split_text(self, text: str) -> List[str]:\\n        return self._split_text(text, self._separators)\\n\\n    @classmethod\\n    def from_language(\\n        cls, language: Language, **kwargs: Any\\n    ) -> RecursiveCharacterTextSplitter:\\n        separators = cls.get_separators_for_language(language)\\n        return cls(separators=separators, is_separator_regex=True, **kwargs)',\n",
              "  'class RecursiveCharacterTextSplitter(TextSplitter):\\n    \"\"\"Splitting text by recursively look at characters.\\n\\n    Recursively tries to split by different characters to find one\\n    that works.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        separators: Optional[List[str]] = None,\\n        keep_separator: bool = True,\\n        is_separator_regex: bool = False,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(keep_separator=keep_separator, **kwargs)\\n        self._separators = separators or [\"\\\\n\\\\n\", \"\\\\n\", \" \", \"\"]\\n        self._is_separator_regex = is_separator_regex\\n\\n    def _split_text(self, text: str, separators: List[str]) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        final_chunks = []\\n        # Get appropriate separator to use\\n        separator = separators[-1]\\n        new_separators = []\\n        for i, _s in enumerate(separators):\\n            _separator = _s if self._is_separator_regex else re.escape(_s)\\n            if _s == \"\":\\n                separator = _s\\n                break\\n            if re.search(_separator, text):\\n                separator = _s\\n                new_separators = separators[i + 1 :]\\n                break\\n\\n        _separator = separator if self._is_separator_regex else re.escape(separator)\\n        splits = _split_text_with_regex(text, _separator, self._keep_separator)',\n",
              "  'class Language(str, Enum):\\n    \"\"\"Enum of the programming languages.\"\"\"\\n\\n    CPP = \"cpp\"\\n    GO = \"go\"\\n    JAVA = \"java\"\\n    KOTLIN = \"kotlin\"\\n    JS = \"js\"\\n    TS = \"ts\"\\n    PHP = \"php\"\\n    PROTO = \"proto\"\\n    PYTHON = \"python\"\\n    RST = \"rst\"\\n    RUBY = \"ruby\"\\n    RUST = \"rust\"\\n    SCALA = \"scala\"\\n    SWIFT = \"swift\"\\n    MARKDOWN = \"markdown\"\\n    LATEX = \"latex\"\\n    HTML = \"html\"\\n    SOL = \"sol\"\\n    CSHARP = \"csharp\"\\n    COBOL = \"cobol\"',\n",
              "  'def split_text(self, text: str) -> List[str]:\\n        def encode_strip_start_and_stop_token_ids(text: str) -> List[int]:\\n            return self._encode(text)[1:-1]\\n\\n        tokenizer = Tokenizer(\\n            chunk_overlap=self._chunk_overlap,\\n            tokens_per_chunk=self.tokens_per_chunk,\\n            decode=self.tokenizer.decode,\\n            encode=encode_strip_start_and_stop_token_ids,\\n        )\\n\\n        return split_text_on_tokens(text=text, tokenizer=tokenizer)\\n\\n    def count_tokens(self, *, text: str) -> int:\\n        return len(self._encode(text))\\n\\n    _max_length_equal_32_bit_integer: int = 2**32\\n\\n    def _encode(self, text: str) -> List[int]:\\n        token_ids_with_start_and_end_token_ids = self.tokenizer.encode(\\n            text,\\n            max_length=self._max_length_equal_32_bit_integer,\\n            truncation=\"do_not_truncate\",\\n        )\\n        return token_ids_with_start_and_end_token_ids',\n",
              "  'class SentenceTransformersTokenTextSplitter(TextSplitter):\\n    \"\"\"Splitting text to tokens using sentence model tokenizer.\"\"\"\\n\\n    def __init__(\\n        self,\\n        chunk_overlap: int = 50,\\n        model_name: str = \"sentence-transformers/all-mpnet-base-v2\",\\n        tokens_per_chunk: Optional[int] = None,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(**kwargs, chunk_overlap=chunk_overlap)\\n\\n        try:\\n            from sentence_transformers import SentenceTransformer\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import sentence_transformer python package. \"\\n                \"This is needed in order to for SentenceTransformersTokenTextSplitter. \"\\n                \"Please install it with `pip install sentence-transformers`.\"\\n            )\\n\\n        self.model_name = model_name\\n        self._model = SentenceTransformer(self.model_name)\\n        self.tokenizer = self._model.tokenizer\\n        self._initialize_chunk_configuration(tokens_per_chunk=tokens_per_chunk)\\n\\n    def _initialize_chunk_configuration(\\n        self, *, tokens_per_chunk: Optional[int]\\n    ) -> None:\\n        self.maximum_tokens_per_chunk = cast(int, self._model.max_seq_length)\\n\\n        if tokens_per_chunk is None:\\n            self.tokens_per_chunk = self.maximum_tokens_per_chunk\\n        else:\\n            self.tokens_per_chunk = tokens_per_chunk\\n\\n        if self.tokens_per_chunk > self.maximum_tokens_per_chunk:\\n            raise ValueError(\\n                f\"The token limit of the models \\'{self.model_name}\\'\"\\n                f\" is: {self.maximum_tokens_per_chunk}.\"\\n                f\" Argument tokens_per_chunk={self.tokens_per_chunk}\"\\n                f\" > maximum token limit.\"\\n            )\\n\\n    def split_text(self, text: str) -> List[str]:\\n        def encode_strip_start_and_stop_token_ids(text: str) -> List[int]:\\n            return self._encode(text)[1:-1]',\n",
              "  'class TokenTextSplitter(TextSplitter):\\n    \"\"\"Splitting text to tokens using model tokenizer.\"\"\"\\n\\n    def __init__(\\n        self,\\n        encoding_name: str = \"gpt2\",\\n        model_name: Optional[str] = None,\\n        allowed_special: Union[Literal[\"all\"], AbstractSet[str]] = set(),\\n        disallowed_special: Union[Literal[\"all\"], Collection[str]] = \"all\",\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(**kwargs)\\n        try:\\n            import tiktoken\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import tiktoken python package. \"\\n                \"This is needed in order to for TokenTextSplitter. \"\\n                \"Please install it with `pip install tiktoken`.\"\\n            )\\n\\n        if model_name is not None:\\n            enc = tiktoken.encoding_for_model(model_name)\\n        else:\\n            enc = tiktoken.get_encoding(encoding_name)\\n        self._tokenizer = enc\\n        self._allowed_special = allowed_special\\n        self._disallowed_special = disallowed_special\\n\\n    def split_text(self, text: str) -> List[str]:\\n        def _encode(_text: str) -> List[int]:\\n            return self._tokenizer.encode(\\n                _text,\\n                allowed_special=self._allowed_special,\\n                disallowed_special=self._disallowed_special,\\n            )\\n\\n        tokenizer = Tokenizer(\\n            chunk_overlap=self._chunk_overlap,\\n            tokens_per_chunk=self._chunk_size,\\n            decode=self._tokenizer.decode,\\n            encode=_encode,\\n        )\\n\\n        return split_text_on_tokens(text=text, tokenizer=tokenizer)',\n",
              "  'class Tokenizer:\\n    \"\"\"Tokenizer data class.\"\"\"\\n\\n    chunk_overlap: int\\n    \"\"\"Overlap in tokens between chunks\"\"\"\\n    tokens_per_chunk: int\\n    \"\"\"Maximum number of tokens per chunk\"\"\"\\n    decode: Callable[[List[int]], str]\\n    \"\"\" Function to decode a list of token ids to a string\"\"\"\\n    encode: Callable[[str], List[int]]\\n    \"\"\" Function to encode a string to a list of token ids\"\"\"\\n\\n\\ndef split_text_on_tokens(*, text: str, tokenizer: Tokenizer) -> List[str]:\\n    \"\"\"Split incoming text and return chunks using tokenizer.\"\"\"\\n    splits: List[str] = []\\n    input_ids = tokenizer.encode(text)\\n    start_idx = 0\\n    cur_idx = min(start_idx + tokenizer.tokens_per_chunk, len(input_ids))\\n    chunk_ids = input_ids[start_idx:cur_idx]\\n    while start_idx < len(input_ids):\\n        splits.append(tokenizer.decode(chunk_ids))\\n        if cur_idx == len(input_ids):\\n            break\\n        start_idx += tokenizer.tokens_per_chunk - tokenizer.chunk_overlap\\n        cur_idx = min(start_idx + tokenizer.tokens_per_chunk, len(input_ids))\\n        chunk_ids = input_ids[start_idx:cur_idx]\\n    return splits',\n",
              "  'def split_text(self, text: str) -> List[Document]:\\n        \"\"\"Split markdown file\\n        Args:\\n            text: Markdown file\"\"\"\\n\\n        # Split the input text by newline character (\"\\\\n\").\\n        lines = text.split(\"\\\\n\")\\n        # Final output\\n        lines_with_metadata: List[LineType] = []\\n        # Content and metadata of the chunk currently being processed\\n        current_content: List[str] = []\\n        current_metadata: Dict[str, str] = {}\\n        # Keep track of the nested header structure\\n        # header_stack: List[Dict[str, Union[int, str]]] = []\\n        header_stack: List[HeaderType] = []\\n        initial_metadata: Dict[str, str] = {}\\n\\n        in_code_block = False\\n        opening_fence = \"\"\\n\\n        for line in lines:\\n            stripped_line = line.strip()\\n\\n            if not in_code_block:\\n                # Exclude inline code spans\\n                if stripped_line.startswith(\"```\") and stripped_line.count(\"```\") == 1:\\n                    in_code_block = True\\n                    opening_fence = \"```\"\\n                elif stripped_line.startswith(\"~~~\"):\\n                    in_code_block = True\\n                    opening_fence = \"~~~\"\\n            else:\\n                if stripped_line.startswith(opening_fence):\\n                    in_code_block = False\\n                    opening_fence = \"\"\\n\\n            if in_code_block:\\n                current_content.append(stripped_line)\\n                continue',\n",
              "  'class Tokenizer:\\n    \"\"\"Tokenizer data class.\"\"\"\\n\\n    chunk_overlap: int\\n    \"\"\"Overlap in tokens between chunks\"\"\"\\n    tokens_per_chunk: int\\n    \"\"\"Maximum number of tokens per chunk\"\"\"\\n    decode: Callable[[List[int]], str]\\n    \"\"\" Function to decode a list of token ids to a string\"\"\"\\n    encode: Callable[[str], List[int]]\\n    \"\"\" Function to encode a string to a list of token ids\"\"\"\\n\\n\\ndef split_text_on_tokens(*, text: str, tokenizer: Tokenizer) -> List[str]:\\n    \"\"\"Split incoming text and return chunks using tokenizer.\"\"\"\\n    splits: List[str] = []\\n    input_ids = tokenizer.encode(text)\\n    start_idx = 0\\n    cur_idx = min(start_idx + tokenizer.tokens_per_chunk, len(input_ids))\\n    chunk_ids = input_ids[start_idx:cur_idx]\\n    while start_idx < len(input_ids):\\n        splits.append(tokenizer.decode(chunk_ids))\\n        if cur_idx == len(input_ids):\\n            break\\n        start_idx += tokenizer.tokens_per_chunk - tokenizer.chunk_overlap\\n        cur_idx = min(start_idx + tokenizer.tokens_per_chunk, len(input_ids))\\n        chunk_ids = input_ids[start_idx:cur_idx]\\n    return splits',\n",
              "  '# map xhtml namespace prefix\\n        ns_map = {\"h\": \"http://www.w3.org/1999/xhtml\"}\\n\\n        # build list of elements from DOM\\n        elements = []\\n        for element in result_dom.findall(\"*//*\", ns_map):\\n            if element.findall(\"*[@class=\\'headers\\']\") or element.findall(\\n                \"*[@class=\\'chunk\\']\"\\n            ):\\n                elements.append(\\n                    ElementType(\\n                        url=file,\\n                        xpath=\"\".join(\\n                            [\\n                                node.text\\n                                for node in element.findall(\"*[@class=\\'xpath\\']\", ns_map)\\n                            ]\\n                        ),\\n                        content=\"\".join(\\n                            [\\n                                node.text\\n                                for node in element.findall(\"*[@class=\\'chunk\\']\", ns_map)\\n                            ]\\n                        ),\\n                        metadata={\\n                            # Add text of specified headers to metadata using header\\n                            # mapping.\\n                            header_mapping[node.tag]: node.text\\n                            for node in filter(\\n                                lambda x: x.tag in header_filter,\\n                                element.findall(\"*[@class=\\'headers\\']/*\", ns_map),\\n                            )\\n                        },\\n                    )\\n                )\\n\\n        if not self.return_each_element:\\n            return self.aggregate_elements_to_chunks(elements)\\n        else:\\n            return [\\n                Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n                for chunk in elements\\n            ]\\n\\n\\n# should be in newer Python versions (3.10+)\\n# @dataclass(frozen=True, kw_only=True, slots=True)\\n@dataclass(frozen=True)',\n",
              "  'return [\\n            Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n            for chunk in aggregated_chunks\\n        ]\\n\\n    def split_text_from_url(self, url: str) -> List[Document]:\\n        \"\"\"Split HTML from web URL\\n\\n        Args:\\n            url: web URL\\n        \"\"\"\\n        r = requests.get(url)\\n        return self.split_text_from_file(BytesIO(r.content))\\n\\n    def split_text(self, text: str) -> List[Document]:\\n        \"\"\"Split HTML text string\\n\\n        Args:\\n            text: HTML text\\n        \"\"\"\\n        return self.split_text_from_file(StringIO(text))\\n\\n    def split_text_from_file(self, file: Any) -> List[Document]:\\n        \"\"\"Split HTML file\\n\\n        Args:\\n            file: HTML file\\n        \"\"\"\\n        try:\\n            from lxml import etree\\n        except ImportError as e:\\n            raise ImportError(\\n                \"Unable to import lxml, please install with `pip install lxml`.\"\\n            ) from e\\n        # use lxml library to parse html document and return xml ElementTree\\n        parser = etree.HTMLParser()\\n        tree = etree.parse(file, parser)\\n\\n        # document transformation for \"structure-aware\" chunking is handled with xsl.\\n        # see comments in html_chunks_with_headers.xslt for more detailed information.\\n        xslt_path = (\\n            pathlib.Path(__file__).parent\\n            / \"document_transformers/xsl/html_chunks_with_headers.xslt\"\\n        )\\n        xslt_tree = etree.parse(xslt_path)\\n        transform = etree.XSLT(xslt_tree)\\n        result = transform(tree)\\n        result_dom = etree.fromstring(str(result))\\n\\n        # create filter and mapping for header metadata\\n        header_filter = [header[0] for header in self.headers_to_split_on]\\n        header_mapping = dict(self.headers_to_split_on)\\n\\n        # map xhtml namespace prefix\\n        ns_map = {\"h\": \"http://www.w3.org/1999/xhtml\"}',\n",
              "  'class HTMLHeaderTextSplitter:\\n    \"\"\"\\n    Splitting HTML files based on specified headers.\\n    Requires lxml package.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        headers_to_split_on: List[Tuple[str, str]],\\n        return_each_element: bool = False,\\n    ):\\n        \"\"\"Create a new HTMLHeaderTextSplitter.\\n\\n        Args:\\n            headers_to_split_on: list of tuples of headers we want to track mapped to\\n                (arbitrary) keys for metadata. Allowed header values: h1, h2, h3, h4,\\n                h5, h6 e.g. [(\"h1\", \"Header 1\"), (\"h2\", \"Header 2)].\\n            return_each_element: Return each element w/ associated headers.\\n        \"\"\"\\n        # Output element-by-element or aggregated into chunks w/ common headers\\n        self.return_each_element = return_each_element\\n        self.headers_to_split_on = sorted(headers_to_split_on)\\n\\n    def aggregate_elements_to_chunks(\\n        self, elements: List[ElementType]\\n    ) -> List[Document]:\\n        \"\"\"Combine elements with common metadata into chunks\\n\\n        Args:\\n            elements: HTML element content with associated identifying info and metadata\\n        \"\"\"\\n        aggregated_chunks: List[ElementType] = []\\n\\n        for element in elements:\\n            if (\\n                aggregated_chunks\\n                and aggregated_chunks[-1][\"metadata\"] == element[\"metadata\"]\\n            ):\\n                # If the last element in the aggregated list\\n                # has the same metadata as the current element,\\n                # append the current content to the last element\\'s content\\n                aggregated_chunks[-1][\"content\"] += \"  \\\\n\" + element[\"content\"]\\n            else:\\n                # Otherwise, append the current element to the aggregated list\\n                aggregated_chunks.append(element)\\n\\n        return [\\n            Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n            for chunk in aggregated_chunks\\n        ]',\n",
              "  'class ElementType(TypedDict):\\n    \"\"\"Element type as typed dict.\"\"\"\\n\\n    url: str\\n    xpath: str\\n    content: str\\n    metadata: Dict[str, str]',\n",
              "  'if current_content:\\n            lines_with_metadata.append(\\n                {\"content\": \"\\\\n\".join(current_content), \"metadata\": current_metadata}\\n            )\\n\\n        # lines_with_metadata has each line with associated header metadata\\n        # aggregate these into chunks based on common metadata\\n        if not self.return_each_line:\\n            return self.aggregate_lines_to_chunks(lines_with_metadata)\\n        else:\\n            return [\\n                Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n                for chunk in lines_with_metadata\\n            ]',\n",
              "  '# Push the current header to the stack\\n                        header: HeaderType = {\\n                            \"level\": current_header_level,\\n                            \"name\": name,\\n                            \"data\": stripped_line[len(sep) :].strip(),\\n                        }\\n                        header_stack.append(header)\\n                        # Update initial_metadata with the current header\\n                        initial_metadata[name] = header[\"data\"]\\n\\n                    # Add the previous line to the lines_with_metadata\\n                    # only if current_content is not empty\\n                    if current_content:\\n                        lines_with_metadata.append(\\n                            {\\n                                \"content\": \"\\\\n\".join(current_content),\\n                                \"metadata\": current_metadata.copy(),\\n                            }\\n                        )\\n                        current_content.clear()\\n\\n                    break\\n            else:\\n                if stripped_line:\\n                    current_content.append(stripped_line)\\n                elif current_content:\\n                    lines_with_metadata.append(\\n                        {\\n                            \"content\": \"\\\\n\".join(current_content),\\n                            \"metadata\": current_metadata.copy(),\\n                        }\\n                    )\\n                    current_content.clear()\\n\\n            current_metadata = initial_metadata.copy()\\n\\n        if current_content:\\n            lines_with_metadata.append(\\n                {\"content\": \"\\\\n\".join(current_content), \"metadata\": current_metadata}\\n            )',\n",
              "  'if in_code_block:\\n                current_content.append(stripped_line)\\n                continue\\n\\n            # Check each line against each of the header types (e.g., #, ##)\\n            for sep, name in self.headers_to_split_on:\\n                # Check if line starts with a header that we intend to split on\\n                if stripped_line.startswith(sep) and (\\n                    # Header with no text OR header is followed by space\\n                    # Both are valid conditions that sep is being used a header\\n                    len(stripped_line) == len(sep) or stripped_line[len(sep)] == \" \"\\n                ):\\n                    # Ensure we are tracking the header as metadata\\n                    if name is not None:\\n                        # Get the current header level\\n                        current_header_level = sep.count(\"#\")\\n\\n                        # Pop out headers of lower or same level from the stack\\n                        while (\\n                            header_stack\\n                            and header_stack[-1][\"level\"] >= current_header_level\\n                        ):\\n                            # We have encountered a new header\\n                            # at the same or higher level\\n                            popped_header = header_stack.pop()\\n                            # Clear the metadata for the\\n                            # popped header in initial_metadata\\n                            if popped_header[\"name\"] in initial_metadata:\\n                                initial_metadata.pop(popped_header[\"name\"])',\n",
              "  '\"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.KOTLIN:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along method definitions\\n                \"\\\\npublic \",\\n                \"\\\\nprotected \",\\n                \"\\\\nprivate \",\\n                \"\\\\ninternal \",\\n                \"\\\\ncompanion \",\\n                \"\\\\nfun \",\\n                \"\\\\nval \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nwhen \",\\n                \"\\\\ncase \",\\n                \"\\\\nelse \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.JS:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunction \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                \"\\\\nvar \",\\n                \"\\\\nclass \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                \"\\\\ndefault \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.TS:\\n            return [\\n                \"\\\\nenum \",\\n                \"\\\\ninterface \",\\n                \"\\\\nnamespace \",\\n                \"\\\\ntype \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along function definitions\\n                \"\\\\nfunction \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements',\n",
              "  'class MarkdownHeaderTextSplitter:\\n    \"\"\"Splitting markdown files based on specified headers.\"\"\"\\n\\n    def __init__(\\n        self, headers_to_split_on: List[Tuple[str, str]], return_each_line: bool = False\\n    ):\\n        \"\"\"Create a new MarkdownHeaderTextSplitter.\\n\\n        Args:\\n            headers_to_split_on: Headers we want to track\\n            return_each_line: Return each line w/ associated headers\\n        \"\"\"\\n        # Output line-by-line or aggregated into chunks w/ common headers\\n        self.return_each_line = return_each_line\\n        # Given the headers we want to split on,\\n        # (e.g., \"#, ##, etc\") order by length\\n        self.headers_to_split_on = sorted(\\n            headers_to_split_on, key=lambda split: len(split[0]), reverse=True\\n        )\\n\\n    def aggregate_lines_to_chunks(self, lines: List[LineType]) -> List[Document]:\\n        \"\"\"Combine lines with common metadata into chunks\\n        Args:\\n            lines: Line of text / associated header metadata\\n        \"\"\"\\n        aggregated_chunks: List[LineType] = []\\n\\n        for line in lines:\\n            if (\\n                aggregated_chunks\\n                and aggregated_chunks[-1][\"metadata\"] == line[\"metadata\"]\\n            ):\\n                # If the last line in the aggregated list\\n                # has the same metadata as the current line,\\n                # append the current content to the last lines\\'s content\\n                aggregated_chunks[-1][\"content\"] += \"  \\\\n\" + line[\"content\"]\\n            else:\\n                # Otherwise, append the current line to the aggregated list\\n                aggregated_chunks.append(line)\\n\\n        return [\\n            Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n            for chunk in aggregated_chunks\\n        ]\\n\\n    def split_text(self, text: str) -> List[Document]:\\n        \"\"\"Split markdown file\\n        Args:\\n            text: Markdown file\"\"\"',\n",
              "  'class CharacterTextSplitter(TextSplitter):\\n    \"\"\"Splitting text that looks at characters.\"\"\"\\n\\n    def __init__(\\n        self, separator: str = \"\\\\n\\\\n\", is_separator_regex: bool = False, **kwargs: Any\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(**kwargs)\\n        self._separator = separator\\n        self._is_separator_regex = is_separator_regex\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        # First we naively split the large input into a bunch of smaller ones.\\n        separator = (\\n            self._separator if self._is_separator_regex else re.escape(self._separator)\\n        )\\n        splits = _split_text_with_regex(text, separator, self._keep_separator)\\n        _separator = \"\" if self._keep_separator else self._separator\\n        return self._merge_splits(splits, _separator)\\n\\n\\nclass LineType(TypedDict):\\n    \"\"\"Line type as typed dict.\"\"\"\\n\\n    metadata: Dict[str, str]\\n    content: str\\n\\n\\nclass HeaderType(TypedDict):\\n    \"\"\"Header type as typed dict.\"\"\"\\n\\n    level: int\\n    name: str\\n    data: str',\n",
              "  'if issubclass(cls, TokenTextSplitter):\\n            extra_kwargs = {\\n                \"encoding_name\": encoding_name,\\n                \"model_name\": model_name,\\n                \"allowed_special\": allowed_special,\\n                \"disallowed_special\": disallowed_special,\\n            }\\n            kwargs = {**kwargs, **extra_kwargs}\\n\\n        return cls(length_function=_tiktoken_encoder, **kwargs)\\n\\n    def transform_documents(\\n        self, documents: Sequence[Document], **kwargs: Any\\n    ) -> Sequence[Document]:\\n        \"\"\"Transform sequence of documents by splitting them.\"\"\"\\n        return self.split_documents(list(documents))\\n\\n    async def atransform_documents(\\n        self, documents: Sequence[Document], **kwargs: Any\\n    ) -> Sequence[Document]:\\n        \"\"\"Asynchronously transform a sequence of documents by splitting them.\"\"\"\\n        return await asyncio.get_running_loop().run_in_executor(\\n            None, partial(self.transform_documents, **kwargs), documents\\n        )',\n",
              "  'if not isinstance(tokenizer, PreTrainedTokenizerBase):\\n                raise ValueError(\\n                    \"Tokenizer received was not an instance of PreTrainedTokenizerBase\"\\n                )\\n\\n            def _huggingface_tokenizer_length(text: str) -> int:\\n                return len(tokenizer.encode(text))\\n\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import transformers python package. \"\\n                \"Please install it with `pip install transformers`.\"\\n            )\\n        return cls(length_function=_huggingface_tokenizer_length, **kwargs)\\n\\n    @classmethod\\n    def from_tiktoken_encoder(\\n        cls: Type[TS],\\n        encoding_name: str = \"gpt2\",\\n        model_name: Optional[str] = None,\\n        allowed_special: Union[Literal[\"all\"], AbstractSet[str]] = set(),\\n        disallowed_special: Union[Literal[\"all\"], Collection[str]] = \"all\",\\n        **kwargs: Any,\\n    ) -> TS:\\n        \"\"\"Text splitter that uses tiktoken encoder to count length.\"\"\"\\n        try:\\n            import tiktoken\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import tiktoken python package. \"\\n                \"This is needed in order to calculate max_tokens_for_prompt. \"\\n                \"Please install it with `pip install tiktoken`.\"\\n            )\\n\\n        if model_name is not None:\\n            enc = tiktoken.encoding_for_model(model_name)\\n        else:\\n            enc = tiktoken.get_encoding(encoding_name)\\n\\n        def _tiktoken_encoder(text: str) -> int:\\n            return len(\\n                enc.encode(\\n                    text,\\n                    allowed_special=allowed_special,\\n                    disallowed_special=disallowed_special,\\n                )\\n            )',\n",
              "  'docs = []\\n        current_doc: List[str] = []\\n        total = 0\\n        for d in splits:\\n            _len = self._length_function(d)\\n            if (\\n                total + _len + (separator_len if len(current_doc) > 0 else 0)\\n                > self._chunk_size\\n            ):\\n                if total > self._chunk_size:\\n                    logger.warning(\\n                        f\"Created a chunk of size {total}, \"\\n                        f\"which is longer than the specified {self._chunk_size}\"\\n                    )\\n                if len(current_doc) > 0:\\n                    doc = self._join_docs(current_doc, separator)\\n                    if doc is not None:\\n                        docs.append(doc)\\n                    # Keep on popping if:\\n                    # - we have a larger chunk than in the chunk overlap\\n                    # - or if we still have any chunks and the length is long\\n                    while total > self._chunk_overlap or (\\n                        total + _len + (separator_len if len(current_doc) > 0 else 0)\\n                        > self._chunk_size\\n                        and total > 0\\n                    ):\\n                        total -= self._length_function(current_doc[0]) + (\\n                            separator_len if len(current_doc) > 1 else 0\\n                        )\\n                        current_doc = current_doc[1:]\\n            current_doc.append(d)\\n            total += _len + (separator_len if len(current_doc) > 1 else 0)\\n        doc = self._join_docs(current_doc, separator)\\n        if doc is not None:\\n            docs.append(doc)\\n        return docs\\n\\n    @classmethod\\n    def from_huggingface_tokenizer(cls, tokenizer: Any, **kwargs: Any) -> TextSplitter:\\n        \"\"\"Text splitter that uses HuggingFace tokenizer to count length.\"\"\"\\n        try:\\n            from transformers import PreTrainedTokenizerBase',\n",
              "  '@abstractmethod\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split text into multiple components.\"\"\"\\n\\n    def create_documents(\\n        self, texts: List[str], metadatas: Optional[List[dict]] = None\\n    ) -> List[Document]:\\n        \"\"\"Create documents from a list of texts.\"\"\"\\n        _metadatas = metadatas or [{}] * len(texts)\\n        documents = []\\n        for i, text in enumerate(texts):\\n            index = -1\\n            for chunk in self.split_text(text):\\n                metadata = copy.deepcopy(_metadatas[i])\\n                if self._add_start_index:\\n                    index = text.find(chunk, index + 1)\\n                    metadata[\"start_index\"] = index\\n                new_doc = Document(page_content=chunk, metadata=metadata)\\n                documents.append(new_doc)\\n        return documents\\n\\n    def split_documents(self, documents: Iterable[Document]) -> List[Document]:\\n        \"\"\"Split documents.\"\"\"\\n        texts, metadatas = [], []\\n        for doc in documents:\\n            texts.append(doc.page_content)\\n            metadatas.append(doc.metadata)\\n        return self.create_documents(texts, metadatas=metadatas)\\n\\n    def _join_docs(self, docs: List[str], separator: str) -> Optional[str]:\\n        text = separator.join(docs)\\n        if self._strip_whitespace:\\n            text = text.strip()\\n        if text == \"\":\\n            return None\\n        else:\\n            return text\\n\\n    def _merge_splits(self, splits: Iterable[str], separator: str) -> List[str]:\\n        # We now want to combine these smaller pieces into medium size\\n        # chunks to send to the LLM.\\n        separator_len = self._length_function(separator)',\n",
              "  'class TextSplitter(BaseDocumentTransformer, ABC):\\n    \"\"\"Interface for splitting text into chunks.\"\"\"\\n\\n    def __init__(\\n        self,\\n        chunk_size: int = 4000,\\n        chunk_overlap: int = 200,\\n        length_function: Callable[[str], int] = len,\\n        keep_separator: bool = False,\\n        add_start_index: bool = False,\\n        strip_whitespace: bool = True,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\\n\\n        Args:\\n            chunk_size: Maximum size of chunks to return\\n            chunk_overlap: Overlap in characters between chunks\\n            length_function: Function that measures the length of given chunks\\n            keep_separator: Whether to keep the separator in the chunks\\n            add_start_index: If `True`, includes chunk\\'s start index in metadata\\n            strip_whitespace: If `True`, strips whitespace from the start and end of\\n                              every document\\n        \"\"\"\\n        if chunk_overlap > chunk_size:\\n            raise ValueError(\\n                f\"Got a larger chunk overlap ({chunk_overlap}) than chunk size \"\\n                f\"({chunk_size}), should be smaller.\"\\n            )\\n        self._chunk_size = chunk_size\\n        self._chunk_overlap = chunk_overlap\\n        self._length_function = length_function\\n        self._keep_separator = keep_separator\\n        self._add_start_index = add_start_index\\n        self._strip_whitespace = strip_whitespace\\n\\n    @abstractmethod\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split text into multiple components.\"\"\"',\n",
              "  'def _split_text_with_regex(\\n    text: str, separator: str, keep_separator: bool\\n) -> List[str]:\\n    # Now that we have the separator, split the text\\n    if separator:\\n        if keep_separator:\\n            # The parentheses in the pattern keep the delimiters in the result.\\n            _splits = re.split(f\"({separator})\", text)\\n            splits = [_splits[i] + _splits[i + 1] for i in range(1, len(_splits), 2)]\\n            if len(_splits) % 2 == 0:\\n                splits += _splits[-1:]\\n            splits = [_splits[0]] + splits\\n        else:\\n            splits = re.split(separator, text)\\n    else:\\n        splits = list(text)\\n    return [s for s in splits if s != \"\"]',\n",
              "  '# Push the current header to the stack\\n                        header: HeaderType = {\\n                            \"level\": current_header_level,\\n                            \"name\": name,\\n                            \"data\": stripped_line[len(sep) :].strip(),\\n                        }\\n                        header_stack.append(header)\\n                        # Update initial_metadata with the current header\\n                        initial_metadata[name] = header[\"data\"]\\n\\n                    # Add the previous line to the lines_with_metadata\\n                    # only if current_content is not empty\\n                    if current_content:\\n                        lines_with_metadata.append(\\n                            {\\n                                \"content\": \"\\\\n\".join(current_content),\\n                                \"metadata\": current_metadata.copy(),\\n                            }\\n                        )\\n                        current_content.clear()\\n\\n                    break\\n            else:\\n                if stripped_line:\\n                    current_content.append(stripped_line)\\n                elif current_content:\\n                    lines_with_metadata.append(\\n                        {\\n                            \"content\": \"\\\\n\".join(current_content),\\n                            \"metadata\": current_metadata.copy(),\\n                        }\\n                    )\\n                    current_content.clear()\\n\\n            current_metadata = initial_metadata.copy()\\n\\n        if current_content:\\n            lines_with_metadata.append(\\n                {\"content\": \"\\\\n\".join(current_content), \"metadata\": current_metadata}\\n            )',\n",
              "  'def split_text(self, text: str) -> List[str]:\\n        def encode_strip_start_and_stop_token_ids(text: str) -> List[int]:\\n            return self._encode(text)[1:-1]\\n\\n        tokenizer = Tokenizer(\\n            chunk_overlap=self._chunk_overlap,\\n            tokens_per_chunk=self.tokens_per_chunk,\\n            decode=self.tokenizer.decode,\\n            encode=encode_strip_start_and_stop_token_ids,\\n        )\\n\\n        return split_text_on_tokens(text=text, tokenizer=tokenizer)\\n\\n    def count_tokens(self, *, text: str) -> int:\\n        return len(self._encode(text))\\n\\n    _max_length_equal_32_bit_integer: int = 2**32\\n\\n    def _encode(self, text: str) -> List[int]:\\n        token_ids_with_start_and_end_token_ids = self.tokenizer.encode(\\n            text,\\n            max_length=self._max_length_equal_32_bit_integer,\\n            truncation=\"do_not_truncate\",\\n        )\\n        return token_ids_with_start_and_end_token_ids',\n",
              "  'class SentenceTransformersTokenTextSplitter(TextSplitter):\\n    \"\"\"Splitting text to tokens using sentence model tokenizer.\"\"\"\\n\\n    def __init__(\\n        self,\\n        chunk_overlap: int = 50,\\n        model_name: str = \"sentence-transformers/all-mpnet-base-v2\",\\n        tokens_per_chunk: Optional[int] = None,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(**kwargs, chunk_overlap=chunk_overlap)\\n\\n        try:\\n            from sentence_transformers import SentenceTransformer\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import sentence_transformer python package. \"\\n                \"This is needed in order to for SentenceTransformersTokenTextSplitter. \"\\n                \"Please install it with `pip install sentence-transformers`.\"\\n            )\\n\\n        self.model_name = model_name\\n        self._model = SentenceTransformer(self.model_name)\\n        self.tokenizer = self._model.tokenizer\\n        self._initialize_chunk_configuration(tokens_per_chunk=tokens_per_chunk)\\n\\n    def _initialize_chunk_configuration(\\n        self, *, tokens_per_chunk: Optional[int]\\n    ) -> None:\\n        self.maximum_tokens_per_chunk = cast(int, self._model.max_seq_length)\\n\\n        if tokens_per_chunk is None:\\n            self.tokens_per_chunk = self.maximum_tokens_per_chunk\\n        else:\\n            self.tokens_per_chunk = tokens_per_chunk\\n\\n        if self.tokens_per_chunk > self.maximum_tokens_per_chunk:\\n            raise ValueError(\\n                f\"The token limit of the models \\'{self.model_name}\\'\"\\n                f\" is: {self.maximum_tokens_per_chunk}.\"\\n                f\" Argument tokens_per_chunk={self.tokens_per_chunk}\"\\n                f\" > maximum token limit.\"\\n            )\\n\\n    def split_text(self, text: str) -> List[str]:\\n        def encode_strip_start_and_stop_token_ids(text: str) -> List[int]:\\n            return self._encode(text)[1:-1]',\n",
              "  'class TokenTextSplitter(TextSplitter):\\n    \"\"\"Splitting text to tokens using model tokenizer.\"\"\"\\n\\n    def __init__(\\n        self,\\n        encoding_name: str = \"gpt2\",\\n        model_name: Optional[str] = None,\\n        allowed_special: Union[Literal[\"all\"], AbstractSet[str]] = set(),\\n        disallowed_special: Union[Literal[\"all\"], Collection[str]] = \"all\",\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(**kwargs)\\n        try:\\n            import tiktoken\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import tiktoken python package. \"\\n                \"This is needed in order to for TokenTextSplitter. \"\\n                \"Please install it with `pip install tiktoken`.\"\\n            )\\n\\n        if model_name is not None:\\n            enc = tiktoken.encoding_for_model(model_name)\\n        else:\\n            enc = tiktoken.get_encoding(encoding_name)\\n        self._tokenizer = enc\\n        self._allowed_special = allowed_special\\n        self._disallowed_special = disallowed_special\\n\\n    def split_text(self, text: str) -> List[str]:\\n        def _encode(_text: str) -> List[int]:\\n            return self._tokenizer.encode(\\n                _text,\\n                allowed_special=self._allowed_special,\\n                disallowed_special=self._disallowed_special,\\n            )\\n\\n        tokenizer = Tokenizer(\\n            chunk_overlap=self._chunk_overlap,\\n            tokens_per_chunk=self._chunk_size,\\n            decode=self._tokenizer.decode,\\n            encode=_encode,\\n        )\\n\\n        return split_text_on_tokens(text=text, tokenizer=tokenizer)',\n",
              "  'class Tokenizer:\\n    \"\"\"Tokenizer data class.\"\"\"\\n\\n    chunk_overlap: int\\n    \"\"\"Overlap in tokens between chunks\"\"\"\\n    tokens_per_chunk: int\\n    \"\"\"Maximum number of tokens per chunk\"\"\"\\n    decode: Callable[[List[int]], str]\\n    \"\"\" Function to decode a list of token ids to a string\"\"\"\\n    encode: Callable[[str], List[int]]\\n    \"\"\" Function to encode a string to a list of token ids\"\"\"\\n\\n\\ndef split_text_on_tokens(*, text: str, tokenizer: Tokenizer) -> List[str]:\\n    \"\"\"Split incoming text and return chunks using tokenizer.\"\"\"\\n    splits: List[str] = []\\n    input_ids = tokenizer.encode(text)\\n    start_idx = 0\\n    cur_idx = min(start_idx + tokenizer.tokens_per_chunk, len(input_ids))\\n    chunk_ids = input_ids[start_idx:cur_idx]\\n    while start_idx < len(input_ids):\\n        splits.append(tokenizer.decode(chunk_ids))\\n        if cur_idx == len(input_ids):\\n            break\\n        start_idx += tokenizer.tokens_per_chunk - tokenizer.chunk_overlap\\n        cur_idx = min(start_idx + tokenizer.tokens_per_chunk, len(input_ids))\\n        chunk_ids = input_ids[start_idx:cur_idx]\\n    return splits',\n",
              "  '# map xhtml namespace prefix\\n        ns_map = {\"h\": \"http://www.w3.org/1999/xhtml\"}\\n\\n        # build list of elements from DOM\\n        elements = []\\n        for element in result_dom.findall(\"*//*\", ns_map):\\n            if element.findall(\"*[@class=\\'headers\\']\") or element.findall(\\n                \"*[@class=\\'chunk\\']\"\\n            ):\\n                elements.append(\\n                    ElementType(\\n                        url=file,\\n                        xpath=\"\".join(\\n                            [\\n                                node.text\\n                                for node in element.findall(\"*[@class=\\'xpath\\']\", ns_map)\\n                            ]\\n                        ),\\n                        content=\"\".join(\\n                            [\\n                                node.text\\n                                for node in element.findall(\"*[@class=\\'chunk\\']\", ns_map)\\n                            ]\\n                        ),\\n                        metadata={\\n                            # Add text of specified headers to metadata using header\\n                            # mapping.\\n                            header_mapping[node.tag]: node.text\\n                            for node in filter(\\n                                lambda x: x.tag in header_filter,\\n                                element.findall(\"*[@class=\\'headers\\']/*\", ns_map),\\n                            )\\n                        },\\n                    )\\n                )\\n\\n        if not self.return_each_element:\\n            return self.aggregate_elements_to_chunks(elements)\\n        else:\\n            return [\\n                Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n                for chunk in elements\\n            ]\\n\\n\\n# should be in newer Python versions (3.10+)\\n# @dataclass(frozen=True, kw_only=True, slots=True)\\n@dataclass(frozen=True)',\n",
              "  'return [\\n            Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n            for chunk in aggregated_chunks\\n        ]\\n\\n    def split_text_from_url(self, url: str) -> List[Document]:\\n        \"\"\"Split HTML from web URL\\n\\n        Args:\\n            url: web URL\\n        \"\"\"\\n        r = requests.get(url)\\n        return self.split_text_from_file(BytesIO(r.content))\\n\\n    def split_text(self, text: str) -> List[Document]:\\n        \"\"\"Split HTML text string\\n\\n        Args:\\n            text: HTML text\\n        \"\"\"\\n        return self.split_text_from_file(StringIO(text))\\n\\n    def split_text_from_file(self, file: Any) -> List[Document]:\\n        \"\"\"Split HTML file\\n\\n        Args:\\n            file: HTML file\\n        \"\"\"\\n        try:\\n            from lxml import etree\\n        except ImportError as e:\\n            raise ImportError(\\n                \"Unable to import lxml, please install with `pip install lxml`.\"\\n            ) from e\\n        # use lxml library to parse html document and return xml ElementTree\\n        parser = etree.HTMLParser()\\n        tree = etree.parse(file, parser)\\n\\n        # document transformation for \"structure-aware\" chunking is handled with xsl.\\n        # see comments in html_chunks_with_headers.xslt for more detailed information.\\n        xslt_path = (\\n            pathlib.Path(__file__).parent\\n            / \"document_transformers/xsl/html_chunks_with_headers.xslt\"\\n        )\\n        xslt_tree = etree.parse(xslt_path)\\n        transform = etree.XSLT(xslt_tree)\\n        result = transform(tree)\\n        result_dom = etree.fromstring(str(result))\\n\\n        # create filter and mapping for header metadata\\n        header_filter = [header[0] for header in self.headers_to_split_on]\\n        header_mapping = dict(self.headers_to_split_on)\\n\\n        # map xhtml namespace prefix\\n        ns_map = {\"h\": \"http://www.w3.org/1999/xhtml\"}',\n",
              "  'class HTMLHeaderTextSplitter:\\n    \"\"\"\\n    Splitting HTML files based on specified headers.\\n    Requires lxml package.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        headers_to_split_on: List[Tuple[str, str]],\\n        return_each_element: bool = False,\\n    ):\\n        \"\"\"Create a new HTMLHeaderTextSplitter.\\n\\n        Args:\\n            headers_to_split_on: list of tuples of headers we want to track mapped to\\n                (arbitrary) keys for metadata. Allowed header values: h1, h2, h3, h4,\\n                h5, h6 e.g. [(\"h1\", \"Header 1\"), (\"h2\", \"Header 2)].\\n            return_each_element: Return each element w/ associated headers.\\n        \"\"\"\\n        # Output element-by-element or aggregated into chunks w/ common headers\\n        self.return_each_element = return_each_element\\n        self.headers_to_split_on = sorted(headers_to_split_on)\\n\\n    def aggregate_elements_to_chunks(\\n        self, elements: List[ElementType]\\n    ) -> List[Document]:\\n        \"\"\"Combine elements with common metadata into chunks\\n\\n        Args:\\n            elements: HTML element content with associated identifying info and metadata\\n        \"\"\"\\n        aggregated_chunks: List[ElementType] = []\\n\\n        for element in elements:\\n            if (\\n                aggregated_chunks\\n                and aggregated_chunks[-1][\"metadata\"] == element[\"metadata\"]\\n            ):\\n                # If the last element in the aggregated list\\n                # has the same metadata as the current element,\\n                # append the current content to the last element\\'s content\\n                aggregated_chunks[-1][\"content\"] += \"  \\\\n\" + element[\"content\"]\\n            else:\\n                # Otherwise, append the current element to the aggregated list\\n                aggregated_chunks.append(element)\\n\\n        return [\\n            Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n            for chunk in aggregated_chunks\\n        ]',\n",
              "  'class ElementType(TypedDict):\\n    \"\"\"Element type as typed dict.\"\"\"\\n\\n    url: str\\n    xpath: str\\n    content: str\\n    metadata: Dict[str, str]',\n",
              "  'if current_content:\\n            lines_with_metadata.append(\\n                {\"content\": \"\\\\n\".join(current_content), \"metadata\": current_metadata}\\n            )\\n\\n        # lines_with_metadata has each line with associated header metadata\\n        # aggregate these into chunks based on common metadata\\n        if not self.return_each_line:\\n            return self.aggregate_lines_to_chunks(lines_with_metadata)\\n        else:\\n            return [\\n                Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n                for chunk in lines_with_metadata\\n            ]',\n",
              "  'class Language(str, Enum):\\n    \"\"\"Enum of the programming languages.\"\"\"\\n\\n    CPP = \"cpp\"\\n    GO = \"go\"\\n    JAVA = \"java\"\\n    KOTLIN = \"kotlin\"\\n    JS = \"js\"\\n    TS = \"ts\"\\n    PHP = \"php\"\\n    PROTO = \"proto\"\\n    PYTHON = \"python\"\\n    RST = \"rst\"\\n    RUBY = \"ruby\"\\n    RUST = \"rust\"\\n    SCALA = \"scala\"\\n    SWIFT = \"swift\"\\n    MARKDOWN = \"markdown\"\\n    LATEX = \"latex\"\\n    HTML = \"html\"\\n    SOL = \"sol\"\\n    CSHARP = \"csharp\"\\n    COBOL = \"cobol\"',\n",
              "  'if in_code_block:\\n                current_content.append(stripped_line)\\n                continue\\n\\n            # Check each line against each of the header types (e.g., #, ##)\\n            for sep, name in self.headers_to_split_on:\\n                # Check if line starts with a header that we intend to split on\\n                if stripped_line.startswith(sep) and (\\n                    # Header with no text OR header is followed by space\\n                    # Both are valid conditions that sep is being used a header\\n                    len(stripped_line) == len(sep) or stripped_line[len(sep)] == \" \"\\n                ):\\n                    # Ensure we are tracking the header as metadata\\n                    if name is not None:\\n                        # Get the current header level\\n                        current_header_level = sep.count(\"#\")\\n\\n                        # Pop out headers of lower or same level from the stack\\n                        while (\\n                            header_stack\\n                            and header_stack[-1][\"level\"] >= current_header_level\\n                        ):\\n                            # We have encountered a new header\\n                            # at the same or higher level\\n                            popped_header = header_stack.pop()\\n                            # Clear the metadata for the\\n                            # popped header in initial_metadata\\n                            if popped_header[\"name\"] in initial_metadata:\\n                                initial_metadata.pop(popped_header[\"name\"])',\n",
              "  'def split_text(self, text: str) -> List[Document]:\\n        \"\"\"Split markdown file\\n        Args:\\n            text: Markdown file\"\"\"\\n\\n        # Split the input text by newline character (\"\\\\n\").\\n        lines = text.split(\"\\\\n\")\\n        # Final output\\n        lines_with_metadata: List[LineType] = []\\n        # Content and metadata of the chunk currently being processed\\n        current_content: List[str] = []\\n        current_metadata: Dict[str, str] = {}\\n        # Keep track of the nested header structure\\n        # header_stack: List[Dict[str, Union[int, str]]] = []\\n        header_stack: List[HeaderType] = []\\n        initial_metadata: Dict[str, str] = {}\\n\\n        in_code_block = False\\n        opening_fence = \"\"\\n\\n        for line in lines:\\n            stripped_line = line.strip()\\n\\n            if not in_code_block:\\n                # Exclude inline code spans\\n                if stripped_line.startswith(\"```\") and stripped_line.count(\"```\") == 1:\\n                    in_code_block = True\\n                    opening_fence = \"```\"\\n                elif stripped_line.startswith(\"~~~\"):\\n                    in_code_block = True\\n                    opening_fence = \"~~~\"\\n            else:\\n                if stripped_line.startswith(opening_fence):\\n                    in_code_block = False\\n                    opening_fence = \"\"\\n\\n            if in_code_block:\\n                current_content.append(stripped_line)\\n                continue',\n",
              "  'class MarkdownHeaderTextSplitter:\\n    \"\"\"Splitting markdown files based on specified headers.\"\"\"\\n\\n    def __init__(\\n        self, headers_to_split_on: List[Tuple[str, str]], return_each_line: bool = False\\n    ):\\n        \"\"\"Create a new MarkdownHeaderTextSplitter.\\n\\n        Args:\\n            headers_to_split_on: Headers we want to track\\n            return_each_line: Return each line w/ associated headers\\n        \"\"\"\\n        # Output line-by-line or aggregated into chunks w/ common headers\\n        self.return_each_line = return_each_line\\n        # Given the headers we want to split on,\\n        # (e.g., \"#, ##, etc\") order by length\\n        self.headers_to_split_on = sorted(\\n            headers_to_split_on, key=lambda split: len(split[0]), reverse=True\\n        )\\n\\n    def aggregate_lines_to_chunks(self, lines: List[LineType]) -> List[Document]:\\n        \"\"\"Combine lines with common metadata into chunks\\n        Args:\\n            lines: Line of text / associated header metadata\\n        \"\"\"\\n        aggregated_chunks: List[LineType] = []\\n\\n        for line in lines:\\n            if (\\n                aggregated_chunks\\n                and aggregated_chunks[-1][\"metadata\"] == line[\"metadata\"]\\n            ):\\n                # If the last line in the aggregated list\\n                # has the same metadata as the current line,\\n                # append the current content to the last lines\\'s content\\n                aggregated_chunks[-1][\"content\"] += \"  \\\\n\" + line[\"content\"]\\n            else:\\n                # Otherwise, append the current line to the aggregated list\\n                aggregated_chunks.append(line)\\n\\n        return [\\n            Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n            for chunk in aggregated_chunks\\n        ]\\n\\n    def split_text(self, text: str) -> List[Document]:\\n        \"\"\"Split markdown file\\n        Args:\\n            text: Markdown file\"\"\"',\n",
              "  'class CharacterTextSplitter(TextSplitter):\\n    \"\"\"Splitting text that looks at characters.\"\"\"\\n\\n    def __init__(\\n        self, separator: str = \"\\\\n\\\\n\", is_separator_regex: bool = False, **kwargs: Any\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(**kwargs)\\n        self._separator = separator\\n        self._is_separator_regex = is_separator_regex\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        # First we naively split the large input into a bunch of smaller ones.\\n        separator = (\\n            self._separator if self._is_separator_regex else re.escape(self._separator)\\n        )\\n        splits = _split_text_with_regex(text, separator, self._keep_separator)\\n        _separator = \"\" if self._keep_separator else self._separator\\n        return self._merge_splits(splits, _separator)\\n\\n\\nclass LineType(TypedDict):\\n    \"\"\"Line type as typed dict.\"\"\"\\n\\n    metadata: Dict[str, str]\\n    content: str\\n\\n\\nclass HeaderType(TypedDict):\\n    \"\"\"Header type as typed dict.\"\"\"\\n\\n    level: int\\n    name: str\\n    data: str',\n",
              "  'if issubclass(cls, TokenTextSplitter):\\n            extra_kwargs = {\\n                \"encoding_name\": encoding_name,\\n                \"model_name\": model_name,\\n                \"allowed_special\": allowed_special,\\n                \"disallowed_special\": disallowed_special,\\n            }\\n            kwargs = {**kwargs, **extra_kwargs}\\n\\n        return cls(length_function=_tiktoken_encoder, **kwargs)\\n\\n    def transform_documents(\\n        self, documents: Sequence[Document], **kwargs: Any\\n    ) -> Sequence[Document]:\\n        \"\"\"Transform sequence of documents by splitting them.\"\"\"\\n        return self.split_documents(list(documents))\\n\\n    async def atransform_documents(\\n        self, documents: Sequence[Document], **kwargs: Any\\n    ) -> Sequence[Document]:\\n        \"\"\"Asynchronously transform a sequence of documents by splitting them.\"\"\"\\n        return await asyncio.get_running_loop().run_in_executor(\\n            None, partial(self.transform_documents, **kwargs), documents\\n        )',\n",
              "  'if not isinstance(tokenizer, PreTrainedTokenizerBase):\\n                raise ValueError(\\n                    \"Tokenizer received was not an instance of PreTrainedTokenizerBase\"\\n                )\\n\\n            def _huggingface_tokenizer_length(text: str) -> int:\\n                return len(tokenizer.encode(text))\\n\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import transformers python package. \"\\n                \"Please install it with `pip install transformers`.\"\\n            )\\n        return cls(length_function=_huggingface_tokenizer_length, **kwargs)\\n\\n    @classmethod\\n    def from_tiktoken_encoder(\\n        cls: Type[TS],\\n        encoding_name: str = \"gpt2\",\\n        model_name: Optional[str] = None,\\n        allowed_special: Union[Literal[\"all\"], AbstractSet[str]] = set(),\\n        disallowed_special: Union[Literal[\"all\"], Collection[str]] = \"all\",\\n        **kwargs: Any,\\n    ) -> TS:\\n        \"\"\"Text splitter that uses tiktoken encoder to count length.\"\"\"\\n        try:\\n            import tiktoken\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import tiktoken python package. \"\\n                \"This is needed in order to calculate max_tokens_for_prompt. \"\\n                \"Please install it with `pip install tiktoken`.\"\\n            )\\n\\n        if model_name is not None:\\n            enc = tiktoken.encoding_for_model(model_name)\\n        else:\\n            enc = tiktoken.get_encoding(encoding_name)\\n\\n        def _tiktoken_encoder(text: str) -> int:\\n            return len(\\n                enc.encode(\\n                    text,\\n                    allowed_special=allowed_special,\\n                    disallowed_special=disallowed_special,\\n                )\\n            )',\n",
              "  'docs = []\\n        current_doc: List[str] = []\\n        total = 0\\n        for d in splits:\\n            _len = self._length_function(d)\\n            if (\\n                total + _len + (separator_len if len(current_doc) > 0 else 0)\\n                > self._chunk_size\\n            ):\\n                if total > self._chunk_size:\\n                    logger.warning(\\n                        f\"Created a chunk of size {total}, \"\\n                        f\"which is longer than the specified {self._chunk_size}\"\\n                    )\\n                if len(current_doc) > 0:\\n                    doc = self._join_docs(current_doc, separator)\\n                    if doc is not None:\\n                        docs.append(doc)\\n                    # Keep on popping if:\\n                    # - we have a larger chunk than in the chunk overlap\\n                    # - or if we still have any chunks and the length is long\\n                    while total > self._chunk_overlap or (\\n                        total + _len + (separator_len if len(current_doc) > 0 else 0)\\n                        > self._chunk_size\\n                        and total > 0\\n                    ):\\n                        total -= self._length_function(current_doc[0]) + (\\n                            separator_len if len(current_doc) > 1 else 0\\n                        )\\n                        current_doc = current_doc[1:]\\n            current_doc.append(d)\\n            total += _len + (separator_len if len(current_doc) > 1 else 0)\\n        doc = self._join_docs(current_doc, separator)\\n        if doc is not None:\\n            docs.append(doc)\\n        return docs\\n\\n    @classmethod\\n    def from_huggingface_tokenizer(cls, tokenizer: Any, **kwargs: Any) -> TextSplitter:\\n        \"\"\"Text splitter that uses HuggingFace tokenizer to count length.\"\"\"\\n        try:\\n            from transformers import PreTrainedTokenizerBase',\n",
              "  '@abstractmethod\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split text into multiple components.\"\"\"\\n\\n    def create_documents(\\n        self, texts: List[str], metadatas: Optional[List[dict]] = None\\n    ) -> List[Document]:\\n        \"\"\"Create documents from a list of texts.\"\"\"\\n        _metadatas = metadatas or [{}] * len(texts)\\n        documents = []\\n        for i, text in enumerate(texts):\\n            index = -1\\n            for chunk in self.split_text(text):\\n                metadata = copy.deepcopy(_metadatas[i])\\n                if self._add_start_index:\\n                    index = text.find(chunk, index + 1)\\n                    metadata[\"start_index\"] = index\\n                new_doc = Document(page_content=chunk, metadata=metadata)\\n                documents.append(new_doc)\\n        return documents\\n\\n    def split_documents(self, documents: Iterable[Document]) -> List[Document]:\\n        \"\"\"Split documents.\"\"\"\\n        texts, metadatas = [], []\\n        for doc in documents:\\n            texts.append(doc.page_content)\\n            metadatas.append(doc.metadata)\\n        return self.create_documents(texts, metadatas=metadatas)\\n\\n    def _join_docs(self, docs: List[str], separator: str) -> Optional[str]:\\n        text = separator.join(docs)\\n        if self._strip_whitespace:\\n            text = text.strip()\\n        if text == \"\":\\n            return None\\n        else:\\n            return text\\n\\n    def _merge_splits(self, splits: Iterable[str], separator: str) -> List[str]:\\n        # We now want to combine these smaller pieces into medium size\\n        # chunks to send to the LLM.\\n        separator_len = self._length_function(separator)',\n",
              "  '\"<nav\",\\n                # Head\\n                \"<head\",\\n                \"<style\",\\n                \"<script\",\\n                \"<meta\",\\n                \"<title\",\\n                \"\",\\n            ]\\n        elif language == Language.CSHARP:\\n            return [\\n                \"\\\\ninterface \",\\n                \"\\\\nenum \",\\n                \"\\\\nimplements \",\\n                \"\\\\ndelegate \",\\n                \"\\\\nevent \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nabstract \",\\n                # Split along method definitions\\n                \"\\\\npublic \",\\n                \"\\\\nprotected \",\\n                \"\\\\nprivate \",\\n                \"\\\\nstatic \",\\n                \"\\\\nreturn \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\ncontinue \",\\n                \"\\\\nfor \",\\n                \"\\\\nforeach \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\nbreak \",\\n                \"\\\\ncase \",\\n                \"\\\\nelse \",\\n                # Split by exceptions\\n                \"\\\\ntry \",\\n                \"\\\\nthrow \",\\n                \"\\\\nfinally \",\\n                \"\\\\ncatch \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SOL:\\n            return [\\n                # Split along compiler information definitions\\n                \"\\\\npragma \",\\n                \"\\\\nusing \",\\n                # Split along contract definitions\\n                \"\\\\ncontract \",\\n                \"\\\\ninterface \",\\n                \"\\\\nlibrary \",\\n                # Split along method definitions\\n                \"\\\\nconstructor \",\\n                \"\\\\ntype \",\\n                \"\\\\nfunction \",\\n                \"\\\\nevent \",\\n                \"\\\\nmodifier \",\\n                \"\\\\nerror \",\\n                \"\\\\nstruct \",\\n                \"\\\\nenum \",\\n                # Split along control flow statements',\n",
              "  'docs = []\\n        current_doc: List[str] = []\\n        total = 0\\n        for d in splits:\\n            _len = self._length_function(d)\\n            if (\\n                total + _len + (separator_len if len(current_doc) > 0 else 0)\\n                > self._chunk_size\\n            ):\\n                if total > self._chunk_size:\\n                    logger.warning(\\n                        f\"Created a chunk of size {total}, \"\\n                        f\"which is longer than the specified {self._chunk_size}\"\\n                    )\\n                if len(current_doc) > 0:\\n                    doc = self._join_docs(current_doc, separator)\\n                    if doc is not None:\\n                        docs.append(doc)\\n                    # Keep on popping if:\\n                    # - we have a larger chunk than in the chunk overlap\\n                    # - or if we still have any chunks and the length is long\\n                    while total > self._chunk_overlap or (\\n                        total + _len + (separator_len if len(current_doc) > 0 else 0)\\n                        > self._chunk_size\\n                        and total > 0\\n                    ):\\n                        total -= self._length_function(current_doc[0]) + (\\n                            separator_len if len(current_doc) > 1 else 0\\n                        )\\n                        current_doc = current_doc[1:]\\n            current_doc.append(d)\\n            total += _len + (separator_len if len(current_doc) > 1 else 0)\\n        doc = self._join_docs(current_doc, separator)\\n        if doc is not None:\\n            docs.append(doc)\\n        return docs\\n\\n    @classmethod\\n    def from_huggingface_tokenizer(cls, tokenizer: Any, **kwargs: Any) -> TextSplitter:\\n        \"\"\"Text splitter that uses HuggingFace tokenizer to count length.\"\"\"\\n        try:\\n            from transformers import PreTrainedTokenizerBase',\n",
              "  '@abstractmethod\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split text into multiple components.\"\"\"\\n\\n    def create_documents(\\n        self, texts: List[str], metadatas: Optional[List[dict]] = None\\n    ) -> List[Document]:\\n        \"\"\"Create documents from a list of texts.\"\"\"\\n        _metadatas = metadatas or [{}] * len(texts)\\n        documents = []\\n        for i, text in enumerate(texts):\\n            index = -1\\n            for chunk in self.split_text(text):\\n                metadata = copy.deepcopy(_metadatas[i])\\n                if self._add_start_index:\\n                    index = text.find(chunk, index + 1)\\n                    metadata[\"start_index\"] = index\\n                new_doc = Document(page_content=chunk, metadata=metadata)\\n                documents.append(new_doc)\\n        return documents\\n\\n    def split_documents(self, documents: Iterable[Document]) -> List[Document]:\\n        \"\"\"Split documents.\"\"\"\\n        texts, metadatas = [], []\\n        for doc in documents:\\n            texts.append(doc.page_content)\\n            metadatas.append(doc.metadata)\\n        return self.create_documents(texts, metadatas=metadatas)\\n\\n    def _join_docs(self, docs: List[str], separator: str) -> Optional[str]:\\n        text = separator.join(docs)\\n        if self._strip_whitespace:\\n            text = text.strip()\\n        if text == \"\":\\n            return None\\n        else:\\n            return text\\n\\n    def _merge_splits(self, splits: Iterable[str], separator: str) -> List[str]:\\n        # We now want to combine these smaller pieces into medium size\\n        # chunks to send to the LLM.\\n        separator_len = self._length_function(separator)',\n",
              "  'class TextSplitter(BaseDocumentTransformer, ABC):\\n    \"\"\"Interface for splitting text into chunks.\"\"\"\\n\\n    def __init__(\\n        self,\\n        chunk_size: int = 4000,\\n        chunk_overlap: int = 200,\\n        length_function: Callable[[str], int] = len,\\n        keep_separator: bool = False,\\n        add_start_index: bool = False,\\n        strip_whitespace: bool = True,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\\n\\n        Args:\\n            chunk_size: Maximum size of chunks to return\\n            chunk_overlap: Overlap in characters between chunks\\n            length_function: Function that measures the length of given chunks\\n            keep_separator: Whether to keep the separator in the chunks\\n            add_start_index: If `True`, includes chunk\\'s start index in metadata\\n            strip_whitespace: If `True`, strips whitespace from the start and end of\\n                              every document\\n        \"\"\"\\n        if chunk_overlap > chunk_size:\\n            raise ValueError(\\n                f\"Got a larger chunk overlap ({chunk_overlap}) than chunk size \"\\n                f\"({chunk_size}), should be smaller.\"\\n            )\\n        self._chunk_size = chunk_size\\n        self._chunk_overlap = chunk_overlap\\n        self._length_function = length_function\\n        self._keep_separator = keep_separator\\n        self._add_start_index = add_start_index\\n        self._strip_whitespace = strip_whitespace\\n\\n    @abstractmethod\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split text into multiple components.\"\"\"',\n",
              "  'def _split_text_with_regex(\\n    text: str, separator: str, keep_separator: bool\\n) -> List[str]:\\n    # Now that we have the separator, split the text\\n    if separator:\\n        if keep_separator:\\n            # The parentheses in the pattern keep the delimiters in the result.\\n            _splits = re.split(f\"({separator})\", text)\\n            splits = [_splits[i] + _splits[i + 1] for i in range(1, len(_splits), 2)]\\n            if len(_splits) % 2 == 0:\\n                splits += _splits[-1:]\\n            splits = [_splits[0]] + splits\\n        else:\\n            splits = re.split(separator, text)\\n    else:\\n        splits = list(text)\\n    return [s for s in splits if s != \"\"]',\n",
              "  '\"\"\"**Text Splitters** are classes for splitting text.\\n\\n\\n**Class hierarchy:**\\n\\n.. code-block::\\n\\n    BaseDocumentTransformer --> TextSplitter --> <name>TextSplitter  # Example: CharacterTextSplitter\\n                                                 RecursiveCharacterTextSplitter -->  <name>TextSplitter\\n\\nNote: **MarkdownHeaderTextSplitter** and **HTMLHeaderTextSplitter do not derive from TextSplitter.\\n\\n\\n**Main helpers:**\\n\\n.. code-block::\\n\\n    Document, Tokenizer, Language, LineType, HeaderType\\n\\n\"\"\"  # noqa: E501\\n\\nfrom __future__ import annotations\\n\\nimport asyncio\\nimport copy\\nimport logging\\nimport pathlib\\nimport re\\nfrom abc import ABC, abstractmethod\\nfrom dataclasses import dataclass\\nfrom enum import Enum\\nfrom functools import partial\\nfrom io import BytesIO, StringIO\\nfrom typing import (\\n    AbstractSet,\\n    Any,\\n    Callable,\\n    Collection,\\n    Dict,\\n    Iterable,\\n    List,\\n    Literal,\\n    Optional,\\n    Sequence,\\n    Tuple,\\n    Type,\\n    TypedDict,\\n    TypeVar,\\n    Union,\\n    cast,\\n)\\n\\nimport requests\\nfrom langchain_core.documents import BaseDocumentTransformer, Document\\n\\nlogger = logging.getLogger(__name__)\\n\\nTS = TypeVar(\"TS\", bound=\"TextSplitter\")\\n\\n\\ndef _make_spacy_pipeline_for_splitting(\\n    pipeline: str, *, max_length: int = 1_000_000\\n) -> Any:  # avoid importing spacy\\n    try:\\n        import spacy\\n    except ImportError:\\n        raise ImportError(\\n            \"Spacy is not installed, please install it with `pip install spacy`.\"\\n        )\\n    if pipeline == \"sentencizer\":\\n        from spacy.lang.en import English\\n\\n        sentencizer = English()\\n        sentencizer.add_pipe(\"sentencizer\")\\n    else:\\n        sentencizer = spacy.load(pipeline, exclude=[\"ner\", \"tagger\"])\\n        sentencizer.max_length = max_length\\n    return sentencizer',\n",
              "  'class PythonCodeTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Python syntax.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a PythonCodeTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.PYTHON)\\n        super().__init__(separators=separators, **kwargs)\\n\\n\\nclass MarkdownTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Markdown-formatted headings.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a MarkdownTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.MARKDOWN)\\n        super().__init__(separators=separators, **kwargs)\\n\\n\\nclass LatexTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Latex-formatted layout elements.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a LatexTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.LATEX)\\n        super().__init__(separators=separators, **kwargs)',\n",
              "  'class NLTKTextSplitter(TextSplitter):\\n    \"\"\"Splitting text using NLTK package.\"\"\"\\n\\n    def __init__(\\n        self, separator: str = \"\\\\n\\\\n\", language: str = \"english\", **kwargs: Any\\n    ) -> None:\\n        \"\"\"Initialize the NLTK splitter.\"\"\"\\n        super().__init__(**kwargs)\\n        try:\\n            from nltk.tokenize import sent_tokenize\\n\\n            self._tokenizer = sent_tokenize\\n        except ImportError:\\n            raise ImportError(\\n                \"NLTK is not installed, please install it with `pip install nltk`.\"\\n            )\\n        self._separator = separator\\n        self._language = language\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        # First we naively split the large input into a bunch of smaller ones.\\n        splits = self._tokenizer(text, language=self._language)\\n        return self._merge_splits(splits, self._separator)\\n\\n\\nclass SpacyTextSplitter(TextSplitter):\\n    \"\"\"Splitting text using Spacy package.\\n\\n\\n    Per default, Spacy\\'s `en_core_web_sm` model is used and\\n    its default max_length is 1000000 (it is the length of maximum character\\n    this model takes which can be increased for large files). For a faster, but\\n    potentially less accurate splitting, you can use `pipeline=\\'sentencizer\\'`.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        separator: str = \"\\\\n\\\\n\",\\n        pipeline: str = \"en_core_web_sm\",\\n        max_length: int = 1_000_000,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Initialize the spacy text splitter.\"\"\"\\n        super().__init__(**kwargs)\\n        self._tokenizer = _make_spacy_pipeline_for_splitting(\\n            pipeline, max_length=max_length\\n        )\\n        self._separator = separator\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        splits = (s.text for s in self._tokenizer(text).sents)\\n        return self._merge_splits(splits, self._separator)\\n\\n\\n# For backwards compatibility',\n",
              "  'else:\\n            raise ValueError(\\n                f\"Language {language} is not supported! \"\\n                f\"Please choose from {list(Language)}\"\\n            )',\n",
              "  '\"\\\\nevent \",\\n                \"\\\\nmodifier \",\\n                \"\\\\nerror \",\\n                \"\\\\nstruct \",\\n                \"\\\\nenum \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo while \",\\n                \"\\\\nassembly \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.COBOL:\\n            return [\\n                # Split along divisions\\n                \"\\\\nIDENTIFICATION DIVISION.\",\\n                \"\\\\nENVIRONMENT DIVISION.\",\\n                \"\\\\nDATA DIVISION.\",\\n                \"\\\\nPROCEDURE DIVISION.\",\\n                # Split along sections within DATA DIVISION\\n                \"\\\\nWORKING-STORAGE SECTION.\",\\n                \"\\\\nLINKAGE SECTION.\",\\n                \"\\\\nFILE SECTION.\",\\n                # Split along sections within PROCEDURE DIVISION\\n                \"\\\\nINPUT-OUTPUT SECTION.\",\\n                # Split along paragraphs and common statements\\n                \"\\\\nOPEN \",\\n                \"\\\\nCLOSE \",\\n                \"\\\\nREAD \",\\n                \"\\\\nWRITE \",\\n                \"\\\\nIF \",\\n                \"\\\\nELSE \",\\n                \"\\\\nMOVE \",\\n                \"\\\\nPERFORM \",\\n                \"\\\\nUNTIL \",\\n                \"\\\\nVARYING \",\\n                \"\\\\nACCEPT \",\\n                \"\\\\nDISPLAY \",\\n                \"\\\\nSTOP RUN.\",\\n                # Split by the normal type of lines\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]',\n",
              "  'class TextSplitter(BaseDocumentTransformer, ABC):\\n    \"\"\"Interface for splitting text into chunks.\"\"\"\\n\\n    def __init__(\\n        self,\\n        chunk_size: int = 4000,\\n        chunk_overlap: int = 200,\\n        length_function: Callable[[str], int] = len,\\n        keep_separator: bool = False,\\n        add_start_index: bool = False,\\n        strip_whitespace: bool = True,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\\n\\n        Args:\\n            chunk_size: Maximum size of chunks to return\\n            chunk_overlap: Overlap in characters between chunks\\n            length_function: Function that measures the length of given chunks\\n            keep_separator: Whether to keep the separator in the chunks\\n            add_start_index: If `True`, includes chunk\\'s start index in metadata\\n            strip_whitespace: If `True`, strips whitespace from the start and end of\\n                              every document\\n        \"\"\"\\n        if chunk_overlap > chunk_size:\\n            raise ValueError(\\n                f\"Got a larger chunk overlap ({chunk_overlap}) than chunk size \"\\n                f\"({chunk_size}), should be smaller.\"\\n            )\\n        self._chunk_size = chunk_size\\n        self._chunk_overlap = chunk_overlap\\n        self._length_function = length_function\\n        self._keep_separator = keep_separator\\n        self._add_start_index = add_start_index\\n        self._strip_whitespace = strip_whitespace\\n\\n    @abstractmethod\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split text into multiple components.\"\"\"',\n",
              "  '\"\\\\n\\\\\\\\*\\\\\\\\*\\\\\\\\*+\\\\n\",\\n                \"\\\\n---+\\\\n\",\\n                \"\\\\n___+\\\\n\",\\n                # Note that this splitter doesn\\'t handle horizontal lines defined\\n                # by *three or more* of ***, ---, or ___, but this is not handled\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.LATEX:\\n            return [\\n                # First, try to split along Latex sections\\n                \"\\\\n\\\\\\\\\\\\\\\\chapter{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\section{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\subsection{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\subsubsection{\",\\n                # Now split by environments\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{enumerate}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{itemize}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{description}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{list}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{quote}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{quotation}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{verse}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{verbatim}\",\\n                # Now split by math environments\\n                \"\\\\n\\\\\\\\\\\\begin{align}\",\\n                \"$$\",\\n                \"$\",\\n                # Now split by the normal type of lines\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.HTML:\\n            return [\\n                # First, try to split along HTML tags\\n                \"<body\",\\n                \"<div\",\\n                \"<p\",\\n                \"<br\",\\n                \"<li\",\\n                \"<h1\",\\n                \"<h2\",\\n                \"<h3\",\\n                \"<h4\",\\n                \"<h5\",\\n                \"<h6\",\\n                \"<span\",\\n                \"<table\",\\n                \"<tr\",\\n                \"<td\",\\n                \"<th\",\\n                \"<ul\",\\n                \"<ol\",\\n                \"<header\",\\n                \"<footer\",\\n                \"<nav\",\\n                # Head\\n                \"<head\",\\n                \"<style\",\\n                \"<script\",\\n                \"<meta\",\\n                \"<title\",',\n",
              "  '\" \",\\n                \"\",\\n            ]\\n        elif language == Language.SCALA:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nobject \",\\n                # Split along method definitions\\n                \"\\\\ndef \",\\n                \"\\\\nval \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nmatch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SWIFT:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunc \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nstruct \",\\n                \"\\\\nenum \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.MARKDOWN:\\n            return [\\n                # First, try to split along Markdown headings (starting with level 2)\\n                \"\\\\n#{1,6} \",\\n                # Note the alternative syntax for headings (below) is not handled here\\n                # Heading level 2\\n                # ---------------\\n                # End of code block\\n                \"```\\\\n\",\\n                # Horizontal lines\\n                \"\\\\n\\\\\\\\*\\\\\\\\*\\\\\\\\*+\\\\n\",\\n                \"\\\\n---+\\\\n\",\\n                \"\\\\n___+\\\\n\",\\n                # Note that this splitter doesn\\'t handle horizontal lines defined',\n",
              "  'return [\\n                # First, try to split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\ndef \",\\n                \"\\\\n\\\\tdef \",\\n                # Now split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.RST:\\n            return [\\n                # Split along section titles\\n                \"\\\\n=+\\\\n\",\\n                \"\\\\n-+\\\\n\",\\n                \"\\\\n\\\\\\\\*+\\\\n\",\\n                # Split along directive markers\\n                \"\\\\n\\\\n.. *\\\\n\\\\n\",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.RUBY:\\n            return [\\n                # Split along method definitions\\n                \"\\\\ndef \",\\n                \"\\\\nclass \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nunless \",\\n                \"\\\\nwhile \",\\n                \"\\\\nfor \",\\n                \"\\\\ndo \",\\n                \"\\\\nbegin \",\\n                \"\\\\nrescue \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.RUST:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfn \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nwhile \",\\n                \"\\\\nfor \",\\n                \"\\\\nloop \",\\n                \"\\\\nmatch \",\\n                \"\\\\nconst \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SCALA:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",',\n",
              "  '\"\\\\nfunction \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                \"\\\\ndefault \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.PHP:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunction \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nforeach \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.PROTO:\\n            return [\\n                # Split along message definitions\\n                \"\\\\nmessage \",\\n                # Split along service definitions\\n                \"\\\\nservice \",\\n                # Split along enum definitions\\n                \"\\\\nenum \",\\n                # Split along option definitions\\n                \"\\\\noption \",\\n                # Split along import statements\\n                \"\\\\nimport \",\\n                # Split along syntax declarations\\n                \"\\\\nsyntax \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.PYTHON:\\n            return [\\n                # First, try to split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\ndef \",\\n                \"\\\\n\\\\tdef \",',\n",
              "  '\"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.KOTLIN:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along method definitions\\n                \"\\\\npublic \",\\n                \"\\\\nprotected \",\\n                \"\\\\nprivate \",\\n                \"\\\\ninternal \",\\n                \"\\\\ncompanion \",\\n                \"\\\\nfun \",\\n                \"\\\\nval \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nwhen \",\\n                \"\\\\ncase \",\\n                \"\\\\nelse \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.JS:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunction \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                \"\\\\nvar \",\\n                \"\\\\nclass \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                \"\\\\ndefault \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.TS:\\n            return [\\n                \"\\\\nenum \",\\n                \"\\\\ninterface \",\\n                \"\\\\nnamespace \",\\n                \"\\\\ntype \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along function definitions\\n                \"\\\\nfunction \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements',\n",
              "  '@staticmethod\\n    def get_separators_for_language(language: Language) -> List[str]:\\n        if language == Language.CPP:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along function definitions\\n                \"\\\\nvoid \",\\n                \"\\\\nint \",\\n                \"\\\\nfloat \",\\n                \"\\\\ndouble \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.GO:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunc \",\\n                \"\\\\nvar \",\\n                \"\\\\nconst \",\\n                \"\\\\ntype \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.JAVA:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along method definitions\\n                \"\\\\npublic \",\\n                \"\\\\nprotected \",\\n                \"\\\\nprivate \",\\n                \"\\\\nstatic \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.KOTLIN:\\n            return [',\n",
              "  '_separator = separator if self._is_separator_regex else re.escape(separator)\\n        splits = _split_text_with_regex(text, _separator, self._keep_separator)\\n\\n        # Now go merging things, recursively splitting longer texts.\\n        _good_splits = []\\n        _separator = \"\" if self._keep_separator else separator\\n        for s in splits:\\n            if self._length_function(s) < self._chunk_size:\\n                _good_splits.append(s)\\n            else:\\n                if _good_splits:\\n                    merged_text = self._merge_splits(_good_splits, _separator)\\n                    final_chunks.extend(merged_text)\\n                    _good_splits = []\\n                if not new_separators:\\n                    final_chunks.append(s)\\n                else:\\n                    other_info = self._split_text(s, new_separators)\\n                    final_chunks.extend(other_info)\\n        if _good_splits:\\n            merged_text = self._merge_splits(_good_splits, _separator)\\n            final_chunks.extend(merged_text)\\n        return final_chunks\\n\\n    def split_text(self, text: str) -> List[str]:\\n        return self._split_text(text, self._separators)\\n\\n    @classmethod\\n    def from_language(\\n        cls, language: Language, **kwargs: Any\\n    ) -> RecursiveCharacterTextSplitter:\\n        separators = cls.get_separators_for_language(language)\\n        return cls(separators=separators, is_separator_regex=True, **kwargs)',\n",
              "  'class RecursiveCharacterTextSplitter(TextSplitter):\\n    \"\"\"Splitting text by recursively look at characters.\\n\\n    Recursively tries to split by different characters to find one\\n    that works.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        separators: Optional[List[str]] = None,\\n        keep_separator: bool = True,\\n        is_separator_regex: bool = False,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(keep_separator=keep_separator, **kwargs)\\n        self._separators = separators or [\"\\\\n\\\\n\", \"\\\\n\", \" \", \"\"]\\n        self._is_separator_regex = is_separator_regex\\n\\n    def _split_text(self, text: str, separators: List[str]) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        final_chunks = []\\n        # Get appropriate separator to use\\n        separator = separators[-1]\\n        new_separators = []\\n        for i, _s in enumerate(separators):\\n            _separator = _s if self._is_separator_regex else re.escape(_s)\\n            if _s == \"\":\\n                separator = _s\\n                break\\n            if re.search(_separator, text):\\n                separator = _s\\n                new_separators = separators[i + 1 :]\\n                break\\n\\n        _separator = separator if self._is_separator_regex else re.escape(separator)\\n        splits = _split_text_with_regex(text, _separator, self._keep_separator)',\n",
              "  'def split_text(self, text: str) -> List[Document]:\\n        \"\"\"Split markdown file\\n        Args:\\n            text: Markdown file\"\"\"\\n\\n        # Split the input text by newline character (\"\\\\n\").\\n        lines = text.split(\"\\\\n\")\\n        # Final output\\n        lines_with_metadata: List[LineType] = []\\n        # Content and metadata of the chunk currently being processed\\n        current_content: List[str] = []\\n        current_metadata: Dict[str, str] = {}\\n        # Keep track of the nested header structure\\n        # header_stack: List[Dict[str, Union[int, str]]] = []\\n        header_stack: List[HeaderType] = []\\n        initial_metadata: Dict[str, str] = {}\\n\\n        in_code_block = False\\n        opening_fence = \"\"\\n\\n        for line in lines:\\n            stripped_line = line.strip()\\n\\n            if not in_code_block:\\n                # Exclude inline code spans\\n                if stripped_line.startswith(\"```\") and stripped_line.count(\"```\") == 1:\\n                    in_code_block = True\\n                    opening_fence = \"```\"\\n                elif stripped_line.startswith(\"~~~\"):\\n                    in_code_block = True\\n                    opening_fence = \"~~~\"\\n            else:\\n                if stripped_line.startswith(opening_fence):\\n                    in_code_block = False\\n                    opening_fence = \"\"\\n\\n            if in_code_block:\\n                current_content.append(stripped_line)\\n                continue',\n",
              "  'class TokenTextSplitter(TextSplitter):\\n    \"\"\"Splitting text to tokens using model tokenizer.\"\"\"\\n\\n    def __init__(\\n        self,\\n        encoding_name: str = \"gpt2\",\\n        model_name: Optional[str] = None,\\n        allowed_special: Union[Literal[\"all\"], AbstractSet[str]] = set(),\\n        disallowed_special: Union[Literal[\"all\"], Collection[str]] = \"all\",\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(**kwargs)\\n        try:\\n            import tiktoken\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import tiktoken python package. \"\\n                \"This is needed in order to for TokenTextSplitter. \"\\n                \"Please install it with `pip install tiktoken`.\"\\n            )\\n\\n        if model_name is not None:\\n            enc = tiktoken.encoding_for_model(model_name)\\n        else:\\n            enc = tiktoken.get_encoding(encoding_name)\\n        self._tokenizer = enc\\n        self._allowed_special = allowed_special\\n        self._disallowed_special = disallowed_special\\n\\n    def split_text(self, text: str) -> List[str]:\\n        def _encode(_text: str) -> List[int]:\\n            return self._tokenizer.encode(\\n                _text,\\n                allowed_special=self._allowed_special,\\n                disallowed_special=self._disallowed_special,\\n            )\\n\\n        tokenizer = Tokenizer(\\n            chunk_overlap=self._chunk_overlap,\\n            tokens_per_chunk=self._chunk_size,\\n            decode=self._tokenizer.decode,\\n            encode=_encode,\\n        )\\n\\n        return split_text_on_tokens(text=text, tokenizer=tokenizer)',\n",
              "  'class Tokenizer:\\n    \"\"\"Tokenizer data class.\"\"\"\\n\\n    chunk_overlap: int\\n    \"\"\"Overlap in tokens between chunks\"\"\"\\n    tokens_per_chunk: int\\n    \"\"\"Maximum number of tokens per chunk\"\"\"\\n    decode: Callable[[List[int]], str]\\n    \"\"\" Function to decode a list of token ids to a string\"\"\"\\n    encode: Callable[[str], List[int]]\\n    \"\"\" Function to encode a string to a list of token ids\"\"\"\\n\\n\\ndef split_text_on_tokens(*, text: str, tokenizer: Tokenizer) -> List[str]:\\n    \"\"\"Split incoming text and return chunks using tokenizer.\"\"\"\\n    splits: List[str] = []\\n    input_ids = tokenizer.encode(text)\\n    start_idx = 0\\n    cur_idx = min(start_idx + tokenizer.tokens_per_chunk, len(input_ids))\\n    chunk_ids = input_ids[start_idx:cur_idx]\\n    while start_idx < len(input_ids):\\n        splits.append(tokenizer.decode(chunk_ids))\\n        if cur_idx == len(input_ids):\\n            break\\n        start_idx += tokenizer.tokens_per_chunk - tokenizer.chunk_overlap\\n        cur_idx = min(start_idx + tokenizer.tokens_per_chunk, len(input_ids))\\n        chunk_ids = input_ids[start_idx:cur_idx]\\n    return splits',\n",
              "  '# map xhtml namespace prefix\\n        ns_map = {\"h\": \"http://www.w3.org/1999/xhtml\"}\\n\\n        # build list of elements from DOM\\n        elements = []\\n        for element in result_dom.findall(\"*//*\", ns_map):\\n            if element.findall(\"*[@class=\\'headers\\']\") or element.findall(\\n                \"*[@class=\\'chunk\\']\"\\n            ):\\n                elements.append(\\n                    ElementType(\\n                        url=file,\\n                        xpath=\"\".join(\\n                            [\\n                                node.text\\n                                for node in element.findall(\"*[@class=\\'xpath\\']\", ns_map)\\n                            ]\\n                        ),\\n                        content=\"\".join(\\n                            [\\n                                node.text\\n                                for node in element.findall(\"*[@class=\\'chunk\\']\", ns_map)\\n                            ]\\n                        ),\\n                        metadata={\\n                            # Add text of specified headers to metadata using header\\n                            # mapping.\\n                            header_mapping[node.tag]: node.text\\n                            for node in filter(\\n                                lambda x: x.tag in header_filter,\\n                                element.findall(\"*[@class=\\'headers\\']/*\", ns_map),\\n                            )\\n                        },\\n                    )\\n                )\\n\\n        if not self.return_each_element:\\n            return self.aggregate_elements_to_chunks(elements)\\n        else:\\n            return [\\n                Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n                for chunk in elements\\n            ]\\n\\n\\n# should be in newer Python versions (3.10+)\\n# @dataclass(frozen=True, kw_only=True, slots=True)\\n@dataclass(frozen=True)',\n",
              "  'return [\\n            Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n            for chunk in aggregated_chunks\\n        ]\\n\\n    def split_text_from_url(self, url: str) -> List[Document]:\\n        \"\"\"Split HTML from web URL\\n\\n        Args:\\n            url: web URL\\n        \"\"\"\\n        r = requests.get(url)\\n        return self.split_text_from_file(BytesIO(r.content))\\n\\n    def split_text(self, text: str) -> List[Document]:\\n        \"\"\"Split HTML text string\\n\\n        Args:\\n            text: HTML text\\n        \"\"\"\\n        return self.split_text_from_file(StringIO(text))\\n\\n    def split_text_from_file(self, file: Any) -> List[Document]:\\n        \"\"\"Split HTML file\\n\\n        Args:\\n            file: HTML file\\n        \"\"\"\\n        try:\\n            from lxml import etree\\n        except ImportError as e:\\n            raise ImportError(\\n                \"Unable to import lxml, please install with `pip install lxml`.\"\\n            ) from e\\n        # use lxml library to parse html document and return xml ElementTree\\n        parser = etree.HTMLParser()\\n        tree = etree.parse(file, parser)\\n\\n        # document transformation for \"structure-aware\" chunking is handled with xsl.\\n        # see comments in html_chunks_with_headers.xslt for more detailed information.\\n        xslt_path = (\\n            pathlib.Path(__file__).parent\\n            / \"document_transformers/xsl/html_chunks_with_headers.xslt\"\\n        )\\n        xslt_tree = etree.parse(xslt_path)\\n        transform = etree.XSLT(xslt_tree)\\n        result = transform(tree)\\n        result_dom = etree.fromstring(str(result))\\n\\n        # create filter and mapping for header metadata\\n        header_filter = [header[0] for header in self.headers_to_split_on]\\n        header_mapping = dict(self.headers_to_split_on)\\n\\n        # map xhtml namespace prefix\\n        ns_map = {\"h\": \"http://www.w3.org/1999/xhtml\"}',\n",
              "  'class HTMLHeaderTextSplitter:\\n    \"\"\"\\n    Splitting HTML files based on specified headers.\\n    Requires lxml package.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        headers_to_split_on: List[Tuple[str, str]],\\n        return_each_element: bool = False,\\n    ):\\n        \"\"\"Create a new HTMLHeaderTextSplitter.\\n\\n        Args:\\n            headers_to_split_on: list of tuples of headers we want to track mapped to\\n                (arbitrary) keys for metadata. Allowed header values: h1, h2, h3, h4,\\n                h5, h6 e.g. [(\"h1\", \"Header 1\"), (\"h2\", \"Header 2)].\\n            return_each_element: Return each element w/ associated headers.\\n        \"\"\"\\n        # Output element-by-element or aggregated into chunks w/ common headers\\n        self.return_each_element = return_each_element\\n        self.headers_to_split_on = sorted(headers_to_split_on)\\n\\n    def aggregate_elements_to_chunks(\\n        self, elements: List[ElementType]\\n    ) -> List[Document]:\\n        \"\"\"Combine elements with common metadata into chunks\\n\\n        Args:\\n            elements: HTML element content with associated identifying info and metadata\\n        \"\"\"\\n        aggregated_chunks: List[ElementType] = []\\n\\n        for element in elements:\\n            if (\\n                aggregated_chunks\\n                and aggregated_chunks[-1][\"metadata\"] == element[\"metadata\"]\\n            ):\\n                # If the last element in the aggregated list\\n                # has the same metadata as the current element,\\n                # append the current content to the last element\\'s content\\n                aggregated_chunks[-1][\"content\"] += \"  \\\\n\" + element[\"content\"]\\n            else:\\n                # Otherwise, append the current element to the aggregated list\\n                aggregated_chunks.append(element)\\n\\n        return [\\n            Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n            for chunk in aggregated_chunks\\n        ]',\n",
              "  'class ElementType(TypedDict):\\n    \"\"\"Element type as typed dict.\"\"\"\\n\\n    url: str\\n    xpath: str\\n    content: str\\n    metadata: Dict[str, str]',\n",
              "  'if current_content:\\n            lines_with_metadata.append(\\n                {\"content\": \"\\\\n\".join(current_content), \"metadata\": current_metadata}\\n            )\\n\\n        # lines_with_metadata has each line with associated header metadata\\n        # aggregate these into chunks based on common metadata\\n        if not self.return_each_line:\\n            return self.aggregate_lines_to_chunks(lines_with_metadata)\\n        else:\\n            return [\\n                Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n                for chunk in lines_with_metadata\\n            ]',\n",
              "  '# Push the current header to the stack\\n                        header: HeaderType = {\\n                            \"level\": current_header_level,\\n                            \"name\": name,\\n                            \"data\": stripped_line[len(sep) :].strip(),\\n                        }\\n                        header_stack.append(header)\\n                        # Update initial_metadata with the current header\\n                        initial_metadata[name] = header[\"data\"]\\n\\n                    # Add the previous line to the lines_with_metadata\\n                    # only if current_content is not empty\\n                    if current_content:\\n                        lines_with_metadata.append(\\n                            {\\n                                \"content\": \"\\\\n\".join(current_content),\\n                                \"metadata\": current_metadata.copy(),\\n                            }\\n                        )\\n                        current_content.clear()\\n\\n                    break\\n            else:\\n                if stripped_line:\\n                    current_content.append(stripped_line)\\n                elif current_content:\\n                    lines_with_metadata.append(\\n                        {\\n                            \"content\": \"\\\\n\".join(current_content),\\n                            \"metadata\": current_metadata.copy(),\\n                        }\\n                    )\\n                    current_content.clear()\\n\\n            current_metadata = initial_metadata.copy()\\n\\n        if current_content:\\n            lines_with_metadata.append(\\n                {\"content\": \"\\\\n\".join(current_content), \"metadata\": current_metadata}\\n            )',\n",
              "  'if in_code_block:\\n                current_content.append(stripped_line)\\n                continue\\n\\n            # Check each line against each of the header types (e.g., #, ##)\\n            for sep, name in self.headers_to_split_on:\\n                # Check if line starts with a header that we intend to split on\\n                if stripped_line.startswith(sep) and (\\n                    # Header with no text OR header is followed by space\\n                    # Both are valid conditions that sep is being used a header\\n                    len(stripped_line) == len(sep) or stripped_line[len(sep)] == \" \"\\n                ):\\n                    # Ensure we are tracking the header as metadata\\n                    if name is not None:\\n                        # Get the current header level\\n                        current_header_level = sep.count(\"#\")\\n\\n                        # Pop out headers of lower or same level from the stack\\n                        while (\\n                            header_stack\\n                            and header_stack[-1][\"level\"] >= current_header_level\\n                        ):\\n                            # We have encountered a new header\\n                            # at the same or higher level\\n                            popped_header = header_stack.pop()\\n                            # Clear the metadata for the\\n                            # popped header in initial_metadata\\n                            if popped_header[\"name\"] in initial_metadata:\\n                                initial_metadata.pop(popped_header[\"name\"])',\n",
              "  'class SentenceTransformersTokenTextSplitter(TextSplitter):\\n    \"\"\"Splitting text to tokens using sentence model tokenizer.\"\"\"\\n\\n    def __init__(\\n        self,\\n        chunk_overlap: int = 50,\\n        model_name: str = \"sentence-transformers/all-mpnet-base-v2\",\\n        tokens_per_chunk: Optional[int] = None,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(**kwargs, chunk_overlap=chunk_overlap)\\n\\n        try:\\n            from sentence_transformers import SentenceTransformer\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import sentence_transformer python package. \"\\n                \"This is needed in order to for SentenceTransformersTokenTextSplitter. \"\\n                \"Please install it with `pip install sentence-transformers`.\"\\n            )\\n\\n        self.model_name = model_name\\n        self._model = SentenceTransformer(self.model_name)\\n        self.tokenizer = self._model.tokenizer\\n        self._initialize_chunk_configuration(tokens_per_chunk=tokens_per_chunk)\\n\\n    def _initialize_chunk_configuration(\\n        self, *, tokens_per_chunk: Optional[int]\\n    ) -> None:\\n        self.maximum_tokens_per_chunk = cast(int, self._model.max_seq_length)\\n\\n        if tokens_per_chunk is None:\\n            self.tokens_per_chunk = self.maximum_tokens_per_chunk\\n        else:\\n            self.tokens_per_chunk = tokens_per_chunk\\n\\n        if self.tokens_per_chunk > self.maximum_tokens_per_chunk:\\n            raise ValueError(\\n                f\"The token limit of the models \\'{self.model_name}\\'\"\\n                f\" is: {self.maximum_tokens_per_chunk}.\"\\n                f\" Argument tokens_per_chunk={self.tokens_per_chunk}\"\\n                f\" > maximum token limit.\"\\n            )\\n\\n    def split_text(self, text: str) -> List[str]:\\n        def encode_strip_start_and_stop_token_ids(text: str) -> List[int]:\\n            return self._encode(text)[1:-1]',\n",
              "  'class MarkdownHeaderTextSplitter:\\n    \"\"\"Splitting markdown files based on specified headers.\"\"\"\\n\\n    def __init__(\\n        self, headers_to_split_on: List[Tuple[str, str]], return_each_line: bool = False\\n    ):\\n        \"\"\"Create a new MarkdownHeaderTextSplitter.\\n\\n        Args:\\n            headers_to_split_on: Headers we want to track\\n            return_each_line: Return each line w/ associated headers\\n        \"\"\"\\n        # Output line-by-line or aggregated into chunks w/ common headers\\n        self.return_each_line = return_each_line\\n        # Given the headers we want to split on,\\n        # (e.g., \"#, ##, etc\") order by length\\n        self.headers_to_split_on = sorted(\\n            headers_to_split_on, key=lambda split: len(split[0]), reverse=True\\n        )\\n\\n    def aggregate_lines_to_chunks(self, lines: List[LineType]) -> List[Document]:\\n        \"\"\"Combine lines with common metadata into chunks\\n        Args:\\n            lines: Line of text / associated header metadata\\n        \"\"\"\\n        aggregated_chunks: List[LineType] = []\\n\\n        for line in lines:\\n            if (\\n                aggregated_chunks\\n                and aggregated_chunks[-1][\"metadata\"] == line[\"metadata\"]\\n            ):\\n                # If the last line in the aggregated list\\n                # has the same metadata as the current line,\\n                # append the current content to the last lines\\'s content\\n                aggregated_chunks[-1][\"content\"] += \"  \\\\n\" + line[\"content\"]\\n            else:\\n                # Otherwise, append the current line to the aggregated list\\n                aggregated_chunks.append(line)\\n\\n        return [\\n            Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n            for chunk in aggregated_chunks\\n        ]\\n\\n    def split_text(self, text: str) -> List[Document]:\\n        \"\"\"Split markdown file\\n        Args:\\n            text: Markdown file\"\"\"',\n",
              "  'class CharacterTextSplitter(TextSplitter):\\n    \"\"\"Splitting text that looks at characters.\"\"\"\\n\\n    def __init__(\\n        self, separator: str = \"\\\\n\\\\n\", is_separator_regex: bool = False, **kwargs: Any\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(**kwargs)\\n        self._separator = separator\\n        self._is_separator_regex = is_separator_regex\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        # First we naively split the large input into a bunch of smaller ones.\\n        separator = (\\n            self._separator if self._is_separator_regex else re.escape(self._separator)\\n        )\\n        splits = _split_text_with_regex(text, separator, self._keep_separator)\\n        _separator = \"\" if self._keep_separator else self._separator\\n        return self._merge_splits(splits, _separator)\\n\\n\\nclass LineType(TypedDict):\\n    \"\"\"Line type as typed dict.\"\"\"\\n\\n    metadata: Dict[str, str]\\n    content: str\\n\\n\\nclass HeaderType(TypedDict):\\n    \"\"\"Header type as typed dict.\"\"\"\\n\\n    level: int\\n    name: str\\n    data: str',\n",
              "  'if issubclass(cls, TokenTextSplitter):\\n            extra_kwargs = {\\n                \"encoding_name\": encoding_name,\\n                \"model_name\": model_name,\\n                \"allowed_special\": allowed_special,\\n                \"disallowed_special\": disallowed_special,\\n            }\\n            kwargs = {**kwargs, **extra_kwargs}\\n\\n        return cls(length_function=_tiktoken_encoder, **kwargs)\\n\\n    def transform_documents(\\n        self, documents: Sequence[Document], **kwargs: Any\\n    ) -> Sequence[Document]:\\n        \"\"\"Transform sequence of documents by splitting them.\"\"\"\\n        return self.split_documents(list(documents))\\n\\n    async def atransform_documents(\\n        self, documents: Sequence[Document], **kwargs: Any\\n    ) -> Sequence[Document]:\\n        \"\"\"Asynchronously transform a sequence of documents by splitting them.\"\"\"\\n        return await asyncio.get_running_loop().run_in_executor(\\n            None, partial(self.transform_documents, **kwargs), documents\\n        )',\n",
              "  'if not isinstance(tokenizer, PreTrainedTokenizerBase):\\n                raise ValueError(\\n                    \"Tokenizer received was not an instance of PreTrainedTokenizerBase\"\\n                )\\n\\n            def _huggingface_tokenizer_length(text: str) -> int:\\n                return len(tokenizer.encode(text))\\n\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import transformers python package. \"\\n                \"Please install it with `pip install transformers`.\"\\n            )\\n        return cls(length_function=_huggingface_tokenizer_length, **kwargs)\\n\\n    @classmethod\\n    def from_tiktoken_encoder(\\n        cls: Type[TS],\\n        encoding_name: str = \"gpt2\",\\n        model_name: Optional[str] = None,\\n        allowed_special: Union[Literal[\"all\"], AbstractSet[str]] = set(),\\n        disallowed_special: Union[Literal[\"all\"], Collection[str]] = \"all\",\\n        **kwargs: Any,\\n    ) -> TS:\\n        \"\"\"Text splitter that uses tiktoken encoder to count length.\"\"\"\\n        try:\\n            import tiktoken\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import tiktoken python package. \"\\n                \"This is needed in order to calculate max_tokens_for_prompt. \"\\n                \"Please install it with `pip install tiktoken`.\"\\n            )\\n\\n        if model_name is not None:\\n            enc = tiktoken.encoding_for_model(model_name)\\n        else:\\n            enc = tiktoken.get_encoding(encoding_name)\\n\\n        def _tiktoken_encoder(text: str) -> int:\\n            return len(\\n                enc.encode(\\n                    text,\\n                    allowed_special=allowed_special,\\n                    disallowed_special=disallowed_special,\\n                )\\n            )',\n",
              "  'docs = []\\n        current_doc: List[str] = []\\n        total = 0\\n        for d in splits:\\n            _len = self._length_function(d)\\n            if (\\n                total + _len + (separator_len if len(current_doc) > 0 else 0)\\n                > self._chunk_size\\n            ):\\n                if total > self._chunk_size:\\n                    logger.warning(\\n                        f\"Created a chunk of size {total}, \"\\n                        f\"which is longer than the specified {self._chunk_size}\"\\n                    )\\n                if len(current_doc) > 0:\\n                    doc = self._join_docs(current_doc, separator)\\n                    if doc is not None:\\n                        docs.append(doc)\\n                    # Keep on popping if:\\n                    # - we have a larger chunk than in the chunk overlap\\n                    # - or if we still have any chunks and the length is long\\n                    while total > self._chunk_overlap or (\\n                        total + _len + (separator_len if len(current_doc) > 0 else 0)\\n                        > self._chunk_size\\n                        and total > 0\\n                    ):\\n                        total -= self._length_function(current_doc[0]) + (\\n                            separator_len if len(current_doc) > 1 else 0\\n                        )\\n                        current_doc = current_doc[1:]\\n            current_doc.append(d)\\n            total += _len + (separator_len if len(current_doc) > 1 else 0)\\n        doc = self._join_docs(current_doc, separator)\\n        if doc is not None:\\n            docs.append(doc)\\n        return docs\\n\\n    @classmethod\\n    def from_huggingface_tokenizer(cls, tokenizer: Any, **kwargs: Any) -> TextSplitter:\\n        \"\"\"Text splitter that uses HuggingFace tokenizer to count length.\"\"\"\\n        try:\\n            from transformers import PreTrainedTokenizerBase',\n",
              "  '@abstractmethod\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split text into multiple components.\"\"\"\\n\\n    def create_documents(\\n        self, texts: List[str], metadatas: Optional[List[dict]] = None\\n    ) -> List[Document]:\\n        \"\"\"Create documents from a list of texts.\"\"\"\\n        _metadatas = metadatas or [{}] * len(texts)\\n        documents = []\\n        for i, text in enumerate(texts):\\n            index = -1\\n            for chunk in self.split_text(text):\\n                metadata = copy.deepcopy(_metadatas[i])\\n                if self._add_start_index:\\n                    index = text.find(chunk, index + 1)\\n                    metadata[\"start_index\"] = index\\n                new_doc = Document(page_content=chunk, metadata=metadata)\\n                documents.append(new_doc)\\n        return documents\\n\\n    def split_documents(self, documents: Iterable[Document]) -> List[Document]:\\n        \"\"\"Split documents.\"\"\"\\n        texts, metadatas = [], []\\n        for doc in documents:\\n            texts.append(doc.page_content)\\n            metadatas.append(doc.metadata)\\n        return self.create_documents(texts, metadatas=metadatas)\\n\\n    def _join_docs(self, docs: List[str], separator: str) -> Optional[str]:\\n        text = separator.join(docs)\\n        if self._strip_whitespace:\\n            text = text.strip()\\n        if text == \"\":\\n            return None\\n        else:\\n            return text\\n\\n    def _merge_splits(self, splits: Iterable[str], separator: str) -> List[str]:\\n        # We now want to combine these smaller pieces into medium size\\n        # chunks to send to the LLM.\\n        separator_len = self._length_function(separator)',\n",
              "  'class TextSplitter(BaseDocumentTransformer, ABC):\\n    \"\"\"Interface for splitting text into chunks.\"\"\"\\n\\n    def __init__(\\n        self,\\n        chunk_size: int = 4000,\\n        chunk_overlap: int = 200,\\n        length_function: Callable[[str], int] = len,\\n        keep_separator: bool = False,\\n        add_start_index: bool = False,\\n        strip_whitespace: bool = True,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\\n\\n        Args:\\n            chunk_size: Maximum size of chunks to return\\n            chunk_overlap: Overlap in characters between chunks\\n            length_function: Function that measures the length of given chunks\\n            keep_separator: Whether to keep the separator in the chunks\\n            add_start_index: If `True`, includes chunk\\'s start index in metadata\\n            strip_whitespace: If `True`, strips whitespace from the start and end of\\n                              every document\\n        \"\"\"\\n        if chunk_overlap > chunk_size:\\n            raise ValueError(\\n                f\"Got a larger chunk overlap ({chunk_overlap}) than chunk size \"\\n                f\"({chunk_size}), should be smaller.\"\\n            )\\n        self._chunk_size = chunk_size\\n        self._chunk_overlap = chunk_overlap\\n        self._length_function = length_function\\n        self._keep_separator = keep_separator\\n        self._add_start_index = add_start_index\\n        self._strip_whitespace = strip_whitespace\\n\\n    @abstractmethod\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split text into multiple components.\"\"\"',\n",
              "  'def _split_text_with_regex(\\n    text: str, separator: str, keep_separator: bool\\n) -> List[str]:\\n    # Now that we have the separator, split the text\\n    if separator:\\n        if keep_separator:\\n            # The parentheses in the pattern keep the delimiters in the result.\\n            _splits = re.split(f\"({separator})\", text)\\n            splits = [_splits[i] + _splits[i + 1] for i in range(1, len(_splits), 2)]\\n            if len(_splits) % 2 == 0:\\n                splits += _splits[-1:]\\n            splits = [_splits[0]] + splits\\n        else:\\n            splits = re.split(separator, text)\\n    else:\\n        splits = list(text)\\n    return [s for s in splits if s != \"\"]',\n",
              "  '\" \",\\n                \"\",\\n            ]\\n        elif language == Language.SCALA:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nobject \",\\n                # Split along method definitions\\n                \"\\\\ndef \",\\n                \"\\\\nval \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nmatch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SWIFT:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunc \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nstruct \",\\n                \"\\\\nenum \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.MARKDOWN:\\n            return [\\n                # First, try to split along Markdown headings (starting with level 2)\\n                \"\\\\n#{1,6} \",\\n                # Note the alternative syntax for headings (below) is not handled here\\n                # Heading level 2\\n                # ---------------\\n                # End of code block\\n                \"```\\\\n\",\\n                # Horizontal lines\\n                \"\\\\n\\\\\\\\*\\\\\\\\*\\\\\\\\*+\\\\n\",\\n                \"\\\\n---+\\\\n\",\\n                \"\\\\n___+\\\\n\",\\n                # Note that this splitter doesn\\'t handle horizontal lines defined',\n",
              "  'def _split_text_with_regex(\\n    text: str, separator: str, keep_separator: bool\\n) -> List[str]:\\n    # Now that we have the separator, split the text\\n    if separator:\\n        if keep_separator:\\n            # The parentheses in the pattern keep the delimiters in the result.\\n            _splits = re.split(f\"({separator})\", text)\\n            splits = [_splits[i] + _splits[i + 1] for i in range(1, len(_splits), 2)]\\n            if len(_splits) % 2 == 0:\\n                splits += _splits[-1:]\\n            splits = [_splits[0]] + splits\\n        else:\\n            splits = re.split(separator, text)\\n    else:\\n        splits = list(text)\\n    return [s for s in splits if s != \"\"]',\n",
              "  '\"\"\"**Text Splitters** are classes for splitting text.\\n\\n\\n**Class hierarchy:**\\n\\n.. code-block::\\n\\n    BaseDocumentTransformer --> TextSplitter --> <name>TextSplitter  # Example: CharacterTextSplitter\\n                                                 RecursiveCharacterTextSplitter -->  <name>TextSplitter\\n\\nNote: **MarkdownHeaderTextSplitter** and **HTMLHeaderTextSplitter do not derive from TextSplitter.\\n\\n\\n**Main helpers:**\\n\\n.. code-block::\\n\\n    Document, Tokenizer, Language, LineType, HeaderType\\n\\n\"\"\"  # noqa: E501\\n\\nfrom __future__ import annotations\\n\\nimport asyncio\\nimport copy\\nimport logging\\nimport pathlib\\nimport re\\nfrom abc import ABC, abstractmethod\\nfrom dataclasses import dataclass\\nfrom enum import Enum\\nfrom functools import partial\\nfrom io import BytesIO, StringIO\\nfrom typing import (\\n    AbstractSet,\\n    Any,\\n    Callable,\\n    Collection,\\n    Dict,\\n    Iterable,\\n    List,\\n    Literal,\\n    Optional,\\n    Sequence,\\n    Tuple,\\n    Type,\\n    TypedDict,\\n    TypeVar,\\n    Union,\\n    cast,\\n)\\n\\nimport requests\\nfrom langchain_core.documents import BaseDocumentTransformer, Document\\n\\nlogger = logging.getLogger(__name__)\\n\\nTS = TypeVar(\"TS\", bound=\"TextSplitter\")\\n\\n\\ndef _make_spacy_pipeline_for_splitting(\\n    pipeline: str, *, max_length: int = 1_000_000\\n) -> Any:  # avoid importing spacy\\n    try:\\n        import spacy\\n    except ImportError:\\n        raise ImportError(\\n            \"Spacy is not installed, please install it with `pip install spacy`.\"\\n        )\\n    if pipeline == \"sentencizer\":\\n        from spacy.lang.en import English\\n\\n        sentencizer = English()\\n        sentencizer.add_pipe(\"sentencizer\")\\n    else:\\n        sentencizer = spacy.load(pipeline, exclude=[\"ner\", \"tagger\"])\\n        sentencizer.max_length = max_length\\n    return sentencizer',\n",
              "  'class PythonCodeTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Python syntax.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a PythonCodeTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.PYTHON)\\n        super().__init__(separators=separators, **kwargs)\\n\\n\\nclass MarkdownTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Markdown-formatted headings.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a MarkdownTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.MARKDOWN)\\n        super().__init__(separators=separators, **kwargs)\\n\\n\\nclass LatexTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Latex-formatted layout elements.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a LatexTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.LATEX)\\n        super().__init__(separators=separators, **kwargs)',\n",
              "  'class NLTKTextSplitter(TextSplitter):\\n    \"\"\"Splitting text using NLTK package.\"\"\"\\n\\n    def __init__(\\n        self, separator: str = \"\\\\n\\\\n\", language: str = \"english\", **kwargs: Any\\n    ) -> None:\\n        \"\"\"Initialize the NLTK splitter.\"\"\"\\n        super().__init__(**kwargs)\\n        try:\\n            from nltk.tokenize import sent_tokenize\\n\\n            self._tokenizer = sent_tokenize\\n        except ImportError:\\n            raise ImportError(\\n                \"NLTK is not installed, please install it with `pip install nltk`.\"\\n            )\\n        self._separator = separator\\n        self._language = language\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        # First we naively split the large input into a bunch of smaller ones.\\n        splits = self._tokenizer(text, language=self._language)\\n        return self._merge_splits(splits, self._separator)\\n\\n\\nclass SpacyTextSplitter(TextSplitter):\\n    \"\"\"Splitting text using Spacy package.\\n\\n\\n    Per default, Spacy\\'s `en_core_web_sm` model is used and\\n    its default max_length is 1000000 (it is the length of maximum character\\n    this model takes which can be increased for large files). For a faster, but\\n    potentially less accurate splitting, you can use `pipeline=\\'sentencizer\\'`.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        separator: str = \"\\\\n\\\\n\",\\n        pipeline: str = \"en_core_web_sm\",\\n        max_length: int = 1_000_000,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Initialize the spacy text splitter.\"\"\"\\n        super().__init__(**kwargs)\\n        self._tokenizer = _make_spacy_pipeline_for_splitting(\\n            pipeline, max_length=max_length\\n        )\\n        self._separator = separator\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        splits = (s.text for s in self._tokenizer(text).sents)\\n        return self._merge_splits(splits, self._separator)\\n\\n\\n# For backwards compatibility',\n",
              "  'else:\\n            raise ValueError(\\n                f\"Language {language} is not supported! \"\\n                f\"Please choose from {list(Language)}\"\\n            )',\n",
              "  '\"\\\\nevent \",\\n                \"\\\\nmodifier \",\\n                \"\\\\nerror \",\\n                \"\\\\nstruct \",\\n                \"\\\\nenum \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo while \",\\n                \"\\\\nassembly \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.COBOL:\\n            return [\\n                # Split along divisions\\n                \"\\\\nIDENTIFICATION DIVISION.\",\\n                \"\\\\nENVIRONMENT DIVISION.\",\\n                \"\\\\nDATA DIVISION.\",\\n                \"\\\\nPROCEDURE DIVISION.\",\\n                # Split along sections within DATA DIVISION\\n                \"\\\\nWORKING-STORAGE SECTION.\",\\n                \"\\\\nLINKAGE SECTION.\",\\n                \"\\\\nFILE SECTION.\",\\n                # Split along sections within PROCEDURE DIVISION\\n                \"\\\\nINPUT-OUTPUT SECTION.\",\\n                # Split along paragraphs and common statements\\n                \"\\\\nOPEN \",\\n                \"\\\\nCLOSE \",\\n                \"\\\\nREAD \",\\n                \"\\\\nWRITE \",\\n                \"\\\\nIF \",\\n                \"\\\\nELSE \",\\n                \"\\\\nMOVE \",\\n                \"\\\\nPERFORM \",\\n                \"\\\\nUNTIL \",\\n                \"\\\\nVARYING \",\\n                \"\\\\nACCEPT \",\\n                \"\\\\nDISPLAY \",\\n                \"\\\\nSTOP RUN.\",\\n                # Split by the normal type of lines\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]',\n",
              "  '\"<nav\",\\n                # Head\\n                \"<head\",\\n                \"<style\",\\n                \"<script\",\\n                \"<meta\",\\n                \"<title\",\\n                \"\",\\n            ]\\n        elif language == Language.CSHARP:\\n            return [\\n                \"\\\\ninterface \",\\n                \"\\\\nenum \",\\n                \"\\\\nimplements \",\\n                \"\\\\ndelegate \",\\n                \"\\\\nevent \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nabstract \",\\n                # Split along method definitions\\n                \"\\\\npublic \",\\n                \"\\\\nprotected \",\\n                \"\\\\nprivate \",\\n                \"\\\\nstatic \",\\n                \"\\\\nreturn \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\ncontinue \",\\n                \"\\\\nfor \",\\n                \"\\\\nforeach \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\nbreak \",\\n                \"\\\\ncase \",\\n                \"\\\\nelse \",\\n                # Split by exceptions\\n                \"\\\\ntry \",\\n                \"\\\\nthrow \",\\n                \"\\\\nfinally \",\\n                \"\\\\ncatch \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SOL:\\n            return [\\n                # Split along compiler information definitions\\n                \"\\\\npragma \",\\n                \"\\\\nusing \",\\n                # Split along contract definitions\\n                \"\\\\ncontract \",\\n                \"\\\\ninterface \",\\n                \"\\\\nlibrary \",\\n                # Split along method definitions\\n                \"\\\\nconstructor \",\\n                \"\\\\ntype \",\\n                \"\\\\nfunction \",\\n                \"\\\\nevent \",\\n                \"\\\\nmodifier \",\\n                \"\\\\nerror \",\\n                \"\\\\nstruct \",\\n                \"\\\\nenum \",\\n                # Split along control flow statements',\n",
              "  '\"\\\\n\\\\\\\\*\\\\\\\\*\\\\\\\\*+\\\\n\",\\n                \"\\\\n---+\\\\n\",\\n                \"\\\\n___+\\\\n\",\\n                # Note that this splitter doesn\\'t handle horizontal lines defined\\n                # by *three or more* of ***, ---, or ___, but this is not handled\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.LATEX:\\n            return [\\n                # First, try to split along Latex sections\\n                \"\\\\n\\\\\\\\\\\\\\\\chapter{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\section{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\subsection{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\subsubsection{\",\\n                # Now split by environments\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{enumerate}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{itemize}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{description}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{list}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{quote}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{quotation}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{verse}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{verbatim}\",\\n                # Now split by math environments\\n                \"\\\\n\\\\\\\\\\\\begin{align}\",\\n                \"$$\",\\n                \"$\",\\n                # Now split by the normal type of lines\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.HTML:\\n            return [\\n                # First, try to split along HTML tags\\n                \"<body\",\\n                \"<div\",\\n                \"<p\",\\n                \"<br\",\\n                \"<li\",\\n                \"<h1\",\\n                \"<h2\",\\n                \"<h3\",\\n                \"<h4\",\\n                \"<h5\",\\n                \"<h6\",\\n                \"<span\",\\n                \"<table\",\\n                \"<tr\",\\n                \"<td\",\\n                \"<th\",\\n                \"<ul\",\\n                \"<ol\",\\n                \"<header\",\\n                \"<footer\",\\n                \"<nav\",\\n                # Head\\n                \"<head\",\\n                \"<style\",\\n                \"<script\",\\n                \"<meta\",\\n                \"<title\",',\n",
              "  'if not isinstance(tokenizer, PreTrainedTokenizerBase):\\n                raise ValueError(\\n                    \"Tokenizer received was not an instance of PreTrainedTokenizerBase\"\\n                )\\n\\n            def _huggingface_tokenizer_length(text: str) -> int:\\n                return len(tokenizer.encode(text))\\n\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import transformers python package. \"\\n                \"Please install it with `pip install transformers`.\"\\n            )\\n        return cls(length_function=_huggingface_tokenizer_length, **kwargs)\\n\\n    @classmethod\\n    def from_tiktoken_encoder(\\n        cls: Type[TS],\\n        encoding_name: str = \"gpt2\",\\n        model_name: Optional[str] = None,\\n        allowed_special: Union[Literal[\"all\"], AbstractSet[str]] = set(),\\n        disallowed_special: Union[Literal[\"all\"], Collection[str]] = \"all\",\\n        **kwargs: Any,\\n    ) -> TS:\\n        \"\"\"Text splitter that uses tiktoken encoder to count length.\"\"\"\\n        try:\\n            import tiktoken\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import tiktoken python package. \"\\n                \"This is needed in order to calculate max_tokens_for_prompt. \"\\n                \"Please install it with `pip install tiktoken`.\"\\n            )\\n\\n        if model_name is not None:\\n            enc = tiktoken.encoding_for_model(model_name)\\n        else:\\n            enc = tiktoken.get_encoding(encoding_name)\\n\\n        def _tiktoken_encoder(text: str) -> int:\\n            return len(\\n                enc.encode(\\n                    text,\\n                    allowed_special=allowed_special,\\n                    disallowed_special=disallowed_special,\\n                )\\n            )',\n",
              "  'return [\\n                # First, try to split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\ndef \",\\n                \"\\\\n\\\\tdef \",\\n                # Now split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.RST:\\n            return [\\n                # Split along section titles\\n                \"\\\\n=+\\\\n\",\\n                \"\\\\n-+\\\\n\",\\n                \"\\\\n\\\\\\\\*+\\\\n\",\\n                # Split along directive markers\\n                \"\\\\n\\\\n.. *\\\\n\\\\n\",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.RUBY:\\n            return [\\n                # Split along method definitions\\n                \"\\\\ndef \",\\n                \"\\\\nclass \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nunless \",\\n                \"\\\\nwhile \",\\n                \"\\\\nfor \",\\n                \"\\\\ndo \",\\n                \"\\\\nbegin \",\\n                \"\\\\nrescue \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.RUST:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfn \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nwhile \",\\n                \"\\\\nfor \",\\n                \"\\\\nloop \",\\n                \"\\\\nmatch \",\\n                \"\\\\nconst \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SCALA:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",',\n",
              "  '\"\\\\nfunction \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                \"\\\\ndefault \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.PHP:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunction \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nforeach \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.PROTO:\\n            return [\\n                # Split along message definitions\\n                \"\\\\nmessage \",\\n                # Split along service definitions\\n                \"\\\\nservice \",\\n                # Split along enum definitions\\n                \"\\\\nenum \",\\n                # Split along option definitions\\n                \"\\\\noption \",\\n                # Split along import statements\\n                \"\\\\nimport \",\\n                # Split along syntax declarations\\n                \"\\\\nsyntax \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.PYTHON:\\n            return [\\n                # First, try to split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\ndef \",\\n                \"\\\\n\\\\tdef \",',\n",
              "  '\"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.KOTLIN:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along method definitions\\n                \"\\\\npublic \",\\n                \"\\\\nprotected \",\\n                \"\\\\nprivate \",\\n                \"\\\\ninternal \",\\n                \"\\\\ncompanion \",\\n                \"\\\\nfun \",\\n                \"\\\\nval \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nwhen \",\\n                \"\\\\ncase \",\\n                \"\\\\nelse \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.JS:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunction \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                \"\\\\nvar \",\\n                \"\\\\nclass \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                \"\\\\ndefault \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.TS:\\n            return [\\n                \"\\\\nenum \",\\n                \"\\\\ninterface \",\\n                \"\\\\nnamespace \",\\n                \"\\\\ntype \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along function definitions\\n                \"\\\\nfunction \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements',\n",
              "  '@staticmethod\\n    def get_separators_for_language(language: Language) -> List[str]:\\n        if language == Language.CPP:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along function definitions\\n                \"\\\\nvoid \",\\n                \"\\\\nint \",\\n                \"\\\\nfloat \",\\n                \"\\\\ndouble \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.GO:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunc \",\\n                \"\\\\nvar \",\\n                \"\\\\nconst \",\\n                \"\\\\ntype \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.JAVA:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along method definitions\\n                \"\\\\npublic \",\\n                \"\\\\nprotected \",\\n                \"\\\\nprivate \",\\n                \"\\\\nstatic \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.KOTLIN:\\n            return [',\n",
              "  '_separator = separator if self._is_separator_regex else re.escape(separator)\\n        splits = _split_text_with_regex(text, _separator, self._keep_separator)\\n\\n        # Now go merging things, recursively splitting longer texts.\\n        _good_splits = []\\n        _separator = \"\" if self._keep_separator else separator\\n        for s in splits:\\n            if self._length_function(s) < self._chunk_size:\\n                _good_splits.append(s)\\n            else:\\n                if _good_splits:\\n                    merged_text = self._merge_splits(_good_splits, _separator)\\n                    final_chunks.extend(merged_text)\\n                    _good_splits = []\\n                if not new_separators:\\n                    final_chunks.append(s)\\n                else:\\n                    other_info = self._split_text(s, new_separators)\\n                    final_chunks.extend(other_info)\\n        if _good_splits:\\n            merged_text = self._merge_splits(_good_splits, _separator)\\n            final_chunks.extend(merged_text)\\n        return final_chunks\\n\\n    def split_text(self, text: str) -> List[str]:\\n        return self._split_text(text, self._separators)\\n\\n    @classmethod\\n    def from_language(\\n        cls, language: Language, **kwargs: Any\\n    ) -> RecursiveCharacterTextSplitter:\\n        separators = cls.get_separators_for_language(language)\\n        return cls(separators=separators, is_separator_regex=True, **kwargs)',\n",
              "  'class RecursiveCharacterTextSplitter(TextSplitter):\\n    \"\"\"Splitting text by recursively look at characters.\\n\\n    Recursively tries to split by different characters to find one\\n    that works.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        separators: Optional[List[str]] = None,\\n        keep_separator: bool = True,\\n        is_separator_regex: bool = False,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(keep_separator=keep_separator, **kwargs)\\n        self._separators = separators or [\"\\\\n\\\\n\", \"\\\\n\", \" \", \"\"]\\n        self._is_separator_regex = is_separator_regex\\n\\n    def _split_text(self, text: str, separators: List[str]) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        final_chunks = []\\n        # Get appropriate separator to use\\n        separator = separators[-1]\\n        new_separators = []\\n        for i, _s in enumerate(separators):\\n            _separator = _s if self._is_separator_regex else re.escape(_s)\\n            if _s == \"\":\\n                separator = _s\\n                break\\n            if re.search(_separator, text):\\n                separator = _s\\n                new_separators = separators[i + 1 :]\\n                break\\n\\n        _separator = separator if self._is_separator_regex else re.escape(separator)\\n        splits = _split_text_with_regex(text, _separator, self._keep_separator)',\n",
              "  'class Language(str, Enum):\\n    \"\"\"Enum of the programming languages.\"\"\"\\n\\n    CPP = \"cpp\"\\n    GO = \"go\"\\n    JAVA = \"java\"\\n    KOTLIN = \"kotlin\"\\n    JS = \"js\"\\n    TS = \"ts\"\\n    PHP = \"php\"\\n    PROTO = \"proto\"\\n    PYTHON = \"python\"\\n    RST = \"rst\"\\n    RUBY = \"ruby\"\\n    RUST = \"rust\"\\n    SCALA = \"scala\"\\n    SWIFT = \"swift\"\\n    MARKDOWN = \"markdown\"\\n    LATEX = \"latex\"\\n    HTML = \"html\"\\n    SOL = \"sol\"\\n    CSHARP = \"csharp\"\\n    COBOL = \"cobol\"',\n",
              "  'def split_text(self, text: str) -> List[str]:\\n        def encode_strip_start_and_stop_token_ids(text: str) -> List[int]:\\n            return self._encode(text)[1:-1]\\n\\n        tokenizer = Tokenizer(\\n            chunk_overlap=self._chunk_overlap,\\n            tokens_per_chunk=self.tokens_per_chunk,\\n            decode=self.tokenizer.decode,\\n            encode=encode_strip_start_and_stop_token_ids,\\n        )\\n\\n        return split_text_on_tokens(text=text, tokenizer=tokenizer)\\n\\n    def count_tokens(self, *, text: str) -> int:\\n        return len(self._encode(text))\\n\\n    _max_length_equal_32_bit_integer: int = 2**32\\n\\n    def _encode(self, text: str) -> List[int]:\\n        token_ids_with_start_and_end_token_ids = self.tokenizer.encode(\\n            text,\\n            max_length=self._max_length_equal_32_bit_integer,\\n            truncation=\"do_not_truncate\",\\n        )\\n        return token_ids_with_start_and_end_token_ids',\n",
              "  '# map xhtml namespace prefix\\n        ns_map = {\"h\": \"http://www.w3.org/1999/xhtml\"}\\n\\n        # build list of elements from DOM\\n        elements = []\\n        for element in result_dom.findall(\"*//*\", ns_map):\\n            if element.findall(\"*[@class=\\'headers\\']\") or element.findall(\\n                \"*[@class=\\'chunk\\']\"\\n            ):\\n                elements.append(\\n                    ElementType(\\n                        url=file,\\n                        xpath=\"\".join(\\n                            [\\n                                node.text\\n                                for node in element.findall(\"*[@class=\\'xpath\\']\", ns_map)\\n                            ]\\n                        ),\\n                        content=\"\".join(\\n                            [\\n                                node.text\\n                                for node in element.findall(\"*[@class=\\'chunk\\']\", ns_map)\\n                            ]\\n                        ),\\n                        metadata={\\n                            # Add text of specified headers to metadata using header\\n                            # mapping.\\n                            header_mapping[node.tag]: node.text\\n                            for node in filter(\\n                                lambda x: x.tag in header_filter,\\n                                element.findall(\"*[@class=\\'headers\\']/*\", ns_map),\\n                            )\\n                        },\\n                    )\\n                )\\n\\n        if not self.return_each_element:\\n            return self.aggregate_elements_to_chunks(elements)\\n        else:\\n            return [\\n                Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n                for chunk in elements\\n            ]\\n\\n\\n# should be in newer Python versions (3.10+)\\n# @dataclass(frozen=True, kw_only=True, slots=True)\\n@dataclass(frozen=True)',\n",
              "  '\"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.KOTLIN:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along method definitions\\n                \"\\\\npublic \",\\n                \"\\\\nprotected \",\\n                \"\\\\nprivate \",\\n                \"\\\\ninternal \",\\n                \"\\\\ncompanion \",\\n                \"\\\\nfun \",\\n                \"\\\\nval \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nwhen \",\\n                \"\\\\ncase \",\\n                \"\\\\nelse \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.JS:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunction \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                \"\\\\nvar \",\\n                \"\\\\nclass \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                \"\\\\ndefault \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.TS:\\n            return [\\n                \"\\\\nenum \",\\n                \"\\\\ninterface \",\\n                \"\\\\nnamespace \",\\n                \"\\\\ntype \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along function definitions\\n                \"\\\\nfunction \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements',\n",
              "  '@staticmethod\\n    def get_separators_for_language(language: Language) -> List[str]:\\n        if language == Language.CPP:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along function definitions\\n                \"\\\\nvoid \",\\n                \"\\\\nint \",\\n                \"\\\\nfloat \",\\n                \"\\\\ndouble \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.GO:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunc \",\\n                \"\\\\nvar \",\\n                \"\\\\nconst \",\\n                \"\\\\ntype \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.JAVA:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along method definitions\\n                \"\\\\npublic \",\\n                \"\\\\nprotected \",\\n                \"\\\\nprivate \",\\n                \"\\\\nstatic \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.KOTLIN:\\n            return [',\n",
              "  '_separator = separator if self._is_separator_regex else re.escape(separator)\\n        splits = _split_text_with_regex(text, _separator, self._keep_separator)\\n\\n        # Now go merging things, recursively splitting longer texts.\\n        _good_splits = []\\n        _separator = \"\" if self._keep_separator else separator\\n        for s in splits:\\n            if self._length_function(s) < self._chunk_size:\\n                _good_splits.append(s)\\n            else:\\n                if _good_splits:\\n                    merged_text = self._merge_splits(_good_splits, _separator)\\n                    final_chunks.extend(merged_text)\\n                    _good_splits = []\\n                if not new_separators:\\n                    final_chunks.append(s)\\n                else:\\n                    other_info = self._split_text(s, new_separators)\\n                    final_chunks.extend(other_info)\\n        if _good_splits:\\n            merged_text = self._merge_splits(_good_splits, _separator)\\n            final_chunks.extend(merged_text)\\n        return final_chunks\\n\\n    def split_text(self, text: str) -> List[str]:\\n        return self._split_text(text, self._separators)\\n\\n    @classmethod\\n    def from_language(\\n        cls, language: Language, **kwargs: Any\\n    ) -> RecursiveCharacterTextSplitter:\\n        separators = cls.get_separators_for_language(language)\\n        return cls(separators=separators, is_separator_regex=True, **kwargs)',\n",
              "  'class RecursiveCharacterTextSplitter(TextSplitter):\\n    \"\"\"Splitting text by recursively look at characters.\\n\\n    Recursively tries to split by different characters to find one\\n    that works.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        separators: Optional[List[str]] = None,\\n        keep_separator: bool = True,\\n        is_separator_regex: bool = False,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(keep_separator=keep_separator, **kwargs)\\n        self._separators = separators or [\"\\\\n\\\\n\", \"\\\\n\", \" \", \"\"]\\n        self._is_separator_regex = is_separator_regex\\n\\n    def _split_text(self, text: str, separators: List[str]) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        final_chunks = []\\n        # Get appropriate separator to use\\n        separator = separators[-1]\\n        new_separators = []\\n        for i, _s in enumerate(separators):\\n            _separator = _s if self._is_separator_regex else re.escape(_s)\\n            if _s == \"\":\\n                separator = _s\\n                break\\n            if re.search(_separator, text):\\n                separator = _s\\n                new_separators = separators[i + 1 :]\\n                break\\n\\n        _separator = separator if self._is_separator_regex else re.escape(separator)\\n        splits = _split_text_with_regex(text, _separator, self._keep_separator)',\n",
              "  'class Language(str, Enum):\\n    \"\"\"Enum of the programming languages.\"\"\"\\n\\n    CPP = \"cpp\"\\n    GO = \"go\"\\n    JAVA = \"java\"\\n    KOTLIN = \"kotlin\"\\n    JS = \"js\"\\n    TS = \"ts\"\\n    PHP = \"php\"\\n    PROTO = \"proto\"\\n    PYTHON = \"python\"\\n    RST = \"rst\"\\n    RUBY = \"ruby\"\\n    RUST = \"rust\"\\n    SCALA = \"scala\"\\n    SWIFT = \"swift\"\\n    MARKDOWN = \"markdown\"\\n    LATEX = \"latex\"\\n    HTML = \"html\"\\n    SOL = \"sol\"\\n    CSHARP = \"csharp\"\\n    COBOL = \"cobol\"',\n",
              "  'def split_text(self, text: str) -> List[str]:\\n        def encode_strip_start_and_stop_token_ids(text: str) -> List[int]:\\n            return self._encode(text)[1:-1]\\n\\n        tokenizer = Tokenizer(\\n            chunk_overlap=self._chunk_overlap,\\n            tokens_per_chunk=self.tokens_per_chunk,\\n            decode=self.tokenizer.decode,\\n            encode=encode_strip_start_and_stop_token_ids,\\n        )\\n\\n        return split_text_on_tokens(text=text, tokenizer=tokenizer)\\n\\n    def count_tokens(self, *, text: str) -> int:\\n        return len(self._encode(text))\\n\\n    _max_length_equal_32_bit_integer: int = 2**32\\n\\n    def _encode(self, text: str) -> List[int]:\\n        token_ids_with_start_and_end_token_ids = self.tokenizer.encode(\\n            text,\\n            max_length=self._max_length_equal_32_bit_integer,\\n            truncation=\"do_not_truncate\",\\n        )\\n        return token_ids_with_start_and_end_token_ids',\n",
              "  'class SentenceTransformersTokenTextSplitter(TextSplitter):\\n    \"\"\"Splitting text to tokens using sentence model tokenizer.\"\"\"\\n\\n    def __init__(\\n        self,\\n        chunk_overlap: int = 50,\\n        model_name: str = \"sentence-transformers/all-mpnet-base-v2\",\\n        tokens_per_chunk: Optional[int] = None,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(**kwargs, chunk_overlap=chunk_overlap)\\n\\n        try:\\n            from sentence_transformers import SentenceTransformer\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import sentence_transformer python package. \"\\n                \"This is needed in order to for SentenceTransformersTokenTextSplitter. \"\\n                \"Please install it with `pip install sentence-transformers`.\"\\n            )\\n\\n        self.model_name = model_name\\n        self._model = SentenceTransformer(self.model_name)\\n        self.tokenizer = self._model.tokenizer\\n        self._initialize_chunk_configuration(tokens_per_chunk=tokens_per_chunk)\\n\\n    def _initialize_chunk_configuration(\\n        self, *, tokens_per_chunk: Optional[int]\\n    ) -> None:\\n        self.maximum_tokens_per_chunk = cast(int, self._model.max_seq_length)\\n\\n        if tokens_per_chunk is None:\\n            self.tokens_per_chunk = self.maximum_tokens_per_chunk\\n        else:\\n            self.tokens_per_chunk = tokens_per_chunk\\n\\n        if self.tokens_per_chunk > self.maximum_tokens_per_chunk:\\n            raise ValueError(\\n                f\"The token limit of the models \\'{self.model_name}\\'\"\\n                f\" is: {self.maximum_tokens_per_chunk}.\"\\n                f\" Argument tokens_per_chunk={self.tokens_per_chunk}\"\\n                f\" > maximum token limit.\"\\n            )\\n\\n    def split_text(self, text: str) -> List[str]:\\n        def encode_strip_start_and_stop_token_ids(text: str) -> List[int]:\\n            return self._encode(text)[1:-1]',\n",
              "  'class TokenTextSplitter(TextSplitter):\\n    \"\"\"Splitting text to tokens using model tokenizer.\"\"\"\\n\\n    def __init__(\\n        self,\\n        encoding_name: str = \"gpt2\",\\n        model_name: Optional[str] = None,\\n        allowed_special: Union[Literal[\"all\"], AbstractSet[str]] = set(),\\n        disallowed_special: Union[Literal[\"all\"], Collection[str]] = \"all\",\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(**kwargs)\\n        try:\\n            import tiktoken\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import tiktoken python package. \"\\n                \"This is needed in order to for TokenTextSplitter. \"\\n                \"Please install it with `pip install tiktoken`.\"\\n            )\\n\\n        if model_name is not None:\\n            enc = tiktoken.encoding_for_model(model_name)\\n        else:\\n            enc = tiktoken.get_encoding(encoding_name)\\n        self._tokenizer = enc\\n        self._allowed_special = allowed_special\\n        self._disallowed_special = disallowed_special\\n\\n    def split_text(self, text: str) -> List[str]:\\n        def _encode(_text: str) -> List[int]:\\n            return self._tokenizer.encode(\\n                _text,\\n                allowed_special=self._allowed_special,\\n                disallowed_special=self._disallowed_special,\\n            )\\n\\n        tokenizer = Tokenizer(\\n            chunk_overlap=self._chunk_overlap,\\n            tokens_per_chunk=self._chunk_size,\\n            decode=self._tokenizer.decode,\\n            encode=_encode,\\n        )\\n\\n        return split_text_on_tokens(text=text, tokenizer=tokenizer)',\n",
              "  'class Tokenizer:\\n    \"\"\"Tokenizer data class.\"\"\"\\n\\n    chunk_overlap: int\\n    \"\"\"Overlap in tokens between chunks\"\"\"\\n    tokens_per_chunk: int\\n    \"\"\"Maximum number of tokens per chunk\"\"\"\\n    decode: Callable[[List[int]], str]\\n    \"\"\" Function to decode a list of token ids to a string\"\"\"\\n    encode: Callable[[str], List[int]]\\n    \"\"\" Function to encode a string to a list of token ids\"\"\"\\n\\n\\ndef split_text_on_tokens(*, text: str, tokenizer: Tokenizer) -> List[str]:\\n    \"\"\"Split incoming text and return chunks using tokenizer.\"\"\"\\n    splits: List[str] = []\\n    input_ids = tokenizer.encode(text)\\n    start_idx = 0\\n    cur_idx = min(start_idx + tokenizer.tokens_per_chunk, len(input_ids))\\n    chunk_ids = input_ids[start_idx:cur_idx]\\n    while start_idx < len(input_ids):\\n        splits.append(tokenizer.decode(chunk_ids))\\n        if cur_idx == len(input_ids):\\n            break\\n        start_idx += tokenizer.tokens_per_chunk - tokenizer.chunk_overlap\\n        cur_idx = min(start_idx + tokenizer.tokens_per_chunk, len(input_ids))\\n        chunk_ids = input_ids[start_idx:cur_idx]\\n    return splits',\n",
              "  '\"\\\\nfunction \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                \"\\\\ndefault \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.PHP:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunction \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nforeach \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.PROTO:\\n            return [\\n                # Split along message definitions\\n                \"\\\\nmessage \",\\n                # Split along service definitions\\n                \"\\\\nservice \",\\n                # Split along enum definitions\\n                \"\\\\nenum \",\\n                # Split along option definitions\\n                \"\\\\noption \",\\n                # Split along import statements\\n                \"\\\\nimport \",\\n                # Split along syntax declarations\\n                \"\\\\nsyntax \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.PYTHON:\\n            return [\\n                # First, try to split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\ndef \",\\n                \"\\\\n\\\\tdef \",',\n",
              "  'return [\\n            Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n            for chunk in aggregated_chunks\\n        ]\\n\\n    def split_text_from_url(self, url: str) -> List[Document]:\\n        \"\"\"Split HTML from web URL\\n\\n        Args:\\n            url: web URL\\n        \"\"\"\\n        r = requests.get(url)\\n        return self.split_text_from_file(BytesIO(r.content))\\n\\n    def split_text(self, text: str) -> List[Document]:\\n        \"\"\"Split HTML text string\\n\\n        Args:\\n            text: HTML text\\n        \"\"\"\\n        return self.split_text_from_file(StringIO(text))\\n\\n    def split_text_from_file(self, file: Any) -> List[Document]:\\n        \"\"\"Split HTML file\\n\\n        Args:\\n            file: HTML file\\n        \"\"\"\\n        try:\\n            from lxml import etree\\n        except ImportError as e:\\n            raise ImportError(\\n                \"Unable to import lxml, please install with `pip install lxml`.\"\\n            ) from e\\n        # use lxml library to parse html document and return xml ElementTree\\n        parser = etree.HTMLParser()\\n        tree = etree.parse(file, parser)\\n\\n        # document transformation for \"structure-aware\" chunking is handled with xsl.\\n        # see comments in html_chunks_with_headers.xslt for more detailed information.\\n        xslt_path = (\\n            pathlib.Path(__file__).parent\\n            / \"document_transformers/xsl/html_chunks_with_headers.xslt\"\\n        )\\n        xslt_tree = etree.parse(xslt_path)\\n        transform = etree.XSLT(xslt_tree)\\n        result = transform(tree)\\n        result_dom = etree.fromstring(str(result))\\n\\n        # create filter and mapping for header metadata\\n        header_filter = [header[0] for header in self.headers_to_split_on]\\n        header_mapping = dict(self.headers_to_split_on)\\n\\n        # map xhtml namespace prefix\\n        ns_map = {\"h\": \"http://www.w3.org/1999/xhtml\"}',\n",
              "  'class HTMLHeaderTextSplitter:\\n    \"\"\"\\n    Splitting HTML files based on specified headers.\\n    Requires lxml package.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        headers_to_split_on: List[Tuple[str, str]],\\n        return_each_element: bool = False,\\n    ):\\n        \"\"\"Create a new HTMLHeaderTextSplitter.\\n\\n        Args:\\n            headers_to_split_on: list of tuples of headers we want to track mapped to\\n                (arbitrary) keys for metadata. Allowed header values: h1, h2, h3, h4,\\n                h5, h6 e.g. [(\"h1\", \"Header 1\"), (\"h2\", \"Header 2)].\\n            return_each_element: Return each element w/ associated headers.\\n        \"\"\"\\n        # Output element-by-element or aggregated into chunks w/ common headers\\n        self.return_each_element = return_each_element\\n        self.headers_to_split_on = sorted(headers_to_split_on)\\n\\n    def aggregate_elements_to_chunks(\\n        self, elements: List[ElementType]\\n    ) -> List[Document]:\\n        \"\"\"Combine elements with common metadata into chunks\\n\\n        Args:\\n            elements: HTML element content with associated identifying info and metadata\\n        \"\"\"\\n        aggregated_chunks: List[ElementType] = []\\n\\n        for element in elements:\\n            if (\\n                aggregated_chunks\\n                and aggregated_chunks[-1][\"metadata\"] == element[\"metadata\"]\\n            ):\\n                # If the last element in the aggregated list\\n                # has the same metadata as the current element,\\n                # append the current content to the last element\\'s content\\n                aggregated_chunks[-1][\"content\"] += \"  \\\\n\" + element[\"content\"]\\n            else:\\n                # Otherwise, append the current element to the aggregated list\\n                aggregated_chunks.append(element)\\n\\n        return [\\n            Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n            for chunk in aggregated_chunks\\n        ]',\n",
              "  'class ElementType(TypedDict):\\n    \"\"\"Element type as typed dict.\"\"\"\\n\\n    url: str\\n    xpath: str\\n    content: str\\n    metadata: Dict[str, str]',\n",
              "  'if current_content:\\n            lines_with_metadata.append(\\n                {\"content\": \"\\\\n\".join(current_content), \"metadata\": current_metadata}\\n            )\\n\\n        # lines_with_metadata has each line with associated header metadata\\n        # aggregate these into chunks based on common metadata\\n        if not self.return_each_line:\\n            return self.aggregate_lines_to_chunks(lines_with_metadata)\\n        else:\\n            return [\\n                Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n                for chunk in lines_with_metadata\\n            ]',\n",
              "  '# Push the current header to the stack\\n                        header: HeaderType = {\\n                            \"level\": current_header_level,\\n                            \"name\": name,\\n                            \"data\": stripped_line[len(sep) :].strip(),\\n                        }\\n                        header_stack.append(header)\\n                        # Update initial_metadata with the current header\\n                        initial_metadata[name] = header[\"data\"]\\n\\n                    # Add the previous line to the lines_with_metadata\\n                    # only if current_content is not empty\\n                    if current_content:\\n                        lines_with_metadata.append(\\n                            {\\n                                \"content\": \"\\\\n\".join(current_content),\\n                                \"metadata\": current_metadata.copy(),\\n                            }\\n                        )\\n                        current_content.clear()\\n\\n                    break\\n            else:\\n                if stripped_line:\\n                    current_content.append(stripped_line)\\n                elif current_content:\\n                    lines_with_metadata.append(\\n                        {\\n                            \"content\": \"\\\\n\".join(current_content),\\n                            \"metadata\": current_metadata.copy(),\\n                        }\\n                    )\\n                    current_content.clear()\\n\\n            current_metadata = initial_metadata.copy()\\n\\n        if current_content:\\n            lines_with_metadata.append(\\n                {\"content\": \"\\\\n\".join(current_content), \"metadata\": current_metadata}\\n            )',\n",
              "  'if in_code_block:\\n                current_content.append(stripped_line)\\n                continue\\n\\n            # Check each line against each of the header types (e.g., #, ##)\\n            for sep, name in self.headers_to_split_on:\\n                # Check if line starts with a header that we intend to split on\\n                if stripped_line.startswith(sep) and (\\n                    # Header with no text OR header is followed by space\\n                    # Both are valid conditions that sep is being used a header\\n                    len(stripped_line) == len(sep) or stripped_line[len(sep)] == \" \"\\n                ):\\n                    # Ensure we are tracking the header as metadata\\n                    if name is not None:\\n                        # Get the current header level\\n                        current_header_level = sep.count(\"#\")\\n\\n                        # Pop out headers of lower or same level from the stack\\n                        while (\\n                            header_stack\\n                            and header_stack[-1][\"level\"] >= current_header_level\\n                        ):\\n                            # We have encountered a new header\\n                            # at the same or higher level\\n                            popped_header = header_stack.pop()\\n                            # Clear the metadata for the\\n                            # popped header in initial_metadata\\n                            if popped_header[\"name\"] in initial_metadata:\\n                                initial_metadata.pop(popped_header[\"name\"])',\n",
              "  'def split_text(self, text: str) -> List[Document]:\\n        \"\"\"Split markdown file\\n        Args:\\n            text: Markdown file\"\"\"\\n\\n        # Split the input text by newline character (\"\\\\n\").\\n        lines = text.split(\"\\\\n\")\\n        # Final output\\n        lines_with_metadata: List[LineType] = []\\n        # Content and metadata of the chunk currently being processed\\n        current_content: List[str] = []\\n        current_metadata: Dict[str, str] = {}\\n        # Keep track of the nested header structure\\n        # header_stack: List[Dict[str, Union[int, str]]] = []\\n        header_stack: List[HeaderType] = []\\n        initial_metadata: Dict[str, str] = {}\\n\\n        in_code_block = False\\n        opening_fence = \"\"\\n\\n        for line in lines:\\n            stripped_line = line.strip()\\n\\n            if not in_code_block:\\n                # Exclude inline code spans\\n                if stripped_line.startswith(\"```\") and stripped_line.count(\"```\") == 1:\\n                    in_code_block = True\\n                    opening_fence = \"```\"\\n                elif stripped_line.startswith(\"~~~\"):\\n                    in_code_block = True\\n                    opening_fence = \"~~~\"\\n            else:\\n                if stripped_line.startswith(opening_fence):\\n                    in_code_block = False\\n                    opening_fence = \"\"\\n\\n            if in_code_block:\\n                current_content.append(stripped_line)\\n                continue',\n",
              "  'class MarkdownHeaderTextSplitter:\\n    \"\"\"Splitting markdown files based on specified headers.\"\"\"\\n\\n    def __init__(\\n        self, headers_to_split_on: List[Tuple[str, str]], return_each_line: bool = False\\n    ):\\n        \"\"\"Create a new MarkdownHeaderTextSplitter.\\n\\n        Args:\\n            headers_to_split_on: Headers we want to track\\n            return_each_line: Return each line w/ associated headers\\n        \"\"\"\\n        # Output line-by-line or aggregated into chunks w/ common headers\\n        self.return_each_line = return_each_line\\n        # Given the headers we want to split on,\\n        # (e.g., \"#, ##, etc\") order by length\\n        self.headers_to_split_on = sorted(\\n            headers_to_split_on, key=lambda split: len(split[0]), reverse=True\\n        )\\n\\n    def aggregate_lines_to_chunks(self, lines: List[LineType]) -> List[Document]:\\n        \"\"\"Combine lines with common metadata into chunks\\n        Args:\\n            lines: Line of text / associated header metadata\\n        \"\"\"\\n        aggregated_chunks: List[LineType] = []\\n\\n        for line in lines:\\n            if (\\n                aggregated_chunks\\n                and aggregated_chunks[-1][\"metadata\"] == line[\"metadata\"]\\n            ):\\n                # If the last line in the aggregated list\\n                # has the same metadata as the current line,\\n                # append the current content to the last lines\\'s content\\n                aggregated_chunks[-1][\"content\"] += \"  \\\\n\" + line[\"content\"]\\n            else:\\n                # Otherwise, append the current line to the aggregated list\\n                aggregated_chunks.append(line)\\n\\n        return [\\n            Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n            for chunk in aggregated_chunks\\n        ]\\n\\n    def split_text(self, text: str) -> List[Document]:\\n        \"\"\"Split markdown file\\n        Args:\\n            text: Markdown file\"\"\"',\n",
              "  '\"\"\"**Text Splitters** are classes for splitting text.\\n\\n\\n**Class hierarchy:**\\n\\n.. code-block::\\n\\n    BaseDocumentTransformer --> TextSplitter --> <name>TextSplitter  # Example: CharacterTextSplitter\\n                                                 RecursiveCharacterTextSplitter -->  <name>TextSplitter\\n\\nNote: **MarkdownHeaderTextSplitter** and **HTMLHeaderTextSplitter do not derive from TextSplitter.\\n\\n\\n**Main helpers:**\\n\\n.. code-block::\\n\\n    Document, Tokenizer, Language, LineType, HeaderType\\n\\n\"\"\"  # noqa: E501\\n\\nfrom __future__ import annotations\\n\\nimport asyncio\\nimport copy\\nimport logging\\nimport pathlib\\nimport re\\nfrom abc import ABC, abstractmethod\\nfrom dataclasses import dataclass\\nfrom enum import Enum\\nfrom functools import partial\\nfrom io import BytesIO, StringIO\\nfrom typing import (\\n    AbstractSet,\\n    Any,\\n    Callable,\\n    Collection,\\n    Dict,\\n    Iterable,\\n    List,\\n    Literal,\\n    Optional,\\n    Sequence,\\n    Tuple,\\n    Type,\\n    TypedDict,\\n    TypeVar,\\n    Union,\\n    cast,\\n)\\n\\nimport requests\\nfrom langchain_core.documents import BaseDocumentTransformer, Document\\n\\nlogger = logging.getLogger(__name__)\\n\\nTS = TypeVar(\"TS\", bound=\"TextSplitter\")\\n\\n\\ndef _make_spacy_pipeline_for_splitting(\\n    pipeline: str, *, max_length: int = 1_000_000\\n) -> Any:  # avoid importing spacy\\n    try:\\n        import spacy\\n    except ImportError:\\n        raise ImportError(\\n            \"Spacy is not installed, please install it with `pip install spacy`.\"\\n        )\\n    if pipeline == \"sentencizer\":\\n        from spacy.lang.en import English\\n\\n        sentencizer = English()\\n        sentencizer.add_pipe(\"sentencizer\")\\n    else:\\n        sentencizer = spacy.load(pipeline, exclude=[\"ner\", \"tagger\"])\\n        sentencizer.max_length = max_length\\n    return sentencizer',\n",
              "  'def split_text(self, text: str) -> List[Document]:\\n        \"\"\"Split markdown file\\n        Args:\\n            text: Markdown file\"\"\"\\n\\n        # Split the input text by newline character (\"\\\\n\").\\n        lines = text.split(\"\\\\n\")\\n        # Final output\\n        lines_with_metadata: List[LineType] = []\\n        # Content and metadata of the chunk currently being processed\\n        current_content: List[str] = []\\n        current_metadata: Dict[str, str] = {}\\n        # Keep track of the nested header structure\\n        # header_stack: List[Dict[str, Union[int, str]]] = []\\n        header_stack: List[HeaderType] = []\\n        initial_metadata: Dict[str, str] = {}\\n\\n        in_code_block = False\\n        opening_fence = \"\"\\n\\n        for line in lines:\\n            stripped_line = line.strip()\\n\\n            if not in_code_block:\\n                # Exclude inline code spans\\n                if stripped_line.startswith(\"```\") and stripped_line.count(\"```\") == 1:\\n                    in_code_block = True\\n                    opening_fence = \"```\"\\n                elif stripped_line.startswith(\"~~~\"):\\n                    in_code_block = True\\n                    opening_fence = \"~~~\"\\n            else:\\n                if stripped_line.startswith(opening_fence):\\n                    in_code_block = False\\n                    opening_fence = \"\"\\n\\n            if in_code_block:\\n                current_content.append(stripped_line)\\n                continue',\n",
              "  'class MarkdownHeaderTextSplitter:\\n    \"\"\"Splitting markdown files based on specified headers.\"\"\"\\n\\n    def __init__(\\n        self, headers_to_split_on: List[Tuple[str, str]], return_each_line: bool = False\\n    ):\\n        \"\"\"Create a new MarkdownHeaderTextSplitter.\\n\\n        Args:\\n            headers_to_split_on: Headers we want to track\\n            return_each_line: Return each line w/ associated headers\\n        \"\"\"\\n        # Output line-by-line or aggregated into chunks w/ common headers\\n        self.return_each_line = return_each_line\\n        # Given the headers we want to split on,\\n        # (e.g., \"#, ##, etc\") order by length\\n        self.headers_to_split_on = sorted(\\n            headers_to_split_on, key=lambda split: len(split[0]), reverse=True\\n        )\\n\\n    def aggregate_lines_to_chunks(self, lines: List[LineType]) -> List[Document]:\\n        \"\"\"Combine lines with common metadata into chunks\\n        Args:\\n            lines: Line of text / associated header metadata\\n        \"\"\"\\n        aggregated_chunks: List[LineType] = []\\n\\n        for line in lines:\\n            if (\\n                aggregated_chunks\\n                and aggregated_chunks[-1][\"metadata\"] == line[\"metadata\"]\\n            ):\\n                # If the last line in the aggregated list\\n                # has the same metadata as the current line,\\n                # append the current content to the last lines\\'s content\\n                aggregated_chunks[-1][\"content\"] += \"  \\\\n\" + line[\"content\"]\\n            else:\\n                # Otherwise, append the current line to the aggregated list\\n                aggregated_chunks.append(line)\\n\\n        return [\\n            Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n            for chunk in aggregated_chunks\\n        ]\\n\\n    def split_text(self, text: str) -> List[Document]:\\n        \"\"\"Split markdown file\\n        Args:\\n            text: Markdown file\"\"\"',\n",
              "  'class CharacterTextSplitter(TextSplitter):\\n    \"\"\"Splitting text that looks at characters.\"\"\"\\n\\n    def __init__(\\n        self, separator: str = \"\\\\n\\\\n\", is_separator_regex: bool = False, **kwargs: Any\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(**kwargs)\\n        self._separator = separator\\n        self._is_separator_regex = is_separator_regex\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        # First we naively split the large input into a bunch of smaller ones.\\n        separator = (\\n            self._separator if self._is_separator_regex else re.escape(self._separator)\\n        )\\n        splits = _split_text_with_regex(text, separator, self._keep_separator)\\n        _separator = \"\" if self._keep_separator else self._separator\\n        return self._merge_splits(splits, _separator)\\n\\n\\nclass LineType(TypedDict):\\n    \"\"\"Line type as typed dict.\"\"\"\\n\\n    metadata: Dict[str, str]\\n    content: str\\n\\n\\nclass HeaderType(TypedDict):\\n    \"\"\"Header type as typed dict.\"\"\"\\n\\n    level: int\\n    name: str\\n    data: str',\n",
              "  'if issubclass(cls, TokenTextSplitter):\\n            extra_kwargs = {\\n                \"encoding_name\": encoding_name,\\n                \"model_name\": model_name,\\n                \"allowed_special\": allowed_special,\\n                \"disallowed_special\": disallowed_special,\\n            }\\n            kwargs = {**kwargs, **extra_kwargs}\\n\\n        return cls(length_function=_tiktoken_encoder, **kwargs)\\n\\n    def transform_documents(\\n        self, documents: Sequence[Document], **kwargs: Any\\n    ) -> Sequence[Document]:\\n        \"\"\"Transform sequence of documents by splitting them.\"\"\"\\n        return self.split_documents(list(documents))\\n\\n    async def atransform_documents(\\n        self, documents: Sequence[Document], **kwargs: Any\\n    ) -> Sequence[Document]:\\n        \"\"\"Asynchronously transform a sequence of documents by splitting them.\"\"\"\\n        return await asyncio.get_running_loop().run_in_executor(\\n            None, partial(self.transform_documents, **kwargs), documents\\n        )',\n",
              "  'if not isinstance(tokenizer, PreTrainedTokenizerBase):\\n                raise ValueError(\\n                    \"Tokenizer received was not an instance of PreTrainedTokenizerBase\"\\n                )\\n\\n            def _huggingface_tokenizer_length(text: str) -> int:\\n                return len(tokenizer.encode(text))\\n\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import transformers python package. \"\\n                \"Please install it with `pip install transformers`.\"\\n            )\\n        return cls(length_function=_huggingface_tokenizer_length, **kwargs)\\n\\n    @classmethod\\n    def from_tiktoken_encoder(\\n        cls: Type[TS],\\n        encoding_name: str = \"gpt2\",\\n        model_name: Optional[str] = None,\\n        allowed_special: Union[Literal[\"all\"], AbstractSet[str]] = set(),\\n        disallowed_special: Union[Literal[\"all\"], Collection[str]] = \"all\",\\n        **kwargs: Any,\\n    ) -> TS:\\n        \"\"\"Text splitter that uses tiktoken encoder to count length.\"\"\"\\n        try:\\n            import tiktoken\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import tiktoken python package. \"\\n                \"This is needed in order to calculate max_tokens_for_prompt. \"\\n                \"Please install it with `pip install tiktoken`.\"\\n            )\\n\\n        if model_name is not None:\\n            enc = tiktoken.encoding_for_model(model_name)\\n        else:\\n            enc = tiktoken.get_encoding(encoding_name)\\n\\n        def _tiktoken_encoder(text: str) -> int:\\n            return len(\\n                enc.encode(\\n                    text,\\n                    allowed_special=allowed_special,\\n                    disallowed_special=disallowed_special,\\n                )\\n            )',\n",
              "  'docs = []\\n        current_doc: List[str] = []\\n        total = 0\\n        for d in splits:\\n            _len = self._length_function(d)\\n            if (\\n                total + _len + (separator_len if len(current_doc) > 0 else 0)\\n                > self._chunk_size\\n            ):\\n                if total > self._chunk_size:\\n                    logger.warning(\\n                        f\"Created a chunk of size {total}, \"\\n                        f\"which is longer than the specified {self._chunk_size}\"\\n                    )\\n                if len(current_doc) > 0:\\n                    doc = self._join_docs(current_doc, separator)\\n                    if doc is not None:\\n                        docs.append(doc)\\n                    # Keep on popping if:\\n                    # - we have a larger chunk than in the chunk overlap\\n                    # - or if we still have any chunks and the length is long\\n                    while total > self._chunk_overlap or (\\n                        total + _len + (separator_len if len(current_doc) > 0 else 0)\\n                        > self._chunk_size\\n                        and total > 0\\n                    ):\\n                        total -= self._length_function(current_doc[0]) + (\\n                            separator_len if len(current_doc) > 1 else 0\\n                        )\\n                        current_doc = current_doc[1:]\\n            current_doc.append(d)\\n            total += _len + (separator_len if len(current_doc) > 1 else 0)\\n        doc = self._join_docs(current_doc, separator)\\n        if doc is not None:\\n            docs.append(doc)\\n        return docs\\n\\n    @classmethod\\n    def from_huggingface_tokenizer(cls, tokenizer: Any, **kwargs: Any) -> TextSplitter:\\n        \"\"\"Text splitter that uses HuggingFace tokenizer to count length.\"\"\"\\n        try:\\n            from transformers import PreTrainedTokenizerBase',\n",
              "  '@abstractmethod\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split text into multiple components.\"\"\"\\n\\n    def create_documents(\\n        self, texts: List[str], metadatas: Optional[List[dict]] = None\\n    ) -> List[Document]:\\n        \"\"\"Create documents from a list of texts.\"\"\"\\n        _metadatas = metadatas or [{}] * len(texts)\\n        documents = []\\n        for i, text in enumerate(texts):\\n            index = -1\\n            for chunk in self.split_text(text):\\n                metadata = copy.deepcopy(_metadatas[i])\\n                if self._add_start_index:\\n                    index = text.find(chunk, index + 1)\\n                    metadata[\"start_index\"] = index\\n                new_doc = Document(page_content=chunk, metadata=metadata)\\n                documents.append(new_doc)\\n        return documents\\n\\n    def split_documents(self, documents: Iterable[Document]) -> List[Document]:\\n        \"\"\"Split documents.\"\"\"\\n        texts, metadatas = [], []\\n        for doc in documents:\\n            texts.append(doc.page_content)\\n            metadatas.append(doc.metadata)\\n        return self.create_documents(texts, metadatas=metadatas)\\n\\n    def _join_docs(self, docs: List[str], separator: str) -> Optional[str]:\\n        text = separator.join(docs)\\n        if self._strip_whitespace:\\n            text = text.strip()\\n        if text == \"\":\\n            return None\\n        else:\\n            return text\\n\\n    def _merge_splits(self, splits: Iterable[str], separator: str) -> List[str]:\\n        # We now want to combine these smaller pieces into medium size\\n        # chunks to send to the LLM.\\n        separator_len = self._length_function(separator)',\n",
              "  'class TextSplitter(BaseDocumentTransformer, ABC):\\n    \"\"\"Interface for splitting text into chunks.\"\"\"\\n\\n    def __init__(\\n        self,\\n        chunk_size: int = 4000,\\n        chunk_overlap: int = 200,\\n        length_function: Callable[[str], int] = len,\\n        keep_separator: bool = False,\\n        add_start_index: bool = False,\\n        strip_whitespace: bool = True,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\\n\\n        Args:\\n            chunk_size: Maximum size of chunks to return\\n            chunk_overlap: Overlap in characters between chunks\\n            length_function: Function that measures the length of given chunks\\n            keep_separator: Whether to keep the separator in the chunks\\n            add_start_index: If `True`, includes chunk\\'s start index in metadata\\n            strip_whitespace: If `True`, strips whitespace from the start and end of\\n                              every document\\n        \"\"\"\\n        if chunk_overlap > chunk_size:\\n            raise ValueError(\\n                f\"Got a larger chunk overlap ({chunk_overlap}) than chunk size \"\\n                f\"({chunk_size}), should be smaller.\"\\n            )\\n        self._chunk_size = chunk_size\\n        self._chunk_overlap = chunk_overlap\\n        self._length_function = length_function\\n        self._keep_separator = keep_separator\\n        self._add_start_index = add_start_index\\n        self._strip_whitespace = strip_whitespace\\n\\n    @abstractmethod\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split text into multiple components.\"\"\"',\n",
              "  'def _split_text_with_regex(\\n    text: str, separator: str, keep_separator: bool\\n) -> List[str]:\\n    # Now that we have the separator, split the text\\n    if separator:\\n        if keep_separator:\\n            # The parentheses in the pattern keep the delimiters in the result.\\n            _splits = re.split(f\"({separator})\", text)\\n            splits = [_splits[i] + _splits[i + 1] for i in range(1, len(_splits), 2)]\\n            if len(_splits) % 2 == 0:\\n                splits += _splits[-1:]\\n            splits = [_splits[0]] + splits\\n        else:\\n            splits = re.split(separator, text)\\n    else:\\n        splits = list(text)\\n    return [s for s in splits if s != \"\"]',\n",
              "  'class CharacterTextSplitter(TextSplitter):\\n    \"\"\"Splitting text that looks at characters.\"\"\"\\n\\n    def __init__(\\n        self, separator: str = \"\\\\n\\\\n\", is_separator_regex: bool = False, **kwargs: Any\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(**kwargs)\\n        self._separator = separator\\n        self._is_separator_regex = is_separator_regex\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        # First we naively split the large input into a bunch of smaller ones.\\n        separator = (\\n            self._separator if self._is_separator_regex else re.escape(self._separator)\\n        )\\n        splits = _split_text_with_regex(text, separator, self._keep_separator)\\n        _separator = \"\" if self._keep_separator else self._separator\\n        return self._merge_splits(splits, _separator)\\n\\n\\nclass LineType(TypedDict):\\n    \"\"\"Line type as typed dict.\"\"\"\\n\\n    metadata: Dict[str, str]\\n    content: str\\n\\n\\nclass HeaderType(TypedDict):\\n    \"\"\"Header type as typed dict.\"\"\"\\n\\n    level: int\\n    name: str\\n    data: str',\n",
              "  'class PythonCodeTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Python syntax.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a PythonCodeTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.PYTHON)\\n        super().__init__(separators=separators, **kwargs)\\n\\n\\nclass MarkdownTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Markdown-formatted headings.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a MarkdownTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.MARKDOWN)\\n        super().__init__(separators=separators, **kwargs)\\n\\n\\nclass LatexTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Latex-formatted layout elements.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a LatexTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.LATEX)\\n        super().__init__(separators=separators, **kwargs)',\n",
              "  'class NLTKTextSplitter(TextSplitter):\\n    \"\"\"Splitting text using NLTK package.\"\"\"\\n\\n    def __init__(\\n        self, separator: str = \"\\\\n\\\\n\", language: str = \"english\", **kwargs: Any\\n    ) -> None:\\n        \"\"\"Initialize the NLTK splitter.\"\"\"\\n        super().__init__(**kwargs)\\n        try:\\n            from nltk.tokenize import sent_tokenize\\n\\n            self._tokenizer = sent_tokenize\\n        except ImportError:\\n            raise ImportError(\\n                \"NLTK is not installed, please install it with `pip install nltk`.\"\\n            )\\n        self._separator = separator\\n        self._language = language\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        # First we naively split the large input into a bunch of smaller ones.\\n        splits = self._tokenizer(text, language=self._language)\\n        return self._merge_splits(splits, self._separator)\\n\\n\\nclass SpacyTextSplitter(TextSplitter):\\n    \"\"\"Splitting text using Spacy package.\\n\\n\\n    Per default, Spacy\\'s `en_core_web_sm` model is used and\\n    its default max_length is 1000000 (it is the length of maximum character\\n    this model takes which can be increased for large files). For a faster, but\\n    potentially less accurate splitting, you can use `pipeline=\\'sentencizer\\'`.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        separator: str = \"\\\\n\\\\n\",\\n        pipeline: str = \"en_core_web_sm\",\\n        max_length: int = 1_000_000,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Initialize the spacy text splitter.\"\"\"\\n        super().__init__(**kwargs)\\n        self._tokenizer = _make_spacy_pipeline_for_splitting(\\n            pipeline, max_length=max_length\\n        )\\n        self._separator = separator\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        splits = (s.text for s in self._tokenizer(text).sents)\\n        return self._merge_splits(splits, self._separator)\\n\\n\\n# For backwards compatibility',\n",
              "  'else:\\n            raise ValueError(\\n                f\"Language {language} is not supported! \"\\n                f\"Please choose from {list(Language)}\"\\n            )',\n",
              "  '\"\\\\nevent \",\\n                \"\\\\nmodifier \",\\n                \"\\\\nerror \",\\n                \"\\\\nstruct \",\\n                \"\\\\nenum \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo while \",\\n                \"\\\\nassembly \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.COBOL:\\n            return [\\n                # Split along divisions\\n                \"\\\\nIDENTIFICATION DIVISION.\",\\n                \"\\\\nENVIRONMENT DIVISION.\",\\n                \"\\\\nDATA DIVISION.\",\\n                \"\\\\nPROCEDURE DIVISION.\",\\n                # Split along sections within DATA DIVISION\\n                \"\\\\nWORKING-STORAGE SECTION.\",\\n                \"\\\\nLINKAGE SECTION.\",\\n                \"\\\\nFILE SECTION.\",\\n                # Split along sections within PROCEDURE DIVISION\\n                \"\\\\nINPUT-OUTPUT SECTION.\",\\n                # Split along paragraphs and common statements\\n                \"\\\\nOPEN \",\\n                \"\\\\nCLOSE \",\\n                \"\\\\nREAD \",\\n                \"\\\\nWRITE \",\\n                \"\\\\nIF \",\\n                \"\\\\nELSE \",\\n                \"\\\\nMOVE \",\\n                \"\\\\nPERFORM \",\\n                \"\\\\nUNTIL \",\\n                \"\\\\nVARYING \",\\n                \"\\\\nACCEPT \",\\n                \"\\\\nDISPLAY \",\\n                \"\\\\nSTOP RUN.\",\\n                # Split by the normal type of lines\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]',\n",
              "  '\"<nav\",\\n                # Head\\n                \"<head\",\\n                \"<style\",\\n                \"<script\",\\n                \"<meta\",\\n                \"<title\",\\n                \"\",\\n            ]\\n        elif language == Language.CSHARP:\\n            return [\\n                \"\\\\ninterface \",\\n                \"\\\\nenum \",\\n                \"\\\\nimplements \",\\n                \"\\\\ndelegate \",\\n                \"\\\\nevent \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nabstract \",\\n                # Split along method definitions\\n                \"\\\\npublic \",\\n                \"\\\\nprotected \",\\n                \"\\\\nprivate \",\\n                \"\\\\nstatic \",\\n                \"\\\\nreturn \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\ncontinue \",\\n                \"\\\\nfor \",\\n                \"\\\\nforeach \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\nbreak \",\\n                \"\\\\ncase \",\\n                \"\\\\nelse \",\\n                # Split by exceptions\\n                \"\\\\ntry \",\\n                \"\\\\nthrow \",\\n                \"\\\\nfinally \",\\n                \"\\\\ncatch \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SOL:\\n            return [\\n                # Split along compiler information definitions\\n                \"\\\\npragma \",\\n                \"\\\\nusing \",\\n                # Split along contract definitions\\n                \"\\\\ncontract \",\\n                \"\\\\ninterface \",\\n                \"\\\\nlibrary \",\\n                # Split along method definitions\\n                \"\\\\nconstructor \",\\n                \"\\\\ntype \",\\n                \"\\\\nfunction \",\\n                \"\\\\nevent \",\\n                \"\\\\nmodifier \",\\n                \"\\\\nerror \",\\n                \"\\\\nstruct \",\\n                \"\\\\nenum \",\\n                # Split along control flow statements',\n",
              "  '\"\\\\n\\\\\\\\*\\\\\\\\*\\\\\\\\*+\\\\n\",\\n                \"\\\\n---+\\\\n\",\\n                \"\\\\n___+\\\\n\",\\n                # Note that this splitter doesn\\'t handle horizontal lines defined\\n                # by *three or more* of ***, ---, or ___, but this is not handled\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.LATEX:\\n            return [\\n                # First, try to split along Latex sections\\n                \"\\\\n\\\\\\\\\\\\\\\\chapter{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\section{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\subsection{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\subsubsection{\",\\n                # Now split by environments\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{enumerate}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{itemize}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{description}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{list}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{quote}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{quotation}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{verse}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{verbatim}\",\\n                # Now split by math environments\\n                \"\\\\n\\\\\\\\\\\\begin{align}\",\\n                \"$$\",\\n                \"$\",\\n                # Now split by the normal type of lines\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.HTML:\\n            return [\\n                # First, try to split along HTML tags\\n                \"<body\",\\n                \"<div\",\\n                \"<p\",\\n                \"<br\",\\n                \"<li\",\\n                \"<h1\",\\n                \"<h2\",\\n                \"<h3\",\\n                \"<h4\",\\n                \"<h5\",\\n                \"<h6\",\\n                \"<span\",\\n                \"<table\",\\n                \"<tr\",\\n                \"<td\",\\n                \"<th\",\\n                \"<ul\",\\n                \"<ol\",\\n                \"<header\",\\n                \"<footer\",\\n                \"<nav\",\\n                # Head\\n                \"<head\",\\n                \"<style\",\\n                \"<script\",\\n                \"<meta\",\\n                \"<title\",',\n",
              "  '\" \",\\n                \"\",\\n            ]\\n        elif language == Language.SCALA:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nobject \",\\n                # Split along method definitions\\n                \"\\\\ndef \",\\n                \"\\\\nval \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nmatch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SWIFT:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunc \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nstruct \",\\n                \"\\\\nenum \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.MARKDOWN:\\n            return [\\n                # First, try to split along Markdown headings (starting with level 2)\\n                \"\\\\n#{1,6} \",\\n                # Note the alternative syntax for headings (below) is not handled here\\n                # Heading level 2\\n                # ---------------\\n                # End of code block\\n                \"```\\\\n\",\\n                # Horizontal lines\\n                \"\\\\n\\\\\\\\*\\\\\\\\*\\\\\\\\*+\\\\n\",\\n                \"\\\\n---+\\\\n\",\\n                \"\\\\n___+\\\\n\",\\n                # Note that this splitter doesn\\'t handle horizontal lines defined',\n",
              "  'return [\\n                # First, try to split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\ndef \",\\n                \"\\\\n\\\\tdef \",\\n                # Now split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.RST:\\n            return [\\n                # Split along section titles\\n                \"\\\\n=+\\\\n\",\\n                \"\\\\n-+\\\\n\",\\n                \"\\\\n\\\\\\\\*+\\\\n\",\\n                # Split along directive markers\\n                \"\\\\n\\\\n.. *\\\\n\\\\n\",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.RUBY:\\n            return [\\n                # Split along method definitions\\n                \"\\\\ndef \",\\n                \"\\\\nclass \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nunless \",\\n                \"\\\\nwhile \",\\n                \"\\\\nfor \",\\n                \"\\\\ndo \",\\n                \"\\\\nbegin \",\\n                \"\\\\nrescue \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.RUST:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfn \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nwhile \",\\n                \"\\\\nfor \",\\n                \"\\\\nloop \",\\n                \"\\\\nmatch \",\\n                \"\\\\nconst \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SCALA:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",',\n",
              "  'class HTMLHeaderTextSplitter:\\n    \"\"\"\\n    Splitting HTML files based on specified headers.\\n    Requires lxml package.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        headers_to_split_on: List[Tuple[str, str]],\\n        return_each_element: bool = False,\\n    ):\\n        \"\"\"Create a new HTMLHeaderTextSplitter.\\n\\n        Args:\\n            headers_to_split_on: list of tuples of headers we want to track mapped to\\n                (arbitrary) keys for metadata. Allowed header values: h1, h2, h3, h4,\\n                h5, h6 e.g. [(\"h1\", \"Header 1\"), (\"h2\", \"Header 2)].\\n            return_each_element: Return each element w/ associated headers.\\n        \"\"\"\\n        # Output element-by-element or aggregated into chunks w/ common headers\\n        self.return_each_element = return_each_element\\n        self.headers_to_split_on = sorted(headers_to_split_on)\\n\\n    def aggregate_elements_to_chunks(\\n        self, elements: List[ElementType]\\n    ) -> List[Document]:\\n        \"\"\"Combine elements with common metadata into chunks\\n\\n        Args:\\n            elements: HTML element content with associated identifying info and metadata\\n        \"\"\"\\n        aggregated_chunks: List[ElementType] = []\\n\\n        for element in elements:\\n            if (\\n                aggregated_chunks\\n                and aggregated_chunks[-1][\"metadata\"] == element[\"metadata\"]\\n            ):\\n                # If the last element in the aggregated list\\n                # has the same metadata as the current element,\\n                # append the current content to the last element\\'s content\\n                aggregated_chunks[-1][\"content\"] += \"  \\\\n\" + element[\"content\"]\\n            else:\\n                # Otherwise, append the current element to the aggregated list\\n                aggregated_chunks.append(element)\\n\\n        return [\\n            Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n            for chunk in aggregated_chunks\\n        ]',\n",
              "  '_separator = separator if self._is_separator_regex else re.escape(separator)\\n        splits = _split_text_with_regex(text, _separator, self._keep_separator)\\n\\n        # Now go merging things, recursively splitting longer texts.\\n        _good_splits = []\\n        _separator = \"\" if self._keep_separator else separator\\n        for s in splits:\\n            if self._length_function(s) < self._chunk_size:\\n                _good_splits.append(s)\\n            else:\\n                if _good_splits:\\n                    merged_text = self._merge_splits(_good_splits, _separator)\\n                    final_chunks.extend(merged_text)\\n                    _good_splits = []\\n                if not new_separators:\\n                    final_chunks.append(s)\\n                else:\\n                    other_info = self._split_text(s, new_separators)\\n                    final_chunks.extend(other_info)\\n        if _good_splits:\\n            merged_text = self._merge_splits(_good_splits, _separator)\\n            final_chunks.extend(merged_text)\\n        return final_chunks\\n\\n    def split_text(self, text: str) -> List[str]:\\n        return self._split_text(text, self._separators)\\n\\n    @classmethod\\n    def from_language(\\n        cls, language: Language, **kwargs: Any\\n    ) -> RecursiveCharacterTextSplitter:\\n        separators = cls.get_separators_for_language(language)\\n        return cls(separators=separators, is_separator_regex=True, **kwargs)',\n",
              "  'class RecursiveCharacterTextSplitter(TextSplitter):\\n    \"\"\"Splitting text by recursively look at characters.\\n\\n    Recursively tries to split by different characters to find one\\n    that works.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        separators: Optional[List[str]] = None,\\n        keep_separator: bool = True,\\n        is_separator_regex: bool = False,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(keep_separator=keep_separator, **kwargs)\\n        self._separators = separators or [\"\\\\n\\\\n\", \"\\\\n\", \" \", \"\"]\\n        self._is_separator_regex = is_separator_regex\\n\\n    def _split_text(self, text: str, separators: List[str]) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        final_chunks = []\\n        # Get appropriate separator to use\\n        separator = separators[-1]\\n        new_separators = []\\n        for i, _s in enumerate(separators):\\n            _separator = _s if self._is_separator_regex else re.escape(_s)\\n            if _s == \"\":\\n                separator = _s\\n                break\\n            if re.search(_separator, text):\\n                separator = _s\\n                new_separators = separators[i + 1 :]\\n                break\\n\\n        _separator = separator if self._is_separator_regex else re.escape(separator)\\n        splits = _split_text_with_regex(text, _separator, self._keep_separator)',\n",
              "  'class Language(str, Enum):\\n    \"\"\"Enum of the programming languages.\"\"\"\\n\\n    CPP = \"cpp\"\\n    GO = \"go\"\\n    JAVA = \"java\"\\n    KOTLIN = \"kotlin\"\\n    JS = \"js\"\\n    TS = \"ts\"\\n    PHP = \"php\"\\n    PROTO = \"proto\"\\n    PYTHON = \"python\"\\n    RST = \"rst\"\\n    RUBY = \"ruby\"\\n    RUST = \"rust\"\\n    SCALA = \"scala\"\\n    SWIFT = \"swift\"\\n    MARKDOWN = \"markdown\"\\n    LATEX = \"latex\"\\n    HTML = \"html\"\\n    SOL = \"sol\"\\n    CSHARP = \"csharp\"\\n    COBOL = \"cobol\"',\n",
              "  'def split_text(self, text: str) -> List[str]:\\n        def encode_strip_start_and_stop_token_ids(text: str) -> List[int]:\\n            return self._encode(text)[1:-1]\\n\\n        tokenizer = Tokenizer(\\n            chunk_overlap=self._chunk_overlap,\\n            tokens_per_chunk=self.tokens_per_chunk,\\n            decode=self.tokenizer.decode,\\n            encode=encode_strip_start_and_stop_token_ids,\\n        )\\n\\n        return split_text_on_tokens(text=text, tokenizer=tokenizer)\\n\\n    def count_tokens(self, *, text: str) -> int:\\n        return len(self._encode(text))\\n\\n    _max_length_equal_32_bit_integer: int = 2**32\\n\\n    def _encode(self, text: str) -> List[int]:\\n        token_ids_with_start_and_end_token_ids = self.tokenizer.encode(\\n            text,\\n            max_length=self._max_length_equal_32_bit_integer,\\n            truncation=\"do_not_truncate\",\\n        )\\n        return token_ids_with_start_and_end_token_ids',\n",
              "  'class SentenceTransformersTokenTextSplitter(TextSplitter):\\n    \"\"\"Splitting text to tokens using sentence model tokenizer.\"\"\"\\n\\n    def __init__(\\n        self,\\n        chunk_overlap: int = 50,\\n        model_name: str = \"sentence-transformers/all-mpnet-base-v2\",\\n        tokens_per_chunk: Optional[int] = None,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(**kwargs, chunk_overlap=chunk_overlap)\\n\\n        try:\\n            from sentence_transformers import SentenceTransformer\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import sentence_transformer python package. \"\\n                \"This is needed in order to for SentenceTransformersTokenTextSplitter. \"\\n                \"Please install it with `pip install sentence-transformers`.\"\\n            )\\n\\n        self.model_name = model_name\\n        self._model = SentenceTransformer(self.model_name)\\n        self.tokenizer = self._model.tokenizer\\n        self._initialize_chunk_configuration(tokens_per_chunk=tokens_per_chunk)\\n\\n    def _initialize_chunk_configuration(\\n        self, *, tokens_per_chunk: Optional[int]\\n    ) -> None:\\n        self.maximum_tokens_per_chunk = cast(int, self._model.max_seq_length)\\n\\n        if tokens_per_chunk is None:\\n            self.tokens_per_chunk = self.maximum_tokens_per_chunk\\n        else:\\n            self.tokens_per_chunk = tokens_per_chunk\\n\\n        if self.tokens_per_chunk > self.maximum_tokens_per_chunk:\\n            raise ValueError(\\n                f\"The token limit of the models \\'{self.model_name}\\'\"\\n                f\" is: {self.maximum_tokens_per_chunk}.\"\\n                f\" Argument tokens_per_chunk={self.tokens_per_chunk}\"\\n                f\" > maximum token limit.\"\\n            )\\n\\n    def split_text(self, text: str) -> List[str]:\\n        def encode_strip_start_and_stop_token_ids(text: str) -> List[int]:\\n            return self._encode(text)[1:-1]',\n",
              "  'class TokenTextSplitter(TextSplitter):\\n    \"\"\"Splitting text to tokens using model tokenizer.\"\"\"\\n\\n    def __init__(\\n        self,\\n        encoding_name: str = \"gpt2\",\\n        model_name: Optional[str] = None,\\n        allowed_special: Union[Literal[\"all\"], AbstractSet[str]] = set(),\\n        disallowed_special: Union[Literal[\"all\"], Collection[str]] = \"all\",\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(**kwargs)\\n        try:\\n            import tiktoken\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import tiktoken python package. \"\\n                \"This is needed in order to for TokenTextSplitter. \"\\n                \"Please install it with `pip install tiktoken`.\"\\n            )\\n\\n        if model_name is not None:\\n            enc = tiktoken.encoding_for_model(model_name)\\n        else:\\n            enc = tiktoken.get_encoding(encoding_name)\\n        self._tokenizer = enc\\n        self._allowed_special = allowed_special\\n        self._disallowed_special = disallowed_special\\n\\n    def split_text(self, text: str) -> List[str]:\\n        def _encode(_text: str) -> List[int]:\\n            return self._tokenizer.encode(\\n                _text,\\n                allowed_special=self._allowed_special,\\n                disallowed_special=self._disallowed_special,\\n            )\\n\\n        tokenizer = Tokenizer(\\n            chunk_overlap=self._chunk_overlap,\\n            tokens_per_chunk=self._chunk_size,\\n            decode=self._tokenizer.decode,\\n            encode=_encode,\\n        )\\n\\n        return split_text_on_tokens(text=text, tokenizer=tokenizer)',\n",
              "  'class Tokenizer:\\n    \"\"\"Tokenizer data class.\"\"\"\\n\\n    chunk_overlap: int\\n    \"\"\"Overlap in tokens between chunks\"\"\"\\n    tokens_per_chunk: int\\n    \"\"\"Maximum number of tokens per chunk\"\"\"\\n    decode: Callable[[List[int]], str]\\n    \"\"\" Function to decode a list of token ids to a string\"\"\"\\n    encode: Callable[[str], List[int]]\\n    \"\"\" Function to encode a string to a list of token ids\"\"\"\\n\\n\\ndef split_text_on_tokens(*, text: str, tokenizer: Tokenizer) -> List[str]:\\n    \"\"\"Split incoming text and return chunks using tokenizer.\"\"\"\\n    splits: List[str] = []\\n    input_ids = tokenizer.encode(text)\\n    start_idx = 0\\n    cur_idx = min(start_idx + tokenizer.tokens_per_chunk, len(input_ids))\\n    chunk_ids = input_ids[start_idx:cur_idx]\\n    while start_idx < len(input_ids):\\n        splits.append(tokenizer.decode(chunk_ids))\\n        if cur_idx == len(input_ids):\\n            break\\n        start_idx += tokenizer.tokens_per_chunk - tokenizer.chunk_overlap\\n        cur_idx = min(start_idx + tokenizer.tokens_per_chunk, len(input_ids))\\n        chunk_ids = input_ids[start_idx:cur_idx]\\n    return splits',\n",
              "  '# map xhtml namespace prefix\\n        ns_map = {\"h\": \"http://www.w3.org/1999/xhtml\"}\\n\\n        # build list of elements from DOM\\n        elements = []\\n        for element in result_dom.findall(\"*//*\", ns_map):\\n            if element.findall(\"*[@class=\\'headers\\']\") or element.findall(\\n                \"*[@class=\\'chunk\\']\"\\n            ):\\n                elements.append(\\n                    ElementType(\\n                        url=file,\\n                        xpath=\"\".join(\\n                            [\\n                                node.text\\n                                for node in element.findall(\"*[@class=\\'xpath\\']\", ns_map)\\n                            ]\\n                        ),\\n                        content=\"\".join(\\n                            [\\n                                node.text\\n                                for node in element.findall(\"*[@class=\\'chunk\\']\", ns_map)\\n                            ]\\n                        ),\\n                        metadata={\\n                            # Add text of specified headers to metadata using header\\n                            # mapping.\\n                            header_mapping[node.tag]: node.text\\n                            for node in filter(\\n                                lambda x: x.tag in header_filter,\\n                                element.findall(\"*[@class=\\'headers\\']/*\", ns_map),\\n                            )\\n                        },\\n                    )\\n                )\\n\\n        if not self.return_each_element:\\n            return self.aggregate_elements_to_chunks(elements)\\n        else:\\n            return [\\n                Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n                for chunk in elements\\n            ]\\n\\n\\n# should be in newer Python versions (3.10+)\\n# @dataclass(frozen=True, kw_only=True, slots=True)\\n@dataclass(frozen=True)',\n",
              "  'return [\\n            Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n            for chunk in aggregated_chunks\\n        ]\\n\\n    def split_text_from_url(self, url: str) -> List[Document]:\\n        \"\"\"Split HTML from web URL\\n\\n        Args:\\n            url: web URL\\n        \"\"\"\\n        r = requests.get(url)\\n        return self.split_text_from_file(BytesIO(r.content))\\n\\n    def split_text(self, text: str) -> List[Document]:\\n        \"\"\"Split HTML text string\\n\\n        Args:\\n            text: HTML text\\n        \"\"\"\\n        return self.split_text_from_file(StringIO(text))\\n\\n    def split_text_from_file(self, file: Any) -> List[Document]:\\n        \"\"\"Split HTML file\\n\\n        Args:\\n            file: HTML file\\n        \"\"\"\\n        try:\\n            from lxml import etree\\n        except ImportError as e:\\n            raise ImportError(\\n                \"Unable to import lxml, please install with `pip install lxml`.\"\\n            ) from e\\n        # use lxml library to parse html document and return xml ElementTree\\n        parser = etree.HTMLParser()\\n        tree = etree.parse(file, parser)\\n\\n        # document transformation for \"structure-aware\" chunking is handled with xsl.\\n        # see comments in html_chunks_with_headers.xslt for more detailed information.\\n        xslt_path = (\\n            pathlib.Path(__file__).parent\\n            / \"document_transformers/xsl/html_chunks_with_headers.xslt\"\\n        )\\n        xslt_tree = etree.parse(xslt_path)\\n        transform = etree.XSLT(xslt_tree)\\n        result = transform(tree)\\n        result_dom = etree.fromstring(str(result))\\n\\n        # create filter and mapping for header metadata\\n        header_filter = [header[0] for header in self.headers_to_split_on]\\n        header_mapping = dict(self.headers_to_split_on)\\n\\n        # map xhtml namespace prefix\\n        ns_map = {\"h\": \"http://www.w3.org/1999/xhtml\"}',\n",
              "  '@staticmethod\\n    def get_separators_for_language(language: Language) -> List[str]:\\n        if language == Language.CPP:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along function definitions\\n                \"\\\\nvoid \",\\n                \"\\\\nint \",\\n                \"\\\\nfloat \",\\n                \"\\\\ndouble \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.GO:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunc \",\\n                \"\\\\nvar \",\\n                \"\\\\nconst \",\\n                \"\\\\ntype \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.JAVA:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along method definitions\\n                \"\\\\npublic \",\\n                \"\\\\nprotected \",\\n                \"\\\\nprivate \",\\n                \"\\\\nstatic \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.KOTLIN:\\n            return [',\n",
              "  'class ElementType(TypedDict):\\n    \"\"\"Element type as typed dict.\"\"\"\\n\\n    url: str\\n    xpath: str\\n    content: str\\n    metadata: Dict[str, str]',\n",
              "  'if current_content:\\n            lines_with_metadata.append(\\n                {\"content\": \"\\\\n\".join(current_content), \"metadata\": current_metadata}\\n            )\\n\\n        # lines_with_metadata has each line with associated header metadata\\n        # aggregate these into chunks based on common metadata\\n        if not self.return_each_line:\\n            return self.aggregate_lines_to_chunks(lines_with_metadata)\\n        else:\\n            return [\\n                Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n                for chunk in lines_with_metadata\\n            ]',\n",
              "  '# Push the current header to the stack\\n                        header: HeaderType = {\\n                            \"level\": current_header_level,\\n                            \"name\": name,\\n                            \"data\": stripped_line[len(sep) :].strip(),\\n                        }\\n                        header_stack.append(header)\\n                        # Update initial_metadata with the current header\\n                        initial_metadata[name] = header[\"data\"]\\n\\n                    # Add the previous line to the lines_with_metadata\\n                    # only if current_content is not empty\\n                    if current_content:\\n                        lines_with_metadata.append(\\n                            {\\n                                \"content\": \"\\\\n\".join(current_content),\\n                                \"metadata\": current_metadata.copy(),\\n                            }\\n                        )\\n                        current_content.clear()\\n\\n                    break\\n            else:\\n                if stripped_line:\\n                    current_content.append(stripped_line)\\n                elif current_content:\\n                    lines_with_metadata.append(\\n                        {\\n                            \"content\": \"\\\\n\".join(current_content),\\n                            \"metadata\": current_metadata.copy(),\\n                        }\\n                    )\\n                    current_content.clear()\\n\\n            current_metadata = initial_metadata.copy()\\n\\n        if current_content:\\n            lines_with_metadata.append(\\n                {\"content\": \"\\\\n\".join(current_content), \"metadata\": current_metadata}\\n            )',\n",
              "  'if in_code_block:\\n                current_content.append(stripped_line)\\n                continue\\n\\n            # Check each line against each of the header types (e.g., #, ##)\\n            for sep, name in self.headers_to_split_on:\\n                # Check if line starts with a header that we intend to split on\\n                if stripped_line.startswith(sep) and (\\n                    # Header with no text OR header is followed by space\\n                    # Both are valid conditions that sep is being used a header\\n                    len(stripped_line) == len(sep) or stripped_line[len(sep)] == \" \"\\n                ):\\n                    # Ensure we are tracking the header as metadata\\n                    if name is not None:\\n                        # Get the current header level\\n                        current_header_level = sep.count(\"#\")\\n\\n                        # Pop out headers of lower or same level from the stack\\n                        while (\\n                            header_stack\\n                            and header_stack[-1][\"level\"] >= current_header_level\\n                        ):\\n                            # We have encountered a new header\\n                            # at the same or higher level\\n                            popped_header = header_stack.pop()\\n                            # Clear the metadata for the\\n                            # popped header in initial_metadata\\n                            if popped_header[\"name\"] in initial_metadata:\\n                                initial_metadata.pop(popped_header[\"name\"])',\n",
              "  'def split_text(self, text: str) -> List[Document]:\\n        \"\"\"Split markdown file\\n        Args:\\n            text: Markdown file\"\"\"\\n\\n        # Split the input text by newline character (\"\\\\n\").\\n        lines = text.split(\"\\\\n\")\\n        # Final output\\n        lines_with_metadata: List[LineType] = []\\n        # Content and metadata of the chunk currently being processed\\n        current_content: List[str] = []\\n        current_metadata: Dict[str, str] = {}\\n        # Keep track of the nested header structure\\n        # header_stack: List[Dict[str, Union[int, str]]] = []\\n        header_stack: List[HeaderType] = []\\n        initial_metadata: Dict[str, str] = {}\\n\\n        in_code_block = False\\n        opening_fence = \"\"\\n\\n        for line in lines:\\n            stripped_line = line.strip()\\n\\n            if not in_code_block:\\n                # Exclude inline code spans\\n                if stripped_line.startswith(\"```\") and stripped_line.count(\"```\") == 1:\\n                    in_code_block = True\\n                    opening_fence = \"```\"\\n                elif stripped_line.startswith(\"~~~\"):\\n                    in_code_block = True\\n                    opening_fence = \"~~~\"\\n            else:\\n                if stripped_line.startswith(opening_fence):\\n                    in_code_block = False\\n                    opening_fence = \"\"\\n\\n            if in_code_block:\\n                current_content.append(stripped_line)\\n                continue',\n",
              "  'class MarkdownHeaderTextSplitter:\\n    \"\"\"Splitting markdown files based on specified headers.\"\"\"\\n\\n    def __init__(\\n        self, headers_to_split_on: List[Tuple[str, str]], return_each_line: bool = False\\n    ):\\n        \"\"\"Create a new MarkdownHeaderTextSplitter.\\n\\n        Args:\\n            headers_to_split_on: Headers we want to track\\n            return_each_line: Return each line w/ associated headers\\n        \"\"\"\\n        # Output line-by-line or aggregated into chunks w/ common headers\\n        self.return_each_line = return_each_line\\n        # Given the headers we want to split on,\\n        # (e.g., \"#, ##, etc\") order by length\\n        self.headers_to_split_on = sorted(\\n            headers_to_split_on, key=lambda split: len(split[0]), reverse=True\\n        )\\n\\n    def aggregate_lines_to_chunks(self, lines: List[LineType]) -> List[Document]:\\n        \"\"\"Combine lines with common metadata into chunks\\n        Args:\\n            lines: Line of text / associated header metadata\\n        \"\"\"\\n        aggregated_chunks: List[LineType] = []\\n\\n        for line in lines:\\n            if (\\n                aggregated_chunks\\n                and aggregated_chunks[-1][\"metadata\"] == line[\"metadata\"]\\n            ):\\n                # If the last line in the aggregated list\\n                # has the same metadata as the current line,\\n                # append the current content to the last lines\\'s content\\n                aggregated_chunks[-1][\"content\"] += \"  \\\\n\" + line[\"content\"]\\n            else:\\n                # Otherwise, append the current line to the aggregated list\\n                aggregated_chunks.append(line)\\n\\n        return [\\n            Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n            for chunk in aggregated_chunks\\n        ]\\n\\n    def split_text(self, text: str) -> List[Document]:\\n        \"\"\"Split markdown file\\n        Args:\\n            text: Markdown file\"\"\"',\n",
              "  'class CharacterTextSplitter(TextSplitter):\\n    \"\"\"Splitting text that looks at characters.\"\"\"\\n\\n    def __init__(\\n        self, separator: str = \"\\\\n\\\\n\", is_separator_regex: bool = False, **kwargs: Any\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(**kwargs)\\n        self._separator = separator\\n        self._is_separator_regex = is_separator_regex\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        # First we naively split the large input into a bunch of smaller ones.\\n        separator = (\\n            self._separator if self._is_separator_regex else re.escape(self._separator)\\n        )\\n        splits = _split_text_with_regex(text, separator, self._keep_separator)\\n        _separator = \"\" if self._keep_separator else self._separator\\n        return self._merge_splits(splits, _separator)\\n\\n\\nclass LineType(TypedDict):\\n    \"\"\"Line type as typed dict.\"\"\"\\n\\n    metadata: Dict[str, str]\\n    content: str\\n\\n\\nclass HeaderType(TypedDict):\\n    \"\"\"Header type as typed dict.\"\"\"\\n\\n    level: int\\n    name: str\\n    data: str',\n",
              "  'if issubclass(cls, TokenTextSplitter):\\n            extra_kwargs = {\\n                \"encoding_name\": encoding_name,\\n                \"model_name\": model_name,\\n                \"allowed_special\": allowed_special,\\n                \"disallowed_special\": disallowed_special,\\n            }\\n            kwargs = {**kwargs, **extra_kwargs}\\n\\n        return cls(length_function=_tiktoken_encoder, **kwargs)\\n\\n    def transform_documents(\\n        self, documents: Sequence[Document], **kwargs: Any\\n    ) -> Sequence[Document]:\\n        \"\"\"Transform sequence of documents by splitting them.\"\"\"\\n        return self.split_documents(list(documents))\\n\\n    async def atransform_documents(\\n        self, documents: Sequence[Document], **kwargs: Any\\n    ) -> Sequence[Document]:\\n        \"\"\"Asynchronously transform a sequence of documents by splitting them.\"\"\"\\n        return await asyncio.get_running_loop().run_in_executor(\\n            None, partial(self.transform_documents, **kwargs), documents\\n        )',\n",
              "  'class NLTKTextSplitter(TextSplitter):\\n    \"\"\"Splitting text using NLTK package.\"\"\"\\n\\n    def __init__(\\n        self, separator: str = \"\\\\n\\\\n\", language: str = \"english\", **kwargs: Any\\n    ) -> None:\\n        \"\"\"Initialize the NLTK splitter.\"\"\"\\n        super().__init__(**kwargs)\\n        try:\\n            from nltk.tokenize import sent_tokenize\\n\\n            self._tokenizer = sent_tokenize\\n        except ImportError:\\n            raise ImportError(\\n                \"NLTK is not installed, please install it with `pip install nltk`.\"\\n            )\\n        self._separator = separator\\n        self._language = language\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        # First we naively split the large input into a bunch of smaller ones.\\n        splits = self._tokenizer(text, language=self._language)\\n        return self._merge_splits(splits, self._separator)\\n\\n\\nclass SpacyTextSplitter(TextSplitter):\\n    \"\"\"Splitting text using Spacy package.\\n\\n\\n    Per default, Spacy\\'s `en_core_web_sm` model is used and\\n    its default max_length is 1000000 (it is the length of maximum character\\n    this model takes which can be increased for large files). For a faster, but\\n    potentially less accurate splitting, you can use `pipeline=\\'sentencizer\\'`.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        separator: str = \"\\\\n\\\\n\",\\n        pipeline: str = \"en_core_web_sm\",\\n        max_length: int = 1_000_000,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Initialize the spacy text splitter.\"\"\"\\n        super().__init__(**kwargs)\\n        self._tokenizer = _make_spacy_pipeline_for_splitting(\\n            pipeline, max_length=max_length\\n        )\\n        self._separator = separator\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        splits = (s.text for s in self._tokenizer(text).sents)\\n        return self._merge_splits(splits, self._separator)\\n\\n\\n# For backwards compatibility',\n",
              "  'if issubclass(cls, TokenTextSplitter):\\n            extra_kwargs = {\\n                \"encoding_name\": encoding_name,\\n                \"model_name\": model_name,\\n                \"allowed_special\": allowed_special,\\n                \"disallowed_special\": disallowed_special,\\n            }\\n            kwargs = {**kwargs, **extra_kwargs}\\n\\n        return cls(length_function=_tiktoken_encoder, **kwargs)\\n\\n    def transform_documents(\\n        self, documents: Sequence[Document], **kwargs: Any\\n    ) -> Sequence[Document]:\\n        \"\"\"Transform sequence of documents by splitting them.\"\"\"\\n        return self.split_documents(list(documents))\\n\\n    async def atransform_documents(\\n        self, documents: Sequence[Document], **kwargs: Any\\n    ) -> Sequence[Document]:\\n        \"\"\"Asynchronously transform a sequence of documents by splitting them.\"\"\"\\n        return await asyncio.get_running_loop().run_in_executor(\\n            None, partial(self.transform_documents, **kwargs), documents\\n        )',\n",
              "  'if not isinstance(tokenizer, PreTrainedTokenizerBase):\\n                raise ValueError(\\n                    \"Tokenizer received was not an instance of PreTrainedTokenizerBase\"\\n                )\\n\\n            def _huggingface_tokenizer_length(text: str) -> int:\\n                return len(tokenizer.encode(text))\\n\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import transformers python package. \"\\n                \"Please install it with `pip install transformers`.\"\\n            )\\n        return cls(length_function=_huggingface_tokenizer_length, **kwargs)\\n\\n    @classmethod\\n    def from_tiktoken_encoder(\\n        cls: Type[TS],\\n        encoding_name: str = \"gpt2\",\\n        model_name: Optional[str] = None,\\n        allowed_special: Union[Literal[\"all\"], AbstractSet[str]] = set(),\\n        disallowed_special: Union[Literal[\"all\"], Collection[str]] = \"all\",\\n        **kwargs: Any,\\n    ) -> TS:\\n        \"\"\"Text splitter that uses tiktoken encoder to count length.\"\"\"\\n        try:\\n            import tiktoken\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import tiktoken python package. \"\\n                \"This is needed in order to calculate max_tokens_for_prompt. \"\\n                \"Please install it with `pip install tiktoken`.\"\\n            )\\n\\n        if model_name is not None:\\n            enc = tiktoken.encoding_for_model(model_name)\\n        else:\\n            enc = tiktoken.get_encoding(encoding_name)\\n\\n        def _tiktoken_encoder(text: str) -> int:\\n            return len(\\n                enc.encode(\\n                    text,\\n                    allowed_special=allowed_special,\\n                    disallowed_special=disallowed_special,\\n                )\\n            )',\n",
              "  'docs = []\\n        current_doc: List[str] = []\\n        total = 0\\n        for d in splits:\\n            _len = self._length_function(d)\\n            if (\\n                total + _len + (separator_len if len(current_doc) > 0 else 0)\\n                > self._chunk_size\\n            ):\\n                if total > self._chunk_size:\\n                    logger.warning(\\n                        f\"Created a chunk of size {total}, \"\\n                        f\"which is longer than the specified {self._chunk_size}\"\\n                    )\\n                if len(current_doc) > 0:\\n                    doc = self._join_docs(current_doc, separator)\\n                    if doc is not None:\\n                        docs.append(doc)\\n                    # Keep on popping if:\\n                    # - we have a larger chunk than in the chunk overlap\\n                    # - or if we still have any chunks and the length is long\\n                    while total > self._chunk_overlap or (\\n                        total + _len + (separator_len if len(current_doc) > 0 else 0)\\n                        > self._chunk_size\\n                        and total > 0\\n                    ):\\n                        total -= self._length_function(current_doc[0]) + (\\n                            separator_len if len(current_doc) > 1 else 0\\n                        )\\n                        current_doc = current_doc[1:]\\n            current_doc.append(d)\\n            total += _len + (separator_len if len(current_doc) > 1 else 0)\\n        doc = self._join_docs(current_doc, separator)\\n        if doc is not None:\\n            docs.append(doc)\\n        return docs\\n\\n    @classmethod\\n    def from_huggingface_tokenizer(cls, tokenizer: Any, **kwargs: Any) -> TextSplitter:\\n        \"\"\"Text splitter that uses HuggingFace tokenizer to count length.\"\"\"\\n        try:\\n            from transformers import PreTrainedTokenizerBase',\n",
              "  '@abstractmethod\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split text into multiple components.\"\"\"\\n\\n    def create_documents(\\n        self, texts: List[str], metadatas: Optional[List[dict]] = None\\n    ) -> List[Document]:\\n        \"\"\"Create documents from a list of texts.\"\"\"\\n        _metadatas = metadatas or [{}] * len(texts)\\n        documents = []\\n        for i, text in enumerate(texts):\\n            index = -1\\n            for chunk in self.split_text(text):\\n                metadata = copy.deepcopy(_metadatas[i])\\n                if self._add_start_index:\\n                    index = text.find(chunk, index + 1)\\n                    metadata[\"start_index\"] = index\\n                new_doc = Document(page_content=chunk, metadata=metadata)\\n                documents.append(new_doc)\\n        return documents\\n\\n    def split_documents(self, documents: Iterable[Document]) -> List[Document]:\\n        \"\"\"Split documents.\"\"\"\\n        texts, metadatas = [], []\\n        for doc in documents:\\n            texts.append(doc.page_content)\\n            metadatas.append(doc.metadata)\\n        return self.create_documents(texts, metadatas=metadatas)\\n\\n    def _join_docs(self, docs: List[str], separator: str) -> Optional[str]:\\n        text = separator.join(docs)\\n        if self._strip_whitespace:\\n            text = text.strip()\\n        if text == \"\":\\n            return None\\n        else:\\n            return text\\n\\n    def _merge_splits(self, splits: Iterable[str], separator: str) -> List[str]:\\n        # We now want to combine these smaller pieces into medium size\\n        # chunks to send to the LLM.\\n        separator_len = self._length_function(separator)',\n",
              "  'class TextSplitter(BaseDocumentTransformer, ABC):\\n    \"\"\"Interface for splitting text into chunks.\"\"\"\\n\\n    def __init__(\\n        self,\\n        chunk_size: int = 4000,\\n        chunk_overlap: int = 200,\\n        length_function: Callable[[str], int] = len,\\n        keep_separator: bool = False,\\n        add_start_index: bool = False,\\n        strip_whitespace: bool = True,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\\n\\n        Args:\\n            chunk_size: Maximum size of chunks to return\\n            chunk_overlap: Overlap in characters between chunks\\n            length_function: Function that measures the length of given chunks\\n            keep_separator: Whether to keep the separator in the chunks\\n            add_start_index: If `True`, includes chunk\\'s start index in metadata\\n            strip_whitespace: If `True`, strips whitespace from the start and end of\\n                              every document\\n        \"\"\"\\n        if chunk_overlap > chunk_size:\\n            raise ValueError(\\n                f\"Got a larger chunk overlap ({chunk_overlap}) than chunk size \"\\n                f\"({chunk_size}), should be smaller.\"\\n            )\\n        self._chunk_size = chunk_size\\n        self._chunk_overlap = chunk_overlap\\n        self._length_function = length_function\\n        self._keep_separator = keep_separator\\n        self._add_start_index = add_start_index\\n        self._strip_whitespace = strip_whitespace\\n\\n    @abstractmethod\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split text into multiple components.\"\"\"',\n",
              "  'def _split_text_with_regex(\\n    text: str, separator: str, keep_separator: bool\\n) -> List[str]:\\n    # Now that we have the separator, split the text\\n    if separator:\\n        if keep_separator:\\n            # The parentheses in the pattern keep the delimiters in the result.\\n            _splits = re.split(f\"({separator})\", text)\\n            splits = [_splits[i] + _splits[i + 1] for i in range(1, len(_splits), 2)]\\n            if len(_splits) % 2 == 0:\\n                splits += _splits[-1:]\\n            splits = [_splits[0]] + splits\\n        else:\\n            splits = re.split(separator, text)\\n    else:\\n        splits = list(text)\\n    return [s for s in splits if s != \"\"]',\n",
              "  '\"\"\"**Text Splitters** are classes for splitting text.\\n\\n\\n**Class hierarchy:**\\n\\n.. code-block::\\n\\n    BaseDocumentTransformer --> TextSplitter --> <name>TextSplitter  # Example: CharacterTextSplitter\\n                                                 RecursiveCharacterTextSplitter -->  <name>TextSplitter\\n\\nNote: **MarkdownHeaderTextSplitter** and **HTMLHeaderTextSplitter do not derive from TextSplitter.\\n\\n\\n**Main helpers:**\\n\\n.. code-block::\\n\\n    Document, Tokenizer, Language, LineType, HeaderType\\n\\n\"\"\"  # noqa: E501\\n\\nfrom __future__ import annotations\\n\\nimport asyncio\\nimport copy\\nimport logging\\nimport pathlib\\nimport re\\nfrom abc import ABC, abstractmethod\\nfrom dataclasses import dataclass\\nfrom enum import Enum\\nfrom functools import partial\\nfrom io import BytesIO, StringIO\\nfrom typing import (\\n    AbstractSet,\\n    Any,\\n    Callable,\\n    Collection,\\n    Dict,\\n    Iterable,\\n    List,\\n    Literal,\\n    Optional,\\n    Sequence,\\n    Tuple,\\n    Type,\\n    TypedDict,\\n    TypeVar,\\n    Union,\\n    cast,\\n)\\n\\nimport requests\\nfrom langchain_core.documents import BaseDocumentTransformer, Document\\n\\nlogger = logging.getLogger(__name__)\\n\\nTS = TypeVar(\"TS\", bound=\"TextSplitter\")\\n\\n\\ndef _make_spacy_pipeline_for_splitting(\\n    pipeline: str, *, max_length: int = 1_000_000\\n) -> Any:  # avoid importing spacy\\n    try:\\n        import spacy\\n    except ImportError:\\n        raise ImportError(\\n            \"Spacy is not installed, please install it with `pip install spacy`.\"\\n        )\\n    if pipeline == \"sentencizer\":\\n        from spacy.lang.en import English\\n\\n        sentencizer = English()\\n        sentencizer.add_pipe(\"sentencizer\")\\n    else:\\n        sentencizer = spacy.load(pipeline, exclude=[\"ner\", \"tagger\"])\\n        sentencizer.max_length = max_length\\n    return sentencizer',\n",
              "  'class PythonCodeTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Python syntax.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a PythonCodeTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.PYTHON)\\n        super().__init__(separators=separators, **kwargs)\\n\\n\\nclass MarkdownTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Markdown-formatted headings.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a MarkdownTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.MARKDOWN)\\n        super().__init__(separators=separators, **kwargs)\\n\\n\\nclass LatexTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Latex-formatted layout elements.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a LatexTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.LATEX)\\n        super().__init__(separators=separators, **kwargs)',\n",
              "  '\"\"\"**Text Splitters** are classes for splitting text.\\n\\n\\n**Class hierarchy:**\\n\\n.. code-block::\\n\\n    BaseDocumentTransformer --> TextSplitter --> <name>TextSplitter  # Example: CharacterTextSplitter\\n                                                 RecursiveCharacterTextSplitter -->  <name>TextSplitter\\n\\nNote: **MarkdownHeaderTextSplitter** and **HTMLHeaderTextSplitter do not derive from TextSplitter.\\n\\n\\n**Main helpers:**\\n\\n.. code-block::\\n\\n    Document, Tokenizer, Language, LineType, HeaderType\\n\\n\"\"\"  # noqa: E501\\n\\nfrom __future__ import annotations\\n\\nimport asyncio\\nimport copy\\nimport logging\\nimport pathlib\\nimport re\\nfrom abc import ABC, abstractmethod\\nfrom dataclasses import dataclass\\nfrom enum import Enum\\nfrom functools import partial\\nfrom io import BytesIO, StringIO\\nfrom typing import (\\n    AbstractSet,\\n    Any,\\n    Callable,\\n    Collection,\\n    Dict,\\n    Iterable,\\n    List,\\n    Literal,\\n    Optional,\\n    Sequence,\\n    Tuple,\\n    Type,\\n    TypedDict,\\n    TypeVar,\\n    Union,\\n    cast,\\n)\\n\\nimport requests\\nfrom langchain_core.documents import BaseDocumentTransformer, Document\\n\\nlogger = logging.getLogger(__name__)\\n\\nTS = TypeVar(\"TS\", bound=\"TextSplitter\")\\n\\n\\ndef _make_spacy_pipeline_for_splitting(\\n    pipeline: str, *, max_length: int = 1_000_000\\n) -> Any:  # avoid importing spacy\\n    try:\\n        import spacy\\n    except ImportError:\\n        raise ImportError(\\n            \"Spacy is not installed, please install it with `pip install spacy`.\"\\n        )\\n    if pipeline == \"sentencizer\":\\n        from spacy.lang.en import English\\n\\n        sentencizer = English()\\n        sentencizer.add_pipe(\"sentencizer\")\\n    else:\\n        sentencizer = spacy.load(pipeline, exclude=[\"ner\", \"tagger\"])\\n        sentencizer.max_length = max_length\\n    return sentencizer',\n",
              "  'else:\\n            raise ValueError(\\n                f\"Language {language} is not supported! \"\\n                f\"Please choose from {list(Language)}\"\\n            )',\n",
              "  '\"\\\\nevent \",\\n                \"\\\\nmodifier \",\\n                \"\\\\nerror \",\\n                \"\\\\nstruct \",\\n                \"\\\\nenum \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo while \",\\n                \"\\\\nassembly \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.COBOL:\\n            return [\\n                # Split along divisions\\n                \"\\\\nIDENTIFICATION DIVISION.\",\\n                \"\\\\nENVIRONMENT DIVISION.\",\\n                \"\\\\nDATA DIVISION.\",\\n                \"\\\\nPROCEDURE DIVISION.\",\\n                # Split along sections within DATA DIVISION\\n                \"\\\\nWORKING-STORAGE SECTION.\",\\n                \"\\\\nLINKAGE SECTION.\",\\n                \"\\\\nFILE SECTION.\",\\n                # Split along sections within PROCEDURE DIVISION\\n                \"\\\\nINPUT-OUTPUT SECTION.\",\\n                # Split along paragraphs and common statements\\n                \"\\\\nOPEN \",\\n                \"\\\\nCLOSE \",\\n                \"\\\\nREAD \",\\n                \"\\\\nWRITE \",\\n                \"\\\\nIF \",\\n                \"\\\\nELSE \",\\n                \"\\\\nMOVE \",\\n                \"\\\\nPERFORM \",\\n                \"\\\\nUNTIL \",\\n                \"\\\\nVARYING \",\\n                \"\\\\nACCEPT \",\\n                \"\\\\nDISPLAY \",\\n                \"\\\\nSTOP RUN.\",\\n                # Split by the normal type of lines\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]',\n",
              "  '\"<nav\",\\n                # Head\\n                \"<head\",\\n                \"<style\",\\n                \"<script\",\\n                \"<meta\",\\n                \"<title\",\\n                \"\",\\n            ]\\n        elif language == Language.CSHARP:\\n            return [\\n                \"\\\\ninterface \",\\n                \"\\\\nenum \",\\n                \"\\\\nimplements \",\\n                \"\\\\ndelegate \",\\n                \"\\\\nevent \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nabstract \",\\n                # Split along method definitions\\n                \"\\\\npublic \",\\n                \"\\\\nprotected \",\\n                \"\\\\nprivate \",\\n                \"\\\\nstatic \",\\n                \"\\\\nreturn \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\ncontinue \",\\n                \"\\\\nfor \",\\n                \"\\\\nforeach \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\nbreak \",\\n                \"\\\\ncase \",\\n                \"\\\\nelse \",\\n                # Split by exceptions\\n                \"\\\\ntry \",\\n                \"\\\\nthrow \",\\n                \"\\\\nfinally \",\\n                \"\\\\ncatch \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SOL:\\n            return [\\n                # Split along compiler information definitions\\n                \"\\\\npragma \",\\n                \"\\\\nusing \",\\n                # Split along contract definitions\\n                \"\\\\ncontract \",\\n                \"\\\\ninterface \",\\n                \"\\\\nlibrary \",\\n                # Split along method definitions\\n                \"\\\\nconstructor \",\\n                \"\\\\ntype \",\\n                \"\\\\nfunction \",\\n                \"\\\\nevent \",\\n                \"\\\\nmodifier \",\\n                \"\\\\nerror \",\\n                \"\\\\nstruct \",\\n                \"\\\\nenum \",\\n                # Split along control flow statements',\n",
              "  '\"\\\\n\\\\\\\\*\\\\\\\\*\\\\\\\\*+\\\\n\",\\n                \"\\\\n---+\\\\n\",\\n                \"\\\\n___+\\\\n\",\\n                # Note that this splitter doesn\\'t handle horizontal lines defined\\n                # by *three or more* of ***, ---, or ___, but this is not handled\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.LATEX:\\n            return [\\n                # First, try to split along Latex sections\\n                \"\\\\n\\\\\\\\\\\\\\\\chapter{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\section{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\subsection{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\subsubsection{\",\\n                # Now split by environments\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{enumerate}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{itemize}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{description}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{list}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{quote}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{quotation}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{verse}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{verbatim}\",\\n                # Now split by math environments\\n                \"\\\\n\\\\\\\\\\\\begin{align}\",\\n                \"$$\",\\n                \"$\",\\n                # Now split by the normal type of lines\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.HTML:\\n            return [\\n                # First, try to split along HTML tags\\n                \"<body\",\\n                \"<div\",\\n                \"<p\",\\n                \"<br\",\\n                \"<li\",\\n                \"<h1\",\\n                \"<h2\",\\n                \"<h3\",\\n                \"<h4\",\\n                \"<h5\",\\n                \"<h6\",\\n                \"<span\",\\n                \"<table\",\\n                \"<tr\",\\n                \"<td\",\\n                \"<th\",\\n                \"<ul\",\\n                \"<ol\",\\n                \"<header\",\\n                \"<footer\",\\n                \"<nav\",\\n                # Head\\n                \"<head\",\\n                \"<style\",\\n                \"<script\",\\n                \"<meta\",\\n                \"<title\",',\n",
              "  '\" \",\\n                \"\",\\n            ]\\n        elif language == Language.SCALA:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nobject \",\\n                # Split along method definitions\\n                \"\\\\ndef \",\\n                \"\\\\nval \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nmatch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SWIFT:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunc \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nstruct \",\\n                \"\\\\nenum \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.MARKDOWN:\\n            return [\\n                # First, try to split along Markdown headings (starting with level 2)\\n                \"\\\\n#{1,6} \",\\n                # Note the alternative syntax for headings (below) is not handled here\\n                # Heading level 2\\n                # ---------------\\n                # End of code block\\n                \"```\\\\n\",\\n                # Horizontal lines\\n                \"\\\\n\\\\\\\\*\\\\\\\\*\\\\\\\\*+\\\\n\",\\n                \"\\\\n---+\\\\n\",\\n                \"\\\\n___+\\\\n\",\\n                # Note that this splitter doesn\\'t handle horizontal lines defined',\n",
              "  'return [\\n                # First, try to split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\ndef \",\\n                \"\\\\n\\\\tdef \",\\n                # Now split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.RST:\\n            return [\\n                # Split along section titles\\n                \"\\\\n=+\\\\n\",\\n                \"\\\\n-+\\\\n\",\\n                \"\\\\n\\\\\\\\*+\\\\n\",\\n                # Split along directive markers\\n                \"\\\\n\\\\n.. *\\\\n\\\\n\",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.RUBY:\\n            return [\\n                # Split along method definitions\\n                \"\\\\ndef \",\\n                \"\\\\nclass \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nunless \",\\n                \"\\\\nwhile \",\\n                \"\\\\nfor \",\\n                \"\\\\ndo \",\\n                \"\\\\nbegin \",\\n                \"\\\\nrescue \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.RUST:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfn \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nwhile \",\\n                \"\\\\nfor \",\\n                \"\\\\nloop \",\\n                \"\\\\nmatch \",\\n                \"\\\\nconst \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SCALA:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",',\n",
              "  '\"\\\\nfunction \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                \"\\\\ndefault \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.PHP:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunction \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nforeach \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.PROTO:\\n            return [\\n                # Split along message definitions\\n                \"\\\\nmessage \",\\n                # Split along service definitions\\n                \"\\\\nservice \",\\n                # Split along enum definitions\\n                \"\\\\nenum \",\\n                # Split along option definitions\\n                \"\\\\noption \",\\n                # Split along import statements\\n                \"\\\\nimport \",\\n                # Split along syntax declarations\\n                \"\\\\nsyntax \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.PYTHON:\\n            return [\\n                # First, try to split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\ndef \",\\n                \"\\\\n\\\\tdef \",',\n",
              "  '\"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.KOTLIN:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along method definitions\\n                \"\\\\npublic \",\\n                \"\\\\nprotected \",\\n                \"\\\\nprivate \",\\n                \"\\\\ninternal \",\\n                \"\\\\ncompanion \",\\n                \"\\\\nfun \",\\n                \"\\\\nval \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nwhen \",\\n                \"\\\\ncase \",\\n                \"\\\\nelse \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.JS:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunction \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                \"\\\\nvar \",\\n                \"\\\\nclass \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                \"\\\\ndefault \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.TS:\\n            return [\\n                \"\\\\nenum \",\\n                \"\\\\ninterface \",\\n                \"\\\\nnamespace \",\\n                \"\\\\ntype \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along function definitions\\n                \"\\\\nfunction \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements',\n",
              "  'def _split_text_with_regex(\\n    text: str, separator: str, keep_separator: bool\\n) -> List[str]:\\n    # Now that we have the separator, split the text\\n    if separator:\\n        if keep_separator:\\n            # The parentheses in the pattern keep the delimiters in the result.\\n            _splits = re.split(f\"({separator})\", text)\\n            splits = [_splits[i] + _splits[i + 1] for i in range(1, len(_splits), 2)]\\n            if len(_splits) % 2 == 0:\\n                splits += _splits[-1:]\\n            splits = [_splits[0]] + splits\\n        else:\\n            splits = re.split(separator, text)\\n    else:\\n        splits = list(text)\\n    return [s for s in splits if s != \"\"]',\n",
              "  'if in_code_block:\\n                current_content.append(stripped_line)\\n                continue\\n\\n            # Check each line against each of the header types (e.g., #, ##)\\n            for sep, name in self.headers_to_split_on:\\n                # Check if line starts with a header that we intend to split on\\n                if stripped_line.startswith(sep) and (\\n                    # Header with no text OR header is followed by space\\n                    # Both are valid conditions that sep is being used a header\\n                    len(stripped_line) == len(sep) or stripped_line[len(sep)] == \" \"\\n                ):\\n                    # Ensure we are tracking the header as metadata\\n                    if name is not None:\\n                        # Get the current header level\\n                        current_header_level = sep.count(\"#\")\\n\\n                        # Pop out headers of lower or same level from the stack\\n                        while (\\n                            header_stack\\n                            and header_stack[-1][\"level\"] >= current_header_level\\n                        ):\\n                            # We have encountered a new header\\n                            # at the same or higher level\\n                            popped_header = header_stack.pop()\\n                            # Clear the metadata for the\\n                            # popped header in initial_metadata\\n                            if popped_header[\"name\"] in initial_metadata:\\n                                initial_metadata.pop(popped_header[\"name\"])',\n",
              "  'def split_text(self, text: str) -> List[Document]:\\n        \"\"\"Split markdown file\\n        Args:\\n            text: Markdown file\"\"\"\\n\\n        # Split the input text by newline character (\"\\\\n\").\\n        lines = text.split(\"\\\\n\")\\n        # Final output\\n        lines_with_metadata: List[LineType] = []\\n        # Content and metadata of the chunk currently being processed\\n        current_content: List[str] = []\\n        current_metadata: Dict[str, str] = {}\\n        # Keep track of the nested header structure\\n        # header_stack: List[Dict[str, Union[int, str]]] = []\\n        header_stack: List[HeaderType] = []\\n        initial_metadata: Dict[str, str] = {}\\n\\n        in_code_block = False\\n        opening_fence = \"\"\\n\\n        for line in lines:\\n            stripped_line = line.strip()\\n\\n            if not in_code_block:\\n                # Exclude inline code spans\\n                if stripped_line.startswith(\"```\") and stripped_line.count(\"```\") == 1:\\n                    in_code_block = True\\n                    opening_fence = \"```\"\\n                elif stripped_line.startswith(\"~~~\"):\\n                    in_code_block = True\\n                    opening_fence = \"~~~\"\\n            else:\\n                if stripped_line.startswith(opening_fence):\\n                    in_code_block = False\\n                    opening_fence = \"\"\\n\\n            if in_code_block:\\n                current_content.append(stripped_line)\\n                continue',\n",
              "  'class MarkdownHeaderTextSplitter:\\n    \"\"\"Splitting markdown files based on specified headers.\"\"\"\\n\\n    def __init__(\\n        self, headers_to_split_on: List[Tuple[str, str]], return_each_line: bool = False\\n    ):\\n        \"\"\"Create a new MarkdownHeaderTextSplitter.\\n\\n        Args:\\n            headers_to_split_on: Headers we want to track\\n            return_each_line: Return each line w/ associated headers\\n        \"\"\"\\n        # Output line-by-line or aggregated into chunks w/ common headers\\n        self.return_each_line = return_each_line\\n        # Given the headers we want to split on,\\n        # (e.g., \"#, ##, etc\") order by length\\n        self.headers_to_split_on = sorted(\\n            headers_to_split_on, key=lambda split: len(split[0]), reverse=True\\n        )\\n\\n    def aggregate_lines_to_chunks(self, lines: List[LineType]) -> List[Document]:\\n        \"\"\"Combine lines with common metadata into chunks\\n        Args:\\n            lines: Line of text / associated header metadata\\n        \"\"\"\\n        aggregated_chunks: List[LineType] = []\\n\\n        for line in lines:\\n            if (\\n                aggregated_chunks\\n                and aggregated_chunks[-1][\"metadata\"] == line[\"metadata\"]\\n            ):\\n                # If the last line in the aggregated list\\n                # has the same metadata as the current line,\\n                # append the current content to the last lines\\'s content\\n                aggregated_chunks[-1][\"content\"] += \"  \\\\n\" + line[\"content\"]\\n            else:\\n                # Otherwise, append the current line to the aggregated list\\n                aggregated_chunks.append(line)\\n\\n        return [\\n            Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n            for chunk in aggregated_chunks\\n        ]\\n\\n    def split_text(self, text: str) -> List[Document]:\\n        \"\"\"Split markdown file\\n        Args:\\n            text: Markdown file\"\"\"',\n",
              "  'class CharacterTextSplitter(TextSplitter):\\n    \"\"\"Splitting text that looks at characters.\"\"\"\\n\\n    def __init__(\\n        self, separator: str = \"\\\\n\\\\n\", is_separator_regex: bool = False, **kwargs: Any\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(**kwargs)\\n        self._separator = separator\\n        self._is_separator_regex = is_separator_regex\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        # First we naively split the large input into a bunch of smaller ones.\\n        separator = (\\n            self._separator if self._is_separator_regex else re.escape(self._separator)\\n        )\\n        splits = _split_text_with_regex(text, separator, self._keep_separator)\\n        _separator = \"\" if self._keep_separator else self._separator\\n        return self._merge_splits(splits, _separator)\\n\\n\\nclass LineType(TypedDict):\\n    \"\"\"Line type as typed dict.\"\"\"\\n\\n    metadata: Dict[str, str]\\n    content: str\\n\\n\\nclass HeaderType(TypedDict):\\n    \"\"\"Header type as typed dict.\"\"\"\\n\\n    level: int\\n    name: str\\n    data: str',\n",
              "  'if issubclass(cls, TokenTextSplitter):\\n            extra_kwargs = {\\n                \"encoding_name\": encoding_name,\\n                \"model_name\": model_name,\\n                \"allowed_special\": allowed_special,\\n                \"disallowed_special\": disallowed_special,\\n            }\\n            kwargs = {**kwargs, **extra_kwargs}\\n\\n        return cls(length_function=_tiktoken_encoder, **kwargs)\\n\\n    def transform_documents(\\n        self, documents: Sequence[Document], **kwargs: Any\\n    ) -> Sequence[Document]:\\n        \"\"\"Transform sequence of documents by splitting them.\"\"\"\\n        return self.split_documents(list(documents))\\n\\n    async def atransform_documents(\\n        self, documents: Sequence[Document], **kwargs: Any\\n    ) -> Sequence[Document]:\\n        \"\"\"Asynchronously transform a sequence of documents by splitting them.\"\"\"\\n        return await asyncio.get_running_loop().run_in_executor(\\n            None, partial(self.transform_documents, **kwargs), documents\\n        )',\n",
              "  'if not isinstance(tokenizer, PreTrainedTokenizerBase):\\n                raise ValueError(\\n                    \"Tokenizer received was not an instance of PreTrainedTokenizerBase\"\\n                )\\n\\n            def _huggingface_tokenizer_length(text: str) -> int:\\n                return len(tokenizer.encode(text))\\n\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import transformers python package. \"\\n                \"Please install it with `pip install transformers`.\"\\n            )\\n        return cls(length_function=_huggingface_tokenizer_length, **kwargs)\\n\\n    @classmethod\\n    def from_tiktoken_encoder(\\n        cls: Type[TS],\\n        encoding_name: str = \"gpt2\",\\n        model_name: Optional[str] = None,\\n        allowed_special: Union[Literal[\"all\"], AbstractSet[str]] = set(),\\n        disallowed_special: Union[Literal[\"all\"], Collection[str]] = \"all\",\\n        **kwargs: Any,\\n    ) -> TS:\\n        \"\"\"Text splitter that uses tiktoken encoder to count length.\"\"\"\\n        try:\\n            import tiktoken\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import tiktoken python package. \"\\n                \"This is needed in order to calculate max_tokens_for_prompt. \"\\n                \"Please install it with `pip install tiktoken`.\"\\n            )\\n\\n        if model_name is not None:\\n            enc = tiktoken.encoding_for_model(model_name)\\n        else:\\n            enc = tiktoken.get_encoding(encoding_name)\\n\\n        def _tiktoken_encoder(text: str) -> int:\\n            return len(\\n                enc.encode(\\n                    text,\\n                    allowed_special=allowed_special,\\n                    disallowed_special=disallowed_special,\\n                )\\n            )',\n",
              "  'docs = []\\n        current_doc: List[str] = []\\n        total = 0\\n        for d in splits:\\n            _len = self._length_function(d)\\n            if (\\n                total + _len + (separator_len if len(current_doc) > 0 else 0)\\n                > self._chunk_size\\n            ):\\n                if total > self._chunk_size:\\n                    logger.warning(\\n                        f\"Created a chunk of size {total}, \"\\n                        f\"which is longer than the specified {self._chunk_size}\"\\n                    )\\n                if len(current_doc) > 0:\\n                    doc = self._join_docs(current_doc, separator)\\n                    if doc is not None:\\n                        docs.append(doc)\\n                    # Keep on popping if:\\n                    # - we have a larger chunk than in the chunk overlap\\n                    # - or if we still have any chunks and the length is long\\n                    while total > self._chunk_overlap or (\\n                        total + _len + (separator_len if len(current_doc) > 0 else 0)\\n                        > self._chunk_size\\n                        and total > 0\\n                    ):\\n                        total -= self._length_function(current_doc[0]) + (\\n                            separator_len if len(current_doc) > 1 else 0\\n                        )\\n                        current_doc = current_doc[1:]\\n            current_doc.append(d)\\n            total += _len + (separator_len if len(current_doc) > 1 else 0)\\n        doc = self._join_docs(current_doc, separator)\\n        if doc is not None:\\n            docs.append(doc)\\n        return docs\\n\\n    @classmethod\\n    def from_huggingface_tokenizer(cls, tokenizer: Any, **kwargs: Any) -> TextSplitter:\\n        \"\"\"Text splitter that uses HuggingFace tokenizer to count length.\"\"\"\\n        try:\\n            from transformers import PreTrainedTokenizerBase',\n",
              "  '@abstractmethod\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split text into multiple components.\"\"\"\\n\\n    def create_documents(\\n        self, texts: List[str], metadatas: Optional[List[dict]] = None\\n    ) -> List[Document]:\\n        \"\"\"Create documents from a list of texts.\"\"\"\\n        _metadatas = metadatas or [{}] * len(texts)\\n        documents = []\\n        for i, text in enumerate(texts):\\n            index = -1\\n            for chunk in self.split_text(text):\\n                metadata = copy.deepcopy(_metadatas[i])\\n                if self._add_start_index:\\n                    index = text.find(chunk, index + 1)\\n                    metadata[\"start_index\"] = index\\n                new_doc = Document(page_content=chunk, metadata=metadata)\\n                documents.append(new_doc)\\n        return documents\\n\\n    def split_documents(self, documents: Iterable[Document]) -> List[Document]:\\n        \"\"\"Split documents.\"\"\"\\n        texts, metadatas = [], []\\n        for doc in documents:\\n            texts.append(doc.page_content)\\n            metadatas.append(doc.metadata)\\n        return self.create_documents(texts, metadatas=metadatas)\\n\\n    def _join_docs(self, docs: List[str], separator: str) -> Optional[str]:\\n        text = separator.join(docs)\\n        if self._strip_whitespace:\\n            text = text.strip()\\n        if text == \"\":\\n            return None\\n        else:\\n            return text\\n\\n    def _merge_splits(self, splits: Iterable[str], separator: str) -> List[str]:\\n        # We now want to combine these smaller pieces into medium size\\n        # chunks to send to the LLM.\\n        separator_len = self._length_function(separator)',\n",
              "  'class TextSplitter(BaseDocumentTransformer, ABC):\\n    \"\"\"Interface for splitting text into chunks.\"\"\"\\n\\n    def __init__(\\n        self,\\n        chunk_size: int = 4000,\\n        chunk_overlap: int = 200,\\n        length_function: Callable[[str], int] = len,\\n        keep_separator: bool = False,\\n        add_start_index: bool = False,\\n        strip_whitespace: bool = True,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\\n\\n        Args:\\n            chunk_size: Maximum size of chunks to return\\n            chunk_overlap: Overlap in characters between chunks\\n            length_function: Function that measures the length of given chunks\\n            keep_separator: Whether to keep the separator in the chunks\\n            add_start_index: If `True`, includes chunk\\'s start index in metadata\\n            strip_whitespace: If `True`, strips whitespace from the start and end of\\n                              every document\\n        \"\"\"\\n        if chunk_overlap > chunk_size:\\n            raise ValueError(\\n                f\"Got a larger chunk overlap ({chunk_overlap}) than chunk size \"\\n                f\"({chunk_size}), should be smaller.\"\\n            )\\n        self._chunk_size = chunk_size\\n        self._chunk_overlap = chunk_overlap\\n        self._length_function = length_function\\n        self._keep_separator = keep_separator\\n        self._add_start_index = add_start_index\\n        self._strip_whitespace = strip_whitespace\\n\\n    @abstractmethod\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split text into multiple components.\"\"\"',\n",
              "  '# Push the current header to the stack\\n                        header: HeaderType = {\\n                            \"level\": current_header_level,\\n                            \"name\": name,\\n                            \"data\": stripped_line[len(sep) :].strip(),\\n                        }\\n                        header_stack.append(header)\\n                        # Update initial_metadata with the current header\\n                        initial_metadata[name] = header[\"data\"]\\n\\n                    # Add the previous line to the lines_with_metadata\\n                    # only if current_content is not empty\\n                    if current_content:\\n                        lines_with_metadata.append(\\n                            {\\n                                \"content\": \"\\\\n\".join(current_content),\\n                                \"metadata\": current_metadata.copy(),\\n                            }\\n                        )\\n                        current_content.clear()\\n\\n                    break\\n            else:\\n                if stripped_line:\\n                    current_content.append(stripped_line)\\n                elif current_content:\\n                    lines_with_metadata.append(\\n                        {\\n                            \"content\": \"\\\\n\".join(current_content),\\n                            \"metadata\": current_metadata.copy(),\\n                        }\\n                    )\\n                    current_content.clear()\\n\\n            current_metadata = initial_metadata.copy()\\n\\n        if current_content:\\n            lines_with_metadata.append(\\n                {\"content\": \"\\\\n\".join(current_content), \"metadata\": current_metadata}\\n            )',\n",
              "  '\"\"\"**Text Splitters** are classes for splitting text.\\n\\n\\n**Class hierarchy:**\\n\\n.. code-block::\\n\\n    BaseDocumentTransformer --> TextSplitter --> <name>TextSplitter  # Example: CharacterTextSplitter\\n                                                 RecursiveCharacterTextSplitter -->  <name>TextSplitter\\n\\nNote: **MarkdownHeaderTextSplitter** and **HTMLHeaderTextSplitter do not derive from TextSplitter.\\n\\n\\n**Main helpers:**\\n\\n.. code-block::\\n\\n    Document, Tokenizer, Language, LineType, HeaderType\\n\\n\"\"\"  # noqa: E501\\n\\nfrom __future__ import annotations\\n\\nimport asyncio\\nimport copy\\nimport logging\\nimport pathlib\\nimport re\\nfrom abc import ABC, abstractmethod\\nfrom dataclasses import dataclass\\nfrom enum import Enum\\nfrom functools import partial\\nfrom io import BytesIO, StringIO\\nfrom typing import (\\n    AbstractSet,\\n    Any,\\n    Callable,\\n    Collection,\\n    Dict,\\n    Iterable,\\n    List,\\n    Literal,\\n    Optional,\\n    Sequence,\\n    Tuple,\\n    Type,\\n    TypedDict,\\n    TypeVar,\\n    Union,\\n    cast,\\n)\\n\\nimport requests\\nfrom langchain_core.documents import BaseDocumentTransformer, Document\\n\\nlogger = logging.getLogger(__name__)\\n\\nTS = TypeVar(\"TS\", bound=\"TextSplitter\")\\n\\n\\ndef _make_spacy_pipeline_for_splitting(\\n    pipeline: str, *, max_length: int = 1_000_000\\n) -> Any:  # avoid importing spacy\\n    try:\\n        import spacy\\n    except ImportError:\\n        raise ImportError(\\n            \"Spacy is not installed, please install it with `pip install spacy`.\"\\n        )\\n    if pipeline == \"sentencizer\":\\n        from spacy.lang.en import English\\n\\n        sentencizer = English()\\n        sentencizer.add_pipe(\"sentencizer\")\\n    else:\\n        sentencizer = spacy.load(pipeline, exclude=[\"ner\", \"tagger\"])\\n        sentencizer.max_length = max_length\\n    return sentencizer',\n",
              "  'class PythonCodeTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Python syntax.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a PythonCodeTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.PYTHON)\\n        super().__init__(separators=separators, **kwargs)\\n\\n\\nclass MarkdownTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Markdown-formatted headings.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a MarkdownTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.MARKDOWN)\\n        super().__init__(separators=separators, **kwargs)\\n\\n\\nclass LatexTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Latex-formatted layout elements.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a LatexTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.LATEX)\\n        super().__init__(separators=separators, **kwargs)',\n",
              "  'class NLTKTextSplitter(TextSplitter):\\n    \"\"\"Splitting text using NLTK package.\"\"\"\\n\\n    def __init__(\\n        self, separator: str = \"\\\\n\\\\n\", language: str = \"english\", **kwargs: Any\\n    ) -> None:\\n        \"\"\"Initialize the NLTK splitter.\"\"\"\\n        super().__init__(**kwargs)\\n        try:\\n            from nltk.tokenize import sent_tokenize\\n\\n            self._tokenizer = sent_tokenize\\n        except ImportError:\\n            raise ImportError(\\n                \"NLTK is not installed, please install it with `pip install nltk`.\"\\n            )\\n        self._separator = separator\\n        self._language = language\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        # First we naively split the large input into a bunch of smaller ones.\\n        splits = self._tokenizer(text, language=self._language)\\n        return self._merge_splits(splits, self._separator)\\n\\n\\nclass SpacyTextSplitter(TextSplitter):\\n    \"\"\"Splitting text using Spacy package.\\n\\n\\n    Per default, Spacy\\'s `en_core_web_sm` model is used and\\n    its default max_length is 1000000 (it is the length of maximum character\\n    this model takes which can be increased for large files). For a faster, but\\n    potentially less accurate splitting, you can use `pipeline=\\'sentencizer\\'`.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        separator: str = \"\\\\n\\\\n\",\\n        pipeline: str = \"en_core_web_sm\",\\n        max_length: int = 1_000_000,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Initialize the spacy text splitter.\"\"\"\\n        super().__init__(**kwargs)\\n        self._tokenizer = _make_spacy_pipeline_for_splitting(\\n            pipeline, max_length=max_length\\n        )\\n        self._separator = separator\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        splits = (s.text for s in self._tokenizer(text).sents)\\n        return self._merge_splits(splits, self._separator)\\n\\n\\n# For backwards compatibility',\n",
              "  'else:\\n            raise ValueError(\\n                f\"Language {language} is not supported! \"\\n                f\"Please choose from {list(Language)}\"\\n            )',\n",
              "  '\"\\\\nevent \",\\n                \"\\\\nmodifier \",\\n                \"\\\\nerror \",\\n                \"\\\\nstruct \",\\n                \"\\\\nenum \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo while \",\\n                \"\\\\nassembly \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.COBOL:\\n            return [\\n                # Split along divisions\\n                \"\\\\nIDENTIFICATION DIVISION.\",\\n                \"\\\\nENVIRONMENT DIVISION.\",\\n                \"\\\\nDATA DIVISION.\",\\n                \"\\\\nPROCEDURE DIVISION.\",\\n                # Split along sections within DATA DIVISION\\n                \"\\\\nWORKING-STORAGE SECTION.\",\\n                \"\\\\nLINKAGE SECTION.\",\\n                \"\\\\nFILE SECTION.\",\\n                # Split along sections within PROCEDURE DIVISION\\n                \"\\\\nINPUT-OUTPUT SECTION.\",\\n                # Split along paragraphs and common statements\\n                \"\\\\nOPEN \",\\n                \"\\\\nCLOSE \",\\n                \"\\\\nREAD \",\\n                \"\\\\nWRITE \",\\n                \"\\\\nIF \",\\n                \"\\\\nELSE \",\\n                \"\\\\nMOVE \",\\n                \"\\\\nPERFORM \",\\n                \"\\\\nUNTIL \",\\n                \"\\\\nVARYING \",\\n                \"\\\\nACCEPT \",\\n                \"\\\\nDISPLAY \",\\n                \"\\\\nSTOP RUN.\",\\n                # Split by the normal type of lines\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]',\n",
              "  '\"<nav\",\\n                # Head\\n                \"<head\",\\n                \"<style\",\\n                \"<script\",\\n                \"<meta\",\\n                \"<title\",\\n                \"\",\\n            ]\\n        elif language == Language.CSHARP:\\n            return [\\n                \"\\\\ninterface \",\\n                \"\\\\nenum \",\\n                \"\\\\nimplements \",\\n                \"\\\\ndelegate \",\\n                \"\\\\nevent \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nabstract \",\\n                # Split along method definitions\\n                \"\\\\npublic \",\\n                \"\\\\nprotected \",\\n                \"\\\\nprivate \",\\n                \"\\\\nstatic \",\\n                \"\\\\nreturn \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\ncontinue \",\\n                \"\\\\nfor \",\\n                \"\\\\nforeach \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\nbreak \",\\n                \"\\\\ncase \",\\n                \"\\\\nelse \",\\n                # Split by exceptions\\n                \"\\\\ntry \",\\n                \"\\\\nthrow \",\\n                \"\\\\nfinally \",\\n                \"\\\\ncatch \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SOL:\\n            return [\\n                # Split along compiler information definitions\\n                \"\\\\npragma \",\\n                \"\\\\nusing \",\\n                # Split along contract definitions\\n                \"\\\\ncontract \",\\n                \"\\\\ninterface \",\\n                \"\\\\nlibrary \",\\n                # Split along method definitions\\n                \"\\\\nconstructor \",\\n                \"\\\\ntype \",\\n                \"\\\\nfunction \",\\n                \"\\\\nevent \",\\n                \"\\\\nmodifier \",\\n                \"\\\\nerror \",\\n                \"\\\\nstruct \",\\n                \"\\\\nenum \",\\n                # Split along control flow statements',\n",
              "  '\"\\\\n\\\\\\\\*\\\\\\\\*\\\\\\\\*+\\\\n\",\\n                \"\\\\n---+\\\\n\",\\n                \"\\\\n___+\\\\n\",\\n                # Note that this splitter doesn\\'t handle horizontal lines defined\\n                # by *three or more* of ***, ---, or ___, but this is not handled\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.LATEX:\\n            return [\\n                # First, try to split along Latex sections\\n                \"\\\\n\\\\\\\\\\\\\\\\chapter{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\section{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\subsection{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\subsubsection{\",\\n                # Now split by environments\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{enumerate}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{itemize}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{description}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{list}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{quote}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{quotation}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{verse}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{verbatim}\",\\n                # Now split by math environments\\n                \"\\\\n\\\\\\\\\\\\begin{align}\",\\n                \"$$\",\\n                \"$\",\\n                # Now split by the normal type of lines\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.HTML:\\n            return [\\n                # First, try to split along HTML tags\\n                \"<body\",\\n                \"<div\",\\n                \"<p\",\\n                \"<br\",\\n                \"<li\",\\n                \"<h1\",\\n                \"<h2\",\\n                \"<h3\",\\n                \"<h4\",\\n                \"<h5\",\\n                \"<h6\",\\n                \"<span\",\\n                \"<table\",\\n                \"<tr\",\\n                \"<td\",\\n                \"<th\",\\n                \"<ul\",\\n                \"<ol\",\\n                \"<header\",\\n                \"<footer\",\\n                \"<nav\",\\n                # Head\\n                \"<head\",\\n                \"<style\",\\n                \"<script\",\\n                \"<meta\",\\n                \"<title\",',\n",
              "  '\" \",\\n                \"\",\\n            ]\\n        elif language == Language.SCALA:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nobject \",\\n                # Split along method definitions\\n                \"\\\\ndef \",\\n                \"\\\\nval \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nmatch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SWIFT:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunc \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nstruct \",\\n                \"\\\\nenum \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.MARKDOWN:\\n            return [\\n                # First, try to split along Markdown headings (starting with level 2)\\n                \"\\\\n#{1,6} \",\\n                # Note the alternative syntax for headings (below) is not handled here\\n                # Heading level 2\\n                # ---------------\\n                # End of code block\\n                \"```\\\\n\",\\n                # Horizontal lines\\n                \"\\\\n\\\\\\\\*\\\\\\\\*\\\\\\\\*+\\\\n\",\\n                \"\\\\n---+\\\\n\",\\n                \"\\\\n___+\\\\n\",\\n                # Note that this splitter doesn\\'t handle horizontal lines defined',\n",
              "  'def split_text(self, text: str) -> List[str]:\\n        def encode_strip_start_and_stop_token_ids(text: str) -> List[int]:\\n            return self._encode(text)[1:-1]\\n\\n        tokenizer = Tokenizer(\\n            chunk_overlap=self._chunk_overlap,\\n            tokens_per_chunk=self.tokens_per_chunk,\\n            decode=self.tokenizer.decode,\\n            encode=encode_strip_start_and_stop_token_ids,\\n        )\\n\\n        return split_text_on_tokens(text=text, tokenizer=tokenizer)\\n\\n    def count_tokens(self, *, text: str) -> int:\\n        return len(self._encode(text))\\n\\n    _max_length_equal_32_bit_integer: int = 2**32\\n\\n    def _encode(self, text: str) -> List[int]:\\n        token_ids_with_start_and_end_token_ids = self.tokenizer.encode(\\n            text,\\n            max_length=self._max_length_equal_32_bit_integer,\\n            truncation=\"do_not_truncate\",\\n        )\\n        return token_ids_with_start_and_end_token_ids',\n",
              "  '\"\\\\n\\\\\\\\*\\\\\\\\*\\\\\\\\*+\\\\n\",\\n                \"\\\\n---+\\\\n\",\\n                \"\\\\n___+\\\\n\",\\n                # Note that this splitter doesn\\'t handle horizontal lines defined\\n                # by *three or more* of ***, ---, or ___, but this is not handled\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.LATEX:\\n            return [\\n                # First, try to split along Latex sections\\n                \"\\\\n\\\\\\\\\\\\\\\\chapter{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\section{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\subsection{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\subsubsection{\",\\n                # Now split by environments\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{enumerate}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{itemize}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{description}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{list}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{quote}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{quotation}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{verse}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{verbatim}\",\\n                # Now split by math environments\\n                \"\\\\n\\\\\\\\\\\\begin{align}\",\\n                \"$$\",\\n                \"$\",\\n                # Now split by the normal type of lines\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.HTML:\\n            return [\\n                # First, try to split along HTML tags\\n                \"<body\",\\n                \"<div\",\\n                \"<p\",\\n                \"<br\",\\n                \"<li\",\\n                \"<h1\",\\n                \"<h2\",\\n                \"<h3\",\\n                \"<h4\",\\n                \"<h5\",\\n                \"<h6\",\\n                \"<span\",\\n                \"<table\",\\n                \"<tr\",\\n                \"<td\",\\n                \"<th\",\\n                \"<ul\",\\n                \"<ol\",\\n                \"<header\",\\n                \"<footer\",\\n                \"<nav\",\\n                # Head\\n                \"<head\",\\n                \"<style\",\\n                \"<script\",\\n                \"<meta\",\\n                \"<title\",',\n",
              "  '\" \",\\n                \"\",\\n            ]\\n        elif language == Language.SCALA:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nobject \",\\n                # Split along method definitions\\n                \"\\\\ndef \",\\n                \"\\\\nval \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nmatch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SWIFT:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunc \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nstruct \",\\n                \"\\\\nenum \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.MARKDOWN:\\n            return [\\n                # First, try to split along Markdown headings (starting with level 2)\\n                \"\\\\n#{1,6} \",\\n                # Note the alternative syntax for headings (below) is not handled here\\n                # Heading level 2\\n                # ---------------\\n                # End of code block\\n                \"```\\\\n\",\\n                # Horizontal lines\\n                \"\\\\n\\\\\\\\*\\\\\\\\*\\\\\\\\*+\\\\n\",\\n                \"\\\\n---+\\\\n\",\\n                \"\\\\n___+\\\\n\",\\n                # Note that this splitter doesn\\'t handle horizontal lines defined',\n",
              "  'return [\\n                # First, try to split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\ndef \",\\n                \"\\\\n\\\\tdef \",\\n                # Now split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.RST:\\n            return [\\n                # Split along section titles\\n                \"\\\\n=+\\\\n\",\\n                \"\\\\n-+\\\\n\",\\n                \"\\\\n\\\\\\\\*+\\\\n\",\\n                # Split along directive markers\\n                \"\\\\n\\\\n.. *\\\\n\\\\n\",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.RUBY:\\n            return [\\n                # Split along method definitions\\n                \"\\\\ndef \",\\n                \"\\\\nclass \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nunless \",\\n                \"\\\\nwhile \",\\n                \"\\\\nfor \",\\n                \"\\\\ndo \",\\n                \"\\\\nbegin \",\\n                \"\\\\nrescue \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.RUST:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfn \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nwhile \",\\n                \"\\\\nfor \",\\n                \"\\\\nloop \",\\n                \"\\\\nmatch \",\\n                \"\\\\nconst \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SCALA:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",',\n",
              "  '\"\\\\nfunction \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                \"\\\\ndefault \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.PHP:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunction \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nforeach \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.PROTO:\\n            return [\\n                # Split along message definitions\\n                \"\\\\nmessage \",\\n                # Split along service definitions\\n                \"\\\\nservice \",\\n                # Split along enum definitions\\n                \"\\\\nenum \",\\n                # Split along option definitions\\n                \"\\\\noption \",\\n                # Split along import statements\\n                \"\\\\nimport \",\\n                # Split along syntax declarations\\n                \"\\\\nsyntax \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.PYTHON:\\n            return [\\n                # First, try to split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\ndef \",\\n                \"\\\\n\\\\tdef \",',\n",
              "  '\"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.KOTLIN:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along method definitions\\n                \"\\\\npublic \",\\n                \"\\\\nprotected \",\\n                \"\\\\nprivate \",\\n                \"\\\\ninternal \",\\n                \"\\\\ncompanion \",\\n                \"\\\\nfun \",\\n                \"\\\\nval \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nwhen \",\\n                \"\\\\ncase \",\\n                \"\\\\nelse \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.JS:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunction \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                \"\\\\nvar \",\\n                \"\\\\nclass \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                \"\\\\ndefault \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.TS:\\n            return [\\n                \"\\\\nenum \",\\n                \"\\\\ninterface \",\\n                \"\\\\nnamespace \",\\n                \"\\\\ntype \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along function definitions\\n                \"\\\\nfunction \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements',\n",
              "  '@staticmethod\\n    def get_separators_for_language(language: Language) -> List[str]:\\n        if language == Language.CPP:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along function definitions\\n                \"\\\\nvoid \",\\n                \"\\\\nint \",\\n                \"\\\\nfloat \",\\n                \"\\\\ndouble \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.GO:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunc \",\\n                \"\\\\nvar \",\\n                \"\\\\nconst \",\\n                \"\\\\ntype \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.JAVA:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along method definitions\\n                \"\\\\npublic \",\\n                \"\\\\nprotected \",\\n                \"\\\\nprivate \",\\n                \"\\\\nstatic \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.KOTLIN:\\n            return [',\n",
              "  '_separator = separator if self._is_separator_regex else re.escape(separator)\\n        splits = _split_text_with_regex(text, _separator, self._keep_separator)\\n\\n        # Now go merging things, recursively splitting longer texts.\\n        _good_splits = []\\n        _separator = \"\" if self._keep_separator else separator\\n        for s in splits:\\n            if self._length_function(s) < self._chunk_size:\\n                _good_splits.append(s)\\n            else:\\n                if _good_splits:\\n                    merged_text = self._merge_splits(_good_splits, _separator)\\n                    final_chunks.extend(merged_text)\\n                    _good_splits = []\\n                if not new_separators:\\n                    final_chunks.append(s)\\n                else:\\n                    other_info = self._split_text(s, new_separators)\\n                    final_chunks.extend(other_info)\\n        if _good_splits:\\n            merged_text = self._merge_splits(_good_splits, _separator)\\n            final_chunks.extend(merged_text)\\n        return final_chunks\\n\\n    def split_text(self, text: str) -> List[str]:\\n        return self._split_text(text, self._separators)\\n\\n    @classmethod\\n    def from_language(\\n        cls, language: Language, **kwargs: Any\\n    ) -> RecursiveCharacterTextSplitter:\\n        separators = cls.get_separators_for_language(language)\\n        return cls(separators=separators, is_separator_regex=True, **kwargs)',\n",
              "  'class RecursiveCharacterTextSplitter(TextSplitter):\\n    \"\"\"Splitting text by recursively look at characters.\\n\\n    Recursively tries to split by different characters to find one\\n    that works.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        separators: Optional[List[str]] = None,\\n        keep_separator: bool = True,\\n        is_separator_regex: bool = False,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(keep_separator=keep_separator, **kwargs)\\n        self._separators = separators or [\"\\\\n\\\\n\", \"\\\\n\", \" \", \"\"]\\n        self._is_separator_regex = is_separator_regex\\n\\n    def _split_text(self, text: str, separators: List[str]) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        final_chunks = []\\n        # Get appropriate separator to use\\n        separator = separators[-1]\\n        new_separators = []\\n        for i, _s in enumerate(separators):\\n            _separator = _s if self._is_separator_regex else re.escape(_s)\\n            if _s == \"\":\\n                separator = _s\\n                break\\n            if re.search(_separator, text):\\n                separator = _s\\n                new_separators = separators[i + 1 :]\\n                break\\n\\n        _separator = separator if self._is_separator_regex else re.escape(separator)\\n        splits = _split_text_with_regex(text, _separator, self._keep_separator)',\n",
              "  'class Language(str, Enum):\\n    \"\"\"Enum of the programming languages.\"\"\"\\n\\n    CPP = \"cpp\"\\n    GO = \"go\"\\n    JAVA = \"java\"\\n    KOTLIN = \"kotlin\"\\n    JS = \"js\"\\n    TS = \"ts\"\\n    PHP = \"php\"\\n    PROTO = \"proto\"\\n    PYTHON = \"python\"\\n    RST = \"rst\"\\n    RUBY = \"ruby\"\\n    RUST = \"rust\"\\n    SCALA = \"scala\"\\n    SWIFT = \"swift\"\\n    MARKDOWN = \"markdown\"\\n    LATEX = \"latex\"\\n    HTML = \"html\"\\n    SOL = \"sol\"\\n    CSHARP = \"csharp\"\\n    COBOL = \"cobol\"',\n",
              "  'return [\\n                # First, try to split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\ndef \",\\n                \"\\\\n\\\\tdef \",\\n                # Now split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.RST:\\n            return [\\n                # Split along section titles\\n                \"\\\\n=+\\\\n\",\\n                \"\\\\n-+\\\\n\",\\n                \"\\\\n\\\\\\\\*+\\\\n\",\\n                # Split along directive markers\\n                \"\\\\n\\\\n.. *\\\\n\\\\n\",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.RUBY:\\n            return [\\n                # Split along method definitions\\n                \"\\\\ndef \",\\n                \"\\\\nclass \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nunless \",\\n                \"\\\\nwhile \",\\n                \"\\\\nfor \",\\n                \"\\\\ndo \",\\n                \"\\\\nbegin \",\\n                \"\\\\nrescue \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.RUST:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfn \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nwhile \",\\n                \"\\\\nfor \",\\n                \"\\\\nloop \",\\n                \"\\\\nmatch \",\\n                \"\\\\nconst \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SCALA:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",',\n",
              "  'class SentenceTransformersTokenTextSplitter(TextSplitter):\\n    \"\"\"Splitting text to tokens using sentence model tokenizer.\"\"\"\\n\\n    def __init__(\\n        self,\\n        chunk_overlap: int = 50,\\n        model_name: str = \"sentence-transformers/all-mpnet-base-v2\",\\n        tokens_per_chunk: Optional[int] = None,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(**kwargs, chunk_overlap=chunk_overlap)\\n\\n        try:\\n            from sentence_transformers import SentenceTransformer\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import sentence_transformer python package. \"\\n                \"This is needed in order to for SentenceTransformersTokenTextSplitter. \"\\n                \"Please install it with `pip install sentence-transformers`.\"\\n            )\\n\\n        self.model_name = model_name\\n        self._model = SentenceTransformer(self.model_name)\\n        self.tokenizer = self._model.tokenizer\\n        self._initialize_chunk_configuration(tokens_per_chunk=tokens_per_chunk)\\n\\n    def _initialize_chunk_configuration(\\n        self, *, tokens_per_chunk: Optional[int]\\n    ) -> None:\\n        self.maximum_tokens_per_chunk = cast(int, self._model.max_seq_length)\\n\\n        if tokens_per_chunk is None:\\n            self.tokens_per_chunk = self.maximum_tokens_per_chunk\\n        else:\\n            self.tokens_per_chunk = tokens_per_chunk\\n\\n        if self.tokens_per_chunk > self.maximum_tokens_per_chunk:\\n            raise ValueError(\\n                f\"The token limit of the models \\'{self.model_name}\\'\"\\n                f\" is: {self.maximum_tokens_per_chunk}.\"\\n                f\" Argument tokens_per_chunk={self.tokens_per_chunk}\"\\n                f\" > maximum token limit.\"\\n            )\\n\\n    def split_text(self, text: str) -> List[str]:\\n        def encode_strip_start_and_stop_token_ids(text: str) -> List[int]:\\n            return self._encode(text)[1:-1]',\n",
              "  'class TokenTextSplitter(TextSplitter):\\n    \"\"\"Splitting text to tokens using model tokenizer.\"\"\"\\n\\n    def __init__(\\n        self,\\n        encoding_name: str = \"gpt2\",\\n        model_name: Optional[str] = None,\\n        allowed_special: Union[Literal[\"all\"], AbstractSet[str]] = set(),\\n        disallowed_special: Union[Literal[\"all\"], Collection[str]] = \"all\",\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(**kwargs)\\n        try:\\n            import tiktoken\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import tiktoken python package. \"\\n                \"This is needed in order to for TokenTextSplitter. \"\\n                \"Please install it with `pip install tiktoken`.\"\\n            )\\n\\n        if model_name is not None:\\n            enc = tiktoken.encoding_for_model(model_name)\\n        else:\\n            enc = tiktoken.get_encoding(encoding_name)\\n        self._tokenizer = enc\\n        self._allowed_special = allowed_special\\n        self._disallowed_special = disallowed_special\\n\\n    def split_text(self, text: str) -> List[str]:\\n        def _encode(_text: str) -> List[int]:\\n            return self._tokenizer.encode(\\n                _text,\\n                allowed_special=self._allowed_special,\\n                disallowed_special=self._disallowed_special,\\n            )\\n\\n        tokenizer = Tokenizer(\\n            chunk_overlap=self._chunk_overlap,\\n            tokens_per_chunk=self._chunk_size,\\n            decode=self._tokenizer.decode,\\n            encode=_encode,\\n        )\\n\\n        return split_text_on_tokens(text=text, tokenizer=tokenizer)',\n",
              "  'class Tokenizer:\\n    \"\"\"Tokenizer data class.\"\"\"\\n\\n    chunk_overlap: int\\n    \"\"\"Overlap in tokens between chunks\"\"\"\\n    tokens_per_chunk: int\\n    \"\"\"Maximum number of tokens per chunk\"\"\"\\n    decode: Callable[[List[int]], str]\\n    \"\"\" Function to decode a list of token ids to a string\"\"\"\\n    encode: Callable[[str], List[int]]\\n    \"\"\" Function to encode a string to a list of token ids\"\"\"\\n\\n\\ndef split_text_on_tokens(*, text: str, tokenizer: Tokenizer) -> List[str]:\\n    \"\"\"Split incoming text and return chunks using tokenizer.\"\"\"\\n    splits: List[str] = []\\n    input_ids = tokenizer.encode(text)\\n    start_idx = 0\\n    cur_idx = min(start_idx + tokenizer.tokens_per_chunk, len(input_ids))\\n    chunk_ids = input_ids[start_idx:cur_idx]\\n    while start_idx < len(input_ids):\\n        splits.append(tokenizer.decode(chunk_ids))\\n        if cur_idx == len(input_ids):\\n            break\\n        start_idx += tokenizer.tokens_per_chunk - tokenizer.chunk_overlap\\n        cur_idx = min(start_idx + tokenizer.tokens_per_chunk, len(input_ids))\\n        chunk_ids = input_ids[start_idx:cur_idx]\\n    return splits',\n",
              "  '# map xhtml namespace prefix\\n        ns_map = {\"h\": \"http://www.w3.org/1999/xhtml\"}\\n\\n        # build list of elements from DOM\\n        elements = []\\n        for element in result_dom.findall(\"*//*\", ns_map):\\n            if element.findall(\"*[@class=\\'headers\\']\") or element.findall(\\n                \"*[@class=\\'chunk\\']\"\\n            ):\\n                elements.append(\\n                    ElementType(\\n                        url=file,\\n                        xpath=\"\".join(\\n                            [\\n                                node.text\\n                                for node in element.findall(\"*[@class=\\'xpath\\']\", ns_map)\\n                            ]\\n                        ),\\n                        content=\"\".join(\\n                            [\\n                                node.text\\n                                for node in element.findall(\"*[@class=\\'chunk\\']\", ns_map)\\n                            ]\\n                        ),\\n                        metadata={\\n                            # Add text of specified headers to metadata using header\\n                            # mapping.\\n                            header_mapping[node.tag]: node.text\\n                            for node in filter(\\n                                lambda x: x.tag in header_filter,\\n                                element.findall(\"*[@class=\\'headers\\']/*\", ns_map),\\n                            )\\n                        },\\n                    )\\n                )\\n\\n        if not self.return_each_element:\\n            return self.aggregate_elements_to_chunks(elements)\\n        else:\\n            return [\\n                Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n                for chunk in elements\\n            ]\\n\\n\\n# should be in newer Python versions (3.10+)\\n# @dataclass(frozen=True, kw_only=True, slots=True)\\n@dataclass(frozen=True)',\n",
              "  'return [\\n            Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n            for chunk in aggregated_chunks\\n        ]\\n\\n    def split_text_from_url(self, url: str) -> List[Document]:\\n        \"\"\"Split HTML from web URL\\n\\n        Args:\\n            url: web URL\\n        \"\"\"\\n        r = requests.get(url)\\n        return self.split_text_from_file(BytesIO(r.content))\\n\\n    def split_text(self, text: str) -> List[Document]:\\n        \"\"\"Split HTML text string\\n\\n        Args:\\n            text: HTML text\\n        \"\"\"\\n        return self.split_text_from_file(StringIO(text))\\n\\n    def split_text_from_file(self, file: Any) -> List[Document]:\\n        \"\"\"Split HTML file\\n\\n        Args:\\n            file: HTML file\\n        \"\"\"\\n        try:\\n            from lxml import etree\\n        except ImportError as e:\\n            raise ImportError(\\n                \"Unable to import lxml, please install with `pip install lxml`.\"\\n            ) from e\\n        # use lxml library to parse html document and return xml ElementTree\\n        parser = etree.HTMLParser()\\n        tree = etree.parse(file, parser)\\n\\n        # document transformation for \"structure-aware\" chunking is handled with xsl.\\n        # see comments in html_chunks_with_headers.xslt for more detailed information.\\n        xslt_path = (\\n            pathlib.Path(__file__).parent\\n            / \"document_transformers/xsl/html_chunks_with_headers.xslt\"\\n        )\\n        xslt_tree = etree.parse(xslt_path)\\n        transform = etree.XSLT(xslt_tree)\\n        result = transform(tree)\\n        result_dom = etree.fromstring(str(result))\\n\\n        # create filter and mapping for header metadata\\n        header_filter = [header[0] for header in self.headers_to_split_on]\\n        header_mapping = dict(self.headers_to_split_on)\\n\\n        # map xhtml namespace prefix\\n        ns_map = {\"h\": \"http://www.w3.org/1999/xhtml\"}',\n",
              "  'class HTMLHeaderTextSplitter:\\n    \"\"\"\\n    Splitting HTML files based on specified headers.\\n    Requires lxml package.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        headers_to_split_on: List[Tuple[str, str]],\\n        return_each_element: bool = False,\\n    ):\\n        \"\"\"Create a new HTMLHeaderTextSplitter.\\n\\n        Args:\\n            headers_to_split_on: list of tuples of headers we want to track mapped to\\n                (arbitrary) keys for metadata. Allowed header values: h1, h2, h3, h4,\\n                h5, h6 e.g. [(\"h1\", \"Header 1\"), (\"h2\", \"Header 2)].\\n            return_each_element: Return each element w/ associated headers.\\n        \"\"\"\\n        # Output element-by-element or aggregated into chunks w/ common headers\\n        self.return_each_element = return_each_element\\n        self.headers_to_split_on = sorted(headers_to_split_on)\\n\\n    def aggregate_elements_to_chunks(\\n        self, elements: List[ElementType]\\n    ) -> List[Document]:\\n        \"\"\"Combine elements with common metadata into chunks\\n\\n        Args:\\n            elements: HTML element content with associated identifying info and metadata\\n        \"\"\"\\n        aggregated_chunks: List[ElementType] = []\\n\\n        for element in elements:\\n            if (\\n                aggregated_chunks\\n                and aggregated_chunks[-1][\"metadata\"] == element[\"metadata\"]\\n            ):\\n                # If the last element in the aggregated list\\n                # has the same metadata as the current element,\\n                # append the current content to the last element\\'s content\\n                aggregated_chunks[-1][\"content\"] += \"  \\\\n\" + element[\"content\"]\\n            else:\\n                # Otherwise, append the current element to the aggregated list\\n                aggregated_chunks.append(element)\\n\\n        return [\\n            Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n            for chunk in aggregated_chunks\\n        ]',\n",
              "  'class ElementType(TypedDict):\\n    \"\"\"Element type as typed dict.\"\"\"\\n\\n    url: str\\n    xpath: str\\n    content: str\\n    metadata: Dict[str, str]',\n",
              "  'if current_content:\\n            lines_with_metadata.append(\\n                {\"content\": \"\\\\n\".join(current_content), \"metadata\": current_metadata}\\n            )\\n\\n        # lines_with_metadata has each line with associated header metadata\\n        # aggregate these into chunks based on common metadata\\n        if not self.return_each_line:\\n            return self.aggregate_lines_to_chunks(lines_with_metadata)\\n        else:\\n            return [\\n                Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n                for chunk in lines_with_metadata\\n            ]',\n",
              "  'class PythonCodeTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Python syntax.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a PythonCodeTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.PYTHON)\\n        super().__init__(separators=separators, **kwargs)\\n\\n\\nclass MarkdownTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Markdown-formatted headings.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a MarkdownTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.MARKDOWN)\\n        super().__init__(separators=separators, **kwargs)\\n\\n\\nclass LatexTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Latex-formatted layout elements.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a LatexTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.LATEX)\\n        super().__init__(separators=separators, **kwargs)',\n",
              "  'class MarkdownHeaderTextSplitter:\\n    \"\"\"Splitting markdown files based on specified headers.\"\"\"\\n\\n    def __init__(\\n        self, headers_to_split_on: List[Tuple[str, str]], return_each_line: bool = False\\n    ):\\n        \"\"\"Create a new MarkdownHeaderTextSplitter.\\n\\n        Args:\\n            headers_to_split_on: Headers we want to track\\n            return_each_line: Return each line w/ associated headers\\n        \"\"\"\\n        # Output line-by-line or aggregated into chunks w/ common headers\\n        self.return_each_line = return_each_line\\n        # Given the headers we want to split on,\\n        # (e.g., \"#, ##, etc\") order by length\\n        self.headers_to_split_on = sorted(\\n            headers_to_split_on, key=lambda split: len(split[0]), reverse=True\\n        )\\n\\n    def aggregate_lines_to_chunks(self, lines: List[LineType]) -> List[Document]:\\n        \"\"\"Combine lines with common metadata into chunks\\n        Args:\\n            lines: Line of text / associated header metadata\\n        \"\"\"\\n        aggregated_chunks: List[LineType] = []\\n\\n        for line in lines:\\n            if (\\n                aggregated_chunks\\n                and aggregated_chunks[-1][\"metadata\"] == line[\"metadata\"]\\n            ):\\n                # If the last line in the aggregated list\\n                # has the same metadata as the current line,\\n                # append the current content to the last lines\\'s content\\n                aggregated_chunks[-1][\"content\"] += \"  \\\\n\" + line[\"content\"]\\n            else:\\n                # Otherwise, append the current line to the aggregated list\\n                aggregated_chunks.append(line)\\n\\n        return [\\n            Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n            for chunk in aggregated_chunks\\n        ]\\n\\n    def split_text(self, text: str) -> List[Document]:\\n        \"\"\"Split markdown file\\n        Args:\\n            text: Markdown file\"\"\"',\n",
              "  'class CharacterTextSplitter(TextSplitter):\\n    \"\"\"Splitting text that looks at characters.\"\"\"\\n\\n    def __init__(\\n        self, separator: str = \"\\\\n\\\\n\", is_separator_regex: bool = False, **kwargs: Any\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(**kwargs)\\n        self._separator = separator\\n        self._is_separator_regex = is_separator_regex\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        # First we naively split the large input into a bunch of smaller ones.\\n        separator = (\\n            self._separator if self._is_separator_regex else re.escape(self._separator)\\n        )\\n        splits = _split_text_with_regex(text, separator, self._keep_separator)\\n        _separator = \"\" if self._keep_separator else self._separator\\n        return self._merge_splits(splits, _separator)\\n\\n\\nclass LineType(TypedDict):\\n    \"\"\"Line type as typed dict.\"\"\"\\n\\n    metadata: Dict[str, str]\\n    content: str\\n\\n\\nclass HeaderType(TypedDict):\\n    \"\"\"Header type as typed dict.\"\"\"\\n\\n    level: int\\n    name: str\\n    data: str',\n",
              "  'if issubclass(cls, TokenTextSplitter):\\n            extra_kwargs = {\\n                \"encoding_name\": encoding_name,\\n                \"model_name\": model_name,\\n                \"allowed_special\": allowed_special,\\n                \"disallowed_special\": disallowed_special,\\n            }\\n            kwargs = {**kwargs, **extra_kwargs}\\n\\n        return cls(length_function=_tiktoken_encoder, **kwargs)\\n\\n    def transform_documents(\\n        self, documents: Sequence[Document], **kwargs: Any\\n    ) -> Sequence[Document]:\\n        \"\"\"Transform sequence of documents by splitting them.\"\"\"\\n        return self.split_documents(list(documents))\\n\\n    async def atransform_documents(\\n        self, documents: Sequence[Document], **kwargs: Any\\n    ) -> Sequence[Document]:\\n        \"\"\"Asynchronously transform a sequence of documents by splitting them.\"\"\"\\n        return await asyncio.get_running_loop().run_in_executor(\\n            None, partial(self.transform_documents, **kwargs), documents\\n        )',\n",
              "  'if not isinstance(tokenizer, PreTrainedTokenizerBase):\\n                raise ValueError(\\n                    \"Tokenizer received was not an instance of PreTrainedTokenizerBase\"\\n                )\\n\\n            def _huggingface_tokenizer_length(text: str) -> int:\\n                return len(tokenizer.encode(text))\\n\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import transformers python package. \"\\n                \"Please install it with `pip install transformers`.\"\\n            )\\n        return cls(length_function=_huggingface_tokenizer_length, **kwargs)\\n\\n    @classmethod\\n    def from_tiktoken_encoder(\\n        cls: Type[TS],\\n        encoding_name: str = \"gpt2\",\\n        model_name: Optional[str] = None,\\n        allowed_special: Union[Literal[\"all\"], AbstractSet[str]] = set(),\\n        disallowed_special: Union[Literal[\"all\"], Collection[str]] = \"all\",\\n        **kwargs: Any,\\n    ) -> TS:\\n        \"\"\"Text splitter that uses tiktoken encoder to count length.\"\"\"\\n        try:\\n            import tiktoken\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import tiktoken python package. \"\\n                \"This is needed in order to calculate max_tokens_for_prompt. \"\\n                \"Please install it with `pip install tiktoken`.\"\\n            )\\n\\n        if model_name is not None:\\n            enc = tiktoken.encoding_for_model(model_name)\\n        else:\\n            enc = tiktoken.get_encoding(encoding_name)\\n\\n        def _tiktoken_encoder(text: str) -> int:\\n            return len(\\n                enc.encode(\\n                    text,\\n                    allowed_special=allowed_special,\\n                    disallowed_special=disallowed_special,\\n                )\\n            )',\n",
              "  'docs = []\\n        current_doc: List[str] = []\\n        total = 0\\n        for d in splits:\\n            _len = self._length_function(d)\\n            if (\\n                total + _len + (separator_len if len(current_doc) > 0 else 0)\\n                > self._chunk_size\\n            ):\\n                if total > self._chunk_size:\\n                    logger.warning(\\n                        f\"Created a chunk of size {total}, \"\\n                        f\"which is longer than the specified {self._chunk_size}\"\\n                    )\\n                if len(current_doc) > 0:\\n                    doc = self._join_docs(current_doc, separator)\\n                    if doc is not None:\\n                        docs.append(doc)\\n                    # Keep on popping if:\\n                    # - we have a larger chunk than in the chunk overlap\\n                    # - or if we still have any chunks and the length is long\\n                    while total > self._chunk_overlap or (\\n                        total + _len + (separator_len if len(current_doc) > 0 else 0)\\n                        > self._chunk_size\\n                        and total > 0\\n                    ):\\n                        total -= self._length_function(current_doc[0]) + (\\n                            separator_len if len(current_doc) > 1 else 0\\n                        )\\n                        current_doc = current_doc[1:]\\n            current_doc.append(d)\\n            total += _len + (separator_len if len(current_doc) > 1 else 0)\\n        doc = self._join_docs(current_doc, separator)\\n        if doc is not None:\\n            docs.append(doc)\\n        return docs\\n\\n    @classmethod\\n    def from_huggingface_tokenizer(cls, tokenizer: Any, **kwargs: Any) -> TextSplitter:\\n        \"\"\"Text splitter that uses HuggingFace tokenizer to count length.\"\"\"\\n        try:\\n            from transformers import PreTrainedTokenizerBase',\n",
              "  '@abstractmethod\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split text into multiple components.\"\"\"\\n\\n    def create_documents(\\n        self, texts: List[str], metadatas: Optional[List[dict]] = None\\n    ) -> List[Document]:\\n        \"\"\"Create documents from a list of texts.\"\"\"\\n        _metadatas = metadatas or [{}] * len(texts)\\n        documents = []\\n        for i, text in enumerate(texts):\\n            index = -1\\n            for chunk in self.split_text(text):\\n                metadata = copy.deepcopy(_metadatas[i])\\n                if self._add_start_index:\\n                    index = text.find(chunk, index + 1)\\n                    metadata[\"start_index\"] = index\\n                new_doc = Document(page_content=chunk, metadata=metadata)\\n                documents.append(new_doc)\\n        return documents\\n\\n    def split_documents(self, documents: Iterable[Document]) -> List[Document]:\\n        \"\"\"Split documents.\"\"\"\\n        texts, metadatas = [], []\\n        for doc in documents:\\n            texts.append(doc.page_content)\\n            metadatas.append(doc.metadata)\\n        return self.create_documents(texts, metadatas=metadatas)\\n\\n    def _join_docs(self, docs: List[str], separator: str) -> Optional[str]:\\n        text = separator.join(docs)\\n        if self._strip_whitespace:\\n            text = text.strip()\\n        if text == \"\":\\n            return None\\n        else:\\n            return text\\n\\n    def _merge_splits(self, splits: Iterable[str], separator: str) -> List[str]:\\n        # We now want to combine these smaller pieces into medium size\\n        # chunks to send to the LLM.\\n        separator_len = self._length_function(separator)',\n",
              "  'class TextSplitter(BaseDocumentTransformer, ABC):\\n    \"\"\"Interface for splitting text into chunks.\"\"\"\\n\\n    def __init__(\\n        self,\\n        chunk_size: int = 4000,\\n        chunk_overlap: int = 200,\\n        length_function: Callable[[str], int] = len,\\n        keep_separator: bool = False,\\n        add_start_index: bool = False,\\n        strip_whitespace: bool = True,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\\n\\n        Args:\\n            chunk_size: Maximum size of chunks to return\\n            chunk_overlap: Overlap in characters between chunks\\n            length_function: Function that measures the length of given chunks\\n            keep_separator: Whether to keep the separator in the chunks\\n            add_start_index: If `True`, includes chunk\\'s start index in metadata\\n            strip_whitespace: If `True`, strips whitespace from the start and end of\\n                              every document\\n        \"\"\"\\n        if chunk_overlap > chunk_size:\\n            raise ValueError(\\n                f\"Got a larger chunk overlap ({chunk_overlap}) than chunk size \"\\n                f\"({chunk_size}), should be smaller.\"\\n            )\\n        self._chunk_size = chunk_size\\n        self._chunk_overlap = chunk_overlap\\n        self._length_function = length_function\\n        self._keep_separator = keep_separator\\n        self._add_start_index = add_start_index\\n        self._strip_whitespace = strip_whitespace\\n\\n    @abstractmethod\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split text into multiple components.\"\"\"',\n",
              "  'def _split_text_with_regex(\\n    text: str, separator: str, keep_separator: bool\\n) -> List[str]:\\n    # Now that we have the separator, split the text\\n    if separator:\\n        if keep_separator:\\n            # The parentheses in the pattern keep the delimiters in the result.\\n            _splits = re.split(f\"({separator})\", text)\\n            splits = [_splits[i] + _splits[i + 1] for i in range(1, len(_splits), 2)]\\n            if len(_splits) % 2 == 0:\\n                splits += _splits[-1:]\\n            splits = [_splits[0]] + splits\\n        else:\\n            splits = re.split(separator, text)\\n    else:\\n        splits = list(text)\\n    return [s for s in splits if s != \"\"]',\n",
              "  '\"\"\"**Text Splitters** are classes for splitting text.\\n\\n\\n**Class hierarchy:**\\n\\n.. code-block::\\n\\n    BaseDocumentTransformer --> TextSplitter --> <name>TextSplitter  # Example: CharacterTextSplitter\\n                                                 RecursiveCharacterTextSplitter -->  <name>TextSplitter\\n\\nNote: **MarkdownHeaderTextSplitter** and **HTMLHeaderTextSplitter do not derive from TextSplitter.\\n\\n\\n**Main helpers:**\\n\\n.. code-block::\\n\\n    Document, Tokenizer, Language, LineType, HeaderType\\n\\n\"\"\"  # noqa: E501\\n\\nfrom __future__ import annotations\\n\\nimport asyncio\\nimport copy\\nimport logging\\nimport pathlib\\nimport re\\nfrom abc import ABC, abstractmethod\\nfrom dataclasses import dataclass\\nfrom enum import Enum\\nfrom functools import partial\\nfrom io import BytesIO, StringIO\\nfrom typing import (\\n    AbstractSet,\\n    Any,\\n    Callable,\\n    Collection,\\n    Dict,\\n    Iterable,\\n    List,\\n    Literal,\\n    Optional,\\n    Sequence,\\n    Tuple,\\n    Type,\\n    TypedDict,\\n    TypeVar,\\n    Union,\\n    cast,\\n)\\n\\nimport requests\\nfrom langchain_core.documents import BaseDocumentTransformer, Document\\n\\nlogger = logging.getLogger(__name__)\\n\\nTS = TypeVar(\"TS\", bound=\"TextSplitter\")\\n\\n\\ndef _make_spacy_pipeline_for_splitting(\\n    pipeline: str, *, max_length: int = 1_000_000\\n) -> Any:  # avoid importing spacy\\n    try:\\n        import spacy\\n    except ImportError:\\n        raise ImportError(\\n            \"Spacy is not installed, please install it with `pip install spacy`.\"\\n        )\\n    if pipeline == \"sentencizer\":\\n        from spacy.lang.en import English\\n\\n        sentencizer = English()\\n        sentencizer.add_pipe(\"sentencizer\")\\n    else:\\n        sentencizer = spacy.load(pipeline, exclude=[\"ner\", \"tagger\"])\\n        sentencizer.max_length = max_length\\n    return sentencizer',\n",
              "  'def split_text(self, text: str) -> List[Document]:\\n        \"\"\"Split markdown file\\n        Args:\\n            text: Markdown file\"\"\"\\n\\n        # Split the input text by newline character (\"\\\\n\").\\n        lines = text.split(\"\\\\n\")\\n        # Final output\\n        lines_with_metadata: List[LineType] = []\\n        # Content and metadata of the chunk currently being processed\\n        current_content: List[str] = []\\n        current_metadata: Dict[str, str] = {}\\n        # Keep track of the nested header structure\\n        # header_stack: List[Dict[str, Union[int, str]]] = []\\n        header_stack: List[HeaderType] = []\\n        initial_metadata: Dict[str, str] = {}\\n\\n        in_code_block = False\\n        opening_fence = \"\"\\n\\n        for line in lines:\\n            stripped_line = line.strip()\\n\\n            if not in_code_block:\\n                # Exclude inline code spans\\n                if stripped_line.startswith(\"```\") and stripped_line.count(\"```\") == 1:\\n                    in_code_block = True\\n                    opening_fence = \"```\"\\n                elif stripped_line.startswith(\"~~~\"):\\n                    in_code_block = True\\n                    opening_fence = \"~~~\"\\n            else:\\n                if stripped_line.startswith(opening_fence):\\n                    in_code_block = False\\n                    opening_fence = \"\"\\n\\n            if in_code_block:\\n                current_content.append(stripped_line)\\n                continue',\n",
              "  'class NLTKTextSplitter(TextSplitter):\\n    \"\"\"Splitting text using NLTK package.\"\"\"\\n\\n    def __init__(\\n        self, separator: str = \"\\\\n\\\\n\", language: str = \"english\", **kwargs: Any\\n    ) -> None:\\n        \"\"\"Initialize the NLTK splitter.\"\"\"\\n        super().__init__(**kwargs)\\n        try:\\n            from nltk.tokenize import sent_tokenize\\n\\n            self._tokenizer = sent_tokenize\\n        except ImportError:\\n            raise ImportError(\\n                \"NLTK is not installed, please install it with `pip install nltk`.\"\\n            )\\n        self._separator = separator\\n        self._language = language\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        # First we naively split the large input into a bunch of smaller ones.\\n        splits = self._tokenizer(text, language=self._language)\\n        return self._merge_splits(splits, self._separator)\\n\\n\\nclass SpacyTextSplitter(TextSplitter):\\n    \"\"\"Splitting text using Spacy package.\\n\\n\\n    Per default, Spacy\\'s `en_core_web_sm` model is used and\\n    its default max_length is 1000000 (it is the length of maximum character\\n    this model takes which can be increased for large files). For a faster, but\\n    potentially less accurate splitting, you can use `pipeline=\\'sentencizer\\'`.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        separator: str = \"\\\\n\\\\n\",\\n        pipeline: str = \"en_core_web_sm\",\\n        max_length: int = 1_000_000,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Initialize the spacy text splitter.\"\"\"\\n        super().__init__(**kwargs)\\n        self._tokenizer = _make_spacy_pipeline_for_splitting(\\n            pipeline, max_length=max_length\\n        )\\n        self._separator = separator\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        splits = (s.text for s in self._tokenizer(text).sents)\\n        return self._merge_splits(splits, self._separator)\\n\\n\\n# For backwards compatibility',\n",
              "  'else:\\n            raise ValueError(\\n                f\"Language {language} is not supported! \"\\n                f\"Please choose from {list(Language)}\"\\n            )',\n",
              "  '\"\\\\nevent \",\\n                \"\\\\nmodifier \",\\n                \"\\\\nerror \",\\n                \"\\\\nstruct \",\\n                \"\\\\nenum \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo while \",\\n                \"\\\\nassembly \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.COBOL:\\n            return [\\n                # Split along divisions\\n                \"\\\\nIDENTIFICATION DIVISION.\",\\n                \"\\\\nENVIRONMENT DIVISION.\",\\n                \"\\\\nDATA DIVISION.\",\\n                \"\\\\nPROCEDURE DIVISION.\",\\n                # Split along sections within DATA DIVISION\\n                \"\\\\nWORKING-STORAGE SECTION.\",\\n                \"\\\\nLINKAGE SECTION.\",\\n                \"\\\\nFILE SECTION.\",\\n                # Split along sections within PROCEDURE DIVISION\\n                \"\\\\nINPUT-OUTPUT SECTION.\",\\n                # Split along paragraphs and common statements\\n                \"\\\\nOPEN \",\\n                \"\\\\nCLOSE \",\\n                \"\\\\nREAD \",\\n                \"\\\\nWRITE \",\\n                \"\\\\nIF \",\\n                \"\\\\nELSE \",\\n                \"\\\\nMOVE \",\\n                \"\\\\nPERFORM \",\\n                \"\\\\nUNTIL \",\\n                \"\\\\nVARYING \",\\n                \"\\\\nACCEPT \",\\n                \"\\\\nDISPLAY \",\\n                \"\\\\nSTOP RUN.\",\\n                # Split by the normal type of lines\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]',\n",
              "  '\"<nav\",\\n                # Head\\n                \"<head\",\\n                \"<style\",\\n                \"<script\",\\n                \"<meta\",\\n                \"<title\",\\n                \"\",\\n            ]\\n        elif language == Language.CSHARP:\\n            return [\\n                \"\\\\ninterface \",\\n                \"\\\\nenum \",\\n                \"\\\\nimplements \",\\n                \"\\\\ndelegate \",\\n                \"\\\\nevent \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nabstract \",\\n                # Split along method definitions\\n                \"\\\\npublic \",\\n                \"\\\\nprotected \",\\n                \"\\\\nprivate \",\\n                \"\\\\nstatic \",\\n                \"\\\\nreturn \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\ncontinue \",\\n                \"\\\\nfor \",\\n                \"\\\\nforeach \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\nbreak \",\\n                \"\\\\ncase \",\\n                \"\\\\nelse \",\\n                # Split by exceptions\\n                \"\\\\ntry \",\\n                \"\\\\nthrow \",\\n                \"\\\\nfinally \",\\n                \"\\\\ncatch \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SOL:\\n            return [\\n                # Split along compiler information definitions\\n                \"\\\\npragma \",\\n                \"\\\\nusing \",\\n                # Split along contract definitions\\n                \"\\\\ncontract \",\\n                \"\\\\ninterface \",\\n                \"\\\\nlibrary \",\\n                # Split along method definitions\\n                \"\\\\nconstructor \",\\n                \"\\\\ntype \",\\n                \"\\\\nfunction \",\\n                \"\\\\nevent \",\\n                \"\\\\nmodifier \",\\n                \"\\\\nerror \",\\n                \"\\\\nstruct \",\\n                \"\\\\nenum \",\\n                # Split along control flow statements',\n",
              "  '\"\\\\n\\\\\\\\*\\\\\\\\*\\\\\\\\*+\\\\n\",\\n                \"\\\\n---+\\\\n\",\\n                \"\\\\n___+\\\\n\",\\n                # Note that this splitter doesn\\'t handle horizontal lines defined\\n                # by *three or more* of ***, ---, or ___, but this is not handled\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.LATEX:\\n            return [\\n                # First, try to split along Latex sections\\n                \"\\\\n\\\\\\\\\\\\\\\\chapter{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\section{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\subsection{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\subsubsection{\",\\n                # Now split by environments\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{enumerate}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{itemize}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{description}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{list}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{quote}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{quotation}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{verse}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{verbatim}\",\\n                # Now split by math environments\\n                \"\\\\n\\\\\\\\\\\\begin{align}\",\\n                \"$$\",\\n                \"$\",\\n                # Now split by the normal type of lines\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.HTML:\\n            return [\\n                # First, try to split along HTML tags\\n                \"<body\",\\n                \"<div\",\\n                \"<p\",\\n                \"<br\",\\n                \"<li\",\\n                \"<h1\",\\n                \"<h2\",\\n                \"<h3\",\\n                \"<h4\",\\n                \"<h5\",\\n                \"<h6\",\\n                \"<span\",\\n                \"<table\",\\n                \"<tr\",\\n                \"<td\",\\n                \"<th\",\\n                \"<ul\",\\n                \"<ol\",\\n                \"<header\",\\n                \"<footer\",\\n                \"<nav\",\\n                # Head\\n                \"<head\",\\n                \"<style\",\\n                \"<script\",\\n                \"<meta\",\\n                \"<title\",',\n",
              "  '\" \",\\n                \"\",\\n            ]\\n        elif language == Language.SCALA:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nobject \",\\n                # Split along method definitions\\n                \"\\\\ndef \",\\n                \"\\\\nval \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nmatch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SWIFT:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunc \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nstruct \",\\n                \"\\\\nenum \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.MARKDOWN:\\n            return [\\n                # First, try to split along Markdown headings (starting with level 2)\\n                \"\\\\n#{1,6} \",\\n                # Note the alternative syntax for headings (below) is not handled here\\n                # Heading level 2\\n                # ---------------\\n                # End of code block\\n                \"```\\\\n\",\\n                # Horizontal lines\\n                \"\\\\n\\\\\\\\*\\\\\\\\*\\\\\\\\*+\\\\n\",\\n                \"\\\\n---+\\\\n\",\\n                \"\\\\n___+\\\\n\",\\n                # Note that this splitter doesn\\'t handle horizontal lines defined',\n",
              "  'return [\\n                # First, try to split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\ndef \",\\n                \"\\\\n\\\\tdef \",\\n                # Now split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.RST:\\n            return [\\n                # Split along section titles\\n                \"\\\\n=+\\\\n\",\\n                \"\\\\n-+\\\\n\",\\n                \"\\\\n\\\\\\\\*+\\\\n\",\\n                # Split along directive markers\\n                \"\\\\n\\\\n.. *\\\\n\\\\n\",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.RUBY:\\n            return [\\n                # Split along method definitions\\n                \"\\\\ndef \",\\n                \"\\\\nclass \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nunless \",\\n                \"\\\\nwhile \",\\n                \"\\\\nfor \",\\n                \"\\\\ndo \",\\n                \"\\\\nbegin \",\\n                \"\\\\nrescue \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.RUST:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfn \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nwhile \",\\n                \"\\\\nfor \",\\n                \"\\\\nloop \",\\n                \"\\\\nmatch \",\\n                \"\\\\nconst \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SCALA:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",',\n",
              "  '\"\\\\nfunction \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                \"\\\\ndefault \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.PHP:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunction \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nforeach \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.PROTO:\\n            return [\\n                # Split along message definitions\\n                \"\\\\nmessage \",\\n                # Split along service definitions\\n                \"\\\\nservice \",\\n                # Split along enum definitions\\n                \"\\\\nenum \",\\n                # Split along option definitions\\n                \"\\\\noption \",\\n                # Split along import statements\\n                \"\\\\nimport \",\\n                # Split along syntax declarations\\n                \"\\\\nsyntax \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.PYTHON:\\n            return [\\n                # First, try to split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\ndef \",\\n                \"\\\\n\\\\tdef \",',\n",
              "  'class TokenTextSplitter(TextSplitter):\\n    \"\"\"Splitting text to tokens using model tokenizer.\"\"\"\\n\\n    def __init__(\\n        self,\\n        encoding_name: str = \"gpt2\",\\n        model_name: Optional[str] = None,\\n        allowed_special: Union[Literal[\"all\"], AbstractSet[str]] = set(),\\n        disallowed_special: Union[Literal[\"all\"], Collection[str]] = \"all\",\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(**kwargs)\\n        try:\\n            import tiktoken\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import tiktoken python package. \"\\n                \"This is needed in order to for TokenTextSplitter. \"\\n                \"Please install it with `pip install tiktoken`.\"\\n            )\\n\\n        if model_name is not None:\\n            enc = tiktoken.encoding_for_model(model_name)\\n        else:\\n            enc = tiktoken.get_encoding(encoding_name)\\n        self._tokenizer = enc\\n        self._allowed_special = allowed_special\\n        self._disallowed_special = disallowed_special\\n\\n    def split_text(self, text: str) -> List[str]:\\n        def _encode(_text: str) -> List[int]:\\n            return self._tokenizer.encode(\\n                _text,\\n                allowed_special=self._allowed_special,\\n                disallowed_special=self._disallowed_special,\\n            )\\n\\n        tokenizer = Tokenizer(\\n            chunk_overlap=self._chunk_overlap,\\n            tokens_per_chunk=self._chunk_size,\\n            decode=self._tokenizer.decode,\\n            encode=_encode,\\n        )\\n\\n        return split_text_on_tokens(text=text, tokenizer=tokenizer)',\n",
              "  '\"\\\\nfunction \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                \"\\\\ndefault \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.PHP:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunction \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nforeach \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.PROTO:\\n            return [\\n                # Split along message definitions\\n                \"\\\\nmessage \",\\n                # Split along service definitions\\n                \"\\\\nservice \",\\n                # Split along enum definitions\\n                \"\\\\nenum \",\\n                # Split along option definitions\\n                \"\\\\noption \",\\n                # Split along import statements\\n                \"\\\\nimport \",\\n                # Split along syntax declarations\\n                \"\\\\nsyntax \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.PYTHON:\\n            return [\\n                # First, try to split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\ndef \",\\n                \"\\\\n\\\\tdef \",',\n",
              "  '\"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.KOTLIN:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along method definitions\\n                \"\\\\npublic \",\\n                \"\\\\nprotected \",\\n                \"\\\\nprivate \",\\n                \"\\\\ninternal \",\\n                \"\\\\ncompanion \",\\n                \"\\\\nfun \",\\n                \"\\\\nval \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nwhen \",\\n                \"\\\\ncase \",\\n                \"\\\\nelse \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.JS:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunction \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                \"\\\\nvar \",\\n                \"\\\\nclass \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                \"\\\\ndefault \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.TS:\\n            return [\\n                \"\\\\nenum \",\\n                \"\\\\ninterface \",\\n                \"\\\\nnamespace \",\\n                \"\\\\ntype \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along function definitions\\n                \"\\\\nfunction \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements',\n",
              "  '@staticmethod\\n    def get_separators_for_language(language: Language) -> List[str]:\\n        if language == Language.CPP:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along function definitions\\n                \"\\\\nvoid \",\\n                \"\\\\nint \",\\n                \"\\\\nfloat \",\\n                \"\\\\ndouble \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.GO:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunc \",\\n                \"\\\\nvar \",\\n                \"\\\\nconst \",\\n                \"\\\\ntype \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.JAVA:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along method definitions\\n                \"\\\\npublic \",\\n                \"\\\\nprotected \",\\n                \"\\\\nprivate \",\\n                \"\\\\nstatic \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.KOTLIN:\\n            return [',\n",
              "  '_separator = separator if self._is_separator_regex else re.escape(separator)\\n        splits = _split_text_with_regex(text, _separator, self._keep_separator)\\n\\n        # Now go merging things, recursively splitting longer texts.\\n        _good_splits = []\\n        _separator = \"\" if self._keep_separator else separator\\n        for s in splits:\\n            if self._length_function(s) < self._chunk_size:\\n                _good_splits.append(s)\\n            else:\\n                if _good_splits:\\n                    merged_text = self._merge_splits(_good_splits, _separator)\\n                    final_chunks.extend(merged_text)\\n                    _good_splits = []\\n                if not new_separators:\\n                    final_chunks.append(s)\\n                else:\\n                    other_info = self._split_text(s, new_separators)\\n                    final_chunks.extend(other_info)\\n        if _good_splits:\\n            merged_text = self._merge_splits(_good_splits, _separator)\\n            final_chunks.extend(merged_text)\\n        return final_chunks\\n\\n    def split_text(self, text: str) -> List[str]:\\n        return self._split_text(text, self._separators)\\n\\n    @classmethod\\n    def from_language(\\n        cls, language: Language, **kwargs: Any\\n    ) -> RecursiveCharacterTextSplitter:\\n        separators = cls.get_separators_for_language(language)\\n        return cls(separators=separators, is_separator_regex=True, **kwargs)',\n",
              "  'class RecursiveCharacterTextSplitter(TextSplitter):\\n    \"\"\"Splitting text by recursively look at characters.\\n\\n    Recursively tries to split by different characters to find one\\n    that works.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        separators: Optional[List[str]] = None,\\n        keep_separator: bool = True,\\n        is_separator_regex: bool = False,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(keep_separator=keep_separator, **kwargs)\\n        self._separators = separators or [\"\\\\n\\\\n\", \"\\\\n\", \" \", \"\"]\\n        self._is_separator_regex = is_separator_regex\\n\\n    def _split_text(self, text: str, separators: List[str]) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        final_chunks = []\\n        # Get appropriate separator to use\\n        separator = separators[-1]\\n        new_separators = []\\n        for i, _s in enumerate(separators):\\n            _separator = _s if self._is_separator_regex else re.escape(_s)\\n            if _s == \"\":\\n                separator = _s\\n                break\\n            if re.search(_separator, text):\\n                separator = _s\\n                new_separators = separators[i + 1 :]\\n                break\\n\\n        _separator = separator if self._is_separator_regex else re.escape(separator)\\n        splits = _split_text_with_regex(text, _separator, self._keep_separator)',\n",
              "  'class Language(str, Enum):\\n    \"\"\"Enum of the programming languages.\"\"\"\\n\\n    CPP = \"cpp\"\\n    GO = \"go\"\\n    JAVA = \"java\"\\n    KOTLIN = \"kotlin\"\\n    JS = \"js\"\\n    TS = \"ts\"\\n    PHP = \"php\"\\n    PROTO = \"proto\"\\n    PYTHON = \"python\"\\n    RST = \"rst\"\\n    RUBY = \"ruby\"\\n    RUST = \"rust\"\\n    SCALA = \"scala\"\\n    SWIFT = \"swift\"\\n    MARKDOWN = \"markdown\"\\n    LATEX = \"latex\"\\n    HTML = \"html\"\\n    SOL = \"sol\"\\n    CSHARP = \"csharp\"\\n    COBOL = \"cobol\"',\n",
              "  'def split_text(self, text: str) -> List[str]:\\n        def encode_strip_start_and_stop_token_ids(text: str) -> List[int]:\\n            return self._encode(text)[1:-1]\\n\\n        tokenizer = Tokenizer(\\n            chunk_overlap=self._chunk_overlap,\\n            tokens_per_chunk=self.tokens_per_chunk,\\n            decode=self.tokenizer.decode,\\n            encode=encode_strip_start_and_stop_token_ids,\\n        )\\n\\n        return split_text_on_tokens(text=text, tokenizer=tokenizer)\\n\\n    def count_tokens(self, *, text: str) -> int:\\n        return len(self._encode(text))\\n\\n    _max_length_equal_32_bit_integer: int = 2**32\\n\\n    def _encode(self, text: str) -> List[int]:\\n        token_ids_with_start_and_end_token_ids = self.tokenizer.encode(\\n            text,\\n            max_length=self._max_length_equal_32_bit_integer,\\n            truncation=\"do_not_truncate\",\\n        )\\n        return token_ids_with_start_and_end_token_ids',\n",
              "  'class SentenceTransformersTokenTextSplitter(TextSplitter):\\n    \"\"\"Splitting text to tokens using sentence model tokenizer.\"\"\"\\n\\n    def __init__(\\n        self,\\n        chunk_overlap: int = 50,\\n        model_name: str = \"sentence-transformers/all-mpnet-base-v2\",\\n        tokens_per_chunk: Optional[int] = None,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(**kwargs, chunk_overlap=chunk_overlap)\\n\\n        try:\\n            from sentence_transformers import SentenceTransformer\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import sentence_transformer python package. \"\\n                \"This is needed in order to for SentenceTransformersTokenTextSplitter. \"\\n                \"Please install it with `pip install sentence-transformers`.\"\\n            )\\n\\n        self.model_name = model_name\\n        self._model = SentenceTransformer(self.model_name)\\n        self.tokenizer = self._model.tokenizer\\n        self._initialize_chunk_configuration(tokens_per_chunk=tokens_per_chunk)\\n\\n    def _initialize_chunk_configuration(\\n        self, *, tokens_per_chunk: Optional[int]\\n    ) -> None:\\n        self.maximum_tokens_per_chunk = cast(int, self._model.max_seq_length)\\n\\n        if tokens_per_chunk is None:\\n            self.tokens_per_chunk = self.maximum_tokens_per_chunk\\n        else:\\n            self.tokens_per_chunk = tokens_per_chunk\\n\\n        if self.tokens_per_chunk > self.maximum_tokens_per_chunk:\\n            raise ValueError(\\n                f\"The token limit of the models \\'{self.model_name}\\'\"\\n                f\" is: {self.maximum_tokens_per_chunk}.\"\\n                f\" Argument tokens_per_chunk={self.tokens_per_chunk}\"\\n                f\" > maximum token limit.\"\\n            )\\n\\n    def split_text(self, text: str) -> List[str]:\\n        def encode_strip_start_and_stop_token_ids(text: str) -> List[int]:\\n            return self._encode(text)[1:-1]',\n",
              "  '\"<nav\",\\n                # Head\\n                \"<head\",\\n                \"<style\",\\n                \"<script\",\\n                \"<meta\",\\n                \"<title\",\\n                \"\",\\n            ]\\n        elif language == Language.CSHARP:\\n            return [\\n                \"\\\\ninterface \",\\n                \"\\\\nenum \",\\n                \"\\\\nimplements \",\\n                \"\\\\ndelegate \",\\n                \"\\\\nevent \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nabstract \",\\n                # Split along method definitions\\n                \"\\\\npublic \",\\n                \"\\\\nprotected \",\\n                \"\\\\nprivate \",\\n                \"\\\\nstatic \",\\n                \"\\\\nreturn \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\ncontinue \",\\n                \"\\\\nfor \",\\n                \"\\\\nforeach \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\nbreak \",\\n                \"\\\\ncase \",\\n                \"\\\\nelse \",\\n                # Split by exceptions\\n                \"\\\\ntry \",\\n                \"\\\\nthrow \",\\n                \"\\\\nfinally \",\\n                \"\\\\ncatch \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SOL:\\n            return [\\n                # Split along compiler information definitions\\n                \"\\\\npragma \",\\n                \"\\\\nusing \",\\n                # Split along contract definitions\\n                \"\\\\ncontract \",\\n                \"\\\\ninterface \",\\n                \"\\\\nlibrary \",\\n                # Split along method definitions\\n                \"\\\\nconstructor \",\\n                \"\\\\ntype \",\\n                \"\\\\nfunction \",\\n                \"\\\\nevent \",\\n                \"\\\\nmodifier \",\\n                \"\\\\nerror \",\\n                \"\\\\nstruct \",\\n                \"\\\\nenum \",\\n                # Split along control flow statements',\n",
              "  'class Tokenizer:\\n    \"\"\"Tokenizer data class.\"\"\"\\n\\n    chunk_overlap: int\\n    \"\"\"Overlap in tokens between chunks\"\"\"\\n    tokens_per_chunk: int\\n    \"\"\"Maximum number of tokens per chunk\"\"\"\\n    decode: Callable[[List[int]], str]\\n    \"\"\" Function to decode a list of token ids to a string\"\"\"\\n    encode: Callable[[str], List[int]]\\n    \"\"\" Function to encode a string to a list of token ids\"\"\"\\n\\n\\ndef split_text_on_tokens(*, text: str, tokenizer: Tokenizer) -> List[str]:\\n    \"\"\"Split incoming text and return chunks using tokenizer.\"\"\"\\n    splits: List[str] = []\\n    input_ids = tokenizer.encode(text)\\n    start_idx = 0\\n    cur_idx = min(start_idx + tokenizer.tokens_per_chunk, len(input_ids))\\n    chunk_ids = input_ids[start_idx:cur_idx]\\n    while start_idx < len(input_ids):\\n        splits.append(tokenizer.decode(chunk_ids))\\n        if cur_idx == len(input_ids):\\n            break\\n        start_idx += tokenizer.tokens_per_chunk - tokenizer.chunk_overlap\\n        cur_idx = min(start_idx + tokenizer.tokens_per_chunk, len(input_ids))\\n        chunk_ids = input_ids[start_idx:cur_idx]\\n    return splits',\n",
              "  '# map xhtml namespace prefix\\n        ns_map = {\"h\": \"http://www.w3.org/1999/xhtml\"}\\n\\n        # build list of elements from DOM\\n        elements = []\\n        for element in result_dom.findall(\"*//*\", ns_map):\\n            if element.findall(\"*[@class=\\'headers\\']\") or element.findall(\\n                \"*[@class=\\'chunk\\']\"\\n            ):\\n                elements.append(\\n                    ElementType(\\n                        url=file,\\n                        xpath=\"\".join(\\n                            [\\n                                node.text\\n                                for node in element.findall(\"*[@class=\\'xpath\\']\", ns_map)\\n                            ]\\n                        ),\\n                        content=\"\".join(\\n                            [\\n                                node.text\\n                                for node in element.findall(\"*[@class=\\'chunk\\']\", ns_map)\\n                            ]\\n                        ),\\n                        metadata={\\n                            # Add text of specified headers to metadata using header\\n                            # mapping.\\n                            header_mapping[node.tag]: node.text\\n                            for node in filter(\\n                                lambda x: x.tag in header_filter,\\n                                element.findall(\"*[@class=\\'headers\\']/*\", ns_map),\\n                            )\\n                        },\\n                    )\\n                )\\n\\n        if not self.return_each_element:\\n            return self.aggregate_elements_to_chunks(elements)\\n        else:\\n            return [\\n                Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n                for chunk in elements\\n            ]\\n\\n\\n# should be in newer Python versions (3.10+)\\n# @dataclass(frozen=True, kw_only=True, slots=True)\\n@dataclass(frozen=True)',\n",
              "  'return [\\n            Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n            for chunk in aggregated_chunks\\n        ]\\n\\n    def split_text_from_url(self, url: str) -> List[Document]:\\n        \"\"\"Split HTML from web URL\\n\\n        Args:\\n            url: web URL\\n        \"\"\"\\n        r = requests.get(url)\\n        return self.split_text_from_file(BytesIO(r.content))\\n\\n    def split_text(self, text: str) -> List[Document]:\\n        \"\"\"Split HTML text string\\n\\n        Args:\\n            text: HTML text\\n        \"\"\"\\n        return self.split_text_from_file(StringIO(text))\\n\\n    def split_text_from_file(self, file: Any) -> List[Document]:\\n        \"\"\"Split HTML file\\n\\n        Args:\\n            file: HTML file\\n        \"\"\"\\n        try:\\n            from lxml import etree\\n        except ImportError as e:\\n            raise ImportError(\\n                \"Unable to import lxml, please install with `pip install lxml`.\"\\n            ) from e\\n        # use lxml library to parse html document and return xml ElementTree\\n        parser = etree.HTMLParser()\\n        tree = etree.parse(file, parser)\\n\\n        # document transformation for \"structure-aware\" chunking is handled with xsl.\\n        # see comments in html_chunks_with_headers.xslt for more detailed information.\\n        xslt_path = (\\n            pathlib.Path(__file__).parent\\n            / \"document_transformers/xsl/html_chunks_with_headers.xslt\"\\n        )\\n        xslt_tree = etree.parse(xslt_path)\\n        transform = etree.XSLT(xslt_tree)\\n        result = transform(tree)\\n        result_dom = etree.fromstring(str(result))\\n\\n        # create filter and mapping for header metadata\\n        header_filter = [header[0] for header in self.headers_to_split_on]\\n        header_mapping = dict(self.headers_to_split_on)\\n\\n        # map xhtml namespace prefix\\n        ns_map = {\"h\": \"http://www.w3.org/1999/xhtml\"}',\n",
              "  'class HTMLHeaderTextSplitter:\\n    \"\"\"\\n    Splitting HTML files based on specified headers.\\n    Requires lxml package.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        headers_to_split_on: List[Tuple[str, str]],\\n        return_each_element: bool = False,\\n    ):\\n        \"\"\"Create a new HTMLHeaderTextSplitter.\\n\\n        Args:\\n            headers_to_split_on: list of tuples of headers we want to track mapped to\\n                (arbitrary) keys for metadata. Allowed header values: h1, h2, h3, h4,\\n                h5, h6 e.g. [(\"h1\", \"Header 1\"), (\"h2\", \"Header 2)].\\n            return_each_element: Return each element w/ associated headers.\\n        \"\"\"\\n        # Output element-by-element or aggregated into chunks w/ common headers\\n        self.return_each_element = return_each_element\\n        self.headers_to_split_on = sorted(headers_to_split_on)\\n\\n    def aggregate_elements_to_chunks(\\n        self, elements: List[ElementType]\\n    ) -> List[Document]:\\n        \"\"\"Combine elements with common metadata into chunks\\n\\n        Args:\\n            elements: HTML element content with associated identifying info and metadata\\n        \"\"\"\\n        aggregated_chunks: List[ElementType] = []\\n\\n        for element in elements:\\n            if (\\n                aggregated_chunks\\n                and aggregated_chunks[-1][\"metadata\"] == element[\"metadata\"]\\n            ):\\n                # If the last element in the aggregated list\\n                # has the same metadata as the current element,\\n                # append the current content to the last element\\'s content\\n                aggregated_chunks[-1][\"content\"] += \"  \\\\n\" + element[\"content\"]\\n            else:\\n                # Otherwise, append the current element to the aggregated list\\n                aggregated_chunks.append(element)\\n\\n        return [\\n            Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n            for chunk in aggregated_chunks\\n        ]',\n",
              "  'class ElementType(TypedDict):\\n    \"\"\"Element type as typed dict.\"\"\"\\n\\n    url: str\\n    xpath: str\\n    content: str\\n    metadata: Dict[str, str]',\n",
              "  'if current_content:\\n            lines_with_metadata.append(\\n                {\"content\": \"\\\\n\".join(current_content), \"metadata\": current_metadata}\\n            )\\n\\n        # lines_with_metadata has each line with associated header metadata\\n        # aggregate these into chunks based on common metadata\\n        if not self.return_each_line:\\n            return self.aggregate_lines_to_chunks(lines_with_metadata)\\n        else:\\n            return [\\n                Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n                for chunk in lines_with_metadata\\n            ]',\n",
              "  '# Push the current header to the stack\\n                        header: HeaderType = {\\n                            \"level\": current_header_level,\\n                            \"name\": name,\\n                            \"data\": stripped_line[len(sep) :].strip(),\\n                        }\\n                        header_stack.append(header)\\n                        # Update initial_metadata with the current header\\n                        initial_metadata[name] = header[\"data\"]\\n\\n                    # Add the previous line to the lines_with_metadata\\n                    # only if current_content is not empty\\n                    if current_content:\\n                        lines_with_metadata.append(\\n                            {\\n                                \"content\": \"\\\\n\".join(current_content),\\n                                \"metadata\": current_metadata.copy(),\\n                            }\\n                        )\\n                        current_content.clear()\\n\\n                    break\\n            else:\\n                if stripped_line:\\n                    current_content.append(stripped_line)\\n                elif current_content:\\n                    lines_with_metadata.append(\\n                        {\\n                            \"content\": \"\\\\n\".join(current_content),\\n                            \"metadata\": current_metadata.copy(),\\n                        }\\n                    )\\n                    current_content.clear()\\n\\n            current_metadata = initial_metadata.copy()\\n\\n        if current_content:\\n            lines_with_metadata.append(\\n                {\"content\": \"\\\\n\".join(current_content), \"metadata\": current_metadata}\\n            )',\n",
              "  'if in_code_block:\\n                current_content.append(stripped_line)\\n                continue\\n\\n            # Check each line against each of the header types (e.g., #, ##)\\n            for sep, name in self.headers_to_split_on:\\n                # Check if line starts with a header that we intend to split on\\n                if stripped_line.startswith(sep) and (\\n                    # Header with no text OR header is followed by space\\n                    # Both are valid conditions that sep is being used a header\\n                    len(stripped_line) == len(sep) or stripped_line[len(sep)] == \" \"\\n                ):\\n                    # Ensure we are tracking the header as metadata\\n                    if name is not None:\\n                        # Get the current header level\\n                        current_header_level = sep.count(\"#\")\\n\\n                        # Pop out headers of lower or same level from the stack\\n                        while (\\n                            header_stack\\n                            and header_stack[-1][\"level\"] >= current_header_level\\n                        ):\\n                            # We have encountered a new header\\n                            # at the same or higher level\\n                            popped_header = header_stack.pop()\\n                            # Clear the metadata for the\\n                            # popped header in initial_metadata\\n                            if popped_header[\"name\"] in initial_metadata:\\n                                initial_metadata.pop(popped_header[\"name\"])',\n",
              "  'if issubclass(cls, TokenTextSplitter):\\n            extra_kwargs = {\\n                \"encoding_name\": encoding_name,\\n                \"model_name\": model_name,\\n                \"allowed_special\": allowed_special,\\n                \"disallowed_special\": disallowed_special,\\n            }\\n            kwargs = {**kwargs, **extra_kwargs}\\n\\n        return cls(length_function=_tiktoken_encoder, **kwargs)\\n\\n    def transform_documents(\\n        self, documents: Sequence[Document], **kwargs: Any\\n    ) -> Sequence[Document]:\\n        \"\"\"Transform sequence of documents by splitting them.\"\"\"\\n        return self.split_documents(list(documents))\\n\\n    async def atransform_documents(\\n        self, documents: Sequence[Document], **kwargs: Any\\n    ) -> Sequence[Document]:\\n        \"\"\"Asynchronously transform a sequence of documents by splitting them.\"\"\"\\n        return await asyncio.get_running_loop().run_in_executor(\\n            None, partial(self.transform_documents, **kwargs), documents\\n        )',\n",
              "  'return [\\n            Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n            for chunk in aggregated_chunks\\n        ]\\n\\n    def split_text_from_url(self, url: str) -> List[Document]:\\n        \"\"\"Split HTML from web URL\\n\\n        Args:\\n            url: web URL\\n        \"\"\"\\n        r = requests.get(url)\\n        return self.split_text_from_file(BytesIO(r.content))\\n\\n    def split_text(self, text: str) -> List[Document]:\\n        \"\"\"Split HTML text string\\n\\n        Args:\\n            text: HTML text\\n        \"\"\"\\n        return self.split_text_from_file(StringIO(text))\\n\\n    def split_text_from_file(self, file: Any) -> List[Document]:\\n        \"\"\"Split HTML file\\n\\n        Args:\\n            file: HTML file\\n        \"\"\"\\n        try:\\n            from lxml import etree\\n        except ImportError as e:\\n            raise ImportError(\\n                \"Unable to import lxml, please install with `pip install lxml`.\"\\n            ) from e\\n        # use lxml library to parse html document and return xml ElementTree\\n        parser = etree.HTMLParser()\\n        tree = etree.parse(file, parser)\\n\\n        # document transformation for \"structure-aware\" chunking is handled with xsl.\\n        # see comments in html_chunks_with_headers.xslt for more detailed information.\\n        xslt_path = (\\n            pathlib.Path(__file__).parent\\n            / \"document_transformers/xsl/html_chunks_with_headers.xslt\"\\n        )\\n        xslt_tree = etree.parse(xslt_path)\\n        transform = etree.XSLT(xslt_tree)\\n        result = transform(tree)\\n        result_dom = etree.fromstring(str(result))\\n\\n        # create filter and mapping for header metadata\\n        header_filter = [header[0] for header in self.headers_to_split_on]\\n        header_mapping = dict(self.headers_to_split_on)\\n\\n        # map xhtml namespace prefix\\n        ns_map = {\"h\": \"http://www.w3.org/1999/xhtml\"}',\n",
              "  'class HTMLHeaderTextSplitter:\\n    \"\"\"\\n    Splitting HTML files based on specified headers.\\n    Requires lxml package.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        headers_to_split_on: List[Tuple[str, str]],\\n        return_each_element: bool = False,\\n    ):\\n        \"\"\"Create a new HTMLHeaderTextSplitter.\\n\\n        Args:\\n            headers_to_split_on: list of tuples of headers we want to track mapped to\\n                (arbitrary) keys for metadata. Allowed header values: h1, h2, h3, h4,\\n                h5, h6 e.g. [(\"h1\", \"Header 1\"), (\"h2\", \"Header 2)].\\n            return_each_element: Return each element w/ associated headers.\\n        \"\"\"\\n        # Output element-by-element or aggregated into chunks w/ common headers\\n        self.return_each_element = return_each_element\\n        self.headers_to_split_on = sorted(headers_to_split_on)\\n\\n    def aggregate_elements_to_chunks(\\n        self, elements: List[ElementType]\\n    ) -> List[Document]:\\n        \"\"\"Combine elements with common metadata into chunks\\n\\n        Args:\\n            elements: HTML element content with associated identifying info and metadata\\n        \"\"\"\\n        aggregated_chunks: List[ElementType] = []\\n\\n        for element in elements:\\n            if (\\n                aggregated_chunks\\n                and aggregated_chunks[-1][\"metadata\"] == element[\"metadata\"]\\n            ):\\n                # If the last element in the aggregated list\\n                # has the same metadata as the current element,\\n                # append the current content to the last element\\'s content\\n                aggregated_chunks[-1][\"content\"] += \"  \\\\n\" + element[\"content\"]\\n            else:\\n                # Otherwise, append the current element to the aggregated list\\n                aggregated_chunks.append(element)\\n\\n        return [\\n            Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n            for chunk in aggregated_chunks\\n        ]',\n",
              "  'class ElementType(TypedDict):\\n    \"\"\"Element type as typed dict.\"\"\"\\n\\n    url: str\\n    xpath: str\\n    content: str\\n    metadata: Dict[str, str]',\n",
              "  'if current_content:\\n            lines_with_metadata.append(\\n                {\"content\": \"\\\\n\".join(current_content), \"metadata\": current_metadata}\\n            )\\n\\n        # lines_with_metadata has each line with associated header metadata\\n        # aggregate these into chunks based on common metadata\\n        if not self.return_each_line:\\n            return self.aggregate_lines_to_chunks(lines_with_metadata)\\n        else:\\n            return [\\n                Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n                for chunk in lines_with_metadata\\n            ]',\n",
              "  '# Push the current header to the stack\\n                        header: HeaderType = {\\n                            \"level\": current_header_level,\\n                            \"name\": name,\\n                            \"data\": stripped_line[len(sep) :].strip(),\\n                        }\\n                        header_stack.append(header)\\n                        # Update initial_metadata with the current header\\n                        initial_metadata[name] = header[\"data\"]\\n\\n                    # Add the previous line to the lines_with_metadata\\n                    # only if current_content is not empty\\n                    if current_content:\\n                        lines_with_metadata.append(\\n                            {\\n                                \"content\": \"\\\\n\".join(current_content),\\n                                \"metadata\": current_metadata.copy(),\\n                            }\\n                        )\\n                        current_content.clear()\\n\\n                    break\\n            else:\\n                if stripped_line:\\n                    current_content.append(stripped_line)\\n                elif current_content:\\n                    lines_with_metadata.append(\\n                        {\\n                            \"content\": \"\\\\n\".join(current_content),\\n                            \"metadata\": current_metadata.copy(),\\n                        }\\n                    )\\n                    current_content.clear()\\n\\n            current_metadata = initial_metadata.copy()\\n\\n        if current_content:\\n            lines_with_metadata.append(\\n                {\"content\": \"\\\\n\".join(current_content), \"metadata\": current_metadata}\\n            )',\n",
              "  'if in_code_block:\\n                current_content.append(stripped_line)\\n                continue\\n\\n            # Check each line against each of the header types (e.g., #, ##)\\n            for sep, name in self.headers_to_split_on:\\n                # Check if line starts with a header that we intend to split on\\n                if stripped_line.startswith(sep) and (\\n                    # Header with no text OR header is followed by space\\n                    # Both are valid conditions that sep is being used a header\\n                    len(stripped_line) == len(sep) or stripped_line[len(sep)] == \" \"\\n                ):\\n                    # Ensure we are tracking the header as metadata\\n                    if name is not None:\\n                        # Get the current header level\\n                        current_header_level = sep.count(\"#\")\\n\\n                        # Pop out headers of lower or same level from the stack\\n                        while (\\n                            header_stack\\n                            and header_stack[-1][\"level\"] >= current_header_level\\n                        ):\\n                            # We have encountered a new header\\n                            # at the same or higher level\\n                            popped_header = header_stack.pop()\\n                            # Clear the metadata for the\\n                            # popped header in initial_metadata\\n                            if popped_header[\"name\"] in initial_metadata:\\n                                initial_metadata.pop(popped_header[\"name\"])',\n",
              "  'def split_text(self, text: str) -> List[Document]:\\n        \"\"\"Split markdown file\\n        Args:\\n            text: Markdown file\"\"\"\\n\\n        # Split the input text by newline character (\"\\\\n\").\\n        lines = text.split(\"\\\\n\")\\n        # Final output\\n        lines_with_metadata: List[LineType] = []\\n        # Content and metadata of the chunk currently being processed\\n        current_content: List[str] = []\\n        current_metadata: Dict[str, str] = {}\\n        # Keep track of the nested header structure\\n        # header_stack: List[Dict[str, Union[int, str]]] = []\\n        header_stack: List[HeaderType] = []\\n        initial_metadata: Dict[str, str] = {}\\n\\n        in_code_block = False\\n        opening_fence = \"\"\\n\\n        for line in lines:\\n            stripped_line = line.strip()\\n\\n            if not in_code_block:\\n                # Exclude inline code spans\\n                if stripped_line.startswith(\"```\") and stripped_line.count(\"```\") == 1:\\n                    in_code_block = True\\n                    opening_fence = \"```\"\\n                elif stripped_line.startswith(\"~~~\"):\\n                    in_code_block = True\\n                    opening_fence = \"~~~\"\\n            else:\\n                if stripped_line.startswith(opening_fence):\\n                    in_code_block = False\\n                    opening_fence = \"\"\\n\\n            if in_code_block:\\n                current_content.append(stripped_line)\\n                continue',\n",
              "  'class MarkdownHeaderTextSplitter:\\n    \"\"\"Splitting markdown files based on specified headers.\"\"\"\\n\\n    def __init__(\\n        self, headers_to_split_on: List[Tuple[str, str]], return_each_line: bool = False\\n    ):\\n        \"\"\"Create a new MarkdownHeaderTextSplitter.\\n\\n        Args:\\n            headers_to_split_on: Headers we want to track\\n            return_each_line: Return each line w/ associated headers\\n        \"\"\"\\n        # Output line-by-line or aggregated into chunks w/ common headers\\n        self.return_each_line = return_each_line\\n        # Given the headers we want to split on,\\n        # (e.g., \"#, ##, etc\") order by length\\n        self.headers_to_split_on = sorted(\\n            headers_to_split_on, key=lambda split: len(split[0]), reverse=True\\n        )\\n\\n    def aggregate_lines_to_chunks(self, lines: List[LineType]) -> List[Document]:\\n        \"\"\"Combine lines with common metadata into chunks\\n        Args:\\n            lines: Line of text / associated header metadata\\n        \"\"\"\\n        aggregated_chunks: List[LineType] = []\\n\\n        for line in lines:\\n            if (\\n                aggregated_chunks\\n                and aggregated_chunks[-1][\"metadata\"] == line[\"metadata\"]\\n            ):\\n                # If the last line in the aggregated list\\n                # has the same metadata as the current line,\\n                # append the current content to the last lines\\'s content\\n                aggregated_chunks[-1][\"content\"] += \"  \\\\n\" + line[\"content\"]\\n            else:\\n                # Otherwise, append the current line to the aggregated list\\n                aggregated_chunks.append(line)\\n\\n        return [\\n            Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n            for chunk in aggregated_chunks\\n        ]\\n\\n    def split_text(self, text: str) -> List[Document]:\\n        \"\"\"Split markdown file\\n        Args:\\n            text: Markdown file\"\"\"',\n",
              "  'class CharacterTextSplitter(TextSplitter):\\n    \"\"\"Splitting text that looks at characters.\"\"\"\\n\\n    def __init__(\\n        self, separator: str = \"\\\\n\\\\n\", is_separator_regex: bool = False, **kwargs: Any\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(**kwargs)\\n        self._separator = separator\\n        self._is_separator_regex = is_separator_regex\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        # First we naively split the large input into a bunch of smaller ones.\\n        separator = (\\n            self._separator if self._is_separator_regex else re.escape(self._separator)\\n        )\\n        splits = _split_text_with_regex(text, separator, self._keep_separator)\\n        _separator = \"\" if self._keep_separator else self._separator\\n        return self._merge_splits(splits, _separator)\\n\\n\\nclass LineType(TypedDict):\\n    \"\"\"Line type as typed dict.\"\"\"\\n\\n    metadata: Dict[str, str]\\n    content: str\\n\\n\\nclass HeaderType(TypedDict):\\n    \"\"\"Header type as typed dict.\"\"\"\\n\\n    level: int\\n    name: str\\n    data: str',\n",
              "  '# map xhtml namespace prefix\\n        ns_map = {\"h\": \"http://www.w3.org/1999/xhtml\"}\\n\\n        # build list of elements from DOM\\n        elements = []\\n        for element in result_dom.findall(\"*//*\", ns_map):\\n            if element.findall(\"*[@class=\\'headers\\']\") or element.findall(\\n                \"*[@class=\\'chunk\\']\"\\n            ):\\n                elements.append(\\n                    ElementType(\\n                        url=file,\\n                        xpath=\"\".join(\\n                            [\\n                                node.text\\n                                for node in element.findall(\"*[@class=\\'xpath\\']\", ns_map)\\n                            ]\\n                        ),\\n                        content=\"\".join(\\n                            [\\n                                node.text\\n                                for node in element.findall(\"*[@class=\\'chunk\\']\", ns_map)\\n                            ]\\n                        ),\\n                        metadata={\\n                            # Add text of specified headers to metadata using header\\n                            # mapping.\\n                            header_mapping[node.tag]: node.text\\n                            for node in filter(\\n                                lambda x: x.tag in header_filter,\\n                                element.findall(\"*[@class=\\'headers\\']/*\", ns_map),\\n                            )\\n                        },\\n                    )\\n                )\\n\\n        if not self.return_each_element:\\n            return self.aggregate_elements_to_chunks(elements)\\n        else:\\n            return [\\n                Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n                for chunk in elements\\n            ]\\n\\n\\n# should be in newer Python versions (3.10+)\\n# @dataclass(frozen=True, kw_only=True, slots=True)\\n@dataclass(frozen=True)',\n",
              "  'if not isinstance(tokenizer, PreTrainedTokenizerBase):\\n                raise ValueError(\\n                    \"Tokenizer received was not an instance of PreTrainedTokenizerBase\"\\n                )\\n\\n            def _huggingface_tokenizer_length(text: str) -> int:\\n                return len(tokenizer.encode(text))\\n\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import transformers python package. \"\\n                \"Please install it with `pip install transformers`.\"\\n            )\\n        return cls(length_function=_huggingface_tokenizer_length, **kwargs)\\n\\n    @classmethod\\n    def from_tiktoken_encoder(\\n        cls: Type[TS],\\n        encoding_name: str = \"gpt2\",\\n        model_name: Optional[str] = None,\\n        allowed_special: Union[Literal[\"all\"], AbstractSet[str]] = set(),\\n        disallowed_special: Union[Literal[\"all\"], Collection[str]] = \"all\",\\n        **kwargs: Any,\\n    ) -> TS:\\n        \"\"\"Text splitter that uses tiktoken encoder to count length.\"\"\"\\n        try:\\n            import tiktoken\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import tiktoken python package. \"\\n                \"This is needed in order to calculate max_tokens_for_prompt. \"\\n                \"Please install it with `pip install tiktoken`.\"\\n            )\\n\\n        if model_name is not None:\\n            enc = tiktoken.encoding_for_model(model_name)\\n        else:\\n            enc = tiktoken.get_encoding(encoding_name)\\n\\n        def _tiktoken_encoder(text: str) -> int:\\n            return len(\\n                enc.encode(\\n                    text,\\n                    allowed_special=allowed_special,\\n                    disallowed_special=disallowed_special,\\n                )\\n            )',\n",
              "  'docs = []\\n        current_doc: List[str] = []\\n        total = 0\\n        for d in splits:\\n            _len = self._length_function(d)\\n            if (\\n                total + _len + (separator_len if len(current_doc) > 0 else 0)\\n                > self._chunk_size\\n            ):\\n                if total > self._chunk_size:\\n                    logger.warning(\\n                        f\"Created a chunk of size {total}, \"\\n                        f\"which is longer than the specified {self._chunk_size}\"\\n                    )\\n                if len(current_doc) > 0:\\n                    doc = self._join_docs(current_doc, separator)\\n                    if doc is not None:\\n                        docs.append(doc)\\n                    # Keep on popping if:\\n                    # - we have a larger chunk than in the chunk overlap\\n                    # - or if we still have any chunks and the length is long\\n                    while total > self._chunk_overlap or (\\n                        total + _len + (separator_len if len(current_doc) > 0 else 0)\\n                        > self._chunk_size\\n                        and total > 0\\n                    ):\\n                        total -= self._length_function(current_doc[0]) + (\\n                            separator_len if len(current_doc) > 1 else 0\\n                        )\\n                        current_doc = current_doc[1:]\\n            current_doc.append(d)\\n            total += _len + (separator_len if len(current_doc) > 1 else 0)\\n        doc = self._join_docs(current_doc, separator)\\n        if doc is not None:\\n            docs.append(doc)\\n        return docs\\n\\n    @classmethod\\n    def from_huggingface_tokenizer(cls, tokenizer: Any, **kwargs: Any) -> TextSplitter:\\n        \"\"\"Text splitter that uses HuggingFace tokenizer to count length.\"\"\"\\n        try:\\n            from transformers import PreTrainedTokenizerBase',\n",
              "  '@abstractmethod\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split text into multiple components.\"\"\"\\n\\n    def create_documents(\\n        self, texts: List[str], metadatas: Optional[List[dict]] = None\\n    ) -> List[Document]:\\n        \"\"\"Create documents from a list of texts.\"\"\"\\n        _metadatas = metadatas or [{}] * len(texts)\\n        documents = []\\n        for i, text in enumerate(texts):\\n            index = -1\\n            for chunk in self.split_text(text):\\n                metadata = copy.deepcopy(_metadatas[i])\\n                if self._add_start_index:\\n                    index = text.find(chunk, index + 1)\\n                    metadata[\"start_index\"] = index\\n                new_doc = Document(page_content=chunk, metadata=metadata)\\n                documents.append(new_doc)\\n        return documents\\n\\n    def split_documents(self, documents: Iterable[Document]) -> List[Document]:\\n        \"\"\"Split documents.\"\"\"\\n        texts, metadatas = [], []\\n        for doc in documents:\\n            texts.append(doc.page_content)\\n            metadatas.append(doc.metadata)\\n        return self.create_documents(texts, metadatas=metadatas)\\n\\n    def _join_docs(self, docs: List[str], separator: str) -> Optional[str]:\\n        text = separator.join(docs)\\n        if self._strip_whitespace:\\n            text = text.strip()\\n        if text == \"\":\\n            return None\\n        else:\\n            return text\\n\\n    def _merge_splits(self, splits: Iterable[str], separator: str) -> List[str]:\\n        # We now want to combine these smaller pieces into medium size\\n        # chunks to send to the LLM.\\n        separator_len = self._length_function(separator)',\n",
              "  'class TextSplitter(BaseDocumentTransformer, ABC):\\n    \"\"\"Interface for splitting text into chunks.\"\"\"\\n\\n    def __init__(\\n        self,\\n        chunk_size: int = 4000,\\n        chunk_overlap: int = 200,\\n        length_function: Callable[[str], int] = len,\\n        keep_separator: bool = False,\\n        add_start_index: bool = False,\\n        strip_whitespace: bool = True,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\\n\\n        Args:\\n            chunk_size: Maximum size of chunks to return\\n            chunk_overlap: Overlap in characters between chunks\\n            length_function: Function that measures the length of given chunks\\n            keep_separator: Whether to keep the separator in the chunks\\n            add_start_index: If `True`, includes chunk\\'s start index in metadata\\n            strip_whitespace: If `True`, strips whitespace from the start and end of\\n                              every document\\n        \"\"\"\\n        if chunk_overlap > chunk_size:\\n            raise ValueError(\\n                f\"Got a larger chunk overlap ({chunk_overlap}) than chunk size \"\\n                f\"({chunk_size}), should be smaller.\"\\n            )\\n        self._chunk_size = chunk_size\\n        self._chunk_overlap = chunk_overlap\\n        self._length_function = length_function\\n        self._keep_separator = keep_separator\\n        self._add_start_index = add_start_index\\n        self._strip_whitespace = strip_whitespace\\n\\n    @abstractmethod\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split text into multiple components.\"\"\"',\n",
              "  'def _split_text_with_regex(\\n    text: str, separator: str, keep_separator: bool\\n) -> List[str]:\\n    # Now that we have the separator, split the text\\n    if separator:\\n        if keep_separator:\\n            # The parentheses in the pattern keep the delimiters in the result.\\n            _splits = re.split(f\"({separator})\", text)\\n            splits = [_splits[i] + _splits[i + 1] for i in range(1, len(_splits), 2)]\\n            if len(_splits) % 2 == 0:\\n                splits += _splits[-1:]\\n            splits = [_splits[0]] + splits\\n        else:\\n            splits = re.split(separator, text)\\n    else:\\n        splits = list(text)\\n    return [s for s in splits if s != \"\"]',\n",
              "  '\"\"\"**Text Splitters** are classes for splitting text.\\n\\n\\n**Class hierarchy:**\\n\\n.. code-block::\\n\\n    BaseDocumentTransformer --> TextSplitter --> <name>TextSplitter  # Example: CharacterTextSplitter\\n                                                 RecursiveCharacterTextSplitter -->  <name>TextSplitter\\n\\nNote: **MarkdownHeaderTextSplitter** and **HTMLHeaderTextSplitter do not derive from TextSplitter.\\n\\n\\n**Main helpers:**\\n\\n.. code-block::\\n\\n    Document, Tokenizer, Language, LineType, HeaderType\\n\\n\"\"\"  # noqa: E501\\n\\nfrom __future__ import annotations\\n\\nimport asyncio\\nimport copy\\nimport logging\\nimport pathlib\\nimport re\\nfrom abc import ABC, abstractmethod\\nfrom dataclasses import dataclass\\nfrom enum import Enum\\nfrom functools import partial\\nfrom io import BytesIO, StringIO\\nfrom typing import (\\n    AbstractSet,\\n    Any,\\n    Callable,\\n    Collection,\\n    Dict,\\n    Iterable,\\n    List,\\n    Literal,\\n    Optional,\\n    Sequence,\\n    Tuple,\\n    Type,\\n    TypedDict,\\n    TypeVar,\\n    Union,\\n    cast,\\n)\\n\\nimport requests\\nfrom langchain_core.documents import BaseDocumentTransformer, Document\\n\\nlogger = logging.getLogger(__name__)\\n\\nTS = TypeVar(\"TS\", bound=\"TextSplitter\")\\n\\n\\ndef _make_spacy_pipeline_for_splitting(\\n    pipeline: str, *, max_length: int = 1_000_000\\n) -> Any:  # avoid importing spacy\\n    try:\\n        import spacy\\n    except ImportError:\\n        raise ImportError(\\n            \"Spacy is not installed, please install it with `pip install spacy`.\"\\n        )\\n    if pipeline == \"sentencizer\":\\n        from spacy.lang.en import English\\n\\n        sentencizer = English()\\n        sentencizer.add_pipe(\"sentencizer\")\\n    else:\\n        sentencizer = spacy.load(pipeline, exclude=[\"ner\", \"tagger\"])\\n        sentencizer.max_length = max_length\\n    return sentencizer',\n",
              "  'class PythonCodeTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Python syntax.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a PythonCodeTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.PYTHON)\\n        super().__init__(separators=separators, **kwargs)\\n\\n\\nclass MarkdownTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Markdown-formatted headings.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a MarkdownTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.MARKDOWN)\\n        super().__init__(separators=separators, **kwargs)\\n\\n\\nclass LatexTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Latex-formatted layout elements.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a LatexTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.LATEX)\\n        super().__init__(separators=separators, **kwargs)',\n",
              "  'class NLTKTextSplitter(TextSplitter):\\n    \"\"\"Splitting text using NLTK package.\"\"\"\\n\\n    def __init__(\\n        self, separator: str = \"\\\\n\\\\n\", language: str = \"english\", **kwargs: Any\\n    ) -> None:\\n        \"\"\"Initialize the NLTK splitter.\"\"\"\\n        super().__init__(**kwargs)\\n        try:\\n            from nltk.tokenize import sent_tokenize\\n\\n            self._tokenizer = sent_tokenize\\n        except ImportError:\\n            raise ImportError(\\n                \"NLTK is not installed, please install it with `pip install nltk`.\"\\n            )\\n        self._separator = separator\\n        self._language = language\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        # First we naively split the large input into a bunch of smaller ones.\\n        splits = self._tokenizer(text, language=self._language)\\n        return self._merge_splits(splits, self._separator)\\n\\n\\nclass SpacyTextSplitter(TextSplitter):\\n    \"\"\"Splitting text using Spacy package.\\n\\n\\n    Per default, Spacy\\'s `en_core_web_sm` model is used and\\n    its default max_length is 1000000 (it is the length of maximum character\\n    this model takes which can be increased for large files). For a faster, but\\n    potentially less accurate splitting, you can use `pipeline=\\'sentencizer\\'`.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        separator: str = \"\\\\n\\\\n\",\\n        pipeline: str = \"en_core_web_sm\",\\n        max_length: int = 1_000_000,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Initialize the spacy text splitter.\"\"\"\\n        super().__init__(**kwargs)\\n        self._tokenizer = _make_spacy_pipeline_for_splitting(\\n            pipeline, max_length=max_length\\n        )\\n        self._separator = separator\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        splits = (s.text for s in self._tokenizer(text).sents)\\n        return self._merge_splits(splits, self._separator)\\n\\n\\n# For backwards compatibility',\n",
              "  '\"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.KOTLIN:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along method definitions\\n                \"\\\\npublic \",\\n                \"\\\\nprotected \",\\n                \"\\\\nprivate \",\\n                \"\\\\ninternal \",\\n                \"\\\\ncompanion \",\\n                \"\\\\nfun \",\\n                \"\\\\nval \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nwhen \",\\n                \"\\\\ncase \",\\n                \"\\\\nelse \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.JS:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunction \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                \"\\\\nvar \",\\n                \"\\\\nclass \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                \"\\\\ndefault \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.TS:\\n            return [\\n                \"\\\\nenum \",\\n                \"\\\\ninterface \",\\n                \"\\\\nnamespace \",\\n                \"\\\\ntype \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along function definitions\\n                \"\\\\nfunction \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements',\n",
              "  'class PythonCodeTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Python syntax.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a PythonCodeTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.PYTHON)\\n        super().__init__(separators=separators, **kwargs)\\n\\n\\nclass MarkdownTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Markdown-formatted headings.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a MarkdownTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.MARKDOWN)\\n        super().__init__(separators=separators, **kwargs)\\n\\n\\nclass LatexTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Latex-formatted layout elements.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a LatexTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.LATEX)\\n        super().__init__(separators=separators, **kwargs)',\n",
              "  'class NLTKTextSplitter(TextSplitter):\\n    \"\"\"Splitting text using NLTK package.\"\"\"\\n\\n    def __init__(\\n        self, separator: str = \"\\\\n\\\\n\", language: str = \"english\", **kwargs: Any\\n    ) -> None:\\n        \"\"\"Initialize the NLTK splitter.\"\"\"\\n        super().__init__(**kwargs)\\n        try:\\n            from nltk.tokenize import sent_tokenize\\n\\n            self._tokenizer = sent_tokenize\\n        except ImportError:\\n            raise ImportError(\\n                \"NLTK is not installed, please install it with `pip install nltk`.\"\\n            )\\n        self._separator = separator\\n        self._language = language\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        # First we naively split the large input into a bunch of smaller ones.\\n        splits = self._tokenizer(text, language=self._language)\\n        return self._merge_splits(splits, self._separator)\\n\\n\\nclass SpacyTextSplitter(TextSplitter):\\n    \"\"\"Splitting text using Spacy package.\\n\\n\\n    Per default, Spacy\\'s `en_core_web_sm` model is used and\\n    its default max_length is 1000000 (it is the length of maximum character\\n    this model takes which can be increased for large files). For a faster, but\\n    potentially less accurate splitting, you can use `pipeline=\\'sentencizer\\'`.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        separator: str = \"\\\\n\\\\n\",\\n        pipeline: str = \"en_core_web_sm\",\\n        max_length: int = 1_000_000,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Initialize the spacy text splitter.\"\"\"\\n        super().__init__(**kwargs)\\n        self._tokenizer = _make_spacy_pipeline_for_splitting(\\n            pipeline, max_length=max_length\\n        )\\n        self._separator = separator\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        splits = (s.text for s in self._tokenizer(text).sents)\\n        return self._merge_splits(splits, self._separator)\\n\\n\\n# For backwards compatibility',\n",
              "  'else:\\n            raise ValueError(\\n                f\"Language {language} is not supported! \"\\n                f\"Please choose from {list(Language)}\"\\n            )',\n",
              "  '\"\\\\nevent \",\\n                \"\\\\nmodifier \",\\n                \"\\\\nerror \",\\n                \"\\\\nstruct \",\\n                \"\\\\nenum \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo while \",\\n                \"\\\\nassembly \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.COBOL:\\n            return [\\n                # Split along divisions\\n                \"\\\\nIDENTIFICATION DIVISION.\",\\n                \"\\\\nENVIRONMENT DIVISION.\",\\n                \"\\\\nDATA DIVISION.\",\\n                \"\\\\nPROCEDURE DIVISION.\",\\n                # Split along sections within DATA DIVISION\\n                \"\\\\nWORKING-STORAGE SECTION.\",\\n                \"\\\\nLINKAGE SECTION.\",\\n                \"\\\\nFILE SECTION.\",\\n                # Split along sections within PROCEDURE DIVISION\\n                \"\\\\nINPUT-OUTPUT SECTION.\",\\n                # Split along paragraphs and common statements\\n                \"\\\\nOPEN \",\\n                \"\\\\nCLOSE \",\\n                \"\\\\nREAD \",\\n                \"\\\\nWRITE \",\\n                \"\\\\nIF \",\\n                \"\\\\nELSE \",\\n                \"\\\\nMOVE \",\\n                \"\\\\nPERFORM \",\\n                \"\\\\nUNTIL \",\\n                \"\\\\nVARYING \",\\n                \"\\\\nACCEPT \",\\n                \"\\\\nDISPLAY \",\\n                \"\\\\nSTOP RUN.\",\\n                # Split by the normal type of lines\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]',\n",
              "  '\"<nav\",\\n                # Head\\n                \"<head\",\\n                \"<style\",\\n                \"<script\",\\n                \"<meta\",\\n                \"<title\",\\n                \"\",\\n            ]\\n        elif language == Language.CSHARP:\\n            return [\\n                \"\\\\ninterface \",\\n                \"\\\\nenum \",\\n                \"\\\\nimplements \",\\n                \"\\\\ndelegate \",\\n                \"\\\\nevent \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nabstract \",\\n                # Split along method definitions\\n                \"\\\\npublic \",\\n                \"\\\\nprotected \",\\n                \"\\\\nprivate \",\\n                \"\\\\nstatic \",\\n                \"\\\\nreturn \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\ncontinue \",\\n                \"\\\\nfor \",\\n                \"\\\\nforeach \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\nbreak \",\\n                \"\\\\ncase \",\\n                \"\\\\nelse \",\\n                # Split by exceptions\\n                \"\\\\ntry \",\\n                \"\\\\nthrow \",\\n                \"\\\\nfinally \",\\n                \"\\\\ncatch \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SOL:\\n            return [\\n                # Split along compiler information definitions\\n                \"\\\\npragma \",\\n                \"\\\\nusing \",\\n                # Split along contract definitions\\n                \"\\\\ncontract \",\\n                \"\\\\ninterface \",\\n                \"\\\\nlibrary \",\\n                # Split along method definitions\\n                \"\\\\nconstructor \",\\n                \"\\\\ntype \",\\n                \"\\\\nfunction \",\\n                \"\\\\nevent \",\\n                \"\\\\nmodifier \",\\n                \"\\\\nerror \",\\n                \"\\\\nstruct \",\\n                \"\\\\nenum \",\\n                # Split along control flow statements',\n",
              "  '\"\\\\n\\\\\\\\*\\\\\\\\*\\\\\\\\*+\\\\n\",\\n                \"\\\\n---+\\\\n\",\\n                \"\\\\n___+\\\\n\",\\n                # Note that this splitter doesn\\'t handle horizontal lines defined\\n                # by *three or more* of ***, ---, or ___, but this is not handled\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.LATEX:\\n            return [\\n                # First, try to split along Latex sections\\n                \"\\\\n\\\\\\\\\\\\\\\\chapter{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\section{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\subsection{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\subsubsection{\",\\n                # Now split by environments\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{enumerate}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{itemize}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{description}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{list}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{quote}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{quotation}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{verse}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{verbatim}\",\\n                # Now split by math environments\\n                \"\\\\n\\\\\\\\\\\\begin{align}\",\\n                \"$$\",\\n                \"$\",\\n                # Now split by the normal type of lines\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.HTML:\\n            return [\\n                # First, try to split along HTML tags\\n                \"<body\",\\n                \"<div\",\\n                \"<p\",\\n                \"<br\",\\n                \"<li\",\\n                \"<h1\",\\n                \"<h2\",\\n                \"<h3\",\\n                \"<h4\",\\n                \"<h5\",\\n                \"<h6\",\\n                \"<span\",\\n                \"<table\",\\n                \"<tr\",\\n                \"<td\",\\n                \"<th\",\\n                \"<ul\",\\n                \"<ol\",\\n                \"<header\",\\n                \"<footer\",\\n                \"<nav\",\\n                # Head\\n                \"<head\",\\n                \"<style\",\\n                \"<script\",\\n                \"<meta\",\\n                \"<title\",',\n",
              "  '\" \",\\n                \"\",\\n            ]\\n        elif language == Language.SCALA:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nobject \",\\n                # Split along method definitions\\n                \"\\\\ndef \",\\n                \"\\\\nval \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nmatch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SWIFT:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunc \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nstruct \",\\n                \"\\\\nenum \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.MARKDOWN:\\n            return [\\n                # First, try to split along Markdown headings (starting with level 2)\\n                \"\\\\n#{1,6} \",\\n                # Note the alternative syntax for headings (below) is not handled here\\n                # Heading level 2\\n                # ---------------\\n                # End of code block\\n                \"```\\\\n\",\\n                # Horizontal lines\\n                \"\\\\n\\\\\\\\*\\\\\\\\*\\\\\\\\*+\\\\n\",\\n                \"\\\\n---+\\\\n\",\\n                \"\\\\n___+\\\\n\",\\n                # Note that this splitter doesn\\'t handle horizontal lines defined',\n",
              "  'return [\\n                # First, try to split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\ndef \",\\n                \"\\\\n\\\\tdef \",\\n                # Now split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.RST:\\n            return [\\n                # Split along section titles\\n                \"\\\\n=+\\\\n\",\\n                \"\\\\n-+\\\\n\",\\n                \"\\\\n\\\\\\\\*+\\\\n\",\\n                # Split along directive markers\\n                \"\\\\n\\\\n.. *\\\\n\\\\n\",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.RUBY:\\n            return [\\n                # Split along method definitions\\n                \"\\\\ndef \",\\n                \"\\\\nclass \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nunless \",\\n                \"\\\\nwhile \",\\n                \"\\\\nfor \",\\n                \"\\\\ndo \",\\n                \"\\\\nbegin \",\\n                \"\\\\nrescue \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.RUST:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfn \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nwhile \",\\n                \"\\\\nfor \",\\n                \"\\\\nloop \",\\n                \"\\\\nmatch \",\\n                \"\\\\nconst \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SCALA:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",',\n",
              "  '\"\\\\nfunction \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                \"\\\\ndefault \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.PHP:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunction \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nforeach \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.PROTO:\\n            return [\\n                # Split along message definitions\\n                \"\\\\nmessage \",\\n                # Split along service definitions\\n                \"\\\\nservice \",\\n                # Split along enum definitions\\n                \"\\\\nenum \",\\n                # Split along option definitions\\n                \"\\\\noption \",\\n                # Split along import statements\\n                \"\\\\nimport \",\\n                # Split along syntax declarations\\n                \"\\\\nsyntax \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.PYTHON:\\n            return [\\n                # First, try to split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\ndef \",\\n                \"\\\\n\\\\tdef \",',\n",
              "  'else:\\n            raise ValueError(\\n                f\"Language {language} is not supported! \"\\n                f\"Please choose from {list(Language)}\"\\n            )',\n",
              "  '@staticmethod\\n    def get_separators_for_language(language: Language) -> List[str]:\\n        if language == Language.CPP:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along function definitions\\n                \"\\\\nvoid \",\\n                \"\\\\nint \",\\n                \"\\\\nfloat \",\\n                \"\\\\ndouble \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.GO:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunc \",\\n                \"\\\\nvar \",\\n                \"\\\\nconst \",\\n                \"\\\\ntype \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.JAVA:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along method definitions\\n                \"\\\\npublic \",\\n                \"\\\\nprotected \",\\n                \"\\\\nprivate \",\\n                \"\\\\nstatic \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.KOTLIN:\\n            return [',\n",
              "  '_separator = separator if self._is_separator_regex else re.escape(separator)\\n        splits = _split_text_with_regex(text, _separator, self._keep_separator)\\n\\n        # Now go merging things, recursively splitting longer texts.\\n        _good_splits = []\\n        _separator = \"\" if self._keep_separator else separator\\n        for s in splits:\\n            if self._length_function(s) < self._chunk_size:\\n                _good_splits.append(s)\\n            else:\\n                if _good_splits:\\n                    merged_text = self._merge_splits(_good_splits, _separator)\\n                    final_chunks.extend(merged_text)\\n                    _good_splits = []\\n                if not new_separators:\\n                    final_chunks.append(s)\\n                else:\\n                    other_info = self._split_text(s, new_separators)\\n                    final_chunks.extend(other_info)\\n        if _good_splits:\\n            merged_text = self._merge_splits(_good_splits, _separator)\\n            final_chunks.extend(merged_text)\\n        return final_chunks\\n\\n    def split_text(self, text: str) -> List[str]:\\n        return self._split_text(text, self._separators)\\n\\n    @classmethod\\n    def from_language(\\n        cls, language: Language, **kwargs: Any\\n    ) -> RecursiveCharacterTextSplitter:\\n        separators = cls.get_separators_for_language(language)\\n        return cls(separators=separators, is_separator_regex=True, **kwargs)',\n",
              "  'class RecursiveCharacterTextSplitter(TextSplitter):\\n    \"\"\"Splitting text by recursively look at characters.\\n\\n    Recursively tries to split by different characters to find one\\n    that works.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        separators: Optional[List[str]] = None,\\n        keep_separator: bool = True,\\n        is_separator_regex: bool = False,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(keep_separator=keep_separator, **kwargs)\\n        self._separators = separators or [\"\\\\n\\\\n\", \"\\\\n\", \" \", \"\"]\\n        self._is_separator_regex = is_separator_regex\\n\\n    def _split_text(self, text: str, separators: List[str]) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        final_chunks = []\\n        # Get appropriate separator to use\\n        separator = separators[-1]\\n        new_separators = []\\n        for i, _s in enumerate(separators):\\n            _separator = _s if self._is_separator_regex else re.escape(_s)\\n            if _s == \"\":\\n                separator = _s\\n                break\\n            if re.search(_separator, text):\\n                separator = _s\\n                new_separators = separators[i + 1 :]\\n                break\\n\\n        _separator = separator if self._is_separator_regex else re.escape(separator)\\n        splits = _split_text_with_regex(text, _separator, self._keep_separator)',\n",
              "  'class Language(str, Enum):\\n    \"\"\"Enum of the programming languages.\"\"\"\\n\\n    CPP = \"cpp\"\\n    GO = \"go\"\\n    JAVA = \"java\"\\n    KOTLIN = \"kotlin\"\\n    JS = \"js\"\\n    TS = \"ts\"\\n    PHP = \"php\"\\n    PROTO = \"proto\"\\n    PYTHON = \"python\"\\n    RST = \"rst\"\\n    RUBY = \"ruby\"\\n    RUST = \"rust\"\\n    SCALA = \"scala\"\\n    SWIFT = \"swift\"\\n    MARKDOWN = \"markdown\"\\n    LATEX = \"latex\"\\n    HTML = \"html\"\\n    SOL = \"sol\"\\n    CSHARP = \"csharp\"\\n    COBOL = \"cobol\"',\n",
              "  'def split_text(self, text: str) -> List[str]:\\n        def encode_strip_start_and_stop_token_ids(text: str) -> List[int]:\\n            return self._encode(text)[1:-1]\\n\\n        tokenizer = Tokenizer(\\n            chunk_overlap=self._chunk_overlap,\\n            tokens_per_chunk=self.tokens_per_chunk,\\n            decode=self.tokenizer.decode,\\n            encode=encode_strip_start_and_stop_token_ids,\\n        )\\n\\n        return split_text_on_tokens(text=text, tokenizer=tokenizer)\\n\\n    def count_tokens(self, *, text: str) -> int:\\n        return len(self._encode(text))\\n\\n    _max_length_equal_32_bit_integer: int = 2**32\\n\\n    def _encode(self, text: str) -> List[int]:\\n        token_ids_with_start_and_end_token_ids = self.tokenizer.encode(\\n            text,\\n            max_length=self._max_length_equal_32_bit_integer,\\n            truncation=\"do_not_truncate\",\\n        )\\n        return token_ids_with_start_and_end_token_ids',\n",
              "  'class SentenceTransformersTokenTextSplitter(TextSplitter):\\n    \"\"\"Splitting text to tokens using sentence model tokenizer.\"\"\"\\n\\n    def __init__(\\n        self,\\n        chunk_overlap: int = 50,\\n        model_name: str = \"sentence-transformers/all-mpnet-base-v2\",\\n        tokens_per_chunk: Optional[int] = None,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(**kwargs, chunk_overlap=chunk_overlap)\\n\\n        try:\\n            from sentence_transformers import SentenceTransformer\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import sentence_transformer python package. \"\\n                \"This is needed in order to for SentenceTransformersTokenTextSplitter. \"\\n                \"Please install it with `pip install sentence-transformers`.\"\\n            )\\n\\n        self.model_name = model_name\\n        self._model = SentenceTransformer(self.model_name)\\n        self.tokenizer = self._model.tokenizer\\n        self._initialize_chunk_configuration(tokens_per_chunk=tokens_per_chunk)\\n\\n    def _initialize_chunk_configuration(\\n        self, *, tokens_per_chunk: Optional[int]\\n    ) -> None:\\n        self.maximum_tokens_per_chunk = cast(int, self._model.max_seq_length)\\n\\n        if tokens_per_chunk is None:\\n            self.tokens_per_chunk = self.maximum_tokens_per_chunk\\n        else:\\n            self.tokens_per_chunk = tokens_per_chunk\\n\\n        if self.tokens_per_chunk > self.maximum_tokens_per_chunk:\\n            raise ValueError(\\n                f\"The token limit of the models \\'{self.model_name}\\'\"\\n                f\" is: {self.maximum_tokens_per_chunk}.\"\\n                f\" Argument tokens_per_chunk={self.tokens_per_chunk}\"\\n                f\" > maximum token limit.\"\\n            )\\n\\n    def split_text(self, text: str) -> List[str]:\\n        def encode_strip_start_and_stop_token_ids(text: str) -> List[int]:\\n            return self._encode(text)[1:-1]',\n",
              "  'class TokenTextSplitter(TextSplitter):\\n    \"\"\"Splitting text to tokens using model tokenizer.\"\"\"\\n\\n    def __init__(\\n        self,\\n        encoding_name: str = \"gpt2\",\\n        model_name: Optional[str] = None,\\n        allowed_special: Union[Literal[\"all\"], AbstractSet[str]] = set(),\\n        disallowed_special: Union[Literal[\"all\"], Collection[str]] = \"all\",\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(**kwargs)\\n        try:\\n            import tiktoken\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import tiktoken python package. \"\\n                \"This is needed in order to for TokenTextSplitter. \"\\n                \"Please install it with `pip install tiktoken`.\"\\n            )\\n\\n        if model_name is not None:\\n            enc = tiktoken.encoding_for_model(model_name)\\n        else:\\n            enc = tiktoken.get_encoding(encoding_name)\\n        self._tokenizer = enc\\n        self._allowed_special = allowed_special\\n        self._disallowed_special = disallowed_special\\n\\n    def split_text(self, text: str) -> List[str]:\\n        def _encode(_text: str) -> List[int]:\\n            return self._tokenizer.encode(\\n                _text,\\n                allowed_special=self._allowed_special,\\n                disallowed_special=self._disallowed_special,\\n            )\\n\\n        tokenizer = Tokenizer(\\n            chunk_overlap=self._chunk_overlap,\\n            tokens_per_chunk=self._chunk_size,\\n            decode=self._tokenizer.decode,\\n            encode=_encode,\\n        )\\n\\n        return split_text_on_tokens(text=text, tokenizer=tokenizer)',\n",
              "  'class Tokenizer:\\n    \"\"\"Tokenizer data class.\"\"\"\\n\\n    chunk_overlap: int\\n    \"\"\"Overlap in tokens between chunks\"\"\"\\n    tokens_per_chunk: int\\n    \"\"\"Maximum number of tokens per chunk\"\"\"\\n    decode: Callable[[List[int]], str]\\n    \"\"\" Function to decode a list of token ids to a string\"\"\"\\n    encode: Callable[[str], List[int]]\\n    \"\"\" Function to encode a string to a list of token ids\"\"\"\\n\\n\\ndef split_text_on_tokens(*, text: str, tokenizer: Tokenizer) -> List[str]:\\n    \"\"\"Split incoming text and return chunks using tokenizer.\"\"\"\\n    splits: List[str] = []\\n    input_ids = tokenizer.encode(text)\\n    start_idx = 0\\n    cur_idx = min(start_idx + tokenizer.tokens_per_chunk, len(input_ids))\\n    chunk_ids = input_ids[start_idx:cur_idx]\\n    while start_idx < len(input_ids):\\n        splits.append(tokenizer.decode(chunk_ids))\\n        if cur_idx == len(input_ids):\\n            break\\n        start_idx += tokenizer.tokens_per_chunk - tokenizer.chunk_overlap\\n        cur_idx = min(start_idx + tokenizer.tokens_per_chunk, len(input_ids))\\n        chunk_ids = input_ids[start_idx:cur_idx]\\n    return splits',\n",
              "  'docs = []\\n        current_doc: List[str] = []\\n        total = 0\\n        for d in splits:\\n            _len = self._length_function(d)\\n            if (\\n                total + _len + (separator_len if len(current_doc) > 0 else 0)\\n                > self._chunk_size\\n            ):\\n                if total > self._chunk_size:\\n                    logger.warning(\\n                        f\"Created a chunk of size {total}, \"\\n                        f\"which is longer than the specified {self._chunk_size}\"\\n                    )\\n                if len(current_doc) > 0:\\n                    doc = self._join_docs(current_doc, separator)\\n                    if doc is not None:\\n                        docs.append(doc)\\n                    # Keep on popping if:\\n                    # - we have a larger chunk than in the chunk overlap\\n                    # - or if we still have any chunks and the length is long\\n                    while total > self._chunk_overlap or (\\n                        total + _len + (separator_len if len(current_doc) > 0 else 0)\\n                        > self._chunk_size\\n                        and total > 0\\n                    ):\\n                        total -= self._length_function(current_doc[0]) + (\\n                            separator_len if len(current_doc) > 1 else 0\\n                        )\\n                        current_doc = current_doc[1:]\\n            current_doc.append(d)\\n            total += _len + (separator_len if len(current_doc) > 1 else 0)\\n        doc = self._join_docs(current_doc, separator)\\n        if doc is not None:\\n            docs.append(doc)\\n        return docs\\n\\n    @classmethod\\n    def from_huggingface_tokenizer(cls, tokenizer: Any, **kwargs: Any) -> TextSplitter:\\n        \"\"\"Text splitter that uses HuggingFace tokenizer to count length.\"\"\"\\n        try:\\n            from transformers import PreTrainedTokenizerBase',\n",
              "  'class ElementType(TypedDict):\\n    \"\"\"Element type as typed dict.\"\"\"\\n\\n    url: str\\n    xpath: str\\n    content: str\\n    metadata: Dict[str, str]',\n",
              "  'if current_content:\\n            lines_with_metadata.append(\\n                {\"content\": \"\\\\n\".join(current_content), \"metadata\": current_metadata}\\n            )\\n\\n        # lines_with_metadata has each line with associated header metadata\\n        # aggregate these into chunks based on common metadata\\n        if not self.return_each_line:\\n            return self.aggregate_lines_to_chunks(lines_with_metadata)\\n        else:\\n            return [\\n                Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n                for chunk in lines_with_metadata\\n            ]',\n",
              "  '# Push the current header to the stack\\n                        header: HeaderType = {\\n                            \"level\": current_header_level,\\n                            \"name\": name,\\n                            \"data\": stripped_line[len(sep) :].strip(),\\n                        }\\n                        header_stack.append(header)\\n                        # Update initial_metadata with the current header\\n                        initial_metadata[name] = header[\"data\"]\\n\\n                    # Add the previous line to the lines_with_metadata\\n                    # only if current_content is not empty\\n                    if current_content:\\n                        lines_with_metadata.append(\\n                            {\\n                                \"content\": \"\\\\n\".join(current_content),\\n                                \"metadata\": current_metadata.copy(),\\n                            }\\n                        )\\n                        current_content.clear()\\n\\n                    break\\n            else:\\n                if stripped_line:\\n                    current_content.append(stripped_line)\\n                elif current_content:\\n                    lines_with_metadata.append(\\n                        {\\n                            \"content\": \"\\\\n\".join(current_content),\\n                            \"metadata\": current_metadata.copy(),\\n                        }\\n                    )\\n                    current_content.clear()\\n\\n            current_metadata = initial_metadata.copy()\\n\\n        if current_content:\\n            lines_with_metadata.append(\\n                {\"content\": \"\\\\n\".join(current_content), \"metadata\": current_metadata}\\n            )',\n",
              "  'if in_code_block:\\n                current_content.append(stripped_line)\\n                continue\\n\\n            # Check each line against each of the header types (e.g., #, ##)\\n            for sep, name in self.headers_to_split_on:\\n                # Check if line starts with a header that we intend to split on\\n                if stripped_line.startswith(sep) and (\\n                    # Header with no text OR header is followed by space\\n                    # Both are valid conditions that sep is being used a header\\n                    len(stripped_line) == len(sep) or stripped_line[len(sep)] == \" \"\\n                ):\\n                    # Ensure we are tracking the header as metadata\\n                    if name is not None:\\n                        # Get the current header level\\n                        current_header_level = sep.count(\"#\")\\n\\n                        # Pop out headers of lower or same level from the stack\\n                        while (\\n                            header_stack\\n                            and header_stack[-1][\"level\"] >= current_header_level\\n                        ):\\n                            # We have encountered a new header\\n                            # at the same or higher level\\n                            popped_header = header_stack.pop()\\n                            # Clear the metadata for the\\n                            # popped header in initial_metadata\\n                            if popped_header[\"name\"] in initial_metadata:\\n                                initial_metadata.pop(popped_header[\"name\"])',\n",
              "  'def split_text(self, text: str) -> List[Document]:\\n        \"\"\"Split markdown file\\n        Args:\\n            text: Markdown file\"\"\"\\n\\n        # Split the input text by newline character (\"\\\\n\").\\n        lines = text.split(\"\\\\n\")\\n        # Final output\\n        lines_with_metadata: List[LineType] = []\\n        # Content and metadata of the chunk currently being processed\\n        current_content: List[str] = []\\n        current_metadata: Dict[str, str] = {}\\n        # Keep track of the nested header structure\\n        # header_stack: List[Dict[str, Union[int, str]]] = []\\n        header_stack: List[HeaderType] = []\\n        initial_metadata: Dict[str, str] = {}\\n\\n        in_code_block = False\\n        opening_fence = \"\"\\n\\n        for line in lines:\\n            stripped_line = line.strip()\\n\\n            if not in_code_block:\\n                # Exclude inline code spans\\n                if stripped_line.startswith(\"```\") and stripped_line.count(\"```\") == 1:\\n                    in_code_block = True\\n                    opening_fence = \"```\"\\n                elif stripped_line.startswith(\"~~~\"):\\n                    in_code_block = True\\n                    opening_fence = \"~~~\"\\n            else:\\n                if stripped_line.startswith(opening_fence):\\n                    in_code_block = False\\n                    opening_fence = \"\"\\n\\n            if in_code_block:\\n                current_content.append(stripped_line)\\n                continue',\n",
              "  'class MarkdownHeaderTextSplitter:\\n    \"\"\"Splitting markdown files based on specified headers.\"\"\"\\n\\n    def __init__(\\n        self, headers_to_split_on: List[Tuple[str, str]], return_each_line: bool = False\\n    ):\\n        \"\"\"Create a new MarkdownHeaderTextSplitter.\\n\\n        Args:\\n            headers_to_split_on: Headers we want to track\\n            return_each_line: Return each line w/ associated headers\\n        \"\"\"\\n        # Output line-by-line or aggregated into chunks w/ common headers\\n        self.return_each_line = return_each_line\\n        # Given the headers we want to split on,\\n        # (e.g., \"#, ##, etc\") order by length\\n        self.headers_to_split_on = sorted(\\n            headers_to_split_on, key=lambda split: len(split[0]), reverse=True\\n        )\\n\\n    def aggregate_lines_to_chunks(self, lines: List[LineType]) -> List[Document]:\\n        \"\"\"Combine lines with common metadata into chunks\\n        Args:\\n            lines: Line of text / associated header metadata\\n        \"\"\"\\n        aggregated_chunks: List[LineType] = []\\n\\n        for line in lines:\\n            if (\\n                aggregated_chunks\\n                and aggregated_chunks[-1][\"metadata\"] == line[\"metadata\"]\\n            ):\\n                # If the last line in the aggregated list\\n                # has the same metadata as the current line,\\n                # append the current content to the last lines\\'s content\\n                aggregated_chunks[-1][\"content\"] += \"  \\\\n\" + line[\"content\"]\\n            else:\\n                # Otherwise, append the current line to the aggregated list\\n                aggregated_chunks.append(line)\\n\\n        return [\\n            Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n            for chunk in aggregated_chunks\\n        ]\\n\\n    def split_text(self, text: str) -> List[Document]:\\n        \"\"\"Split markdown file\\n        Args:\\n            text: Markdown file\"\"\"',\n",
              "  'class CharacterTextSplitter(TextSplitter):\\n    \"\"\"Splitting text that looks at characters.\"\"\"\\n\\n    def __init__(\\n        self, separator: str = \"\\\\n\\\\n\", is_separator_regex: bool = False, **kwargs: Any\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(**kwargs)\\n        self._separator = separator\\n        self._is_separator_regex = is_separator_regex\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        # First we naively split the large input into a bunch of smaller ones.\\n        separator = (\\n            self._separator if self._is_separator_regex else re.escape(self._separator)\\n        )\\n        splits = _split_text_with_regex(text, separator, self._keep_separator)\\n        _separator = \"\" if self._keep_separator else self._separator\\n        return self._merge_splits(splits, _separator)\\n\\n\\nclass LineType(TypedDict):\\n    \"\"\"Line type as typed dict.\"\"\"\\n\\n    metadata: Dict[str, str]\\n    content: str\\n\\n\\nclass HeaderType(TypedDict):\\n    \"\"\"Header type as typed dict.\"\"\"\\n\\n    level: int\\n    name: str\\n    data: str',\n",
              "  'if issubclass(cls, TokenTextSplitter):\\n            extra_kwargs = {\\n                \"encoding_name\": encoding_name,\\n                \"model_name\": model_name,\\n                \"allowed_special\": allowed_special,\\n                \"disallowed_special\": disallowed_special,\\n            }\\n            kwargs = {**kwargs, **extra_kwargs}\\n\\n        return cls(length_function=_tiktoken_encoder, **kwargs)\\n\\n    def transform_documents(\\n        self, documents: Sequence[Document], **kwargs: Any\\n    ) -> Sequence[Document]:\\n        \"\"\"Transform sequence of documents by splitting them.\"\"\"\\n        return self.split_documents(list(documents))\\n\\n    async def atransform_documents(\\n        self, documents: Sequence[Document], **kwargs: Any\\n    ) -> Sequence[Document]:\\n        \"\"\"Asynchronously transform a sequence of documents by splitting them.\"\"\"\\n        return await asyncio.get_running_loop().run_in_executor(\\n            None, partial(self.transform_documents, **kwargs), documents\\n        )',\n",
              "  'if not isinstance(tokenizer, PreTrainedTokenizerBase):\\n                raise ValueError(\\n                    \"Tokenizer received was not an instance of PreTrainedTokenizerBase\"\\n                )\\n\\n            def _huggingface_tokenizer_length(text: str) -> int:\\n                return len(tokenizer.encode(text))\\n\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import transformers python package. \"\\n                \"Please install it with `pip install transformers`.\"\\n            )\\n        return cls(length_function=_huggingface_tokenizer_length, **kwargs)\\n\\n    @classmethod\\n    def from_tiktoken_encoder(\\n        cls: Type[TS],\\n        encoding_name: str = \"gpt2\",\\n        model_name: Optional[str] = None,\\n        allowed_special: Union[Literal[\"all\"], AbstractSet[str]] = set(),\\n        disallowed_special: Union[Literal[\"all\"], Collection[str]] = \"all\",\\n        **kwargs: Any,\\n    ) -> TS:\\n        \"\"\"Text splitter that uses tiktoken encoder to count length.\"\"\"\\n        try:\\n            import tiktoken\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import tiktoken python package. \"\\n                \"This is needed in order to calculate max_tokens_for_prompt. \"\\n                \"Please install it with `pip install tiktoken`.\"\\n            )\\n\\n        if model_name is not None:\\n            enc = tiktoken.encoding_for_model(model_name)\\n        else:\\n            enc = tiktoken.get_encoding(encoding_name)\\n\\n        def _tiktoken_encoder(text: str) -> int:\\n            return len(\\n                enc.encode(\\n                    text,\\n                    allowed_special=allowed_special,\\n                    disallowed_special=disallowed_special,\\n                )\\n            )',\n",
              "  'class HTMLHeaderTextSplitter:\\n    \"\"\"\\n    Splitting HTML files based on specified headers.\\n    Requires lxml package.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        headers_to_split_on: List[Tuple[str, str]],\\n        return_each_element: bool = False,\\n    ):\\n        \"\"\"Create a new HTMLHeaderTextSplitter.\\n\\n        Args:\\n            headers_to_split_on: list of tuples of headers we want to track mapped to\\n                (arbitrary) keys for metadata. Allowed header values: h1, h2, h3, h4,\\n                h5, h6 e.g. [(\"h1\", \"Header 1\"), (\"h2\", \"Header 2)].\\n            return_each_element: Return each element w/ associated headers.\\n        \"\"\"\\n        # Output element-by-element or aggregated into chunks w/ common headers\\n        self.return_each_element = return_each_element\\n        self.headers_to_split_on = sorted(headers_to_split_on)\\n\\n    def aggregate_elements_to_chunks(\\n        self, elements: List[ElementType]\\n    ) -> List[Document]:\\n        \"\"\"Combine elements with common metadata into chunks\\n\\n        Args:\\n            elements: HTML element content with associated identifying info and metadata\\n        \"\"\"\\n        aggregated_chunks: List[ElementType] = []\\n\\n        for element in elements:\\n            if (\\n                aggregated_chunks\\n                and aggregated_chunks[-1][\"metadata\"] == element[\"metadata\"]\\n            ):\\n                # If the last element in the aggregated list\\n                # has the same metadata as the current element,\\n                # append the current content to the last element\\'s content\\n                aggregated_chunks[-1][\"content\"] += \"  \\\\n\" + element[\"content\"]\\n            else:\\n                # Otherwise, append the current element to the aggregated list\\n                aggregated_chunks.append(element)\\n\\n        return [\\n            Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n            for chunk in aggregated_chunks\\n        ]',\n",
              "  '@abstractmethod\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split text into multiple components.\"\"\"\\n\\n    def create_documents(\\n        self, texts: List[str], metadatas: Optional[List[dict]] = None\\n    ) -> List[Document]:\\n        \"\"\"Create documents from a list of texts.\"\"\"\\n        _metadatas = metadatas or [{}] * len(texts)\\n        documents = []\\n        for i, text in enumerate(texts):\\n            index = -1\\n            for chunk in self.split_text(text):\\n                metadata = copy.deepcopy(_metadatas[i])\\n                if self._add_start_index:\\n                    index = text.find(chunk, index + 1)\\n                    metadata[\"start_index\"] = index\\n                new_doc = Document(page_content=chunk, metadata=metadata)\\n                documents.append(new_doc)\\n        return documents\\n\\n    def split_documents(self, documents: Iterable[Document]) -> List[Document]:\\n        \"\"\"Split documents.\"\"\"\\n        texts, metadatas = [], []\\n        for doc in documents:\\n            texts.append(doc.page_content)\\n            metadatas.append(doc.metadata)\\n        return self.create_documents(texts, metadatas=metadatas)\\n\\n    def _join_docs(self, docs: List[str], separator: str) -> Optional[str]:\\n        text = separator.join(docs)\\n        if self._strip_whitespace:\\n            text = text.strip()\\n        if text == \"\":\\n            return None\\n        else:\\n            return text\\n\\n    def _merge_splits(self, splits: Iterable[str], separator: str) -> List[str]:\\n        # We now want to combine these smaller pieces into medium size\\n        # chunks to send to the LLM.\\n        separator_len = self._length_function(separator)',\n",
              "  'class TextSplitter(BaseDocumentTransformer, ABC):\\n    \"\"\"Interface for splitting text into chunks.\"\"\"\\n\\n    def __init__(\\n        self,\\n        chunk_size: int = 4000,\\n        chunk_overlap: int = 200,\\n        length_function: Callable[[str], int] = len,\\n        keep_separator: bool = False,\\n        add_start_index: bool = False,\\n        strip_whitespace: bool = True,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\\n\\n        Args:\\n            chunk_size: Maximum size of chunks to return\\n            chunk_overlap: Overlap in characters between chunks\\n            length_function: Function that measures the length of given chunks\\n            keep_separator: Whether to keep the separator in the chunks\\n            add_start_index: If `True`, includes chunk\\'s start index in metadata\\n            strip_whitespace: If `True`, strips whitespace from the start and end of\\n                              every document\\n        \"\"\"\\n        if chunk_overlap > chunk_size:\\n            raise ValueError(\\n                f\"Got a larger chunk overlap ({chunk_overlap}) than chunk size \"\\n                f\"({chunk_size}), should be smaller.\"\\n            )\\n        self._chunk_size = chunk_size\\n        self._chunk_overlap = chunk_overlap\\n        self._length_function = length_function\\n        self._keep_separator = keep_separator\\n        self._add_start_index = add_start_index\\n        self._strip_whitespace = strip_whitespace\\n\\n    @abstractmethod\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split text into multiple components.\"\"\"',\n",
              "  'def _split_text_with_regex(\\n    text: str, separator: str, keep_separator: bool\\n) -> List[str]:\\n    # Now that we have the separator, split the text\\n    if separator:\\n        if keep_separator:\\n            # The parentheses in the pattern keep the delimiters in the result.\\n            _splits = re.split(f\"({separator})\", text)\\n            splits = [_splits[i] + _splits[i + 1] for i in range(1, len(_splits), 2)]\\n            if len(_splits) % 2 == 0:\\n                splits += _splits[-1:]\\n            splits = [_splits[0]] + splits\\n        else:\\n            splits = re.split(separator, text)\\n    else:\\n        splits = list(text)\\n    return [s for s in splits if s != \"\"]',\n",
              "  '\"\"\"**Text Splitters** are classes for splitting text.\\n\\n\\n**Class hierarchy:**\\n\\n.. code-block::\\n\\n    BaseDocumentTransformer --> TextSplitter --> <name>TextSplitter  # Example: CharacterTextSplitter\\n                                                 RecursiveCharacterTextSplitter -->  <name>TextSplitter\\n\\nNote: **MarkdownHeaderTextSplitter** and **HTMLHeaderTextSplitter do not derive from TextSplitter.\\n\\n\\n**Main helpers:**\\n\\n.. code-block::\\n\\n    Document, Tokenizer, Language, LineType, HeaderType\\n\\n\"\"\"  # noqa: E501\\n\\nfrom __future__ import annotations\\n\\nimport asyncio\\nimport copy\\nimport logging\\nimport pathlib\\nimport re\\nfrom abc import ABC, abstractmethod\\nfrom dataclasses import dataclass\\nfrom enum import Enum\\nfrom functools import partial\\nfrom io import BytesIO, StringIO\\nfrom typing import (\\n    AbstractSet,\\n    Any,\\n    Callable,\\n    Collection,\\n    Dict,\\n    Iterable,\\n    List,\\n    Literal,\\n    Optional,\\n    Sequence,\\n    Tuple,\\n    Type,\\n    TypedDict,\\n    TypeVar,\\n    Union,\\n    cast,\\n)\\n\\nimport requests\\nfrom langchain_core.documents import BaseDocumentTransformer, Document\\n\\nlogger = logging.getLogger(__name__)\\n\\nTS = TypeVar(\"TS\", bound=\"TextSplitter\")\\n\\n\\ndef _make_spacy_pipeline_for_splitting(\\n    pipeline: str, *, max_length: int = 1_000_000\\n) -> Any:  # avoid importing spacy\\n    try:\\n        import spacy\\n    except ImportError:\\n        raise ImportError(\\n            \"Spacy is not installed, please install it with `pip install spacy`.\"\\n        )\\n    if pipeline == \"sentencizer\":\\n        from spacy.lang.en import English\\n\\n        sentencizer = English()\\n        sentencizer.add_pipe(\"sentencizer\")\\n    else:\\n        sentencizer = spacy.load(pipeline, exclude=[\"ner\", \"tagger\"])\\n        sentencizer.max_length = max_length\\n    return sentencizer',\n",
              "  'class PythonCodeTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Python syntax.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a PythonCodeTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.PYTHON)\\n        super().__init__(separators=separators, **kwargs)\\n\\n\\nclass MarkdownTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Markdown-formatted headings.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a MarkdownTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.MARKDOWN)\\n        super().__init__(separators=separators, **kwargs)\\n\\n\\nclass LatexTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Latex-formatted layout elements.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a LatexTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.LATEX)\\n        super().__init__(separators=separators, **kwargs)',\n",
              "  'class NLTKTextSplitter(TextSplitter):\\n    \"\"\"Splitting text using NLTK package.\"\"\"\\n\\n    def __init__(\\n        self, separator: str = \"\\\\n\\\\n\", language: str = \"english\", **kwargs: Any\\n    ) -> None:\\n        \"\"\"Initialize the NLTK splitter.\"\"\"\\n        super().__init__(**kwargs)\\n        try:\\n            from nltk.tokenize import sent_tokenize\\n\\n            self._tokenizer = sent_tokenize\\n        except ImportError:\\n            raise ImportError(\\n                \"NLTK is not installed, please install it with `pip install nltk`.\"\\n            )\\n        self._separator = separator\\n        self._language = language\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        # First we naively split the large input into a bunch of smaller ones.\\n        splits = self._tokenizer(text, language=self._language)\\n        return self._merge_splits(splits, self._separator)\\n\\n\\nclass SpacyTextSplitter(TextSplitter):\\n    \"\"\"Splitting text using Spacy package.\\n\\n\\n    Per default, Spacy\\'s `en_core_web_sm` model is used and\\n    its default max_length is 1000000 (it is the length of maximum character\\n    this model takes which can be increased for large files). For a faster, but\\n    potentially less accurate splitting, you can use `pipeline=\\'sentencizer\\'`.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        separator: str = \"\\\\n\\\\n\",\\n        pipeline: str = \"en_core_web_sm\",\\n        max_length: int = 1_000_000,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Initialize the spacy text splitter.\"\"\"\\n        super().__init__(**kwargs)\\n        self._tokenizer = _make_spacy_pipeline_for_splitting(\\n            pipeline, max_length=max_length\\n        )\\n        self._separator = separator\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        splits = (s.text for s in self._tokenizer(text).sents)\\n        return self._merge_splits(splits, self._separator)\\n\\n\\n# For backwards compatibility',\n",
              "  'else:\\n            raise ValueError(\\n                f\"Language {language} is not supported! \"\\n                f\"Please choose from {list(Language)}\"\\n            )',\n",
              "  '\"\\\\nevent \",\\n                \"\\\\nmodifier \",\\n                \"\\\\nerror \",\\n                \"\\\\nstruct \",\\n                \"\\\\nenum \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo while \",\\n                \"\\\\nassembly \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.COBOL:\\n            return [\\n                # Split along divisions\\n                \"\\\\nIDENTIFICATION DIVISION.\",\\n                \"\\\\nENVIRONMENT DIVISION.\",\\n                \"\\\\nDATA DIVISION.\",\\n                \"\\\\nPROCEDURE DIVISION.\",\\n                # Split along sections within DATA DIVISION\\n                \"\\\\nWORKING-STORAGE SECTION.\",\\n                \"\\\\nLINKAGE SECTION.\",\\n                \"\\\\nFILE SECTION.\",\\n                # Split along sections within PROCEDURE DIVISION\\n                \"\\\\nINPUT-OUTPUT SECTION.\",\\n                # Split along paragraphs and common statements\\n                \"\\\\nOPEN \",\\n                \"\\\\nCLOSE \",\\n                \"\\\\nREAD \",\\n                \"\\\\nWRITE \",\\n                \"\\\\nIF \",\\n                \"\\\\nELSE \",\\n                \"\\\\nMOVE \",\\n                \"\\\\nPERFORM \",\\n                \"\\\\nUNTIL \",\\n                \"\\\\nVARYING \",\\n                \"\\\\nACCEPT \",\\n                \"\\\\nDISPLAY \",\\n                \"\\\\nSTOP RUN.\",\\n                # Split by the normal type of lines\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]',\n",
              "  '_separator = separator if self._is_separator_regex else re.escape(separator)\\n        splits = _split_text_with_regex(text, _separator, self._keep_separator)\\n\\n        # Now go merging things, recursively splitting longer texts.\\n        _good_splits = []\\n        _separator = \"\" if self._keep_separator else separator\\n        for s in splits:\\n            if self._length_function(s) < self._chunk_size:\\n                _good_splits.append(s)\\n            else:\\n                if _good_splits:\\n                    merged_text = self._merge_splits(_good_splits, _separator)\\n                    final_chunks.extend(merged_text)\\n                    _good_splits = []\\n                if not new_separators:\\n                    final_chunks.append(s)\\n                else:\\n                    other_info = self._split_text(s, new_separators)\\n                    final_chunks.extend(other_info)\\n        if _good_splits:\\n            merged_text = self._merge_splits(_good_splits, _separator)\\n            final_chunks.extend(merged_text)\\n        return final_chunks\\n\\n    def split_text(self, text: str) -> List[str]:\\n        return self._split_text(text, self._separators)\\n\\n    @classmethod\\n    def from_language(\\n        cls, language: Language, **kwargs: Any\\n    ) -> RecursiveCharacterTextSplitter:\\n        separators = cls.get_separators_for_language(language)\\n        return cls(separators=separators, is_separator_regex=True, **kwargs)',\n",
              "  '\"\\\\nevent \",\\n                \"\\\\nmodifier \",\\n                \"\\\\nerror \",\\n                \"\\\\nstruct \",\\n                \"\\\\nenum \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo while \",\\n                \"\\\\nassembly \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.COBOL:\\n            return [\\n                # Split along divisions\\n                \"\\\\nIDENTIFICATION DIVISION.\",\\n                \"\\\\nENVIRONMENT DIVISION.\",\\n                \"\\\\nDATA DIVISION.\",\\n                \"\\\\nPROCEDURE DIVISION.\",\\n                # Split along sections within DATA DIVISION\\n                \"\\\\nWORKING-STORAGE SECTION.\",\\n                \"\\\\nLINKAGE SECTION.\",\\n                \"\\\\nFILE SECTION.\",\\n                # Split along sections within PROCEDURE DIVISION\\n                \"\\\\nINPUT-OUTPUT SECTION.\",\\n                # Split along paragraphs and common statements\\n                \"\\\\nOPEN \",\\n                \"\\\\nCLOSE \",\\n                \"\\\\nREAD \",\\n                \"\\\\nWRITE \",\\n                \"\\\\nIF \",\\n                \"\\\\nELSE \",\\n                \"\\\\nMOVE \",\\n                \"\\\\nPERFORM \",\\n                \"\\\\nUNTIL \",\\n                \"\\\\nVARYING \",\\n                \"\\\\nACCEPT \",\\n                \"\\\\nDISPLAY \",\\n                \"\\\\nSTOP RUN.\",\\n                # Split by the normal type of lines\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]',\n",
              "  '\"<nav\",\\n                # Head\\n                \"<head\",\\n                \"<style\",\\n                \"<script\",\\n                \"<meta\",\\n                \"<title\",\\n                \"\",\\n            ]\\n        elif language == Language.CSHARP:\\n            return [\\n                \"\\\\ninterface \",\\n                \"\\\\nenum \",\\n                \"\\\\nimplements \",\\n                \"\\\\ndelegate \",\\n                \"\\\\nevent \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nabstract \",\\n                # Split along method definitions\\n                \"\\\\npublic \",\\n                \"\\\\nprotected \",\\n                \"\\\\nprivate \",\\n                \"\\\\nstatic \",\\n                \"\\\\nreturn \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\ncontinue \",\\n                \"\\\\nfor \",\\n                \"\\\\nforeach \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\nbreak \",\\n                \"\\\\ncase \",\\n                \"\\\\nelse \",\\n                # Split by exceptions\\n                \"\\\\ntry \",\\n                \"\\\\nthrow \",\\n                \"\\\\nfinally \",\\n                \"\\\\ncatch \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SOL:\\n            return [\\n                # Split along compiler information definitions\\n                \"\\\\npragma \",\\n                \"\\\\nusing \",\\n                # Split along contract definitions\\n                \"\\\\ncontract \",\\n                \"\\\\ninterface \",\\n                \"\\\\nlibrary \",\\n                # Split along method definitions\\n                \"\\\\nconstructor \",\\n                \"\\\\ntype \",\\n                \"\\\\nfunction \",\\n                \"\\\\nevent \",\\n                \"\\\\nmodifier \",\\n                \"\\\\nerror \",\\n                \"\\\\nstruct \",\\n                \"\\\\nenum \",\\n                # Split along control flow statements',\n",
              "  '\"\\\\n\\\\\\\\*\\\\\\\\*\\\\\\\\*+\\\\n\",\\n                \"\\\\n---+\\\\n\",\\n                \"\\\\n___+\\\\n\",\\n                # Note that this splitter doesn\\'t handle horizontal lines defined\\n                # by *three or more* of ***, ---, or ___, but this is not handled\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.LATEX:\\n            return [\\n                # First, try to split along Latex sections\\n                \"\\\\n\\\\\\\\\\\\\\\\chapter{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\section{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\subsection{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\subsubsection{\",\\n                # Now split by environments\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{enumerate}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{itemize}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{description}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{list}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{quote}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{quotation}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{verse}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{verbatim}\",\\n                # Now split by math environments\\n                \"\\\\n\\\\\\\\\\\\begin{align}\",\\n                \"$$\",\\n                \"$\",\\n                # Now split by the normal type of lines\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.HTML:\\n            return [\\n                # First, try to split along HTML tags\\n                \"<body\",\\n                \"<div\",\\n                \"<p\",\\n                \"<br\",\\n                \"<li\",\\n                \"<h1\",\\n                \"<h2\",\\n                \"<h3\",\\n                \"<h4\",\\n                \"<h5\",\\n                \"<h6\",\\n                \"<span\",\\n                \"<table\",\\n                \"<tr\",\\n                \"<td\",\\n                \"<th\",\\n                \"<ul\",\\n                \"<ol\",\\n                \"<header\",\\n                \"<footer\",\\n                \"<nav\",\\n                # Head\\n                \"<head\",\\n                \"<style\",\\n                \"<script\",\\n                \"<meta\",\\n                \"<title\",',\n",
              "  '\" \",\\n                \"\",\\n            ]\\n        elif language == Language.SCALA:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nobject \",\\n                # Split along method definitions\\n                \"\\\\ndef \",\\n                \"\\\\nval \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nmatch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SWIFT:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunc \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nstruct \",\\n                \"\\\\nenum \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.MARKDOWN:\\n            return [\\n                # First, try to split along Markdown headings (starting with level 2)\\n                \"\\\\n#{1,6} \",\\n                # Note the alternative syntax for headings (below) is not handled here\\n                # Heading level 2\\n                # ---------------\\n                # End of code block\\n                \"```\\\\n\",\\n                # Horizontal lines\\n                \"\\\\n\\\\\\\\*\\\\\\\\*\\\\\\\\*+\\\\n\",\\n                \"\\\\n---+\\\\n\",\\n                \"\\\\n___+\\\\n\",\\n                # Note that this splitter doesn\\'t handle horizontal lines defined',\n",
              "  'return [\\n                # First, try to split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\ndef \",\\n                \"\\\\n\\\\tdef \",\\n                # Now split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.RST:\\n            return [\\n                # Split along section titles\\n                \"\\\\n=+\\\\n\",\\n                \"\\\\n-+\\\\n\",\\n                \"\\\\n\\\\\\\\*+\\\\n\",\\n                # Split along directive markers\\n                \"\\\\n\\\\n.. *\\\\n\\\\n\",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.RUBY:\\n            return [\\n                # Split along method definitions\\n                \"\\\\ndef \",\\n                \"\\\\nclass \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nunless \",\\n                \"\\\\nwhile \",\\n                \"\\\\nfor \",\\n                \"\\\\ndo \",\\n                \"\\\\nbegin \",\\n                \"\\\\nrescue \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.RUST:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfn \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nwhile \",\\n                \"\\\\nfor \",\\n                \"\\\\nloop \",\\n                \"\\\\nmatch \",\\n                \"\\\\nconst \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SCALA:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",',\n",
              "  '\"\\\\nfunction \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                \"\\\\ndefault \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.PHP:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunction \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nforeach \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.PROTO:\\n            return [\\n                # Split along message definitions\\n                \"\\\\nmessage \",\\n                # Split along service definitions\\n                \"\\\\nservice \",\\n                # Split along enum definitions\\n                \"\\\\nenum \",\\n                # Split along option definitions\\n                \"\\\\noption \",\\n                # Split along import statements\\n                \"\\\\nimport \",\\n                # Split along syntax declarations\\n                \"\\\\nsyntax \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.PYTHON:\\n            return [\\n                # First, try to split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\ndef \",\\n                \"\\\\n\\\\tdef \",',\n",
              "  '\"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.KOTLIN:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along method definitions\\n                \"\\\\npublic \",\\n                \"\\\\nprotected \",\\n                \"\\\\nprivate \",\\n                \"\\\\ninternal \",\\n                \"\\\\ncompanion \",\\n                \"\\\\nfun \",\\n                \"\\\\nval \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nwhen \",\\n                \"\\\\ncase \",\\n                \"\\\\nelse \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.JS:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunction \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                \"\\\\nvar \",\\n                \"\\\\nclass \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                \"\\\\ndefault \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.TS:\\n            return [\\n                \"\\\\nenum \",\\n                \"\\\\ninterface \",\\n                \"\\\\nnamespace \",\\n                \"\\\\ntype \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along function definitions\\n                \"\\\\nfunction \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements',\n",
              "  '@staticmethod\\n    def get_separators_for_language(language: Language) -> List[str]:\\n        if language == Language.CPP:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along function definitions\\n                \"\\\\nvoid \",\\n                \"\\\\nint \",\\n                \"\\\\nfloat \",\\n                \"\\\\ndouble \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.GO:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunc \",\\n                \"\\\\nvar \",\\n                \"\\\\nconst \",\\n                \"\\\\ntype \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.JAVA:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along method definitions\\n                \"\\\\npublic \",\\n                \"\\\\nprotected \",\\n                \"\\\\nprivate \",\\n                \"\\\\nstatic \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.KOTLIN:\\n            return [',\n",
              "  'if in_code_block:\\n                current_content.append(stripped_line)\\n                continue\\n\\n            # Check each line against each of the header types (e.g., #, ##)\\n            for sep, name in self.headers_to_split_on:\\n                # Check if line starts with a header that we intend to split on\\n                if stripped_line.startswith(sep) and (\\n                    # Header with no text OR header is followed by space\\n                    # Both are valid conditions that sep is being used a header\\n                    len(stripped_line) == len(sep) or stripped_line[len(sep)] == \" \"\\n                ):\\n                    # Ensure we are tracking the header as metadata\\n                    if name is not None:\\n                        # Get the current header level\\n                        current_header_level = sep.count(\"#\")\\n\\n                        # Pop out headers of lower or same level from the stack\\n                        while (\\n                            header_stack\\n                            and header_stack[-1][\"level\"] >= current_header_level\\n                        ):\\n                            # We have encountered a new header\\n                            # at the same or higher level\\n                            popped_header = header_stack.pop()\\n                            # Clear the metadata for the\\n                            # popped header in initial_metadata\\n                            if popped_header[\"name\"] in initial_metadata:\\n                                initial_metadata.pop(popped_header[\"name\"])',\n",
              "  'class RecursiveCharacterTextSplitter(TextSplitter):\\n    \"\"\"Splitting text by recursively look at characters.\\n\\n    Recursively tries to split by different characters to find one\\n    that works.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        separators: Optional[List[str]] = None,\\n        keep_separator: bool = True,\\n        is_separator_regex: bool = False,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(keep_separator=keep_separator, **kwargs)\\n        self._separators = separators or [\"\\\\n\\\\n\", \"\\\\n\", \" \", \"\"]\\n        self._is_separator_regex = is_separator_regex\\n\\n    def _split_text(self, text: str, separators: List[str]) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        final_chunks = []\\n        # Get appropriate separator to use\\n        separator = separators[-1]\\n        new_separators = []\\n        for i, _s in enumerate(separators):\\n            _separator = _s if self._is_separator_regex else re.escape(_s)\\n            if _s == \"\":\\n                separator = _s\\n                break\\n            if re.search(_separator, text):\\n                separator = _s\\n                new_separators = separators[i + 1 :]\\n                break\\n\\n        _separator = separator if self._is_separator_regex else re.escape(separator)\\n        splits = _split_text_with_regex(text, _separator, self._keep_separator)',\n",
              "  'class Language(str, Enum):\\n    \"\"\"Enum of the programming languages.\"\"\"\\n\\n    CPP = \"cpp\"\\n    GO = \"go\"\\n    JAVA = \"java\"\\n    KOTLIN = \"kotlin\"\\n    JS = \"js\"\\n    TS = \"ts\"\\n    PHP = \"php\"\\n    PROTO = \"proto\"\\n    PYTHON = \"python\"\\n    RST = \"rst\"\\n    RUBY = \"ruby\"\\n    RUST = \"rust\"\\n    SCALA = \"scala\"\\n    SWIFT = \"swift\"\\n    MARKDOWN = \"markdown\"\\n    LATEX = \"latex\"\\n    HTML = \"html\"\\n    SOL = \"sol\"\\n    CSHARP = \"csharp\"\\n    COBOL = \"cobol\"',\n",
              "  'def split_text(self, text: str) -> List[str]:\\n        def encode_strip_start_and_stop_token_ids(text: str) -> List[int]:\\n            return self._encode(text)[1:-1]\\n\\n        tokenizer = Tokenizer(\\n            chunk_overlap=self._chunk_overlap,\\n            tokens_per_chunk=self.tokens_per_chunk,\\n            decode=self.tokenizer.decode,\\n            encode=encode_strip_start_and_stop_token_ids,\\n        )\\n\\n        return split_text_on_tokens(text=text, tokenizer=tokenizer)\\n\\n    def count_tokens(self, *, text: str) -> int:\\n        return len(self._encode(text))\\n\\n    _max_length_equal_32_bit_integer: int = 2**32\\n\\n    def _encode(self, text: str) -> List[int]:\\n        token_ids_with_start_and_end_token_ids = self.tokenizer.encode(\\n            text,\\n            max_length=self._max_length_equal_32_bit_integer,\\n            truncation=\"do_not_truncate\",\\n        )\\n        return token_ids_with_start_and_end_token_ids',\n",
              "  'class SentenceTransformersTokenTextSplitter(TextSplitter):\\n    \"\"\"Splitting text to tokens using sentence model tokenizer.\"\"\"\\n\\n    def __init__(\\n        self,\\n        chunk_overlap: int = 50,\\n        model_name: str = \"sentence-transformers/all-mpnet-base-v2\",\\n        tokens_per_chunk: Optional[int] = None,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(**kwargs, chunk_overlap=chunk_overlap)\\n\\n        try:\\n            from sentence_transformers import SentenceTransformer\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import sentence_transformer python package. \"\\n                \"This is needed in order to for SentenceTransformersTokenTextSplitter. \"\\n                \"Please install it with `pip install sentence-transformers`.\"\\n            )\\n\\n        self.model_name = model_name\\n        self._model = SentenceTransformer(self.model_name)\\n        self.tokenizer = self._model.tokenizer\\n        self._initialize_chunk_configuration(tokens_per_chunk=tokens_per_chunk)\\n\\n    def _initialize_chunk_configuration(\\n        self, *, tokens_per_chunk: Optional[int]\\n    ) -> None:\\n        self.maximum_tokens_per_chunk = cast(int, self._model.max_seq_length)\\n\\n        if tokens_per_chunk is None:\\n            self.tokens_per_chunk = self.maximum_tokens_per_chunk\\n        else:\\n            self.tokens_per_chunk = tokens_per_chunk\\n\\n        if self.tokens_per_chunk > self.maximum_tokens_per_chunk:\\n            raise ValueError(\\n                f\"The token limit of the models \\'{self.model_name}\\'\"\\n                f\" is: {self.maximum_tokens_per_chunk}.\"\\n                f\" Argument tokens_per_chunk={self.tokens_per_chunk}\"\\n                f\" > maximum token limit.\"\\n            )\\n\\n    def split_text(self, text: str) -> List[str]:\\n        def encode_strip_start_and_stop_token_ids(text: str) -> List[int]:\\n            return self._encode(text)[1:-1]',\n",
              "  'class TokenTextSplitter(TextSplitter):\\n    \"\"\"Splitting text to tokens using model tokenizer.\"\"\"\\n\\n    def __init__(\\n        self,\\n        encoding_name: str = \"gpt2\",\\n        model_name: Optional[str] = None,\\n        allowed_special: Union[Literal[\"all\"], AbstractSet[str]] = set(),\\n        disallowed_special: Union[Literal[\"all\"], Collection[str]] = \"all\",\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(**kwargs)\\n        try:\\n            import tiktoken\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import tiktoken python package. \"\\n                \"This is needed in order to for TokenTextSplitter. \"\\n                \"Please install it with `pip install tiktoken`.\"\\n            )\\n\\n        if model_name is not None:\\n            enc = tiktoken.encoding_for_model(model_name)\\n        else:\\n            enc = tiktoken.get_encoding(encoding_name)\\n        self._tokenizer = enc\\n        self._allowed_special = allowed_special\\n        self._disallowed_special = disallowed_special\\n\\n    def split_text(self, text: str) -> List[str]:\\n        def _encode(_text: str) -> List[int]:\\n            return self._tokenizer.encode(\\n                _text,\\n                allowed_special=self._allowed_special,\\n                disallowed_special=self._disallowed_special,\\n            )\\n\\n        tokenizer = Tokenizer(\\n            chunk_overlap=self._chunk_overlap,\\n            tokens_per_chunk=self._chunk_size,\\n            decode=self._tokenizer.decode,\\n            encode=_encode,\\n        )\\n\\n        return split_text_on_tokens(text=text, tokenizer=tokenizer)',\n",
              "  'class Tokenizer:\\n    \"\"\"Tokenizer data class.\"\"\"\\n\\n    chunk_overlap: int\\n    \"\"\"Overlap in tokens between chunks\"\"\"\\n    tokens_per_chunk: int\\n    \"\"\"Maximum number of tokens per chunk\"\"\"\\n    decode: Callable[[List[int]], str]\\n    \"\"\" Function to decode a list of token ids to a string\"\"\"\\n    encode: Callable[[str], List[int]]\\n    \"\"\" Function to encode a string to a list of token ids\"\"\"\\n\\n\\ndef split_text_on_tokens(*, text: str, tokenizer: Tokenizer) -> List[str]:\\n    \"\"\"Split incoming text and return chunks using tokenizer.\"\"\"\\n    splits: List[str] = []\\n    input_ids = tokenizer.encode(text)\\n    start_idx = 0\\n    cur_idx = min(start_idx + tokenizer.tokens_per_chunk, len(input_ids))\\n    chunk_ids = input_ids[start_idx:cur_idx]\\n    while start_idx < len(input_ids):\\n        splits.append(tokenizer.decode(chunk_ids))\\n        if cur_idx == len(input_ids):\\n            break\\n        start_idx += tokenizer.tokens_per_chunk - tokenizer.chunk_overlap\\n        cur_idx = min(start_idx + tokenizer.tokens_per_chunk, len(input_ids))\\n        chunk_ids = input_ids[start_idx:cur_idx]\\n    return splits',\n",
              "  '# map xhtml namespace prefix\\n        ns_map = {\"h\": \"http://www.w3.org/1999/xhtml\"}\\n\\n        # build list of elements from DOM\\n        elements = []\\n        for element in result_dom.findall(\"*//*\", ns_map):\\n            if element.findall(\"*[@class=\\'headers\\']\") or element.findall(\\n                \"*[@class=\\'chunk\\']\"\\n            ):\\n                elements.append(\\n                    ElementType(\\n                        url=file,\\n                        xpath=\"\".join(\\n                            [\\n                                node.text\\n                                for node in element.findall(\"*[@class=\\'xpath\\']\", ns_map)\\n                            ]\\n                        ),\\n                        content=\"\".join(\\n                            [\\n                                node.text\\n                                for node in element.findall(\"*[@class=\\'chunk\\']\", ns_map)\\n                            ]\\n                        ),\\n                        metadata={\\n                            # Add text of specified headers to metadata using header\\n                            # mapping.\\n                            header_mapping[node.tag]: node.text\\n                            for node in filter(\\n                                lambda x: x.tag in header_filter,\\n                                element.findall(\"*[@class=\\'headers\\']/*\", ns_map),\\n                            )\\n                        },\\n                    )\\n                )\\n\\n        if not self.return_each_element:\\n            return self.aggregate_elements_to_chunks(elements)\\n        else:\\n            return [\\n                Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n                for chunk in elements\\n            ]\\n\\n\\n# should be in newer Python versions (3.10+)\\n# @dataclass(frozen=True, kw_only=True, slots=True)\\n@dataclass(frozen=True)',\n",
              "  'return [\\n            Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n            for chunk in aggregated_chunks\\n        ]\\n\\n    def split_text_from_url(self, url: str) -> List[Document]:\\n        \"\"\"Split HTML from web URL\\n\\n        Args:\\n            url: web URL\\n        \"\"\"\\n        r = requests.get(url)\\n        return self.split_text_from_file(BytesIO(r.content))\\n\\n    def split_text(self, text: str) -> List[Document]:\\n        \"\"\"Split HTML text string\\n\\n        Args:\\n            text: HTML text\\n        \"\"\"\\n        return self.split_text_from_file(StringIO(text))\\n\\n    def split_text_from_file(self, file: Any) -> List[Document]:\\n        \"\"\"Split HTML file\\n\\n        Args:\\n            file: HTML file\\n        \"\"\"\\n        try:\\n            from lxml import etree\\n        except ImportError as e:\\n            raise ImportError(\\n                \"Unable to import lxml, please install with `pip install lxml`.\"\\n            ) from e\\n        # use lxml library to parse html document and return xml ElementTree\\n        parser = etree.HTMLParser()\\n        tree = etree.parse(file, parser)\\n\\n        # document transformation for \"structure-aware\" chunking is handled with xsl.\\n        # see comments in html_chunks_with_headers.xslt for more detailed information.\\n        xslt_path = (\\n            pathlib.Path(__file__).parent\\n            / \"document_transformers/xsl/html_chunks_with_headers.xslt\"\\n        )\\n        xslt_tree = etree.parse(xslt_path)\\n        transform = etree.XSLT(xslt_tree)\\n        result = transform(tree)\\n        result_dom = etree.fromstring(str(result))\\n\\n        # create filter and mapping for header metadata\\n        header_filter = [header[0] for header in self.headers_to_split_on]\\n        header_mapping = dict(self.headers_to_split_on)\\n\\n        # map xhtml namespace prefix\\n        ns_map = {\"h\": \"http://www.w3.org/1999/xhtml\"}',\n",
              "  '\"\\\\nevent \",\\n                \"\\\\nmodifier \",\\n                \"\\\\nerror \",\\n                \"\\\\nstruct \",\\n                \"\\\\nenum \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo while \",\\n                \"\\\\nassembly \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.COBOL:\\n            return [\\n                # Split along divisions\\n                \"\\\\nIDENTIFICATION DIVISION.\",\\n                \"\\\\nENVIRONMENT DIVISION.\",\\n                \"\\\\nDATA DIVISION.\",\\n                \"\\\\nPROCEDURE DIVISION.\",\\n                # Split along sections within DATA DIVISION\\n                \"\\\\nWORKING-STORAGE SECTION.\",\\n                \"\\\\nLINKAGE SECTION.\",\\n                \"\\\\nFILE SECTION.\",\\n                # Split along sections within PROCEDURE DIVISION\\n                \"\\\\nINPUT-OUTPUT SECTION.\",\\n                # Split along paragraphs and common statements\\n                \"\\\\nOPEN \",\\n                \"\\\\nCLOSE \",\\n                \"\\\\nREAD \",\\n                \"\\\\nWRITE \",\\n                \"\\\\nIF \",\\n                \"\\\\nELSE \",\\n                \"\\\\nMOVE \",\\n                \"\\\\nPERFORM \",\\n                \"\\\\nUNTIL \",\\n                \"\\\\nVARYING \",\\n                \"\\\\nACCEPT \",\\n                \"\\\\nDISPLAY \",\\n                \"\\\\nSTOP RUN.\",\\n                # Split by the normal type of lines\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]',\n",
              "  'if not isinstance(tokenizer, PreTrainedTokenizerBase):\\n                raise ValueError(\\n                    \"Tokenizer received was not an instance of PreTrainedTokenizerBase\"\\n                )\\n\\n            def _huggingface_tokenizer_length(text: str) -> int:\\n                return len(tokenizer.encode(text))\\n\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import transformers python package. \"\\n                \"Please install it with `pip install transformers`.\"\\n            )\\n        return cls(length_function=_huggingface_tokenizer_length, **kwargs)\\n\\n    @classmethod\\n    def from_tiktoken_encoder(\\n        cls: Type[TS],\\n        encoding_name: str = \"gpt2\",\\n        model_name: Optional[str] = None,\\n        allowed_special: Union[Literal[\"all\"], AbstractSet[str]] = set(),\\n        disallowed_special: Union[Literal[\"all\"], Collection[str]] = \"all\",\\n        **kwargs: Any,\\n    ) -> TS:\\n        \"\"\"Text splitter that uses tiktoken encoder to count length.\"\"\"\\n        try:\\n            import tiktoken\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import tiktoken python package. \"\\n                \"This is needed in order to calculate max_tokens_for_prompt. \"\\n                \"Please install it with `pip install tiktoken`.\"\\n            )\\n\\n        if model_name is not None:\\n            enc = tiktoken.encoding_for_model(model_name)\\n        else:\\n            enc = tiktoken.get_encoding(encoding_name)\\n\\n        def _tiktoken_encoder(text: str) -> int:\\n            return len(\\n                enc.encode(\\n                    text,\\n                    allowed_special=allowed_special,\\n                    disallowed_special=disallowed_special,\\n                )\\n            )',\n",
              "  'docs = []\\n        current_doc: List[str] = []\\n        total = 0\\n        for d in splits:\\n            _len = self._length_function(d)\\n            if (\\n                total + _len + (separator_len if len(current_doc) > 0 else 0)\\n                > self._chunk_size\\n            ):\\n                if total > self._chunk_size:\\n                    logger.warning(\\n                        f\"Created a chunk of size {total}, \"\\n                        f\"which is longer than the specified {self._chunk_size}\"\\n                    )\\n                if len(current_doc) > 0:\\n                    doc = self._join_docs(current_doc, separator)\\n                    if doc is not None:\\n                        docs.append(doc)\\n                    # Keep on popping if:\\n                    # - we have a larger chunk than in the chunk overlap\\n                    # - or if we still have any chunks and the length is long\\n                    while total > self._chunk_overlap or (\\n                        total + _len + (separator_len if len(current_doc) > 0 else 0)\\n                        > self._chunk_size\\n                        and total > 0\\n                    ):\\n                        total -= self._length_function(current_doc[0]) + (\\n                            separator_len if len(current_doc) > 1 else 0\\n                        )\\n                        current_doc = current_doc[1:]\\n            current_doc.append(d)\\n            total += _len + (separator_len if len(current_doc) > 1 else 0)\\n        doc = self._join_docs(current_doc, separator)\\n        if doc is not None:\\n            docs.append(doc)\\n        return docs\\n\\n    @classmethod\\n    def from_huggingface_tokenizer(cls, tokenizer: Any, **kwargs: Any) -> TextSplitter:\\n        \"\"\"Text splitter that uses HuggingFace tokenizer to count length.\"\"\"\\n        try:\\n            from transformers import PreTrainedTokenizerBase',\n",
              "  '@abstractmethod\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split text into multiple components.\"\"\"\\n\\n    def create_documents(\\n        self, texts: List[str], metadatas: Optional[List[dict]] = None\\n    ) -> List[Document]:\\n        \"\"\"Create documents from a list of texts.\"\"\"\\n        _metadatas = metadatas or [{}] * len(texts)\\n        documents = []\\n        for i, text in enumerate(texts):\\n            index = -1\\n            for chunk in self.split_text(text):\\n                metadata = copy.deepcopy(_metadatas[i])\\n                if self._add_start_index:\\n                    index = text.find(chunk, index + 1)\\n                    metadata[\"start_index\"] = index\\n                new_doc = Document(page_content=chunk, metadata=metadata)\\n                documents.append(new_doc)\\n        return documents\\n\\n    def split_documents(self, documents: Iterable[Document]) -> List[Document]:\\n        \"\"\"Split documents.\"\"\"\\n        texts, metadatas = [], []\\n        for doc in documents:\\n            texts.append(doc.page_content)\\n            metadatas.append(doc.metadata)\\n        return self.create_documents(texts, metadatas=metadatas)\\n\\n    def _join_docs(self, docs: List[str], separator: str) -> Optional[str]:\\n        text = separator.join(docs)\\n        if self._strip_whitespace:\\n            text = text.strip()\\n        if text == \"\":\\n            return None\\n        else:\\n            return text\\n\\n    def _merge_splits(self, splits: Iterable[str], separator: str) -> List[str]:\\n        # We now want to combine these smaller pieces into medium size\\n        # chunks to send to the LLM.\\n        separator_len = self._length_function(separator)',\n",
              "  'class TextSplitter(BaseDocumentTransformer, ABC):\\n    \"\"\"Interface for splitting text into chunks.\"\"\"\\n\\n    def __init__(\\n        self,\\n        chunk_size: int = 4000,\\n        chunk_overlap: int = 200,\\n        length_function: Callable[[str], int] = len,\\n        keep_separator: bool = False,\\n        add_start_index: bool = False,\\n        strip_whitespace: bool = True,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\\n\\n        Args:\\n            chunk_size: Maximum size of chunks to return\\n            chunk_overlap: Overlap in characters between chunks\\n            length_function: Function that measures the length of given chunks\\n            keep_separator: Whether to keep the separator in the chunks\\n            add_start_index: If `True`, includes chunk\\'s start index in metadata\\n            strip_whitespace: If `True`, strips whitespace from the start and end of\\n                              every document\\n        \"\"\"\\n        if chunk_overlap > chunk_size:\\n            raise ValueError(\\n                f\"Got a larger chunk overlap ({chunk_overlap}) than chunk size \"\\n                f\"({chunk_size}), should be smaller.\"\\n            )\\n        self._chunk_size = chunk_size\\n        self._chunk_overlap = chunk_overlap\\n        self._length_function = length_function\\n        self._keep_separator = keep_separator\\n        self._add_start_index = add_start_index\\n        self._strip_whitespace = strip_whitespace\\n\\n    @abstractmethod\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split text into multiple components.\"\"\"',\n",
              "  'def _split_text_with_regex(\\n    text: str, separator: str, keep_separator: bool\\n) -> List[str]:\\n    # Now that we have the separator, split the text\\n    if separator:\\n        if keep_separator:\\n            # The parentheses in the pattern keep the delimiters in the result.\\n            _splits = re.split(f\"({separator})\", text)\\n            splits = [_splits[i] + _splits[i + 1] for i in range(1, len(_splits), 2)]\\n            if len(_splits) % 2 == 0:\\n                splits += _splits[-1:]\\n            splits = [_splits[0]] + splits\\n        else:\\n            splits = re.split(separator, text)\\n    else:\\n        splits = list(text)\\n    return [s for s in splits if s != \"\"]',\n",
              "  '\"\"\"**Text Splitters** are classes for splitting text.\\n\\n\\n**Class hierarchy:**\\n\\n.. code-block::\\n\\n    BaseDocumentTransformer --> TextSplitter --> <name>TextSplitter  # Example: CharacterTextSplitter\\n                                                 RecursiveCharacterTextSplitter -->  <name>TextSplitter\\n\\nNote: **MarkdownHeaderTextSplitter** and **HTMLHeaderTextSplitter do not derive from TextSplitter.\\n\\n\\n**Main helpers:**\\n\\n.. code-block::\\n\\n    Document, Tokenizer, Language, LineType, HeaderType\\n\\n\"\"\"  # noqa: E501\\n\\nfrom __future__ import annotations\\n\\nimport asyncio\\nimport copy\\nimport logging\\nimport pathlib\\nimport re\\nfrom abc import ABC, abstractmethod\\nfrom dataclasses import dataclass\\nfrom enum import Enum\\nfrom functools import partial\\nfrom io import BytesIO, StringIO\\nfrom typing import (\\n    AbstractSet,\\n    Any,\\n    Callable,\\n    Collection,\\n    Dict,\\n    Iterable,\\n    List,\\n    Literal,\\n    Optional,\\n    Sequence,\\n    Tuple,\\n    Type,\\n    TypedDict,\\n    TypeVar,\\n    Union,\\n    cast,\\n)\\n\\nimport requests\\nfrom langchain_core.documents import BaseDocumentTransformer, Document\\n\\nlogger = logging.getLogger(__name__)\\n\\nTS = TypeVar(\"TS\", bound=\"TextSplitter\")\\n\\n\\ndef _make_spacy_pipeline_for_splitting(\\n    pipeline: str, *, max_length: int = 1_000_000\\n) -> Any:  # avoid importing spacy\\n    try:\\n        import spacy\\n    except ImportError:\\n        raise ImportError(\\n            \"Spacy is not installed, please install it with `pip install spacy`.\"\\n        )\\n    if pipeline == \"sentencizer\":\\n        from spacy.lang.en import English\\n\\n        sentencizer = English()\\n        sentencizer.add_pipe(\"sentencizer\")\\n    else:\\n        sentencizer = spacy.load(pipeline, exclude=[\"ner\", \"tagger\"])\\n        sentencizer.max_length = max_length\\n    return sentencizer',\n",
              "  'class PythonCodeTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Python syntax.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a PythonCodeTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.PYTHON)\\n        super().__init__(separators=separators, **kwargs)\\n\\n\\nclass MarkdownTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Markdown-formatted headings.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a MarkdownTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.MARKDOWN)\\n        super().__init__(separators=separators, **kwargs)\\n\\n\\nclass LatexTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Latex-formatted layout elements.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a LatexTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.LATEX)\\n        super().__init__(separators=separators, **kwargs)',\n",
              "  'class NLTKTextSplitter(TextSplitter):\\n    \"\"\"Splitting text using NLTK package.\"\"\"\\n\\n    def __init__(\\n        self, separator: str = \"\\\\n\\\\n\", language: str = \"english\", **kwargs: Any\\n    ) -> None:\\n        \"\"\"Initialize the NLTK splitter.\"\"\"\\n        super().__init__(**kwargs)\\n        try:\\n            from nltk.tokenize import sent_tokenize\\n\\n            self._tokenizer = sent_tokenize\\n        except ImportError:\\n            raise ImportError(\\n                \"NLTK is not installed, please install it with `pip install nltk`.\"\\n            )\\n        self._separator = separator\\n        self._language = language\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        # First we naively split the large input into a bunch of smaller ones.\\n        splits = self._tokenizer(text, language=self._language)\\n        return self._merge_splits(splits, self._separator)\\n\\n\\nclass SpacyTextSplitter(TextSplitter):\\n    \"\"\"Splitting text using Spacy package.\\n\\n\\n    Per default, Spacy\\'s `en_core_web_sm` model is used and\\n    its default max_length is 1000000 (it is the length of maximum character\\n    this model takes which can be increased for large files). For a faster, but\\n    potentially less accurate splitting, you can use `pipeline=\\'sentencizer\\'`.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        separator: str = \"\\\\n\\\\n\",\\n        pipeline: str = \"en_core_web_sm\",\\n        max_length: int = 1_000_000,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Initialize the spacy text splitter.\"\"\"\\n        super().__init__(**kwargs)\\n        self._tokenizer = _make_spacy_pipeline_for_splitting(\\n            pipeline, max_length=max_length\\n        )\\n        self._separator = separator\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        splits = (s.text for s in self._tokenizer(text).sents)\\n        return self._merge_splits(splits, self._separator)\\n\\n\\n# For backwards compatibility',\n",
              "  'else:\\n            raise ValueError(\\n                f\"Language {language} is not supported! \"\\n                f\"Please choose from {list(Language)}\"\\n            )',\n",
              "  'if issubclass(cls, TokenTextSplitter):\\n            extra_kwargs = {\\n                \"encoding_name\": encoding_name,\\n                \"model_name\": model_name,\\n                \"allowed_special\": allowed_special,\\n                \"disallowed_special\": disallowed_special,\\n            }\\n            kwargs = {**kwargs, **extra_kwargs}\\n\\n        return cls(length_function=_tiktoken_encoder, **kwargs)\\n\\n    def transform_documents(\\n        self, documents: Sequence[Document], **kwargs: Any\\n    ) -> Sequence[Document]:\\n        \"\"\"Transform sequence of documents by splitting them.\"\"\"\\n        return self.split_documents(list(documents))\\n\\n    async def atransform_documents(\\n        self, documents: Sequence[Document], **kwargs: Any\\n    ) -> Sequence[Document]:\\n        \"\"\"Asynchronously transform a sequence of documents by splitting them.\"\"\"\\n        return await asyncio.get_running_loop().run_in_executor(\\n            None, partial(self.transform_documents, **kwargs), documents\\n        )',\n",
              "  '\"<nav\",\\n                # Head\\n                \"<head\",\\n                \"<style\",\\n                \"<script\",\\n                \"<meta\",\\n                \"<title\",\\n                \"\",\\n            ]\\n        elif language == Language.CSHARP:\\n            return [\\n                \"\\\\ninterface \",\\n                \"\\\\nenum \",\\n                \"\\\\nimplements \",\\n                \"\\\\ndelegate \",\\n                \"\\\\nevent \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nabstract \",\\n                # Split along method definitions\\n                \"\\\\npublic \",\\n                \"\\\\nprotected \",\\n                \"\\\\nprivate \",\\n                \"\\\\nstatic \",\\n                \"\\\\nreturn \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\ncontinue \",\\n                \"\\\\nfor \",\\n                \"\\\\nforeach \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\nbreak \",\\n                \"\\\\ncase \",\\n                \"\\\\nelse \",\\n                # Split by exceptions\\n                \"\\\\ntry \",\\n                \"\\\\nthrow \",\\n                \"\\\\nfinally \",\\n                \"\\\\ncatch \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SOL:\\n            return [\\n                # Split along compiler information definitions\\n                \"\\\\npragma \",\\n                \"\\\\nusing \",\\n                # Split along contract definitions\\n                \"\\\\ncontract \",\\n                \"\\\\ninterface \",\\n                \"\\\\nlibrary \",\\n                # Split along method definitions\\n                \"\\\\nconstructor \",\\n                \"\\\\ntype \",\\n                \"\\\\nfunction \",\\n                \"\\\\nevent \",\\n                \"\\\\nmodifier \",\\n                \"\\\\nerror \",\\n                \"\\\\nstruct \",\\n                \"\\\\nenum \",\\n                # Split along control flow statements',\n",
              "  '\"\\\\n\\\\\\\\*\\\\\\\\*\\\\\\\\*+\\\\n\",\\n                \"\\\\n---+\\\\n\",\\n                \"\\\\n___+\\\\n\",\\n                # Note that this splitter doesn\\'t handle horizontal lines defined\\n                # by *three or more* of ***, ---, or ___, but this is not handled\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.LATEX:\\n            return [\\n                # First, try to split along Latex sections\\n                \"\\\\n\\\\\\\\\\\\\\\\chapter{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\section{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\subsection{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\subsubsection{\",\\n                # Now split by environments\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{enumerate}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{itemize}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{description}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{list}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{quote}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{quotation}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{verse}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{verbatim}\",\\n                # Now split by math environments\\n                \"\\\\n\\\\\\\\\\\\begin{align}\",\\n                \"$$\",\\n                \"$\",\\n                # Now split by the normal type of lines\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.HTML:\\n            return [\\n                # First, try to split along HTML tags\\n                \"<body\",\\n                \"<div\",\\n                \"<p\",\\n                \"<br\",\\n                \"<li\",\\n                \"<h1\",\\n                \"<h2\",\\n                \"<h3\",\\n                \"<h4\",\\n                \"<h5\",\\n                \"<h6\",\\n                \"<span\",\\n                \"<table\",\\n                \"<tr\",\\n                \"<td\",\\n                \"<th\",\\n                \"<ul\",\\n                \"<ol\",\\n                \"<header\",\\n                \"<footer\",\\n                \"<nav\",\\n                # Head\\n                \"<head\",\\n                \"<style\",\\n                \"<script\",\\n                \"<meta\",\\n                \"<title\",',\n",
              "  '\" \",\\n                \"\",\\n            ]\\n        elif language == Language.SCALA:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nobject \",\\n                # Split along method definitions\\n                \"\\\\ndef \",\\n                \"\\\\nval \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nmatch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SWIFT:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunc \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nstruct \",\\n                \"\\\\nenum \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.MARKDOWN:\\n            return [\\n                # First, try to split along Markdown headings (starting with level 2)\\n                \"\\\\n#{1,6} \",\\n                # Note the alternative syntax for headings (below) is not handled here\\n                # Heading level 2\\n                # ---------------\\n                # End of code block\\n                \"```\\\\n\",\\n                # Horizontal lines\\n                \"\\\\n\\\\\\\\*\\\\\\\\*\\\\\\\\*+\\\\n\",\\n                \"\\\\n---+\\\\n\",\\n                \"\\\\n___+\\\\n\",\\n                # Note that this splitter doesn\\'t handle horizontal lines defined',\n",
              "  'return [\\n                # First, try to split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\ndef \",\\n                \"\\\\n\\\\tdef \",\\n                # Now split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.RST:\\n            return [\\n                # Split along section titles\\n                \"\\\\n=+\\\\n\",\\n                \"\\\\n-+\\\\n\",\\n                \"\\\\n\\\\\\\\*+\\\\n\",\\n                # Split along directive markers\\n                \"\\\\n\\\\n.. *\\\\n\\\\n\",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.RUBY:\\n            return [\\n                # Split along method definitions\\n                \"\\\\ndef \",\\n                \"\\\\nclass \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nunless \",\\n                \"\\\\nwhile \",\\n                \"\\\\nfor \",\\n                \"\\\\ndo \",\\n                \"\\\\nbegin \",\\n                \"\\\\nrescue \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.RUST:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfn \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nwhile \",\\n                \"\\\\nfor \",\\n                \"\\\\nloop \",\\n                \"\\\\nmatch \",\\n                \"\\\\nconst \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SCALA:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",',\n",
              "  '\"\\\\nfunction \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                \"\\\\ndefault \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.PHP:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunction \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nforeach \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.PROTO:\\n            return [\\n                # Split along message definitions\\n                \"\\\\nmessage \",\\n                # Split along service definitions\\n                \"\\\\nservice \",\\n                # Split along enum definitions\\n                \"\\\\nenum \",\\n                # Split along option definitions\\n                \"\\\\noption \",\\n                # Split along import statements\\n                \"\\\\nimport \",\\n                # Split along syntax declarations\\n                \"\\\\nsyntax \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.PYTHON:\\n            return [\\n                # First, try to split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\ndef \",\\n                \"\\\\n\\\\tdef \",',\n",
              "  '\"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.KOTLIN:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along method definitions\\n                \"\\\\npublic \",\\n                \"\\\\nprotected \",\\n                \"\\\\nprivate \",\\n                \"\\\\ninternal \",\\n                \"\\\\ncompanion \",\\n                \"\\\\nfun \",\\n                \"\\\\nval \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nwhen \",\\n                \"\\\\ncase \",\\n                \"\\\\nelse \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.JS:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunction \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                \"\\\\nvar \",\\n                \"\\\\nclass \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                \"\\\\ndefault \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.TS:\\n            return [\\n                \"\\\\nenum \",\\n                \"\\\\ninterface \",\\n                \"\\\\nnamespace \",\\n                \"\\\\ntype \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along function definitions\\n                \"\\\\nfunction \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements',\n",
              "  '@staticmethod\\n    def get_separators_for_language(language: Language) -> List[str]:\\n        if language == Language.CPP:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along function definitions\\n                \"\\\\nvoid \",\\n                \"\\\\nint \",\\n                \"\\\\nfloat \",\\n                \"\\\\ndouble \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.GO:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunc \",\\n                \"\\\\nvar \",\\n                \"\\\\nconst \",\\n                \"\\\\ntype \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.JAVA:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along method definitions\\n                \"\\\\npublic \",\\n                \"\\\\nprotected \",\\n                \"\\\\nprivate \",\\n                \"\\\\nstatic \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.KOTLIN:\\n            return [',\n",
              "  '_separator = separator if self._is_separator_regex else re.escape(separator)\\n        splits = _split_text_with_regex(text, _separator, self._keep_separator)\\n\\n        # Now go merging things, recursively splitting longer texts.\\n        _good_splits = []\\n        _separator = \"\" if self._keep_separator else separator\\n        for s in splits:\\n            if self._length_function(s) < self._chunk_size:\\n                _good_splits.append(s)\\n            else:\\n                if _good_splits:\\n                    merged_text = self._merge_splits(_good_splits, _separator)\\n                    final_chunks.extend(merged_text)\\n                    _good_splits = []\\n                if not new_separators:\\n                    final_chunks.append(s)\\n                else:\\n                    other_info = self._split_text(s, new_separators)\\n                    final_chunks.extend(other_info)\\n        if _good_splits:\\n            merged_text = self._merge_splits(_good_splits, _separator)\\n            final_chunks.extend(merged_text)\\n        return final_chunks\\n\\n    def split_text(self, text: str) -> List[str]:\\n        return self._split_text(text, self._separators)\\n\\n    @classmethod\\n    def from_language(\\n        cls, language: Language, **kwargs: Any\\n    ) -> RecursiveCharacterTextSplitter:\\n        separators = cls.get_separators_for_language(language)\\n        return cls(separators=separators, is_separator_regex=True, **kwargs)',\n",
              "  'return [\\n            Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n            for chunk in aggregated_chunks\\n        ]\\n\\n    def split_text_from_url(self, url: str) -> List[Document]:\\n        \"\"\"Split HTML from web URL\\n\\n        Args:\\n            url: web URL\\n        \"\"\"\\n        r = requests.get(url)\\n        return self.split_text_from_file(BytesIO(r.content))\\n\\n    def split_text(self, text: str) -> List[Document]:\\n        \"\"\"Split HTML text string\\n\\n        Args:\\n            text: HTML text\\n        \"\"\"\\n        return self.split_text_from_file(StringIO(text))\\n\\n    def split_text_from_file(self, file: Any) -> List[Document]:\\n        \"\"\"Split HTML file\\n\\n        Args:\\n            file: HTML file\\n        \"\"\"\\n        try:\\n            from lxml import etree\\n        except ImportError as e:\\n            raise ImportError(\\n                \"Unable to import lxml, please install with `pip install lxml`.\"\\n            ) from e\\n        # use lxml library to parse html document and return xml ElementTree\\n        parser = etree.HTMLParser()\\n        tree = etree.parse(file, parser)\\n\\n        # document transformation for \"structure-aware\" chunking is handled with xsl.\\n        # see comments in html_chunks_with_headers.xslt for more detailed information.\\n        xslt_path = (\\n            pathlib.Path(__file__).parent\\n            / \"document_transformers/xsl/html_chunks_with_headers.xslt\"\\n        )\\n        xslt_tree = etree.parse(xslt_path)\\n        transform = etree.XSLT(xslt_tree)\\n        result = transform(tree)\\n        result_dom = etree.fromstring(str(result))\\n\\n        # create filter and mapping for header metadata\\n        header_filter = [header[0] for header in self.headers_to_split_on]\\n        header_mapping = dict(self.headers_to_split_on)\\n\\n        # map xhtml namespace prefix\\n        ns_map = {\"h\": \"http://www.w3.org/1999/xhtml\"}',\n",
              "  '@staticmethod\\n    def get_separators_for_language(language: Language) -> List[str]:\\n        if language == Language.CPP:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along function definitions\\n                \"\\\\nvoid \",\\n                \"\\\\nint \",\\n                \"\\\\nfloat \",\\n                \"\\\\ndouble \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.GO:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunc \",\\n                \"\\\\nvar \",\\n                \"\\\\nconst \",\\n                \"\\\\ntype \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.JAVA:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along method definitions\\n                \"\\\\npublic \",\\n                \"\\\\nprotected \",\\n                \"\\\\nprivate \",\\n                \"\\\\nstatic \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.KOTLIN:\\n            return [',\n",
              "  '_separator = separator if self._is_separator_regex else re.escape(separator)\\n        splits = _split_text_with_regex(text, _separator, self._keep_separator)\\n\\n        # Now go merging things, recursively splitting longer texts.\\n        _good_splits = []\\n        _separator = \"\" if self._keep_separator else separator\\n        for s in splits:\\n            if self._length_function(s) < self._chunk_size:\\n                _good_splits.append(s)\\n            else:\\n                if _good_splits:\\n                    merged_text = self._merge_splits(_good_splits, _separator)\\n                    final_chunks.extend(merged_text)\\n                    _good_splits = []\\n                if not new_separators:\\n                    final_chunks.append(s)\\n                else:\\n                    other_info = self._split_text(s, new_separators)\\n                    final_chunks.extend(other_info)\\n        if _good_splits:\\n            merged_text = self._merge_splits(_good_splits, _separator)\\n            final_chunks.extend(merged_text)\\n        return final_chunks\\n\\n    def split_text(self, text: str) -> List[str]:\\n        return self._split_text(text, self._separators)\\n\\n    @classmethod\\n    def from_language(\\n        cls, language: Language, **kwargs: Any\\n    ) -> RecursiveCharacterTextSplitter:\\n        separators = cls.get_separators_for_language(language)\\n        return cls(separators=separators, is_separator_regex=True, **kwargs)',\n",
              "  'class RecursiveCharacterTextSplitter(TextSplitter):\\n    \"\"\"Splitting text by recursively look at characters.\\n\\n    Recursively tries to split by different characters to find one\\n    that works.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        separators: Optional[List[str]] = None,\\n        keep_separator: bool = True,\\n        is_separator_regex: bool = False,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(keep_separator=keep_separator, **kwargs)\\n        self._separators = separators or [\"\\\\n\\\\n\", \"\\\\n\", \" \", \"\"]\\n        self._is_separator_regex = is_separator_regex\\n\\n    def _split_text(self, text: str, separators: List[str]) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        final_chunks = []\\n        # Get appropriate separator to use\\n        separator = separators[-1]\\n        new_separators = []\\n        for i, _s in enumerate(separators):\\n            _separator = _s if self._is_separator_regex else re.escape(_s)\\n            if _s == \"\":\\n                separator = _s\\n                break\\n            if re.search(_separator, text):\\n                separator = _s\\n                new_separators = separators[i + 1 :]\\n                break\\n\\n        _separator = separator if self._is_separator_regex else re.escape(separator)\\n        splits = _split_text_with_regex(text, _separator, self._keep_separator)',\n",
              "  'class Language(str, Enum):\\n    \"\"\"Enum of the programming languages.\"\"\"\\n\\n    CPP = \"cpp\"\\n    GO = \"go\"\\n    JAVA = \"java\"\\n    KOTLIN = \"kotlin\"\\n    JS = \"js\"\\n    TS = \"ts\"\\n    PHP = \"php\"\\n    PROTO = \"proto\"\\n    PYTHON = \"python\"\\n    RST = \"rst\"\\n    RUBY = \"ruby\"\\n    RUST = \"rust\"\\n    SCALA = \"scala\"\\n    SWIFT = \"swift\"\\n    MARKDOWN = \"markdown\"\\n    LATEX = \"latex\"\\n    HTML = \"html\"\\n    SOL = \"sol\"\\n    CSHARP = \"csharp\"\\n    COBOL = \"cobol\"',\n",
              "  'def split_text(self, text: str) -> List[str]:\\n        def encode_strip_start_and_stop_token_ids(text: str) -> List[int]:\\n            return self._encode(text)[1:-1]\\n\\n        tokenizer = Tokenizer(\\n            chunk_overlap=self._chunk_overlap,\\n            tokens_per_chunk=self.tokens_per_chunk,\\n            decode=self.tokenizer.decode,\\n            encode=encode_strip_start_and_stop_token_ids,\\n        )\\n\\n        return split_text_on_tokens(text=text, tokenizer=tokenizer)\\n\\n    def count_tokens(self, *, text: str) -> int:\\n        return len(self._encode(text))\\n\\n    _max_length_equal_32_bit_integer: int = 2**32\\n\\n    def _encode(self, text: str) -> List[int]:\\n        token_ids_with_start_and_end_token_ids = self.tokenizer.encode(\\n            text,\\n            max_length=self._max_length_equal_32_bit_integer,\\n            truncation=\"do_not_truncate\",\\n        )\\n        return token_ids_with_start_and_end_token_ids',\n",
              "  'class SentenceTransformersTokenTextSplitter(TextSplitter):\\n    \"\"\"Splitting text to tokens using sentence model tokenizer.\"\"\"\\n\\n    def __init__(\\n        self,\\n        chunk_overlap: int = 50,\\n        model_name: str = \"sentence-transformers/all-mpnet-base-v2\",\\n        tokens_per_chunk: Optional[int] = None,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(**kwargs, chunk_overlap=chunk_overlap)\\n\\n        try:\\n            from sentence_transformers import SentenceTransformer\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import sentence_transformer python package. \"\\n                \"This is needed in order to for SentenceTransformersTokenTextSplitter. \"\\n                \"Please install it with `pip install sentence-transformers`.\"\\n            )\\n\\n        self.model_name = model_name\\n        self._model = SentenceTransformer(self.model_name)\\n        self.tokenizer = self._model.tokenizer\\n        self._initialize_chunk_configuration(tokens_per_chunk=tokens_per_chunk)\\n\\n    def _initialize_chunk_configuration(\\n        self, *, tokens_per_chunk: Optional[int]\\n    ) -> None:\\n        self.maximum_tokens_per_chunk = cast(int, self._model.max_seq_length)\\n\\n        if tokens_per_chunk is None:\\n            self.tokens_per_chunk = self.maximum_tokens_per_chunk\\n        else:\\n            self.tokens_per_chunk = tokens_per_chunk\\n\\n        if self.tokens_per_chunk > self.maximum_tokens_per_chunk:\\n            raise ValueError(\\n                f\"The token limit of the models \\'{self.model_name}\\'\"\\n                f\" is: {self.maximum_tokens_per_chunk}.\"\\n                f\" Argument tokens_per_chunk={self.tokens_per_chunk}\"\\n                f\" > maximum token limit.\"\\n            )\\n\\n    def split_text(self, text: str) -> List[str]:\\n        def encode_strip_start_and_stop_token_ids(text: str) -> List[int]:\\n            return self._encode(text)[1:-1]',\n",
              "  'class TokenTextSplitter(TextSplitter):\\n    \"\"\"Splitting text to tokens using model tokenizer.\"\"\"\\n\\n    def __init__(\\n        self,\\n        encoding_name: str = \"gpt2\",\\n        model_name: Optional[str] = None,\\n        allowed_special: Union[Literal[\"all\"], AbstractSet[str]] = set(),\\n        disallowed_special: Union[Literal[\"all\"], Collection[str]] = \"all\",\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(**kwargs)\\n        try:\\n            import tiktoken\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import tiktoken python package. \"\\n                \"This is needed in order to for TokenTextSplitter. \"\\n                \"Please install it with `pip install tiktoken`.\"\\n            )\\n\\n        if model_name is not None:\\n            enc = tiktoken.encoding_for_model(model_name)\\n        else:\\n            enc = tiktoken.get_encoding(encoding_name)\\n        self._tokenizer = enc\\n        self._allowed_special = allowed_special\\n        self._disallowed_special = disallowed_special\\n\\n    def split_text(self, text: str) -> List[str]:\\n        def _encode(_text: str) -> List[int]:\\n            return self._tokenizer.encode(\\n                _text,\\n                allowed_special=self._allowed_special,\\n                disallowed_special=self._disallowed_special,\\n            )\\n\\n        tokenizer = Tokenizer(\\n            chunk_overlap=self._chunk_overlap,\\n            tokens_per_chunk=self._chunk_size,\\n            decode=self._tokenizer.decode,\\n            encode=_encode,\\n        )\\n\\n        return split_text_on_tokens(text=text, tokenizer=tokenizer)',\n",
              "  'class Tokenizer:\\n    \"\"\"Tokenizer data class.\"\"\"\\n\\n    chunk_overlap: int\\n    \"\"\"Overlap in tokens between chunks\"\"\"\\n    tokens_per_chunk: int\\n    \"\"\"Maximum number of tokens per chunk\"\"\"\\n    decode: Callable[[List[int]], str]\\n    \"\"\" Function to decode a list of token ids to a string\"\"\"\\n    encode: Callable[[str], List[int]]\\n    \"\"\" Function to encode a string to a list of token ids\"\"\"\\n\\n\\ndef split_text_on_tokens(*, text: str, tokenizer: Tokenizer) -> List[str]:\\n    \"\"\"Split incoming text and return chunks using tokenizer.\"\"\"\\n    splits: List[str] = []\\n    input_ids = tokenizer.encode(text)\\n    start_idx = 0\\n    cur_idx = min(start_idx + tokenizer.tokens_per_chunk, len(input_ids))\\n    chunk_ids = input_ids[start_idx:cur_idx]\\n    while start_idx < len(input_ids):\\n        splits.append(tokenizer.decode(chunk_ids))\\n        if cur_idx == len(input_ids):\\n            break\\n        start_idx += tokenizer.tokens_per_chunk - tokenizer.chunk_overlap\\n        cur_idx = min(start_idx + tokenizer.tokens_per_chunk, len(input_ids))\\n        chunk_ids = input_ids[start_idx:cur_idx]\\n    return splits',\n",
              "  '# map xhtml namespace prefix\\n        ns_map = {\"h\": \"http://www.w3.org/1999/xhtml\"}\\n\\n        # build list of elements from DOM\\n        elements = []\\n        for element in result_dom.findall(\"*//*\", ns_map):\\n            if element.findall(\"*[@class=\\'headers\\']\") or element.findall(\\n                \"*[@class=\\'chunk\\']\"\\n            ):\\n                elements.append(\\n                    ElementType(\\n                        url=file,\\n                        xpath=\"\".join(\\n                            [\\n                                node.text\\n                                for node in element.findall(\"*[@class=\\'xpath\\']\", ns_map)\\n                            ]\\n                        ),\\n                        content=\"\".join(\\n                            [\\n                                node.text\\n                                for node in element.findall(\"*[@class=\\'chunk\\']\", ns_map)\\n                            ]\\n                        ),\\n                        metadata={\\n                            # Add text of specified headers to metadata using header\\n                            # mapping.\\n                            header_mapping[node.tag]: node.text\\n                            for node in filter(\\n                                lambda x: x.tag in header_filter,\\n                                element.findall(\"*[@class=\\'headers\\']/*\", ns_map),\\n                            )\\n                        },\\n                    )\\n                )\\n\\n        if not self.return_each_element:\\n            return self.aggregate_elements_to_chunks(elements)\\n        else:\\n            return [\\n                Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n                for chunk in elements\\n            ]\\n\\n\\n# should be in newer Python versions (3.10+)\\n# @dataclass(frozen=True, kw_only=True, slots=True)\\n@dataclass(frozen=True)',\n",
              "  'class RecursiveCharacterTextSplitter(TextSplitter):\\n    \"\"\"Splitting text by recursively look at characters.\\n\\n    Recursively tries to split by different characters to find one\\n    that works.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        separators: Optional[List[str]] = None,\\n        keep_separator: bool = True,\\n        is_separator_regex: bool = False,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(keep_separator=keep_separator, **kwargs)\\n        self._separators = separators or [\"\\\\n\\\\n\", \"\\\\n\", \" \", \"\"]\\n        self._is_separator_regex = is_separator_regex\\n\\n    def _split_text(self, text: str, separators: List[str]) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        final_chunks = []\\n        # Get appropriate separator to use\\n        separator = separators[-1]\\n        new_separators = []\\n        for i, _s in enumerate(separators):\\n            _separator = _s if self._is_separator_regex else re.escape(_s)\\n            if _s == \"\":\\n                separator = _s\\n                break\\n            if re.search(_separator, text):\\n                separator = _s\\n                new_separators = separators[i + 1 :]\\n                break\\n\\n        _separator = separator if self._is_separator_regex else re.escape(separator)\\n        splits = _split_text_with_regex(text, _separator, self._keep_separator)',\n",
              "  'class HTMLHeaderTextSplitter:\\n    \"\"\"\\n    Splitting HTML files based on specified headers.\\n    Requires lxml package.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        headers_to_split_on: List[Tuple[str, str]],\\n        return_each_element: bool = False,\\n    ):\\n        \"\"\"Create a new HTMLHeaderTextSplitter.\\n\\n        Args:\\n            headers_to_split_on: list of tuples of headers we want to track mapped to\\n                (arbitrary) keys for metadata. Allowed header values: h1, h2, h3, h4,\\n                h5, h6 e.g. [(\"h1\", \"Header 1\"), (\"h2\", \"Header 2)].\\n            return_each_element: Return each element w/ associated headers.\\n        \"\"\"\\n        # Output element-by-element or aggregated into chunks w/ common headers\\n        self.return_each_element = return_each_element\\n        self.headers_to_split_on = sorted(headers_to_split_on)\\n\\n    def aggregate_elements_to_chunks(\\n        self, elements: List[ElementType]\\n    ) -> List[Document]:\\n        \"\"\"Combine elements with common metadata into chunks\\n\\n        Args:\\n            elements: HTML element content with associated identifying info and metadata\\n        \"\"\"\\n        aggregated_chunks: List[ElementType] = []\\n\\n        for element in elements:\\n            if (\\n                aggregated_chunks\\n                and aggregated_chunks[-1][\"metadata\"] == element[\"metadata\"]\\n            ):\\n                # If the last element in the aggregated list\\n                # has the same metadata as the current element,\\n                # append the current content to the last element\\'s content\\n                aggregated_chunks[-1][\"content\"] += \"  \\\\n\" + element[\"content\"]\\n            else:\\n                # Otherwise, append the current element to the aggregated list\\n                aggregated_chunks.append(element)\\n\\n        return [\\n            Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n            for chunk in aggregated_chunks\\n        ]',\n",
              "  'class ElementType(TypedDict):\\n    \"\"\"Element type as typed dict.\"\"\"\\n\\n    url: str\\n    xpath: str\\n    content: str\\n    metadata: Dict[str, str]',\n",
              "  'if current_content:\\n            lines_with_metadata.append(\\n                {\"content\": \"\\\\n\".join(current_content), \"metadata\": current_metadata}\\n            )\\n\\n        # lines_with_metadata has each line with associated header metadata\\n        # aggregate these into chunks based on common metadata\\n        if not self.return_each_line:\\n            return self.aggregate_lines_to_chunks(lines_with_metadata)\\n        else:\\n            return [\\n                Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n                for chunk in lines_with_metadata\\n            ]',\n",
              "  '# Push the current header to the stack\\n                        header: HeaderType = {\\n                            \"level\": current_header_level,\\n                            \"name\": name,\\n                            \"data\": stripped_line[len(sep) :].strip(),\\n                        }\\n                        header_stack.append(header)\\n                        # Update initial_metadata with the current header\\n                        initial_metadata[name] = header[\"data\"]\\n\\n                    # Add the previous line to the lines_with_metadata\\n                    # only if current_content is not empty\\n                    if current_content:\\n                        lines_with_metadata.append(\\n                            {\\n                                \"content\": \"\\\\n\".join(current_content),\\n                                \"metadata\": current_metadata.copy(),\\n                            }\\n                        )\\n                        current_content.clear()\\n\\n                    break\\n            else:\\n                if stripped_line:\\n                    current_content.append(stripped_line)\\n                elif current_content:\\n                    lines_with_metadata.append(\\n                        {\\n                            \"content\": \"\\\\n\".join(current_content),\\n                            \"metadata\": current_metadata.copy(),\\n                        }\\n                    )\\n                    current_content.clear()\\n\\n            current_metadata = initial_metadata.copy()\\n\\n        if current_content:\\n            lines_with_metadata.append(\\n                {\"content\": \"\\\\n\".join(current_content), \"metadata\": current_metadata}\\n            )',\n",
              "  'if in_code_block:\\n                current_content.append(stripped_line)\\n                continue\\n\\n            # Check each line against each of the header types (e.g., #, ##)\\n            for sep, name in self.headers_to_split_on:\\n                # Check if line starts with a header that we intend to split on\\n                if stripped_line.startswith(sep) and (\\n                    # Header with no text OR header is followed by space\\n                    # Both are valid conditions that sep is being used a header\\n                    len(stripped_line) == len(sep) or stripped_line[len(sep)] == \" \"\\n                ):\\n                    # Ensure we are tracking the header as metadata\\n                    if name is not None:\\n                        # Get the current header level\\n                        current_header_level = sep.count(\"#\")\\n\\n                        # Pop out headers of lower or same level from the stack\\n                        while (\\n                            header_stack\\n                            and header_stack[-1][\"level\"] >= current_header_level\\n                        ):\\n                            # We have encountered a new header\\n                            # at the same or higher level\\n                            popped_header = header_stack.pop()\\n                            # Clear the metadata for the\\n                            # popped header in initial_metadata\\n                            if popped_header[\"name\"] in initial_metadata:\\n                                initial_metadata.pop(popped_header[\"name\"])',\n",
              "  'def split_text(self, text: str) -> List[Document]:\\n        \"\"\"Split markdown file\\n        Args:\\n            text: Markdown file\"\"\"\\n\\n        # Split the input text by newline character (\"\\\\n\").\\n        lines = text.split(\"\\\\n\")\\n        # Final output\\n        lines_with_metadata: List[LineType] = []\\n        # Content and metadata of the chunk currently being processed\\n        current_content: List[str] = []\\n        current_metadata: Dict[str, str] = {}\\n        # Keep track of the nested header structure\\n        # header_stack: List[Dict[str, Union[int, str]]] = []\\n        header_stack: List[HeaderType] = []\\n        initial_metadata: Dict[str, str] = {}\\n\\n        in_code_block = False\\n        opening_fence = \"\"\\n\\n        for line in lines:\\n            stripped_line = line.strip()\\n\\n            if not in_code_block:\\n                # Exclude inline code spans\\n                if stripped_line.startswith(\"```\") and stripped_line.count(\"```\") == 1:\\n                    in_code_block = True\\n                    opening_fence = \"```\"\\n                elif stripped_line.startswith(\"~~~\"):\\n                    in_code_block = True\\n                    opening_fence = \"~~~\"\\n            else:\\n                if stripped_line.startswith(opening_fence):\\n                    in_code_block = False\\n                    opening_fence = \"\"\\n\\n            if in_code_block:\\n                current_content.append(stripped_line)\\n                continue',\n",
              "  'class MarkdownHeaderTextSplitter:\\n    \"\"\"Splitting markdown files based on specified headers.\"\"\"\\n\\n    def __init__(\\n        self, headers_to_split_on: List[Tuple[str, str]], return_each_line: bool = False\\n    ):\\n        \"\"\"Create a new MarkdownHeaderTextSplitter.\\n\\n        Args:\\n            headers_to_split_on: Headers we want to track\\n            return_each_line: Return each line w/ associated headers\\n        \"\"\"\\n        # Output line-by-line or aggregated into chunks w/ common headers\\n        self.return_each_line = return_each_line\\n        # Given the headers we want to split on,\\n        # (e.g., \"#, ##, etc\") order by length\\n        self.headers_to_split_on = sorted(\\n            headers_to_split_on, key=lambda split: len(split[0]), reverse=True\\n        )\\n\\n    def aggregate_lines_to_chunks(self, lines: List[LineType]) -> List[Document]:\\n        \"\"\"Combine lines with common metadata into chunks\\n        Args:\\n            lines: Line of text / associated header metadata\\n        \"\"\"\\n        aggregated_chunks: List[LineType] = []\\n\\n        for line in lines:\\n            if (\\n                aggregated_chunks\\n                and aggregated_chunks[-1][\"metadata\"] == line[\"metadata\"]\\n            ):\\n                # If the last line in the aggregated list\\n                # has the same metadata as the current line,\\n                # append the current content to the last lines\\'s content\\n                aggregated_chunks[-1][\"content\"] += \"  \\\\n\" + line[\"content\"]\\n            else:\\n                # Otherwise, append the current line to the aggregated list\\n                aggregated_chunks.append(line)\\n\\n        return [\\n            Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n            for chunk in aggregated_chunks\\n        ]\\n\\n    def split_text(self, text: str) -> List[Document]:\\n        \"\"\"Split markdown file\\n        Args:\\n            text: Markdown file\"\"\"',\n",
              "  'class CharacterTextSplitter(TextSplitter):\\n    \"\"\"Splitting text that looks at characters.\"\"\"\\n\\n    def __init__(\\n        self, separator: str = \"\\\\n\\\\n\", is_separator_regex: bool = False, **kwargs: Any\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(**kwargs)\\n        self._separator = separator\\n        self._is_separator_regex = is_separator_regex\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        # First we naively split the large input into a bunch of smaller ones.\\n        separator = (\\n            self._separator if self._is_separator_regex else re.escape(self._separator)\\n        )\\n        splits = _split_text_with_regex(text, separator, self._keep_separator)\\n        _separator = \"\" if self._keep_separator else self._separator\\n        return self._merge_splits(splits, _separator)\\n\\n\\nclass LineType(TypedDict):\\n    \"\"\"Line type as typed dict.\"\"\"\\n\\n    metadata: Dict[str, str]\\n    content: str\\n\\n\\nclass HeaderType(TypedDict):\\n    \"\"\"Header type as typed dict.\"\"\"\\n\\n    level: int\\n    name: str\\n    data: str',\n",
              "  '\"\\\\n\\\\\\\\*\\\\\\\\*\\\\\\\\*+\\\\n\",\\n                \"\\\\n---+\\\\n\",\\n                \"\\\\n___+\\\\n\",\\n                # Note that this splitter doesn\\'t handle horizontal lines defined\\n                # by *three or more* of ***, ---, or ___, but this is not handled\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.LATEX:\\n            return [\\n                # First, try to split along Latex sections\\n                \"\\\\n\\\\\\\\\\\\\\\\chapter{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\section{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\subsection{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\subsubsection{\",\\n                # Now split by environments\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{enumerate}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{itemize}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{description}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{list}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{quote}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{quotation}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{verse}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{verbatim}\",\\n                # Now split by math environments\\n                \"\\\\n\\\\\\\\\\\\begin{align}\",\\n                \"$$\",\\n                \"$\",\\n                # Now split by the normal type of lines\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.HTML:\\n            return [\\n                # First, try to split along HTML tags\\n                \"<body\",\\n                \"<div\",\\n                \"<p\",\\n                \"<br\",\\n                \"<li\",\\n                \"<h1\",\\n                \"<h2\",\\n                \"<h3\",\\n                \"<h4\",\\n                \"<h5\",\\n                \"<h6\",\\n                \"<span\",\\n                \"<table\",\\n                \"<tr\",\\n                \"<td\",\\n                \"<th\",\\n                \"<ul\",\\n                \"<ol\",\\n                \"<header\",\\n                \"<footer\",\\n                \"<nav\",\\n                # Head\\n                \"<head\",\\n                \"<style\",\\n                \"<script\",\\n                \"<meta\",\\n                \"<title\",',\n",
              "  '@abstractmethod\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split text into multiple components.\"\"\"\\n\\n    def create_documents(\\n        self, texts: List[str], metadatas: Optional[List[dict]] = None\\n    ) -> List[Document]:\\n        \"\"\"Create documents from a list of texts.\"\"\"\\n        _metadatas = metadatas or [{}] * len(texts)\\n        documents = []\\n        for i, text in enumerate(texts):\\n            index = -1\\n            for chunk in self.split_text(text):\\n                metadata = copy.deepcopy(_metadatas[i])\\n                if self._add_start_index:\\n                    index = text.find(chunk, index + 1)\\n                    metadata[\"start_index\"] = index\\n                new_doc = Document(page_content=chunk, metadata=metadata)\\n                documents.append(new_doc)\\n        return documents\\n\\n    def split_documents(self, documents: Iterable[Document]) -> List[Document]:\\n        \"\"\"Split documents.\"\"\"\\n        texts, metadatas = [], []\\n        for doc in documents:\\n            texts.append(doc.page_content)\\n            metadatas.append(doc.metadata)\\n        return self.create_documents(texts, metadatas=metadatas)\\n\\n    def _join_docs(self, docs: List[str], separator: str) -> Optional[str]:\\n        text = separator.join(docs)\\n        if self._strip_whitespace:\\n            text = text.strip()\\n        if text == \"\":\\n            return None\\n        else:\\n            return text\\n\\n    def _merge_splits(self, splits: Iterable[str], separator: str) -> List[str]:\\n        # We now want to combine these smaller pieces into medium size\\n        # chunks to send to the LLM.\\n        separator_len = self._length_function(separator)',\n",
              "  'class TextSplitter(BaseDocumentTransformer, ABC):\\n    \"\"\"Interface for splitting text into chunks.\"\"\"\\n\\n    def __init__(\\n        self,\\n        chunk_size: int = 4000,\\n        chunk_overlap: int = 200,\\n        length_function: Callable[[str], int] = len,\\n        keep_separator: bool = False,\\n        add_start_index: bool = False,\\n        strip_whitespace: bool = True,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\\n\\n        Args:\\n            chunk_size: Maximum size of chunks to return\\n            chunk_overlap: Overlap in characters between chunks\\n            length_function: Function that measures the length of given chunks\\n            keep_separator: Whether to keep the separator in the chunks\\n            add_start_index: If `True`, includes chunk\\'s start index in metadata\\n            strip_whitespace: If `True`, strips whitespace from the start and end of\\n                              every document\\n        \"\"\"\\n        if chunk_overlap > chunk_size:\\n            raise ValueError(\\n                f\"Got a larger chunk overlap ({chunk_overlap}) than chunk size \"\\n                f\"({chunk_size}), should be smaller.\"\\n            )\\n        self._chunk_size = chunk_size\\n        self._chunk_overlap = chunk_overlap\\n        self._length_function = length_function\\n        self._keep_separator = keep_separator\\n        self._add_start_index = add_start_index\\n        self._strip_whitespace = strip_whitespace\\n\\n    @abstractmethod\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split text into multiple components.\"\"\"',\n",
              "  'def _split_text_with_regex(\\n    text: str, separator: str, keep_separator: bool\\n) -> List[str]:\\n    # Now that we have the separator, split the text\\n    if separator:\\n        if keep_separator:\\n            # The parentheses in the pattern keep the delimiters in the result.\\n            _splits = re.split(f\"({separator})\", text)\\n            splits = [_splits[i] + _splits[i + 1] for i in range(1, len(_splits), 2)]\\n            if len(_splits) % 2 == 0:\\n                splits += _splits[-1:]\\n            splits = [_splits[0]] + splits\\n        else:\\n            splits = re.split(separator, text)\\n    else:\\n        splits = list(text)\\n    return [s for s in splits if s != \"\"]',\n",
              "  '\"\"\"**Text Splitters** are classes for splitting text.\\n\\n\\n**Class hierarchy:**\\n\\n.. code-block::\\n\\n    BaseDocumentTransformer --> TextSplitter --> <name>TextSplitter  # Example: CharacterTextSplitter\\n                                                 RecursiveCharacterTextSplitter -->  <name>TextSplitter\\n\\nNote: **MarkdownHeaderTextSplitter** and **HTMLHeaderTextSplitter do not derive from TextSplitter.\\n\\n\\n**Main helpers:**\\n\\n.. code-block::\\n\\n    Document, Tokenizer, Language, LineType, HeaderType\\n\\n\"\"\"  # noqa: E501\\n\\nfrom __future__ import annotations\\n\\nimport asyncio\\nimport copy\\nimport logging\\nimport pathlib\\nimport re\\nfrom abc import ABC, abstractmethod\\nfrom dataclasses import dataclass\\nfrom enum import Enum\\nfrom functools import partial\\nfrom io import BytesIO, StringIO\\nfrom typing import (\\n    AbstractSet,\\n    Any,\\n    Callable,\\n    Collection,\\n    Dict,\\n    Iterable,\\n    List,\\n    Literal,\\n    Optional,\\n    Sequence,\\n    Tuple,\\n    Type,\\n    TypedDict,\\n    TypeVar,\\n    Union,\\n    cast,\\n)\\n\\nimport requests\\nfrom langchain_core.documents import BaseDocumentTransformer, Document\\n\\nlogger = logging.getLogger(__name__)\\n\\nTS = TypeVar(\"TS\", bound=\"TextSplitter\")\\n\\n\\ndef _make_spacy_pipeline_for_splitting(\\n    pipeline: str, *, max_length: int = 1_000_000\\n) -> Any:  # avoid importing spacy\\n    try:\\n        import spacy\\n    except ImportError:\\n        raise ImportError(\\n            \"Spacy is not installed, please install it with `pip install spacy`.\"\\n        )\\n    if pipeline == \"sentencizer\":\\n        from spacy.lang.en import English\\n\\n        sentencizer = English()\\n        sentencizer.add_pipe(\"sentencizer\")\\n    else:\\n        sentencizer = spacy.load(pipeline, exclude=[\"ner\", \"tagger\"])\\n        sentencizer.max_length = max_length\\n    return sentencizer',\n",
              "  'class PythonCodeTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Python syntax.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a PythonCodeTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.PYTHON)\\n        super().__init__(separators=separators, **kwargs)\\n\\n\\nclass MarkdownTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Markdown-formatted headings.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a MarkdownTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.MARKDOWN)\\n        super().__init__(separators=separators, **kwargs)\\n\\n\\nclass LatexTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Latex-formatted layout elements.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a LatexTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.LATEX)\\n        super().__init__(separators=separators, **kwargs)',\n",
              "  'class NLTKTextSplitter(TextSplitter):\\n    \"\"\"Splitting text using NLTK package.\"\"\"\\n\\n    def __init__(\\n        self, separator: str = \"\\\\n\\\\n\", language: str = \"english\", **kwargs: Any\\n    ) -> None:\\n        \"\"\"Initialize the NLTK splitter.\"\"\"\\n        super().__init__(**kwargs)\\n        try:\\n            from nltk.tokenize import sent_tokenize\\n\\n            self._tokenizer = sent_tokenize\\n        except ImportError:\\n            raise ImportError(\\n                \"NLTK is not installed, please install it with `pip install nltk`.\"\\n            )\\n        self._separator = separator\\n        self._language = language\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        # First we naively split the large input into a bunch of smaller ones.\\n        splits = self._tokenizer(text, language=self._language)\\n        return self._merge_splits(splits, self._separator)\\n\\n\\nclass SpacyTextSplitter(TextSplitter):\\n    \"\"\"Splitting text using Spacy package.\\n\\n\\n    Per default, Spacy\\'s `en_core_web_sm` model is used and\\n    its default max_length is 1000000 (it is the length of maximum character\\n    this model takes which can be increased for large files). For a faster, but\\n    potentially less accurate splitting, you can use `pipeline=\\'sentencizer\\'`.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        separator: str = \"\\\\n\\\\n\",\\n        pipeline: str = \"en_core_web_sm\",\\n        max_length: int = 1_000_000,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Initialize the spacy text splitter.\"\"\"\\n        super().__init__(**kwargs)\\n        self._tokenizer = _make_spacy_pipeline_for_splitting(\\n            pipeline, max_length=max_length\\n        )\\n        self._separator = separator\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        splits = (s.text for s in self._tokenizer(text).sents)\\n        return self._merge_splits(splits, self._separator)\\n\\n\\n# For backwards compatibility',\n",
              "  'else:\\n            raise ValueError(\\n                f\"Language {language} is not supported! \"\\n                f\"Please choose from {list(Language)}\"\\n            )',\n",
              "  '\"\\\\nevent \",\\n                \"\\\\nmodifier \",\\n                \"\\\\nerror \",\\n                \"\\\\nstruct \",\\n                \"\\\\nenum \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo while \",\\n                \"\\\\nassembly \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.COBOL:\\n            return [\\n                # Split along divisions\\n                \"\\\\nIDENTIFICATION DIVISION.\",\\n                \"\\\\nENVIRONMENT DIVISION.\",\\n                \"\\\\nDATA DIVISION.\",\\n                \"\\\\nPROCEDURE DIVISION.\",\\n                # Split along sections within DATA DIVISION\\n                \"\\\\nWORKING-STORAGE SECTION.\",\\n                \"\\\\nLINKAGE SECTION.\",\\n                \"\\\\nFILE SECTION.\",\\n                # Split along sections within PROCEDURE DIVISION\\n                \"\\\\nINPUT-OUTPUT SECTION.\",\\n                # Split along paragraphs and common statements\\n                \"\\\\nOPEN \",\\n                \"\\\\nCLOSE \",\\n                \"\\\\nREAD \",\\n                \"\\\\nWRITE \",\\n                \"\\\\nIF \",\\n                \"\\\\nELSE \",\\n                \"\\\\nMOVE \",\\n                \"\\\\nPERFORM \",\\n                \"\\\\nUNTIL \",\\n                \"\\\\nVARYING \",\\n                \"\\\\nACCEPT \",\\n                \"\\\\nDISPLAY \",\\n                \"\\\\nSTOP RUN.\",\\n                # Split by the normal type of lines\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]',\n",
              "  '\"<nav\",\\n                # Head\\n                \"<head\",\\n                \"<style\",\\n                \"<script\",\\n                \"<meta\",\\n                \"<title\",\\n                \"\",\\n            ]\\n        elif language == Language.CSHARP:\\n            return [\\n                \"\\\\ninterface \",\\n                \"\\\\nenum \",\\n                \"\\\\nimplements \",\\n                \"\\\\ndelegate \",\\n                \"\\\\nevent \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nabstract \",\\n                # Split along method definitions\\n                \"\\\\npublic \",\\n                \"\\\\nprotected \",\\n                \"\\\\nprivate \",\\n                \"\\\\nstatic \",\\n                \"\\\\nreturn \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\ncontinue \",\\n                \"\\\\nfor \",\\n                \"\\\\nforeach \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\nbreak \",\\n                \"\\\\ncase \",\\n                \"\\\\nelse \",\\n                # Split by exceptions\\n                \"\\\\ntry \",\\n                \"\\\\nthrow \",\\n                \"\\\\nfinally \",\\n                \"\\\\ncatch \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SOL:\\n            return [\\n                # Split along compiler information definitions\\n                \"\\\\npragma \",\\n                \"\\\\nusing \",\\n                # Split along contract definitions\\n                \"\\\\ncontract \",\\n                \"\\\\ninterface \",\\n                \"\\\\nlibrary \",\\n                # Split along method definitions\\n                \"\\\\nconstructor \",\\n                \"\\\\ntype \",\\n                \"\\\\nfunction \",\\n                \"\\\\nevent \",\\n                \"\\\\nmodifier \",\\n                \"\\\\nerror \",\\n                \"\\\\nstruct \",\\n                \"\\\\nenum \",\\n                # Split along control flow statements',\n",
              "  'docs = []\\n        current_doc: List[str] = []\\n        total = 0\\n        for d in splits:\\n            _len = self._length_function(d)\\n            if (\\n                total + _len + (separator_len if len(current_doc) > 0 else 0)\\n                > self._chunk_size\\n            ):\\n                if total > self._chunk_size:\\n                    logger.warning(\\n                        f\"Created a chunk of size {total}, \"\\n                        f\"which is longer than the specified {self._chunk_size}\"\\n                    )\\n                if len(current_doc) > 0:\\n                    doc = self._join_docs(current_doc, separator)\\n                    if doc is not None:\\n                        docs.append(doc)\\n                    # Keep on popping if:\\n                    # - we have a larger chunk than in the chunk overlap\\n                    # - or if we still have any chunks and the length is long\\n                    while total > self._chunk_overlap or (\\n                        total + _len + (separator_len if len(current_doc) > 0 else 0)\\n                        > self._chunk_size\\n                        and total > 0\\n                    ):\\n                        total -= self._length_function(current_doc[0]) + (\\n                            separator_len if len(current_doc) > 1 else 0\\n                        )\\n                        current_doc = current_doc[1:]\\n            current_doc.append(d)\\n            total += _len + (separator_len if len(current_doc) > 1 else 0)\\n        doc = self._join_docs(current_doc, separator)\\n        if doc is not None:\\n            docs.append(doc)\\n        return docs\\n\\n    @classmethod\\n    def from_huggingface_tokenizer(cls, tokenizer: Any, **kwargs: Any) -> TextSplitter:\\n        \"\"\"Text splitter that uses HuggingFace tokenizer to count length.\"\"\"\\n        try:\\n            from transformers import PreTrainedTokenizerBase',\n",
              "  '\" \",\\n                \"\",\\n            ]\\n        elif language == Language.SCALA:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nobject \",\\n                # Split along method definitions\\n                \"\\\\ndef \",\\n                \"\\\\nval \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nmatch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SWIFT:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunc \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nstruct \",\\n                \"\\\\nenum \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.MARKDOWN:\\n            return [\\n                # First, try to split along Markdown headings (starting with level 2)\\n                \"\\\\n#{1,6} \",\\n                # Note the alternative syntax for headings (below) is not handled here\\n                # Heading level 2\\n                # ---------------\\n                # End of code block\\n                \"```\\\\n\",\\n                # Horizontal lines\\n                \"\\\\n\\\\\\\\*\\\\\\\\*\\\\\\\\*+\\\\n\",\\n                \"\\\\n---+\\\\n\",\\n                \"\\\\n___+\\\\n\",\\n                # Note that this splitter doesn\\'t handle horizontal lines defined',\n",
              "  'return [\\n                # First, try to split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\ndef \",\\n                \"\\\\n\\\\tdef \",\\n                # Now split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.RST:\\n            return [\\n                # Split along section titles\\n                \"\\\\n=+\\\\n\",\\n                \"\\\\n-+\\\\n\",\\n                \"\\\\n\\\\\\\\*+\\\\n\",\\n                # Split along directive markers\\n                \"\\\\n\\\\n.. *\\\\n\\\\n\",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.RUBY:\\n            return [\\n                # Split along method definitions\\n                \"\\\\ndef \",\\n                \"\\\\nclass \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nunless \",\\n                \"\\\\nwhile \",\\n                \"\\\\nfor \",\\n                \"\\\\ndo \",\\n                \"\\\\nbegin \",\\n                \"\\\\nrescue \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.RUST:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfn \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nwhile \",\\n                \"\\\\nfor \",\\n                \"\\\\nloop \",\\n                \"\\\\nmatch \",\\n                \"\\\\nconst \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SCALA:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",',\n",
              "  '\"\\\\nfunction \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                \"\\\\ndefault \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.PHP:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunction \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nforeach \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.PROTO:\\n            return [\\n                # Split along message definitions\\n                \"\\\\nmessage \",\\n                # Split along service definitions\\n                \"\\\\nservice \",\\n                # Split along enum definitions\\n                \"\\\\nenum \",\\n                # Split along option definitions\\n                \"\\\\noption \",\\n                # Split along import statements\\n                \"\\\\nimport \",\\n                # Split along syntax declarations\\n                \"\\\\nsyntax \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.PYTHON:\\n            return [\\n                # First, try to split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\ndef \",\\n                \"\\\\n\\\\tdef \",',\n",
              "  '\"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.KOTLIN:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along method definitions\\n                \"\\\\npublic \",\\n                \"\\\\nprotected \",\\n                \"\\\\nprivate \",\\n                \"\\\\ninternal \",\\n                \"\\\\ncompanion \",\\n                \"\\\\nfun \",\\n                \"\\\\nval \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nwhen \",\\n                \"\\\\ncase \",\\n                \"\\\\nelse \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.JS:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunction \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                \"\\\\nvar \",\\n                \"\\\\nclass \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                \"\\\\ndefault \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.TS:\\n            return [\\n                \"\\\\nenum \",\\n                \"\\\\ninterface \",\\n                \"\\\\nnamespace \",\\n                \"\\\\ntype \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along function definitions\\n                \"\\\\nfunction \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements',\n",
              "  '@staticmethod\\n    def get_separators_for_language(language: Language) -> List[str]:\\n        if language == Language.CPP:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along function definitions\\n                \"\\\\nvoid \",\\n                \"\\\\nint \",\\n                \"\\\\nfloat \",\\n                \"\\\\ndouble \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.GO:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunc \",\\n                \"\\\\nvar \",\\n                \"\\\\nconst \",\\n                \"\\\\ntype \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.JAVA:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along method definitions\\n                \"\\\\npublic \",\\n                \"\\\\nprotected \",\\n                \"\\\\nprivate \",\\n                \"\\\\nstatic \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.KOTLIN:\\n            return [',\n",
              "  '_separator = separator if self._is_separator_regex else re.escape(separator)\\n        splits = _split_text_with_regex(text, _separator, self._keep_separator)\\n\\n        # Now go merging things, recursively splitting longer texts.\\n        _good_splits = []\\n        _separator = \"\" if self._keep_separator else separator\\n        for s in splits:\\n            if self._length_function(s) < self._chunk_size:\\n                _good_splits.append(s)\\n            else:\\n                if _good_splits:\\n                    merged_text = self._merge_splits(_good_splits, _separator)\\n                    final_chunks.extend(merged_text)\\n                    _good_splits = []\\n                if not new_separators:\\n                    final_chunks.append(s)\\n                else:\\n                    other_info = self._split_text(s, new_separators)\\n                    final_chunks.extend(other_info)\\n        if _good_splits:\\n            merged_text = self._merge_splits(_good_splits, _separator)\\n            final_chunks.extend(merged_text)\\n        return final_chunks\\n\\n    def split_text(self, text: str) -> List[str]:\\n        return self._split_text(text, self._separators)\\n\\n    @classmethod\\n    def from_language(\\n        cls, language: Language, **kwargs: Any\\n    ) -> RecursiveCharacterTextSplitter:\\n        separators = cls.get_separators_for_language(language)\\n        return cls(separators=separators, is_separator_regex=True, **kwargs)',\n",
              "  'class RecursiveCharacterTextSplitter(TextSplitter):\\n    \"\"\"Splitting text by recursively look at characters.\\n\\n    Recursively tries to split by different characters to find one\\n    that works.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        separators: Optional[List[str]] = None,\\n        keep_separator: bool = True,\\n        is_separator_regex: bool = False,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(keep_separator=keep_separator, **kwargs)\\n        self._separators = separators or [\"\\\\n\\\\n\", \"\\\\n\", \" \", \"\"]\\n        self._is_separator_regex = is_separator_regex\\n\\n    def _split_text(self, text: str, separators: List[str]) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        final_chunks = []\\n        # Get appropriate separator to use\\n        separator = separators[-1]\\n        new_separators = []\\n        for i, _s in enumerate(separators):\\n            _separator = _s if self._is_separator_regex else re.escape(_s)\\n            if _s == \"\":\\n                separator = _s\\n                break\\n            if re.search(_separator, text):\\n                separator = _s\\n                new_separators = separators[i + 1 :]\\n                break\\n\\n        _separator = separator if self._is_separator_regex else re.escape(separator)\\n        splits = _split_text_with_regex(text, _separator, self._keep_separator)',\n",
              "  'class Language(str, Enum):\\n    \"\"\"Enum of the programming languages.\"\"\"\\n\\n    CPP = \"cpp\"\\n    GO = \"go\"\\n    JAVA = \"java\"\\n    KOTLIN = \"kotlin\"\\n    JS = \"js\"\\n    TS = \"ts\"\\n    PHP = \"php\"\\n    PROTO = \"proto\"\\n    PYTHON = \"python\"\\n    RST = \"rst\"\\n    RUBY = \"ruby\"\\n    RUST = \"rust\"\\n    SCALA = \"scala\"\\n    SWIFT = \"swift\"\\n    MARKDOWN = \"markdown\"\\n    LATEX = \"latex\"\\n    HTML = \"html\"\\n    SOL = \"sol\"\\n    CSHARP = \"csharp\"\\n    COBOL = \"cobol\"',\n",
              "  'class ElementType(TypedDict):\\n    \"\"\"Element type as typed dict.\"\"\"\\n\\n    url: str\\n    xpath: str\\n    content: str\\n    metadata: Dict[str, str]',\n",
              "  'class Language(str, Enum):\\n    \"\"\"Enum of the programming languages.\"\"\"\\n\\n    CPP = \"cpp\"\\n    GO = \"go\"\\n    JAVA = \"java\"\\n    KOTLIN = \"kotlin\"\\n    JS = \"js\"\\n    TS = \"ts\"\\n    PHP = \"php\"\\n    PROTO = \"proto\"\\n    PYTHON = \"python\"\\n    RST = \"rst\"\\n    RUBY = \"ruby\"\\n    RUST = \"rust\"\\n    SCALA = \"scala\"\\n    SWIFT = \"swift\"\\n    MARKDOWN = \"markdown\"\\n    LATEX = \"latex\"\\n    HTML = \"html\"\\n    SOL = \"sol\"\\n    CSHARP = \"csharp\"\\n    COBOL = \"cobol\"',\n",
              "  'def split_text(self, text: str) -> List[str]:\\n        def encode_strip_start_and_stop_token_ids(text: str) -> List[int]:\\n            return self._encode(text)[1:-1]\\n\\n        tokenizer = Tokenizer(\\n            chunk_overlap=self._chunk_overlap,\\n            tokens_per_chunk=self.tokens_per_chunk,\\n            decode=self.tokenizer.decode,\\n            encode=encode_strip_start_and_stop_token_ids,\\n        )\\n\\n        return split_text_on_tokens(text=text, tokenizer=tokenizer)\\n\\n    def count_tokens(self, *, text: str) -> int:\\n        return len(self._encode(text))\\n\\n    _max_length_equal_32_bit_integer: int = 2**32\\n\\n    def _encode(self, text: str) -> List[int]:\\n        token_ids_with_start_and_end_token_ids = self.tokenizer.encode(\\n            text,\\n            max_length=self._max_length_equal_32_bit_integer,\\n            truncation=\"do_not_truncate\",\\n        )\\n        return token_ids_with_start_and_end_token_ids',\n",
              "  'class SentenceTransformersTokenTextSplitter(TextSplitter):\\n    \"\"\"Splitting text to tokens using sentence model tokenizer.\"\"\"\\n\\n    def __init__(\\n        self,\\n        chunk_overlap: int = 50,\\n        model_name: str = \"sentence-transformers/all-mpnet-base-v2\",\\n        tokens_per_chunk: Optional[int] = None,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(**kwargs, chunk_overlap=chunk_overlap)\\n\\n        try:\\n            from sentence_transformers import SentenceTransformer\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import sentence_transformer python package. \"\\n                \"This is needed in order to for SentenceTransformersTokenTextSplitter. \"\\n                \"Please install it with `pip install sentence-transformers`.\"\\n            )\\n\\n        self.model_name = model_name\\n        self._model = SentenceTransformer(self.model_name)\\n        self.tokenizer = self._model.tokenizer\\n        self._initialize_chunk_configuration(tokens_per_chunk=tokens_per_chunk)\\n\\n    def _initialize_chunk_configuration(\\n        self, *, tokens_per_chunk: Optional[int]\\n    ) -> None:\\n        self.maximum_tokens_per_chunk = cast(int, self._model.max_seq_length)\\n\\n        if tokens_per_chunk is None:\\n            self.tokens_per_chunk = self.maximum_tokens_per_chunk\\n        else:\\n            self.tokens_per_chunk = tokens_per_chunk\\n\\n        if self.tokens_per_chunk > self.maximum_tokens_per_chunk:\\n            raise ValueError(\\n                f\"The token limit of the models \\'{self.model_name}\\'\"\\n                f\" is: {self.maximum_tokens_per_chunk}.\"\\n                f\" Argument tokens_per_chunk={self.tokens_per_chunk}\"\\n                f\" > maximum token limit.\"\\n            )\\n\\n    def split_text(self, text: str) -> List[str]:\\n        def encode_strip_start_and_stop_token_ids(text: str) -> List[int]:\\n            return self._encode(text)[1:-1]',\n",
              "  'class TokenTextSplitter(TextSplitter):\\n    \"\"\"Splitting text to tokens using model tokenizer.\"\"\"\\n\\n    def __init__(\\n        self,\\n        encoding_name: str = \"gpt2\",\\n        model_name: Optional[str] = None,\\n        allowed_special: Union[Literal[\"all\"], AbstractSet[str]] = set(),\\n        disallowed_special: Union[Literal[\"all\"], Collection[str]] = \"all\",\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(**kwargs)\\n        try:\\n            import tiktoken\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import tiktoken python package. \"\\n                \"This is needed in order to for TokenTextSplitter. \"\\n                \"Please install it with `pip install tiktoken`.\"\\n            )\\n\\n        if model_name is not None:\\n            enc = tiktoken.encoding_for_model(model_name)\\n        else:\\n            enc = tiktoken.get_encoding(encoding_name)\\n        self._tokenizer = enc\\n        self._allowed_special = allowed_special\\n        self._disallowed_special = disallowed_special\\n\\n    def split_text(self, text: str) -> List[str]:\\n        def _encode(_text: str) -> List[int]:\\n            return self._tokenizer.encode(\\n                _text,\\n                allowed_special=self._allowed_special,\\n                disallowed_special=self._disallowed_special,\\n            )\\n\\n        tokenizer = Tokenizer(\\n            chunk_overlap=self._chunk_overlap,\\n            tokens_per_chunk=self._chunk_size,\\n            decode=self._tokenizer.decode,\\n            encode=_encode,\\n        )\\n\\n        return split_text_on_tokens(text=text, tokenizer=tokenizer)',\n",
              "  'class Tokenizer:\\n    \"\"\"Tokenizer data class.\"\"\"\\n\\n    chunk_overlap: int\\n    \"\"\"Overlap in tokens between chunks\"\"\"\\n    tokens_per_chunk: int\\n    \"\"\"Maximum number of tokens per chunk\"\"\"\\n    decode: Callable[[List[int]], str]\\n    \"\"\" Function to decode a list of token ids to a string\"\"\"\\n    encode: Callable[[str], List[int]]\\n    \"\"\" Function to encode a string to a list of token ids\"\"\"\\n\\n\\ndef split_text_on_tokens(*, text: str, tokenizer: Tokenizer) -> List[str]:\\n    \"\"\"Split incoming text and return chunks using tokenizer.\"\"\"\\n    splits: List[str] = []\\n    input_ids = tokenizer.encode(text)\\n    start_idx = 0\\n    cur_idx = min(start_idx + tokenizer.tokens_per_chunk, len(input_ids))\\n    chunk_ids = input_ids[start_idx:cur_idx]\\n    while start_idx < len(input_ids):\\n        splits.append(tokenizer.decode(chunk_ids))\\n        if cur_idx == len(input_ids):\\n            break\\n        start_idx += tokenizer.tokens_per_chunk - tokenizer.chunk_overlap\\n        cur_idx = min(start_idx + tokenizer.tokens_per_chunk, len(input_ids))\\n        chunk_ids = input_ids[start_idx:cur_idx]\\n    return splits',\n",
              "  '# map xhtml namespace prefix\\n        ns_map = {\"h\": \"http://www.w3.org/1999/xhtml\"}\\n\\n        # build list of elements from DOM\\n        elements = []\\n        for element in result_dom.findall(\"*//*\", ns_map):\\n            if element.findall(\"*[@class=\\'headers\\']\") or element.findall(\\n                \"*[@class=\\'chunk\\']\"\\n            ):\\n                elements.append(\\n                    ElementType(\\n                        url=file,\\n                        xpath=\"\".join(\\n                            [\\n                                node.text\\n                                for node in element.findall(\"*[@class=\\'xpath\\']\", ns_map)\\n                            ]\\n                        ),\\n                        content=\"\".join(\\n                            [\\n                                node.text\\n                                for node in element.findall(\"*[@class=\\'chunk\\']\", ns_map)\\n                            ]\\n                        ),\\n                        metadata={\\n                            # Add text of specified headers to metadata using header\\n                            # mapping.\\n                            header_mapping[node.tag]: node.text\\n                            for node in filter(\\n                                lambda x: x.tag in header_filter,\\n                                element.findall(\"*[@class=\\'headers\\']/*\", ns_map),\\n                            )\\n                        },\\n                    )\\n                )\\n\\n        if not self.return_each_element:\\n            return self.aggregate_elements_to_chunks(elements)\\n        else:\\n            return [\\n                Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n                for chunk in elements\\n            ]\\n\\n\\n# should be in newer Python versions (3.10+)\\n# @dataclass(frozen=True, kw_only=True, slots=True)\\n@dataclass(frozen=True)',\n",
              "  'return [\\n            Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n            for chunk in aggregated_chunks\\n        ]\\n\\n    def split_text_from_url(self, url: str) -> List[Document]:\\n        \"\"\"Split HTML from web URL\\n\\n        Args:\\n            url: web URL\\n        \"\"\"\\n        r = requests.get(url)\\n        return self.split_text_from_file(BytesIO(r.content))\\n\\n    def split_text(self, text: str) -> List[Document]:\\n        \"\"\"Split HTML text string\\n\\n        Args:\\n            text: HTML text\\n        \"\"\"\\n        return self.split_text_from_file(StringIO(text))\\n\\n    def split_text_from_file(self, file: Any) -> List[Document]:\\n        \"\"\"Split HTML file\\n\\n        Args:\\n            file: HTML file\\n        \"\"\"\\n        try:\\n            from lxml import etree\\n        except ImportError as e:\\n            raise ImportError(\\n                \"Unable to import lxml, please install with `pip install lxml`.\"\\n            ) from e\\n        # use lxml library to parse html document and return xml ElementTree\\n        parser = etree.HTMLParser()\\n        tree = etree.parse(file, parser)\\n\\n        # document transformation for \"structure-aware\" chunking is handled with xsl.\\n        # see comments in html_chunks_with_headers.xslt for more detailed information.\\n        xslt_path = (\\n            pathlib.Path(__file__).parent\\n            / \"document_transformers/xsl/html_chunks_with_headers.xslt\"\\n        )\\n        xslt_tree = etree.parse(xslt_path)\\n        transform = etree.XSLT(xslt_tree)\\n        result = transform(tree)\\n        result_dom = etree.fromstring(str(result))\\n\\n        # create filter and mapping for header metadata\\n        header_filter = [header[0] for header in self.headers_to_split_on]\\n        header_mapping = dict(self.headers_to_split_on)\\n\\n        # map xhtml namespace prefix\\n        ns_map = {\"h\": \"http://www.w3.org/1999/xhtml\"}',\n",
              "  'class HTMLHeaderTextSplitter:\\n    \"\"\"\\n    Splitting HTML files based on specified headers.\\n    Requires lxml package.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        headers_to_split_on: List[Tuple[str, str]],\\n        return_each_element: bool = False,\\n    ):\\n        \"\"\"Create a new HTMLHeaderTextSplitter.\\n\\n        Args:\\n            headers_to_split_on: list of tuples of headers we want to track mapped to\\n                (arbitrary) keys for metadata. Allowed header values: h1, h2, h3, h4,\\n                h5, h6 e.g. [(\"h1\", \"Header 1\"), (\"h2\", \"Header 2)].\\n            return_each_element: Return each element w/ associated headers.\\n        \"\"\"\\n        # Output element-by-element or aggregated into chunks w/ common headers\\n        self.return_each_element = return_each_element\\n        self.headers_to_split_on = sorted(headers_to_split_on)\\n\\n    def aggregate_elements_to_chunks(\\n        self, elements: List[ElementType]\\n    ) -> List[Document]:\\n        \"\"\"Combine elements with common metadata into chunks\\n\\n        Args:\\n            elements: HTML element content with associated identifying info and metadata\\n        \"\"\"\\n        aggregated_chunks: List[ElementType] = []\\n\\n        for element in elements:\\n            if (\\n                aggregated_chunks\\n                and aggregated_chunks[-1][\"metadata\"] == element[\"metadata\"]\\n            ):\\n                # If the last element in the aggregated list\\n                # has the same metadata as the current element,\\n                # append the current content to the last element\\'s content\\n                aggregated_chunks[-1][\"content\"] += \"  \\\\n\" + element[\"content\"]\\n            else:\\n                # Otherwise, append the current element to the aggregated list\\n                aggregated_chunks.append(element)\\n\\n        return [\\n            Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n            for chunk in aggregated_chunks\\n        ]',\n",
              "  '\"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.KOTLIN:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along method definitions\\n                \"\\\\npublic \",\\n                \"\\\\nprotected \",\\n                \"\\\\nprivate \",\\n                \"\\\\ninternal \",\\n                \"\\\\ncompanion \",\\n                \"\\\\nfun \",\\n                \"\\\\nval \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nwhen \",\\n                \"\\\\ncase \",\\n                \"\\\\nelse \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.JS:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunction \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                \"\\\\nvar \",\\n                \"\\\\nclass \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                \"\\\\ndefault \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.TS:\\n            return [\\n                \"\\\\nenum \",\\n                \"\\\\ninterface \",\\n                \"\\\\nnamespace \",\\n                \"\\\\ntype \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along function definitions\\n                \"\\\\nfunction \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements',\n",
              "  'if current_content:\\n            lines_with_metadata.append(\\n                {\"content\": \"\\\\n\".join(current_content), \"metadata\": current_metadata}\\n            )\\n\\n        # lines_with_metadata has each line with associated header metadata\\n        # aggregate these into chunks based on common metadata\\n        if not self.return_each_line:\\n            return self.aggregate_lines_to_chunks(lines_with_metadata)\\n        else:\\n            return [\\n                Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n                for chunk in lines_with_metadata\\n            ]',\n",
              "  '# Push the current header to the stack\\n                        header: HeaderType = {\\n                            \"level\": current_header_level,\\n                            \"name\": name,\\n                            \"data\": stripped_line[len(sep) :].strip(),\\n                        }\\n                        header_stack.append(header)\\n                        # Update initial_metadata with the current header\\n                        initial_metadata[name] = header[\"data\"]\\n\\n                    # Add the previous line to the lines_with_metadata\\n                    # only if current_content is not empty\\n                    if current_content:\\n                        lines_with_metadata.append(\\n                            {\\n                                \"content\": \"\\\\n\".join(current_content),\\n                                \"metadata\": current_metadata.copy(),\\n                            }\\n                        )\\n                        current_content.clear()\\n\\n                    break\\n            else:\\n                if stripped_line:\\n                    current_content.append(stripped_line)\\n                elif current_content:\\n                    lines_with_metadata.append(\\n                        {\\n                            \"content\": \"\\\\n\".join(current_content),\\n                            \"metadata\": current_metadata.copy(),\\n                        }\\n                    )\\n                    current_content.clear()\\n\\n            current_metadata = initial_metadata.copy()\\n\\n        if current_content:\\n            lines_with_metadata.append(\\n                {\"content\": \"\\\\n\".join(current_content), \"metadata\": current_metadata}\\n            )',\n",
              "  'if in_code_block:\\n                current_content.append(stripped_line)\\n                continue\\n\\n            # Check each line against each of the header types (e.g., #, ##)\\n            for sep, name in self.headers_to_split_on:\\n                # Check if line starts with a header that we intend to split on\\n                if stripped_line.startswith(sep) and (\\n                    # Header with no text OR header is followed by space\\n                    # Both are valid conditions that sep is being used a header\\n                    len(stripped_line) == len(sep) or stripped_line[len(sep)] == \" \"\\n                ):\\n                    # Ensure we are tracking the header as metadata\\n                    if name is not None:\\n                        # Get the current header level\\n                        current_header_level = sep.count(\"#\")\\n\\n                        # Pop out headers of lower or same level from the stack\\n                        while (\\n                            header_stack\\n                            and header_stack[-1][\"level\"] >= current_header_level\\n                        ):\\n                            # We have encountered a new header\\n                            # at the same or higher level\\n                            popped_header = header_stack.pop()\\n                            # Clear the metadata for the\\n                            # popped header in initial_metadata\\n                            if popped_header[\"name\"] in initial_metadata:\\n                                initial_metadata.pop(popped_header[\"name\"])',\n",
              "  'def split_text(self, text: str) -> List[Document]:\\n        \"\"\"Split markdown file\\n        Args:\\n            text: Markdown file\"\"\"\\n\\n        # Split the input text by newline character (\"\\\\n\").\\n        lines = text.split(\"\\\\n\")\\n        # Final output\\n        lines_with_metadata: List[LineType] = []\\n        # Content and metadata of the chunk currently being processed\\n        current_content: List[str] = []\\n        current_metadata: Dict[str, str] = {}\\n        # Keep track of the nested header structure\\n        # header_stack: List[Dict[str, Union[int, str]]] = []\\n        header_stack: List[HeaderType] = []\\n        initial_metadata: Dict[str, str] = {}\\n\\n        in_code_block = False\\n        opening_fence = \"\"\\n\\n        for line in lines:\\n            stripped_line = line.strip()\\n\\n            if not in_code_block:\\n                # Exclude inline code spans\\n                if stripped_line.startswith(\"```\") and stripped_line.count(\"```\") == 1:\\n                    in_code_block = True\\n                    opening_fence = \"```\"\\n                elif stripped_line.startswith(\"~~~\"):\\n                    in_code_block = True\\n                    opening_fence = \"~~~\"\\n            else:\\n                if stripped_line.startswith(opening_fence):\\n                    in_code_block = False\\n                    opening_fence = \"\"\\n\\n            if in_code_block:\\n                current_content.append(stripped_line)\\n                continue',\n",
              "  'class MarkdownHeaderTextSplitter:\\n    \"\"\"Splitting markdown files based on specified headers.\"\"\"\\n\\n    def __init__(\\n        self, headers_to_split_on: List[Tuple[str, str]], return_each_line: bool = False\\n    ):\\n        \"\"\"Create a new MarkdownHeaderTextSplitter.\\n\\n        Args:\\n            headers_to_split_on: Headers we want to track\\n            return_each_line: Return each line w/ associated headers\\n        \"\"\"\\n        # Output line-by-line or aggregated into chunks w/ common headers\\n        self.return_each_line = return_each_line\\n        # Given the headers we want to split on,\\n        # (e.g., \"#, ##, etc\") order by length\\n        self.headers_to_split_on = sorted(\\n            headers_to_split_on, key=lambda split: len(split[0]), reverse=True\\n        )\\n\\n    def aggregate_lines_to_chunks(self, lines: List[LineType]) -> List[Document]:\\n        \"\"\"Combine lines with common metadata into chunks\\n        Args:\\n            lines: Line of text / associated header metadata\\n        \"\"\"\\n        aggregated_chunks: List[LineType] = []\\n\\n        for line in lines:\\n            if (\\n                aggregated_chunks\\n                and aggregated_chunks[-1][\"metadata\"] == line[\"metadata\"]\\n            ):\\n                # If the last line in the aggregated list\\n                # has the same metadata as the current line,\\n                # append the current content to the last lines\\'s content\\n                aggregated_chunks[-1][\"content\"] += \"  \\\\n\" + line[\"content\"]\\n            else:\\n                # Otherwise, append the current line to the aggregated list\\n                aggregated_chunks.append(line)\\n\\n        return [\\n            Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n            for chunk in aggregated_chunks\\n        ]\\n\\n    def split_text(self, text: str) -> List[Document]:\\n        \"\"\"Split markdown file\\n        Args:\\n            text: Markdown file\"\"\"',\n",
              "  'class CharacterTextSplitter(TextSplitter):\\n    \"\"\"Splitting text that looks at characters.\"\"\"\\n\\n    def __init__(\\n        self, separator: str = \"\\\\n\\\\n\", is_separator_regex: bool = False, **kwargs: Any\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(**kwargs)\\n        self._separator = separator\\n        self._is_separator_regex = is_separator_regex\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        # First we naively split the large input into a bunch of smaller ones.\\n        separator = (\\n            self._separator if self._is_separator_regex else re.escape(self._separator)\\n        )\\n        splits = _split_text_with_regex(text, separator, self._keep_separator)\\n        _separator = \"\" if self._keep_separator else self._separator\\n        return self._merge_splits(splits, _separator)\\n\\n\\nclass LineType(TypedDict):\\n    \"\"\"Line type as typed dict.\"\"\"\\n\\n    metadata: Dict[str, str]\\n    content: str\\n\\n\\nclass HeaderType(TypedDict):\\n    \"\"\"Header type as typed dict.\"\"\"\\n\\n    level: int\\n    name: str\\n    data: str',\n",
              "  'if issubclass(cls, TokenTextSplitter):\\n            extra_kwargs = {\\n                \"encoding_name\": encoding_name,\\n                \"model_name\": model_name,\\n                \"allowed_special\": allowed_special,\\n                \"disallowed_special\": disallowed_special,\\n            }\\n            kwargs = {**kwargs, **extra_kwargs}\\n\\n        return cls(length_function=_tiktoken_encoder, **kwargs)\\n\\n    def transform_documents(\\n        self, documents: Sequence[Document], **kwargs: Any\\n    ) -> Sequence[Document]:\\n        \"\"\"Transform sequence of documents by splitting them.\"\"\"\\n        return self.split_documents(list(documents))\\n\\n    async def atransform_documents(\\n        self, documents: Sequence[Document], **kwargs: Any\\n    ) -> Sequence[Document]:\\n        \"\"\"Asynchronously transform a sequence of documents by splitting them.\"\"\"\\n        return await asyncio.get_running_loop().run_in_executor(\\n            None, partial(self.transform_documents, **kwargs), documents\\n        )',\n",
              "  'if not isinstance(tokenizer, PreTrainedTokenizerBase):\\n                raise ValueError(\\n                    \"Tokenizer received was not an instance of PreTrainedTokenizerBase\"\\n                )\\n\\n            def _huggingface_tokenizer_length(text: str) -> int:\\n                return len(tokenizer.encode(text))\\n\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import transformers python package. \"\\n                \"Please install it with `pip install transformers`.\"\\n            )\\n        return cls(length_function=_huggingface_tokenizer_length, **kwargs)\\n\\n    @classmethod\\n    def from_tiktoken_encoder(\\n        cls: Type[TS],\\n        encoding_name: str = \"gpt2\",\\n        model_name: Optional[str] = None,\\n        allowed_special: Union[Literal[\"all\"], AbstractSet[str]] = set(),\\n        disallowed_special: Union[Literal[\"all\"], Collection[str]] = \"all\",\\n        **kwargs: Any,\\n    ) -> TS:\\n        \"\"\"Text splitter that uses tiktoken encoder to count length.\"\"\"\\n        try:\\n            import tiktoken\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import tiktoken python package. \"\\n                \"This is needed in order to calculate max_tokens_for_prompt. \"\\n                \"Please install it with `pip install tiktoken`.\"\\n            )\\n\\n        if model_name is not None:\\n            enc = tiktoken.encoding_for_model(model_name)\\n        else:\\n            enc = tiktoken.get_encoding(encoding_name)\\n\\n        def _tiktoken_encoder(text: str) -> int:\\n            return len(\\n                enc.encode(\\n                    text,\\n                    allowed_special=allowed_special,\\n                    disallowed_special=disallowed_special,\\n                )\\n            )',\n",
              "  'from typing import Tuple, Union\\n\\nfrom langchain.chains.query_constructor.ir import (\\n    Comparator,\\n    Comparison,\\n    Operation,\\n    Operator,\\n    StructuredQuery,\\n    Visitor,\\n)\\n\\n\\ndef process_value(value: Union[int, float, str]) -> str:\\n    \"\"\"Convert a value to a string and add single quotes if it is a string.\"\"\"\\n    if isinstance(value, str):\\n        return f\"\\'{value}\\'\"\\n    else:\\n        return str(value)',\n",
              "  'if not isinstance(tokenizer, PreTrainedTokenizerBase):\\n                raise ValueError(\\n                    \"Tokenizer received was not an instance of PreTrainedTokenizerBase\"\\n                )\\n\\n            def _huggingface_tokenizer_length(text: str) -> int:\\n                return len(tokenizer.encode(text))\\n\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import transformers python package. \"\\n                \"Please install it with `pip install transformers`.\"\\n            )\\n        return cls(length_function=_huggingface_tokenizer_length, **kwargs)\\n\\n    @classmethod\\n    def from_tiktoken_encoder(\\n        cls: Type[TS],\\n        encoding_name: str = \"gpt2\",\\n        model_name: Optional[str] = None,\\n        allowed_special: Union[Literal[\"all\"], AbstractSet[str]] = set(),\\n        disallowed_special: Union[Literal[\"all\"], Collection[str]] = \"all\",\\n        **kwargs: Any,\\n    ) -> TS:\\n        \"\"\"Text splitter that uses tiktoken encoder to count length.\"\"\"\\n        try:\\n            import tiktoken\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import tiktoken python package. \"\\n                \"This is needed in order to calculate max_tokens_for_prompt. \"\\n                \"Please install it with `pip install tiktoken`.\"\\n            )\\n\\n        if model_name is not None:\\n            enc = tiktoken.encoding_for_model(model_name)\\n        else:\\n            enc = tiktoken.get_encoding(encoding_name)\\n\\n        def _tiktoken_encoder(text: str) -> int:\\n            return len(\\n                enc.encode(\\n                    text,\\n                    allowed_special=allowed_special,\\n                    disallowed_special=disallowed_special,\\n                )\\n            )',\n",
              "  'docs = []\\n        current_doc: List[str] = []\\n        total = 0\\n        for d in splits:\\n            _len = self._length_function(d)\\n            if (\\n                total + _len + (separator_len if len(current_doc) > 0 else 0)\\n                > self._chunk_size\\n            ):\\n                if total > self._chunk_size:\\n                    logger.warning(\\n                        f\"Created a chunk of size {total}, \"\\n                        f\"which is longer than the specified {self._chunk_size}\"\\n                    )\\n                if len(current_doc) > 0:\\n                    doc = self._join_docs(current_doc, separator)\\n                    if doc is not None:\\n                        docs.append(doc)\\n                    # Keep on popping if:\\n                    # - we have a larger chunk than in the chunk overlap\\n                    # - or if we still have any chunks and the length is long\\n                    while total > self._chunk_overlap or (\\n                        total + _len + (separator_len if len(current_doc) > 0 else 0)\\n                        > self._chunk_size\\n                        and total > 0\\n                    ):\\n                        total -= self._length_function(current_doc[0]) + (\\n                            separator_len if len(current_doc) > 1 else 0\\n                        )\\n                        current_doc = current_doc[1:]\\n            current_doc.append(d)\\n            total += _len + (separator_len if len(current_doc) > 1 else 0)\\n        doc = self._join_docs(current_doc, separator)\\n        if doc is not None:\\n            docs.append(doc)\\n        return docs\\n\\n    @classmethod\\n    def from_huggingface_tokenizer(cls, tokenizer: Any, **kwargs: Any) -> TextSplitter:\\n        \"\"\"Text splitter that uses HuggingFace tokenizer to count length.\"\"\"\\n        try:\\n            from transformers import PreTrainedTokenizerBase',\n",
              "  '@abstractmethod\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split text into multiple components.\"\"\"\\n\\n    def create_documents(\\n        self, texts: List[str], metadatas: Optional[List[dict]] = None\\n    ) -> List[Document]:\\n        \"\"\"Create documents from a list of texts.\"\"\"\\n        _metadatas = metadatas or [{}] * len(texts)\\n        documents = []\\n        for i, text in enumerate(texts):\\n            index = -1\\n            for chunk in self.split_text(text):\\n                metadata = copy.deepcopy(_metadatas[i])\\n                if self._add_start_index:\\n                    index = text.find(chunk, index + 1)\\n                    metadata[\"start_index\"] = index\\n                new_doc = Document(page_content=chunk, metadata=metadata)\\n                documents.append(new_doc)\\n        return documents\\n\\n    def split_documents(self, documents: Iterable[Document]) -> List[Document]:\\n        \"\"\"Split documents.\"\"\"\\n        texts, metadatas = [], []\\n        for doc in documents:\\n            texts.append(doc.page_content)\\n            metadatas.append(doc.metadata)\\n        return self.create_documents(texts, metadatas=metadatas)\\n\\n    def _join_docs(self, docs: List[str], separator: str) -> Optional[str]:\\n        text = separator.join(docs)\\n        if self._strip_whitespace:\\n            text = text.strip()\\n        if text == \"\":\\n            return None\\n        else:\\n            return text\\n\\n    def _merge_splits(self, splits: Iterable[str], separator: str) -> List[str]:\\n        # We now want to combine these smaller pieces into medium size\\n        # chunks to send to the LLM.\\n        separator_len = self._length_function(separator)',\n",
              "  'class TextSplitter(BaseDocumentTransformer, ABC):\\n    \"\"\"Interface for splitting text into chunks.\"\"\"\\n\\n    def __init__(\\n        self,\\n        chunk_size: int = 4000,\\n        chunk_overlap: int = 200,\\n        length_function: Callable[[str], int] = len,\\n        keep_separator: bool = False,\\n        add_start_index: bool = False,\\n        strip_whitespace: bool = True,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\\n\\n        Args:\\n            chunk_size: Maximum size of chunks to return\\n            chunk_overlap: Overlap in characters between chunks\\n            length_function: Function that measures the length of given chunks\\n            keep_separator: Whether to keep the separator in the chunks\\n            add_start_index: If `True`, includes chunk\\'s start index in metadata\\n            strip_whitespace: If `True`, strips whitespace from the start and end of\\n                              every document\\n        \"\"\"\\n        if chunk_overlap > chunk_size:\\n            raise ValueError(\\n                f\"Got a larger chunk overlap ({chunk_overlap}) than chunk size \"\\n                f\"({chunk_size}), should be smaller.\"\\n            )\\n        self._chunk_size = chunk_size\\n        self._chunk_overlap = chunk_overlap\\n        self._length_function = length_function\\n        self._keep_separator = keep_separator\\n        self._add_start_index = add_start_index\\n        self._strip_whitespace = strip_whitespace\\n\\n    @abstractmethod\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split text into multiple components.\"\"\"',\n",
              "  'def _split_text_with_regex(\\n    text: str, separator: str, keep_separator: bool\\n) -> List[str]:\\n    # Now that we have the separator, split the text\\n    if separator:\\n        if keep_separator:\\n            # The parentheses in the pattern keep the delimiters in the result.\\n            _splits = re.split(f\"({separator})\", text)\\n            splits = [_splits[i] + _splits[i + 1] for i in range(1, len(_splits), 2)]\\n            if len(_splits) % 2 == 0:\\n                splits += _splits[-1:]\\n            splits = [_splits[0]] + splits\\n        else:\\n            splits = re.split(separator, text)\\n    else:\\n        splits = list(text)\\n    return [s for s in splits if s != \"\"]',\n",
              "  '\"\"\"**Text Splitters** are classes for splitting text.\\n\\n\\n**Class hierarchy:**\\n\\n.. code-block::\\n\\n    BaseDocumentTransformer --> TextSplitter --> <name>TextSplitter  # Example: CharacterTextSplitter\\n                                                 RecursiveCharacterTextSplitter -->  <name>TextSplitter\\n\\nNote: **MarkdownHeaderTextSplitter** and **HTMLHeaderTextSplitter do not derive from TextSplitter.\\n\\n\\n**Main helpers:**\\n\\n.. code-block::\\n\\n    Document, Tokenizer, Language, LineType, HeaderType\\n\\n\"\"\"  # noqa: E501\\n\\nfrom __future__ import annotations\\n\\nimport asyncio\\nimport copy\\nimport logging\\nimport pathlib\\nimport re\\nfrom abc import ABC, abstractmethod\\nfrom dataclasses import dataclass\\nfrom enum import Enum\\nfrom functools import partial\\nfrom io import BytesIO, StringIO\\nfrom typing import (\\n    AbstractSet,\\n    Any,\\n    Callable,\\n    Collection,\\n    Dict,\\n    Iterable,\\n    List,\\n    Literal,\\n    Optional,\\n    Sequence,\\n    Tuple,\\n    Type,\\n    TypedDict,\\n    TypeVar,\\n    Union,\\n    cast,\\n)\\n\\nimport requests\\nfrom langchain_core.documents import BaseDocumentTransformer, Document\\n\\nlogger = logging.getLogger(__name__)\\n\\nTS = TypeVar(\"TS\", bound=\"TextSplitter\")\\n\\n\\ndef _make_spacy_pipeline_for_splitting(\\n    pipeline: str, *, max_length: int = 1_000_000\\n) -> Any:  # avoid importing spacy\\n    try:\\n        import spacy\\n    except ImportError:\\n        raise ImportError(\\n            \"Spacy is not installed, please install it with `pip install spacy`.\"\\n        )\\n    if pipeline == \"sentencizer\":\\n        from spacy.lang.en import English\\n\\n        sentencizer = English()\\n        sentencizer.add_pipe(\"sentencizer\")\\n    else:\\n        sentencizer = spacy.load(pipeline, exclude=[\"ner\", \"tagger\"])\\n        sentencizer.max_length = max_length\\n    return sentencizer',\n",
              "  'class VectaraTranslator(Visitor):\\n    \"\"\"Translate `Vectara` internal query language elements to valid filters.\"\"\"\\n\\n    allowed_operators = [Operator.AND, Operator.OR]\\n    \"\"\"Subset of allowed logical operators.\"\"\"\\n    allowed_comparators = [\\n        Comparator.EQ,\\n        Comparator.NE,\\n        Comparator.GT,\\n        Comparator.GTE,\\n        Comparator.LT,\\n        Comparator.LTE,\\n    ]\\n    \"\"\"Subset of allowed logical comparators.\"\"\"\\n\\n    def _format_func(self, func: Union[Operator, Comparator]) -> str:\\n        map_dict = {\\n            Operator.AND: \" and \",\\n            Operator.OR: \" or \",\\n            Comparator.EQ: \"=\",\\n            Comparator.NE: \"!=\",\\n            Comparator.GT: \">\",\\n            Comparator.GTE: \">=\",\\n            Comparator.LT: \"<\",\\n            Comparator.LTE: \"<=\",\\n        }\\n        self._validate_func(func)\\n        return map_dict[func]\\n\\n    def visit_operation(self, operation: Operation) -> str:\\n        args = [arg.accept(self) for arg in operation.arguments]\\n        operator = self._format_func(operation.operator)\\n        return \"( \" + operator.join(args) + \" )\"\\n\\n    def visit_comparison(self, comparison: Comparison) -> str:\\n        comparator = self._format_func(comparison.comparator)\\n        processed_value = process_value(comparison.value)\\n        attribute = comparison.attribute\\n        return (\\n            \"( \" + \"doc.\" + attribute + \" \" + comparator + \" \" + processed_value + \" )\"\\n        )\\n\\n    def visit_structured_query(\\n        self, structured_query: StructuredQuery\\n    ) -> Tuple[str, dict]:\\n        if structured_query.filter is None:\\n            kwargs = {}\\n        else:\\n            kwargs = {\"filter\": structured_query.filter.accept(self)}\\n        return structured_query.query, kwargs',\n",
              "  'from typing import Tuple, Union\\n\\nfrom langchain.chains.query_constructor.ir import (\\n    Comparator,\\n    Comparison,\\n    Operation,\\n    Operator,\\n    StructuredQuery,\\n    Visitor,\\n)\\n\\n\\ndef process_value(value: Union[int, float, str]) -> str:\\n    \"\"\"Convert a value to a string and add single quotes if it is a string.\"\"\"\\n    if isinstance(value, str):\\n        return f\"\\'{value}\\'\"\\n    else:\\n        return str(value)',\n",
              "  'class VectaraTranslator(Visitor):\\n    \"\"\"Translate `Vectara` internal query language elements to valid filters.\"\"\"\\n\\n    allowed_operators = [Operator.AND, Operator.OR]\\n    \"\"\"Subset of allowed logical operators.\"\"\"\\n    allowed_comparators = [\\n        Comparator.EQ,\\n        Comparator.NE,\\n        Comparator.GT,\\n        Comparator.GTE,\\n        Comparator.LT,\\n        Comparator.LTE,\\n    ]\\n    \"\"\"Subset of allowed logical comparators.\"\"\"\\n\\n    def _format_func(self, func: Union[Operator, Comparator]) -> str:\\n        map_dict = {\\n            Operator.AND: \" and \",\\n            Operator.OR: \" or \",\\n            Comparator.EQ: \"=\",\\n            Comparator.NE: \"!=\",\\n            Comparator.GT: \">\",\\n            Comparator.GTE: \">=\",\\n            Comparator.LT: \"<\",\\n            Comparator.LTE: \"<=\",\\n        }\\n        self._validate_func(func)\\n        return map_dict[func]\\n\\n    def visit_operation(self, operation: Operation) -> str:\\n        args = [arg.accept(self) for arg in operation.arguments]\\n        operator = self._format_func(operation.operator)\\n        return \"( \" + operator.join(args) + \" )\"\\n\\n    def visit_comparison(self, comparison: Comparison) -> str:\\n        comparator = self._format_func(comparison.comparator)\\n        processed_value = process_value(comparison.value)\\n        attribute = comparison.attribute\\n        return (\\n            \"( \" + \"doc.\" + attribute + \" \" + comparator + \" \" + processed_value + \" )\"\\n        )\\n\\n    def visit_structured_query(\\n        self, structured_query: StructuredQuery\\n    ) -> Tuple[str, dict]:\\n        if structured_query.filter is None:\\n            kwargs = {}\\n        else:\\n            kwargs = {\"filter\": structured_query.filter.accept(self)}\\n        return structured_query.query, kwargs',\n",
              "  'if issubclass(cls, TokenTextSplitter):\\n            extra_kwargs = {\\n                \"encoding_name\": encoding_name,\\n                \"model_name\": model_name,\\n                \"allowed_special\": allowed_special,\\n                \"disallowed_special\": disallowed_special,\\n            }\\n            kwargs = {**kwargs, **extra_kwargs}\\n\\n        return cls(length_function=_tiktoken_encoder, **kwargs)\\n\\n    def transform_documents(\\n        self, documents: Sequence[Document], **kwargs: Any\\n    ) -> Sequence[Document]:\\n        \"\"\"Transform sequence of documents by splitting them.\"\"\"\\n        return self.split_documents(list(documents))\\n\\n    async def atransform_documents(\\n        self, documents: Sequence[Document], **kwargs: Any\\n    ) -> Sequence[Document]:\\n        \"\"\"Asynchronously transform a sequence of documents by splitting them.\"\"\"\\n        return await asyncio.get_running_loop().run_in_executor(\\n            None, partial(self.transform_documents, **kwargs), documents\\n        )',\n",
              "  'from langchain_community.tools.vectorstore.tool import (\\n    VectorStoreQATool,\\n    VectorStoreQAWithSourcesTool,\\n)\\n\\n__all__ = [\\n    \"VectorStoreQATool\",\\n    \"VectorStoreQAWithSourcesTool\",\\n]',\n",
              "  'class PythonCodeTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Python syntax.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a PythonCodeTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.PYTHON)\\n        super().__init__(separators=separators, **kwargs)\\n\\n\\nclass MarkdownTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Markdown-formatted headings.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a MarkdownTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.MARKDOWN)\\n        super().__init__(separators=separators, **kwargs)\\n\\n\\nclass LatexTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Latex-formatted layout elements.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a LatexTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.LATEX)\\n        super().__init__(separators=separators, **kwargs)',\n",
              "  'class NLTKTextSplitter(TextSplitter):\\n    \"\"\"Splitting text using NLTK package.\"\"\"\\n\\n    def __init__(\\n        self, separator: str = \"\\\\n\\\\n\", language: str = \"english\", **kwargs: Any\\n    ) -> None:\\n        \"\"\"Initialize the NLTK splitter.\"\"\"\\n        super().__init__(**kwargs)\\n        try:\\n            from nltk.tokenize import sent_tokenize\\n\\n            self._tokenizer = sent_tokenize\\n        except ImportError:\\n            raise ImportError(\\n                \"NLTK is not installed, please install it with `pip install nltk`.\"\\n            )\\n        self._separator = separator\\n        self._language = language\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        # First we naively split the large input into a bunch of smaller ones.\\n        splits = self._tokenizer(text, language=self._language)\\n        return self._merge_splits(splits, self._separator)\\n\\n\\nclass SpacyTextSplitter(TextSplitter):\\n    \"\"\"Splitting text using Spacy package.\\n\\n\\n    Per default, Spacy\\'s `en_core_web_sm` model is used and\\n    its default max_length is 1000000 (it is the length of maximum character\\n    this model takes which can be increased for large files). For a faster, but\\n    potentially less accurate splitting, you can use `pipeline=\\'sentencizer\\'`.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        separator: str = \"\\\\n\\\\n\",\\n        pipeline: str = \"en_core_web_sm\",\\n        max_length: int = 1_000_000,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Initialize the spacy text splitter.\"\"\"\\n        super().__init__(**kwargs)\\n        self._tokenizer = _make_spacy_pipeline_for_splitting(\\n            pipeline, max_length=max_length\\n        )\\n        self._separator = separator\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        splits = (s.text for s in self._tokenizer(text).sents)\\n        return self._merge_splits(splits, self._separator)\\n\\n\\n# For backwards compatibility',\n",
              "  'else:\\n            raise ValueError(\\n                f\"Language {language} is not supported! \"\\n                f\"Please choose from {list(Language)}\"\\n            )',\n",
              "  '\"\\\\nevent \",\\n                \"\\\\nmodifier \",\\n                \"\\\\nerror \",\\n                \"\\\\nstruct \",\\n                \"\\\\nenum \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo while \",\\n                \"\\\\nassembly \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.COBOL:\\n            return [\\n                # Split along divisions\\n                \"\\\\nIDENTIFICATION DIVISION.\",\\n                \"\\\\nENVIRONMENT DIVISION.\",\\n                \"\\\\nDATA DIVISION.\",\\n                \"\\\\nPROCEDURE DIVISION.\",\\n                # Split along sections within DATA DIVISION\\n                \"\\\\nWORKING-STORAGE SECTION.\",\\n                \"\\\\nLINKAGE SECTION.\",\\n                \"\\\\nFILE SECTION.\",\\n                # Split along sections within PROCEDURE DIVISION\\n                \"\\\\nINPUT-OUTPUT SECTION.\",\\n                # Split along paragraphs and common statements\\n                \"\\\\nOPEN \",\\n                \"\\\\nCLOSE \",\\n                \"\\\\nREAD \",\\n                \"\\\\nWRITE \",\\n                \"\\\\nIF \",\\n                \"\\\\nELSE \",\\n                \"\\\\nMOVE \",\\n                \"\\\\nPERFORM \",\\n                \"\\\\nUNTIL \",\\n                \"\\\\nVARYING \",\\n                \"\\\\nACCEPT \",\\n                \"\\\\nDISPLAY \",\\n                \"\\\\nSTOP RUN.\",\\n                # Split by the normal type of lines\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]',\n",
              "  '\"<nav\",\\n                # Head\\n                \"<head\",\\n                \"<style\",\\n                \"<script\",\\n                \"<meta\",\\n                \"<title\",\\n                \"\",\\n            ]\\n        elif language == Language.CSHARP:\\n            return [\\n                \"\\\\ninterface \",\\n                \"\\\\nenum \",\\n                \"\\\\nimplements \",\\n                \"\\\\ndelegate \",\\n                \"\\\\nevent \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nabstract \",\\n                # Split along method definitions\\n                \"\\\\npublic \",\\n                \"\\\\nprotected \",\\n                \"\\\\nprivate \",\\n                \"\\\\nstatic \",\\n                \"\\\\nreturn \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\ncontinue \",\\n                \"\\\\nfor \",\\n                \"\\\\nforeach \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\nbreak \",\\n                \"\\\\ncase \",\\n                \"\\\\nelse \",\\n                # Split by exceptions\\n                \"\\\\ntry \",\\n                \"\\\\nthrow \",\\n                \"\\\\nfinally \",\\n                \"\\\\ncatch \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SOL:\\n            return [\\n                # Split along compiler information definitions\\n                \"\\\\npragma \",\\n                \"\\\\nusing \",\\n                # Split along contract definitions\\n                \"\\\\ncontract \",\\n                \"\\\\ninterface \",\\n                \"\\\\nlibrary \",\\n                # Split along method definitions\\n                \"\\\\nconstructor \",\\n                \"\\\\ntype \",\\n                \"\\\\nfunction \",\\n                \"\\\\nevent \",\\n                \"\\\\nmodifier \",\\n                \"\\\\nerror \",\\n                \"\\\\nstruct \",\\n                \"\\\\nenum \",\\n                # Split along control flow statements',\n",
              "  '\"\\\\n\\\\\\\\*\\\\\\\\*\\\\\\\\*+\\\\n\",\\n                \"\\\\n---+\\\\n\",\\n                \"\\\\n___+\\\\n\",\\n                # Note that this splitter doesn\\'t handle horizontal lines defined\\n                # by *three or more* of ***, ---, or ___, but this is not handled\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.LATEX:\\n            return [\\n                # First, try to split along Latex sections\\n                \"\\\\n\\\\\\\\\\\\\\\\chapter{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\section{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\subsection{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\subsubsection{\",\\n                # Now split by environments\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{enumerate}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{itemize}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{description}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{list}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{quote}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{quotation}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{verse}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{verbatim}\",\\n                # Now split by math environments\\n                \"\\\\n\\\\\\\\\\\\begin{align}\",\\n                \"$$\",\\n                \"$\",\\n                # Now split by the normal type of lines\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.HTML:\\n            return [\\n                # First, try to split along HTML tags\\n                \"<body\",\\n                \"<div\",\\n                \"<p\",\\n                \"<br\",\\n                \"<li\",\\n                \"<h1\",\\n                \"<h2\",\\n                \"<h3\",\\n                \"<h4\",\\n                \"<h5\",\\n                \"<h6\",\\n                \"<span\",\\n                \"<table\",\\n                \"<tr\",\\n                \"<td\",\\n                \"<th\",\\n                \"<ul\",\\n                \"<ol\",\\n                \"<header\",\\n                \"<footer\",\\n                \"<nav\",\\n                # Head\\n                \"<head\",\\n                \"<style\",\\n                \"<script\",\\n                \"<meta\",\\n                \"<title\",',\n",
              "  '\" \",\\n                \"\",\\n            ]\\n        elif language == Language.SCALA:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nobject \",\\n                # Split along method definitions\\n                \"\\\\ndef \",\\n                \"\\\\nval \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nmatch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SWIFT:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunc \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nstruct \",\\n                \"\\\\nenum \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.MARKDOWN:\\n            return [\\n                # First, try to split along Markdown headings (starting with level 2)\\n                \"\\\\n#{1,6} \",\\n                # Note the alternative syntax for headings (below) is not handled here\\n                # Heading level 2\\n                # ---------------\\n                # End of code block\\n                \"```\\\\n\",\\n                # Horizontal lines\\n                \"\\\\n\\\\\\\\*\\\\\\\\*\\\\\\\\*+\\\\n\",\\n                \"\\\\n---+\\\\n\",\\n                \"\\\\n___+\\\\n\",\\n                # Note that this splitter doesn\\'t handle horizontal lines defined',\n",
              "  'return [\\n            Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n            for chunk in aggregated_chunks\\n        ]\\n\\n    def split_text_from_url(self, url: str) -> List[Document]:\\n        \"\"\"Split HTML from web URL\\n\\n        Args:\\n            url: web URL\\n        \"\"\"\\n        r = requests.get(url)\\n        return self.split_text_from_file(BytesIO(r.content))\\n\\n    def split_text(self, text: str) -> List[Document]:\\n        \"\"\"Split HTML text string\\n\\n        Args:\\n            text: HTML text\\n        \"\"\"\\n        return self.split_text_from_file(StringIO(text))\\n\\n    def split_text_from_file(self, file: Any) -> List[Document]:\\n        \"\"\"Split HTML file\\n\\n        Args:\\n            file: HTML file\\n        \"\"\"\\n        try:\\n            from lxml import etree\\n        except ImportError as e:\\n            raise ImportError(\\n                \"Unable to import lxml, please install with `pip install lxml`.\"\\n            ) from e\\n        # use lxml library to parse html document and return xml ElementTree\\n        parser = etree.HTMLParser()\\n        tree = etree.parse(file, parser)\\n\\n        # document transformation for \"structure-aware\" chunking is handled with xsl.\\n        # see comments in html_chunks_with_headers.xslt for more detailed information.\\n        xslt_path = (\\n            pathlib.Path(__file__).parent\\n            / \"document_transformers/xsl/html_chunks_with_headers.xslt\"\\n        )\\n        xslt_tree = etree.parse(xslt_path)\\n        transform = etree.XSLT(xslt_tree)\\n        result = transform(tree)\\n        result_dom = etree.fromstring(str(result))\\n\\n        # create filter and mapping for header metadata\\n        header_filter = [header[0] for header in self.headers_to_split_on]\\n        header_mapping = dict(self.headers_to_split_on)\\n\\n        # map xhtml namespace prefix\\n        ns_map = {\"h\": \"http://www.w3.org/1999/xhtml\"}',\n",
              "  '@staticmethod\\n    def get_separators_for_language(language: Language) -> List[str]:\\n        if language == Language.CPP:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along function definitions\\n                \"\\\\nvoid \",\\n                \"\\\\nint \",\\n                \"\\\\nfloat \",\\n                \"\\\\ndouble \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.GO:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunc \",\\n                \"\\\\nvar \",\\n                \"\\\\nconst \",\\n                \"\\\\ntype \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.JAVA:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along method definitions\\n                \"\\\\npublic \",\\n                \"\\\\nprotected \",\\n                \"\\\\nprivate \",\\n                \"\\\\nstatic \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.KOTLIN:\\n            return [',\n",
              "  '_separator = separator if self._is_separator_regex else re.escape(separator)\\n        splits = _split_text_with_regex(text, _separator, self._keep_separator)\\n\\n        # Now go merging things, recursively splitting longer texts.\\n        _good_splits = []\\n        _separator = \"\" if self._keep_separator else separator\\n        for s in splits:\\n            if self._length_function(s) < self._chunk_size:\\n                _good_splits.append(s)\\n            else:\\n                if _good_splits:\\n                    merged_text = self._merge_splits(_good_splits, _separator)\\n                    final_chunks.extend(merged_text)\\n                    _good_splits = []\\n                if not new_separators:\\n                    final_chunks.append(s)\\n                else:\\n                    other_info = self._split_text(s, new_separators)\\n                    final_chunks.extend(other_info)\\n        if _good_splits:\\n            merged_text = self._merge_splits(_good_splits, _separator)\\n            final_chunks.extend(merged_text)\\n        return final_chunks\\n\\n    def split_text(self, text: str) -> List[str]:\\n        return self._split_text(text, self._separators)\\n\\n    @classmethod\\n    def from_language(\\n        cls, language: Language, **kwargs: Any\\n    ) -> RecursiveCharacterTextSplitter:\\n        separators = cls.get_separators_for_language(language)\\n        return cls(separators=separators, is_separator_regex=True, **kwargs)',\n",
              "  'class RecursiveCharacterTextSplitter(TextSplitter):\\n    \"\"\"Splitting text by recursively look at characters.\\n\\n    Recursively tries to split by different characters to find one\\n    that works.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        separators: Optional[List[str]] = None,\\n        keep_separator: bool = True,\\n        is_separator_regex: bool = False,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(keep_separator=keep_separator, **kwargs)\\n        self._separators = separators or [\"\\\\n\\\\n\", \"\\\\n\", \" \", \"\"]\\n        self._is_separator_regex = is_separator_regex\\n\\n    def _split_text(self, text: str, separators: List[str]) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        final_chunks = []\\n        # Get appropriate separator to use\\n        separator = separators[-1]\\n        new_separators = []\\n        for i, _s in enumerate(separators):\\n            _separator = _s if self._is_separator_regex else re.escape(_s)\\n            if _s == \"\":\\n                separator = _s\\n                break\\n            if re.search(_separator, text):\\n                separator = _s\\n                new_separators = separators[i + 1 :]\\n                break\\n\\n        _separator = separator if self._is_separator_regex else re.escape(separator)\\n        splits = _split_text_with_regex(text, _separator, self._keep_separator)',\n",
              "  'class Language(str, Enum):\\n    \"\"\"Enum of the programming languages.\"\"\"\\n\\n    CPP = \"cpp\"\\n    GO = \"go\"\\n    JAVA = \"java\"\\n    KOTLIN = \"kotlin\"\\n    JS = \"js\"\\n    TS = \"ts\"\\n    PHP = \"php\"\\n    PROTO = \"proto\"\\n    PYTHON = \"python\"\\n    RST = \"rst\"\\n    RUBY = \"ruby\"\\n    RUST = \"rust\"\\n    SCALA = \"scala\"\\n    SWIFT = \"swift\"\\n    MARKDOWN = \"markdown\"\\n    LATEX = \"latex\"\\n    HTML = \"html\"\\n    SOL = \"sol\"\\n    CSHARP = \"csharp\"\\n    COBOL = \"cobol\"',\n",
              "  ...],\n",
              " 'metadatas': [None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  None,\n",
              "  ...]}"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#db.persist()"
      ],
      "metadata": {
        "id": "jnXxAywN1XE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IVfrtDbxCs_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JUFTArGGCtCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x_-emNF5CtFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0gkg5rC8CtHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i6JPoGz6c76C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KaAWyiJ5CtKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "#from langchain.loaders import GenericLoader\n",
        "#from langchain.parsers import LanguageParser, Language\n",
        "\n",
        "from langchain.document_loaders.generic import GenericLoader\n",
        "from langchain.document_loaders.parsers import LanguageParser\n",
        "\n",
        "# 假設您已經初始化了以下變數\n",
        "#定義路徑\n",
        "persist_directory = '/content/db'\n",
        "source_directory = '/content/docs'\n",
        "embeddings_model_name = 'shibing624/text2vec-base-chinese'\n",
        "\n",
        "# 創建嵌入\n",
        "embeddings = HuggingFaceEmbeddings(model_name=embeddings_model_name)\n",
        "\n",
        "# 創建向量資料庫實例\n",
        "db = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n",
        "\n",
        "# 使用 GenericLoader 進行文檔加載\n",
        "repo_path = source_directory\n",
        "loader = GenericLoader.from_filesystem(\n",
        "    repo_path+\"/libs/langchain/langchain\",\n",
        "    glob=\"**/*\",\n",
        "    suffixes=[\".py\"],\n",
        "    parser=LanguageParser(language=Language.PYTHON, parser_threshold=500)\n",
        ")\n",
        "documents = loader.load()\n",
        "\n",
        "# 使用 RecursiveCharacterTextSplitter 分割文檔\n",
        "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
        "    language=Language.PYTHON,\n",
        "    chunk_size=2000,\n",
        "    chunk_overlap=200\n",
        ")\n",
        "texts = python_splitter.split_documents(documents)\n",
        "\n",
        "# 將分割後的文檔加入向量資料庫\n",
        "db.add_documents(texts)\n",
        "\n",
        "# 打印相似度搜索結果\n",
        "search_results = db.similarity_search(\"litellm\")\n",
        "print(\"Search Results:\")\n",
        "for result in search_results:\n",
        "    print(result.metadata)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8AaB4vB606op",
        "outputId": "982112a9-4292-4858-fd63-12d3e19a8869"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Search Results:\n",
            "{'source': '/content/docs/libs/langchain/langchain/chains/constitutional_ai/principles.py', 'language': 'python'}\n",
            "{'source': '/content/docs/libs/langchain/langchain/chains/constitutional_ai/principles.py', 'language': 'python'}\n",
            "{'source': '/content/docs/libs/langchain/langchain/chains/qa_with_sources/stuff_prompt.py', 'language': 'python'}\n",
            "{'source': '/content/docs/libs/langchain/langchain/chains/constitutional_ai/principles.py', 'language': 'python'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "db.similarity_search(\"litellm\")"
      ],
      "metadata": {
        "id": "f7jxft3t2n4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K4FIZsSlA1o4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db.persist()"
      ],
      "metadata": {
        "id": "qVjbWyGAA1sB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install reportlab\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDOzMI3nA1uq",
        "outputId": "8498abc7-8ac9-4be4-aef7-ac6981187003"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting reportlab\n",
            "  Downloading reportlab-4.0.8-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from reportlab) (10.1.0)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from reportlab) (5.2.0)\n",
            "Installing collected packages: reportlab\n",
            "Successfully installed reportlab-4.0.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MJ8wizE1BC_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from reportlab.pdfgen import canvas\n",
        "\n",
        "def create_pdf(input_file_path, output_file_path):\n",
        "    # 創建PDF文檔\n",
        "    pdf_canvas = canvas.Canvas(output_file_path)\n",
        "\n",
        "    # 讀取輸入檔案的內容，並將其寫入PDF\n",
        "    with open(input_file_path, \"r\", encoding=\"utf-8\") as input_file:\n",
        "        try:\n",
        "            content = input_file.read()\n",
        "        except UnicodeDecodeError:\n",
        "            # 如果使用utf-8編碼失敗，嘗試使用latin-1\n",
        "            input_file = open(input_file_path, \"r\", encoding=\"latin-1\")\n",
        "            content = input_file.read()\n",
        "\n",
        "        pdf_canvas.drawString(72, 800, content)\n",
        "\n",
        "    # 保存PDF文檔\n",
        "    pdf_canvas.save()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    input_folder_path = \"/content/docs/cookbook\"\n",
        "    output_folder_path = \"/content/docs_temp\"\n",
        "\n",
        "    convert_files_to_pdf(input_folder_path, output_folder_path)\n"
      ],
      "metadata": {
        "id": "xw5q8iQIA1xr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from reportlab.pdfgen import canvas\n",
        "import os\n",
        "from reportlab.pdfgen import canvas\n",
        "\n",
        "def convert_files_to_pdf(input_folder, output_folder):\n",
        "    # 確保輸出資料夾存在\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    # 遍歷輸入資料夾中的所有檔案\n",
        "    for root, dirs, files in os.walk(input_folder):\n",
        "        for file_name in files:\n",
        "            # 創建PDF文件的路徑\n",
        "            pdf_path = os.path.join(output_folder, os.path.splitext(file_name)[0] + \".pdf\")\n",
        "\n",
        "            # 創建PDF文件\n",
        "            create_pdf(os.path.join(root, file_name), pdf_path)\n",
        "\n",
        "def create_pdf(input_file_path, output_file_path):\n",
        "    # 創建PDF文檔\n",
        "    pdf_canvas = canvas.Canvas(output_file_path)\n",
        "\n",
        "    # 讀取輸入檔案的內容，並將其寫入PDF\n",
        "    with open(input_file_path, \"r\", encoding=\"utf-8\") as input_file:\n",
        "        content = input_file.read()\n",
        "        pdf_canvas.drawString(72, 800, content)\n",
        "\n",
        "    # 保存PDF文檔\n",
        "    pdf_canvas.save()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    input_folder_path = \"/content/docs/libs/langchain/langchain\"\n",
        "    output_folder_path = \"/content/docs_temp\"\n",
        "\n",
        "    convert_files_to_pdf(input_folder_path, output_folder_path)\n"
      ],
      "metadata": {
        "id": "Oa-nVtBGCF-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/docs/libs/langchain/scripts/check_imports.py test"
      ],
      "metadata": {
        "id": "BOgpw6dlFDD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_folder_path = \"test\"\n",
        "output_folder_path = \"/content/docs_temp\"\n",
        "\n",
        "convert_files_to_pdf(input_folder_path, output_folder_path)"
      ],
      "metadata": {
        "id": "WTQqkR1FIaSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "OK02x2mvIrP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "llraxpBjdztg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "def load_and_split_documents(source_dir: str, chunk_size: int = 2000, chunk_overlap: int = 200) -> List[str]:\n",
        "    # 使用 GenericLoader 進行文檔加載\n",
        "    repo_path = source_dir\n",
        "    loader = GenericLoader.from_filesystem(\n",
        "        repo_path+\"/libs/langchain/langchain\",\n",
        "        glob=\"**/*\",\n",
        "        suffixes=[\".py\"],\n",
        "        parser=LanguageParser(language=Language.PYTHON, parser_threshold=500)\n",
        "    )\n",
        "    documents = loader.load()\n",
        "\n",
        "    # 使用 RecursiveCharacterTextSplitter 分割文檔\n",
        "    python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
        "        language=Language.PYTHON,\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap\n",
        "    )\n",
        "    texts = python_splitter.split_documents(documents)\n",
        "\n",
        "    return texts\n",
        "\n",
        "\n",
        "# 創建向量資料庫實例\n",
        "db = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n",
        "\n",
        "# 手動實現文檔加載和分割\n",
        "texts = load_and_split_documents(source_directory)\n",
        "\n",
        "# 將分割後的文檔加入向量資料庫\n",
        "db.add_documents(texts)\n",
        "\n",
        "# 打印相似度搜索結果\n",
        "search_results = db.similarity_search(\"litellm\")\n",
        "print(\"Search Results:\")\n",
        "for result in search_results:\n",
        "    print(result.metadata)\n"
      ],
      "metadata": {
        "id": "asMNzs8wdzwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ClbE4Nnmdzz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xMKnZdRLdz2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders.generic import GenericLoader\n",
        "from langchain.document_loaders.parsers import LanguageParser\n",
        "from langchain.text_splitter import Language\n",
        "from typing import List\n",
        "\n",
        "# 假設您已經初始化了以下變數\n",
        "persist_directory = r'D:\\python 免安裝10 3.10\\code\\112.12.25_解釋程式碼\\db'\n",
        "source_directory = r'D:\\python 免安裝10 3.10\\code\\112.12.25_解釋程式碼\\docs'\n",
        "embeddings_model_name = 'shibing624/text2vec-base-chinese'\n",
        "\n",
        "# 創建嵌入\n",
        "embeddings = HuggingFaceEmbeddings(model_name=embeddings_model_name)\n",
        "\n",
        "# 創建向量資料庫實例\n",
        "db = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n",
        "\n",
        "def load_and_split_documents(source_dir: str, chunk_size: int = 2000, chunk_overlap: int = 200) -> List[Document]:\n",
        "    # 使用 GenericLoader 進行文檔加載\n",
        "    repo_path = source_dir\n",
        "    loader = GenericLoader.from_filesystem(\n",
        "        repo_path+\"/libs/langchain/langchain\",\n",
        "        glob=\"**/*\",\n",
        "        suffixes=[\".py\"],\n",
        "        parser=LanguageParser(language=Language.PYTHON, parser_threshold=500)\n",
        "    )\n",
        "    documents = loader.load()\n",
        "\n",
        "    # 使用 RecursiveCharacterTextSplitter 分割文檔\n",
        "    python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
        "        language=Language.PYTHON,\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap\n",
        "    )\n",
        "    texts = python_splitter.split_documents(documents)\n",
        "\n",
        "    # 確保每個文檔都有元數據\n",
        "    for text in texts:\n",
        "        if not text.metadata:\n",
        "            text.metadata = {\"source\": \"unknown_source\"}  # 預設元數據\n",
        "\n",
        "    return texts\n",
        "\n",
        "# 手動實現文檔加載和分割\n",
        "texts = load_and_split_documents(source_directory)\n",
        "\n",
        "# 將分割後的文檔加入向量資料庫\n",
        "db.add_documents(texts)\n",
        "\n",
        "# 打印相似度搜索結果\n",
        "search_results = db.similarity_search(\"litellm\")\n",
        "print(\"Search Results:\")\n",
        "for result in search_results:\n",
        "    print(result.metadata)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "2XSwpOiTdz46",
        "outputId": "7594180e-eaf9-4cc7-cf98-5612dcd38393"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-ccf8715bdba9>\u001b[0m in \u001b[0;36m<cell line: 51>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m# 將分割後的文檔加入向量資料庫\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;31m# 打印相似度搜索結果\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/vectorstores.py\u001b[0m in \u001b[0;36madd_documents\u001b[0;34m(self, documents, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_content\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mmetadatas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadatas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     async def aadd_documents(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_community/vectorstores/chroma.py\u001b[0m in \u001b[0;36madd_texts\u001b[0;34m(self, texts, metadatas, ids, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m                 )\n\u001b[1;32m    323\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m             self._collection.upsert(\n\u001b[0m\u001b[1;32m    325\u001b[0m                 \u001b[0membeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m                 \u001b[0mdocuments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/chromadb/api/models/Collection.py\u001b[0m in \u001b[0;36mupsert\u001b[0;34m(self, ids, embeddings, metadatas, documents, increment_index)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \"\"\"\n\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         ids, embeddings, metadatas, documents = self._validate_embedding_set(\n\u001b[0m\u001b[1;32m    296\u001b[0m             \u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadatas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/chromadb/api/models/Collection.py\u001b[0m in \u001b[0;36m_validate_embedding_set\u001b[0;34m(self, ids, embeddings, metadatas, documents, require_embeddings_or_documents)\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDocument\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     ]:\n\u001b[0;32m--> 347\u001b[0;31m         \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_cast_one_to_many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m         embeddings = (\n\u001b[1;32m    349\u001b[0m             \u001b[0mvalidate_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_cast_one_to_many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/chromadb/api/types.py\u001b[0m in \u001b[0;36mmaybe_cast_one_to_many\u001b[0;34m(target)\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;31m# One Embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;31m# One Metadata dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2nN-9U-5ehcP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}